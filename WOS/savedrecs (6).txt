FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Assuncao, Marcos D.
   Calheiros, Rodrigo N.
   Bianchi, Silvia
   Netto, Marco A. S.
   Buyya, Rajkumar
TI Big Data computing and clouds: Trends and future directions
SO JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING
VL 79-80
SI SI
BP 3
EP 15
DI 10.1016/j.jpdc.2014.08.003
DT Article
PD MAY 2015
PY 2015
AB This paper discusses approaches and environments for carrying out
   analytics on Clouds for Big Data applications. It revolves around four
   important areas of analytics and Big Data, namely (i) data management
   and supporting architectures; (ii) model development and scoring; (iii)
   visualisation and user interaction; and (iv) business models. Through a
   detailed survey, we identify possible gaps in technology and provide
   recommendations for the research community on future directions on
   Cloud-supported Big Data computing and analytics solutions. (C) 2014
   Elsevier Inc. All rights reserved.
RI Netto, Marco/GLT-9968-2022; Buyya, Rajkumar/C-3424-2009; Calheiros, Roigo/B-5155-2008; Dias de Assuncao, Marcos/
OI Calheiros, Roigo/0000-0001-7435-2445; Dias de Assuncao,
   Marcos/0000-0002-4218-0260
ZB 9
TC 436
ZS 3
ZR 0
ZA 1
Z8 7
Z9 555
U1 4
U2 295
SN 0743-7315
EI 1096-0848
DA 2015-05-01
UT WOS:000356189400002
ER

PT C
AU Wang, Lei
   Zhan, Jianfeng
   Luo, Chunjie
   Zhu, Yuqing
   Yang, Qiang
   He, Yongqiang
   Gao, Wanling
   Jia, Zhen
   Shi, Yingje
   Zhang, Shuji
   Zheng, Chen
   Lu, Gang
   Zhan, Kent
   Li, Xiaona
   Qiu, Bizhu
GP IEEE
TI BigDataBench: a Big Data Benchmark Suite from Internet Services
SO 2014 20TH IEEE INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE COMPUTER
   ARCHITECTURE (HPCA-20)
SE International Symposium on High-Performance Computer
   Architecture-Proceedings
BP 488
EP 499
DT Proceedings Paper
PD 2014
PY 2014
AB As architecture, systems, and data management communities pay greater
   attention to innovative big data systems and architecture, the pressure
   of benchmarking and evaluating these systems rises. However, the
   complexity, diversity, frequently changed workloads, and rapid evolution
   of big data systems raise great challenges in big data benchmarking.
   Considering the broad use of big data systems, for the sake of fairness,
   big data benchmarks must include diversity of data and workloads, which
   is the prerequisite for evaluating big data systems and architecture.
   Most of the state-of-the-art big data benchmarking efforts target
   evaluating specific types of applications or system software stacks, and
   hence they are not qualified for serving the purposes mentioned above.
   This paper presents our joint research efforts on this issue with
   several industrial partners. Our big data benchmark suite-BigDataBench
   not only covers broad application scenarios, but also includes diverse
   and representative data sets. Currently, we choose 19 big data
   benchmarks from dimensions of application scenarios, operations/
   algorithms, data types, data sources, software stacks, and application
   types, and they are comprehensive for fairly measuring and evaluating
   big data systems and architecture. Big-DataBench is publicly available
   from the project home page http://prof.ict.ac.cn/BigDataBench.
   Also, we comprehensively characterize 19 big data workloads included in
   BigDataBench with varying data inputs. On a typical state-of-practice
   processor, Intel Xeon E5645, we have the following observations: First,
   in comparison with the traditional benchmarks: including PARSEC. HPCC,
   and SPECCPU, big data applications have very low operation intensity,
   which measures the ratio of the total number of instructions divided by
   the total byte number of memory accesses; Second, the volume of data
   input has non-negligible impact on micro-architecture characteristics,
   which may impose challenges for simulation-based big data architecture
   research; Last but not least, corroborating the observations in
   CloudSuite and DCBench (which use smaller data inputs), we find that the
   numbers of Li instruction cache (LII) misses per 1000 instructions (in
   short, MPKI) of the big data applications are higher than in the
   traditional benchmarks; also, we find that L3 caches are effective for
   the big data applications, corroborating the observation in DCBench.
CT 20th IEEE International Symposium on High Performance Computer
   Architecture (HPCA)
CY FEB 15-19, 2013
CL Orlando, FL
SP IEEE; IEEE Comp Soc TC Comp Architecture; Intel; Nvidia; IBM; ARM;
   Facebook; Microsoft Res; Qualcomm
RI Zhu, Yuqing/KLZ-7450-2024
OI Zhu, Yuqing/0000-0001-5431-8581
TC 305
ZB 0
Z8 10
ZR 0
ZS 0
ZA 0
Z9 365
U1 0
U2 11
SN 1530-0897
BN 978-1-4799-3097-5
DA 2014-01-01
UT WOS:000357020300042
ER

PT J
AU He, Xing
   Ai, Qian
   Qiu, Robert Caiming
   Huang, Wentao
   Piao, Longjian
   Liu, Haichun
TI A Big Data Architecture Design for Smart Grids Based on Random Matrix
   Theory
SO IEEE TRANSACTIONS ON SMART GRID
VL 8
IS 2
BP 674
EP 686
DI 10.1109/TSG.2015.2445828
DT Article
PD MAR 2017
PY 2017
AB Model-based analysis tools, built on assumptions and simplifications,
   are difficult to handle smart grids with data characterized by volume,
   velocity, variety, and veracity (i.e., 4Vs data). This paper, using
   random matrix theory (RMT), motivates data-driven tools to perceive the
   complex grids in high-dimension; meanwhile, an architecture with
   detailed procedures is proposed. In algorithm perspective, the
   architecture performs a high-dimensional analysis and compares the
   findings with RMT predictions to conduct anomaly detections. Mean
   spectral radius (MSR), as a statistical indicator, is defined to reflect
   the correlations of system data in different dimensions. In management
   mode perspective, a group-work mode is discussed for smart grids
   operation. This mode breaks through regional limitations for energy
   flows and data flows, and makes advanced big data analyses possible. For
   a specific large-scale zone-dividing system with multiple connected
   utilities, each site, operating under the group-work mode, is able to
   work out the regional MSR only with its own measured/simulated data. The
   large-scale interconnected system, in this way, is naturally decoupled
   from statistical parameters perspective, rather than from engineering
   models perspective. Furthermore, a comparative analysis of these
   distributed MSRs, even with imperceptible different raw data, will
   produce a contour line to detect the event and locate the source. It
   demonstrates that the architecture is compatible with the block
   calculation only using the regional small database; beyond that, this
   architecture, as a data-driven solution, is sensitive to system
   situation awareness, and practical for real large-scale interconnected
   systems. Five case studies and their visualizations validate the
   designed architecture in various fields of power systems. To our best
   knowledge, this paper is the first attempt to apply big data technology
   into smart grids.
RI He, Xing/AGX-8002-2022; HUANG, WENTAO/AAA-9948-2022
OI He, Xing/0000-0002-2527-7423; 
ZR 0
Z8 53
TC 222
ZB 2
ZA 0
ZS 0
Z9 291
U1 5
U2 190
SN 1949-3053
EI 1949-3061
DA 2017-03-29
UT WOS:000395831200016
ER

PT J
AU Cui, Laizhong
   Yu, F. Richard
   Yan, Qiao
TI When Big Data Meets Software-Defined Networking: SDN for Big Data and
   Big Data for SDN
SO IEEE NETWORK
VL 30
IS 1
BP 58
EP 65
DT Article
PD JAN-FEB 2016
PY 2016
AB Both big data and software-defined networking (SDN) have attracted great
   interests from both academia and industry. These two important areas
   have traditionally been addressed separately in the most of previous
   works. However, on the one hand, the good features of SDN can greatly
   facilitate big data acquisition, transmission, storage, and processing.
   On the other hand, big data will have profound impacts on the design and
   operation of SDN. In this paper, we present the good features of SDN in
   solving several issues prevailing with big data applications, including
   big data processing in cloud data centers, data delivery, joint
   optimization, scientific big data architectures and scheduling issues.
   We show that SDN can manage the network efficiently for improving the
   performance of big data applications. In addition, we show that big data
   can benefit SDN as well, including traffic engineering, cross-layer
   design, defeating security attacks, and SDN-based intra and inter data
   center networks. Moreover, we discuss a number of open issues that need
   to be addressed to jointly consider big data and SDN in future research.
RI Cui, Laizhong/AAX-9571-2020; YAN, QIAO/AAN-2319-2020; Yu, F Richard/B-3182-2018
OI Yu, F Richard/0000-0003-1006-7594
ZA 0
TC 219
ZS 0
ZB 1
ZR 0
Z8 7
Z9 252
U1 0
U2 87
SN 0890-8044
EI 1558-156X
DA 2016-02-17
UT WOS:000368832500011
ER

PT C
AU Demchenko, Yuri
   de Laat, Cees
   Membrey, Peter
BE Smari, WW
   Fox, GC
   Nygard, M
TI Defining Architecture Components of the Big Data Ecosystem
SO PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON COLLABORATION
   TECHNOLOGIES AND SYSTEMS (CTS)
BP 104
EP 112
DT Proceedings Paper
PD 2014
PY 2014
AB Big Data are becoming a new technology focus both in science and in
   industry and motivate technology shift to data centric architecture and
   operational models. There is a vital need to define the basic
   information/semantic models, architecture components and operational
   models that together comprise a so-called Big Data Ecosystem. This paper
   discusses a nature of Big Data that may originate from different
   scientific, industry and social activity domains and proposes improved
   Big Data definition that includes the following parts: Big Data
   properties (also called Big Data 5V: Volume, Velocity, Variety, Value
   and Veracity), data models and structures, data analytics,
   infrastructure and security. The paper discusses paradigm change from
   traditional host or service based to data centric architecture and
   operational models in Big Data. The Big Data Architecture Framework
   (BDAF) is proposed to address all aspects of the Big Data Ecosystem and
   includes the following components: Big Data Infrastructure, Big Data
   Analytics, Data structures and models, Big Data Lifecycle Management,
   Big Data Security. The paper analyses requirements to and provides
   suggestions how the mentioned above components can address the main Big
   Data challenges. The presented work intends to provide a consolidated
   view of the Big Data phenomena and related challenges to modern
   technologies, and initiate wide discussion.
CT International Conference on Collaboration Technologies and Systems (CTS)
CY MAY 19-23, 2014
CL Minneapolis, MN
SP Intelligence Adv Res Projects Activ, Off Director Natl Intelligence;
   Honeywell Int Inc; Adventium Labs; Intelligent Automat Inc; Knowledge
   Based Syst Inc; LexisNexis Corp; Ball Aerosp & Technologies Corp; Intel
   Corp; MEI Res Ltd; Microsoft Res; Springer Verlag; IEEE
OI Membrey, Peter/0000-0002-6034-3799; Demchenko, Yuri/0000-0001-7474-9506
ZA 0
ZS 0
TC 179
Z8 1
ZR 0
ZB 3
Z9 232
U1 15
U2 463
BN 978-1-4799-5158-1
DA 2015-01-07
UT WOS:000345833000016
ER

PT J
AU Emani, Cheikh Kacfah
   Cullot, Nadine
   Nicolle, Christophe
TI Understandable Big Data: A survey
SO COMPUTER SCIENCE REVIEW
VL 17
BP 70
EP 81
DI 10.1016/j.cosrev.2015.05.002
DT Article
PD AUG 2015
PY 2015
AB This survey presents the concept of Big Data. Firstly, a definition and
   the features of Big Data are given. Secondly, the different steps for
   Big Data data processing and the main problems encountered in big data
   management are described. Next, a general overview of an architecture
   for handling it is depicted. Then, the problem of merging Big Data
   architecture in an already existing information system is discussed.
   Finally this survey tackles semantics (reasoning, coreference
   resolution, entity linking, information extraction, consolidation,
   paraphrase resolution, ontology alignment) in the Big Data context. (C)
   2015 Elsevier Inc. All rights reserved.
RI Nicolle, Christophe/ABE-5885-2021
OI Nicolle, Christophe/0000-0002-8118-5005
Z8 5
ZR 0
ZA 0
ZB 7
ZS 0
TC 174
Z9 214
U1 0
U2 51
SN 1574-0137
EI 1876-7745
DA 2015-10-19
UT WOS:000361964800004
ER

PT J
AU Sundarakani, Balan
   Ajaykumar, Aneesh
   Gunasekaran, Angappa
TI Big data driven supply chain design and applications for blockchain: An
   action research using case study approach
SO OMEGA-INTERNATIONAL JOURNAL OF MANAGEMENT SCIENCE
VL 102
AR 102452
DI 10.1016/j.omega.2021.102452
EA APR 2021
DT Article
PD JUL 2021
PY 2021
AB Blockchain appears to still be nascent in its growth and a relatively
   untapped asset. This research investigates the need of blockchain in
   Industry 4.0 environment from Big Data perspective in supply chain
   management. The research method used in this study involves a
   combination of an Action Research method and Case Study research. More
   specifically, the action research method was applied in two industry
   case studies that implemented and tested the designed architecture in a
   global logistics environment. Case Study A examined the blockchain
   application in cross-border cargo movements whereas Case Study B
   investigated the application in a liquid chemical logistics company
   serving to petroleum industries. Our research analysis has identified
   that the Case A subject had disconnected systems and services for
   blockchain wherein the big data interactions had failed (failure case).
   Whereas in Case B, the company has achieved nearly 25% increase in
   revenue through its customer service after the blockchain implementation
   and thereby reduction in paperwork and carbon emissions (success case).
   This research contributes to the advancement of the body of knowledge to
   big data and blockchain by identifying key implementation guideline and
   issues for blockchain in supply chain management. Further, action-based
   research coupled with a case study approach has been used to evaluate
   the application aspects of the architecture's scalability and
   functionality of bigdata and blockchain in supply chain management.
   (c) 2021 Elsevier Ltd. All rights reserved.
RI Sundarakani, Balan/C-2368-2019; Gunasekaran, Angappa/AEZ-0121-2022
OI Sundarakani, Balan/0000-0002-3510-8746; 
ZA 0
ZB 3
ZR 0
Z8 3
ZS 2
TC 151
Z9 178
U1 24
U2 398
SN 0305-0483
EI 1873-5274
DA 2021-05-19
UT WOS:000641439200017
ER

PT C
AU Hai, Rihan
   Geisler, Sandra
   Quix, Christoph
GP ACM SIGMOD
TI Constance: An Intelligent Data Lake System
SO SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON
   MANAGEMENT OF DATA
BP 2097
EP 2100
DI 10.1145/2882903.2899389
DT Proceedings Paper
PD 2016
PY 2016
AB As the challenge of our time, Big Data still has many research hassles,
   especially the variety of data. The high diversity of data sources often
   results in information silos, a collection of non-integrated data
   management systems with heterogeneous schemas, query languages, and
   APIs. Data Lake systems have been proposed as a solution to this
   problem, by providing a schema-less repository for raw data with a
   common access interface. However, just dumping all data into a data lake
   without any metadata management, would only lead to a 'data swamp'. To
   avoid this, we propose Constance', a Data Lake system with sophisticated
   metadata management over raw data extracted from heterogeneous data
   sources. Constance discovers, extracts, and summarizes the structural
   metadata from the data sources, and annotates data and metadata with
   semantic information to avoid ambiguities. With embedded query rewriting
   engines supporting structured data and semi-structured data, Constance
   provides users a unified interface for query processing and data
   exploration. During the demo, we will walk through each functional
   component of Constance. Constance will be applied to two real-life use
   cases in order to show attendees the importance and usefulness of our
   generic and extensible data lake system.
CT ACM SIGMOD International Conference on Management of Data
CY JUN 26-JUL 01, 2016
CL San Francisco, CA
SP ACM SIGMOD; Assoc Comp Machinery; Microsoft; Oracle; Tableau; Alibaba
   com; AT & T; Facebook; Google; IBM Res; Infosys; Platfora; Recruit;
   Salesforce; SAP; Snowflake; Amazon Web Serv; Cloudera; Esgyn; HP; Intel;
   LinkedIn; LogicBlox; Memsql; Splice Machine; Visa Res; Natl Sci Fdn
RI Hai, Rihan/AAO-3865-2020; Quix, Christoph/
OI Quix, Christoph/0000-0002-1698-4345
ZR 0
Z8 6
ZA 0
ZB 6
TC 141
ZS 0
Z9 172
U1 1
U2 16
BN 978-1-4503-3531-7
DA 2016-01-01
UT WOS:000452538600145
ER

PT J
AU Santos, Maribel Yasmina
   Oliveira e Sa, Jorge
   Andrade, Carina
   Lima, Francisca Vale
   Costa, Eduarda
   Costa, Carlos
   Martinho, Bruno
   Galvao, Joao
TI A Big Data system supporting Bosch Braga Industry 4.0 strategy
SO INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
VL 37
IS 6
BP 750
EP 760
DI 10.1016/j.ijinfomgt.2017.07.012
DT Article
PD DEC 2017
PY 2017
AB People, devices, infrastructures and sensors can constantly communicate
   exchanging data and generating new data that trace many of these
   exchanges. This leads to vast volumes of data collected at ever
   increasing velocities and of different variety, a phenomenon currently
   known as Big Data. In particular, recent developments in Information and
   Communications Technologies are pushing the fourth industrial
   revolution, Industry 4.0, being data generated by several sources like
   machine controllers, sensors, manufacturing systems, among others.
   Joining volume, variety and velocity of data, with Industry 4.0, makes
   the opportunity to enhance sustainable innovation in the Factories of
   the Future. In this, the collection, integration, storage, processing
   and analysis of data is a key challenge, being Big Data systems needed
   to link all the entities and data needs of the factory. Thereby, this
   paper addresses this key challenge, proposing and implementing a Big
   Data Analytics architecture, using a multinational organisation (Bosch
   Car Multimedia - Braga) as a case study. In this work, all the data
   lifecycle, from collection to analysis, is handled, taking into
   consideration the different data processing speeds that can exist in the
   real environment of a factory (batch or stream).
RI Oliveira e Sá, Jorge/B-7176-2012; Anade, Carina/K-6539-2016; Galvão, João/N-8673-2015; Costa, Carlos/P-3314-2019; Santos, Maribel Yasmina/M-5214-2013
OI Oliveira e Sá, Jorge/0000-0003-4095-3431; Anade,
   Carina/0000-0001-8783-9412; Costa, Carlos/0000-0003-0011-6030; Santos,
   Maribel Yasmina/0000-0002-3249-6229
ZA 0
TC 123
ZB 3
ZR 0
ZS 2
Z8 0
Z9 166
U1 1
U2 161
SN 0268-4012
EI 1873-4707
DA 2017-11-19
UT WOS:000413649100027
ER

PT J
AU Yu, Wenjin
   Dillon, Tharam
   Mostafa, Fahed
   Rahayu, Wenny
   Liu, Yuehua
TI A Global Manufacturing Big Data Ecosystem for Fault Detection in
   Predictive Maintenance
SO IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS
VL 16
IS 1
BP 183
EP 192
DI 10.1109/TII.2019.2915846
DT Article
PD JAN 2020
PY 2020
AB Artificial intelligence, big data, machine learning, cloud computing,
   and Internet of Things (IoT) are terms which have driven the fourth
   industrial revolution. The digital revolution has transformed the
   manufacturing industry into smart manufacturing through the development
   of intelligent systems. In this paper, a big data ecosystem is presented
   for the implementation of fault detection and diagnosis in predictive
   maintenance with real industrial big data gathered directly from
   large-scale global manufacturing plants, aiming to provide a complete
   architecture which could be used in industrial IoT-based smart
   manufacturing in an industrial 4.0 system. The proposed architecture
   overcomes multiple challenges including big data ingestion, integration,
   transformation, storage, analytics, and visualization in a real-time
   environment using various technologies such as the data lake, NoSQL
   database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and
   other techniques. Transformation protocols, authentication, and data
   encryption methods are also utilized to address data and network
   security issues. A MapReduce-based distributed PCA model is designed for
   fault detection and diagnosis. In a large-scale manufacturing system,
   not all kinds of failure data are accessible, and the absence of labels
   precludes all the supervised methods in the predictive phase.
   Furthermore, the proposed framework takes advantage of some of the
   characteristics of PCA such as its ease of implementation on Spark, its
   simple algorithmic structure, and its real-time processing ability. All
   these elements are essential for smart manufacturing in the evolution to
   Industry 4.0. The proposed detection system has been implemented into
   the real-time industrial production system in a cooperated company,
   running for several years, and the results successfully provide an alarm
   warning several days before the fault happens. A test case involving
   several outages in 2014 is reported and analyzed in detail during the
   experiment section.
RI Rahayu, Wenny/ABA-5515-2020; Dillon, Tharam/; Yu, Wenjin/ABA-5516-2020
OI Dillon, Tharam/0000-0002-7527-129X; Yu, Wenjin/0000-0002-1101-5588
Z8 2
ZS 0
ZA 0
ZR 0
TC 144
ZB 0
Z9 163
U1 8
U2 160
SN 1551-3203
EI 1941-0050
DA 2020-01-30
UT WOS:000508428900017
ER

PT J
AU Chong, Alain Yee Loong
   Ch'ng, Eugene
   Liu, Martin J.
   Li, Boying
TI Predicting consumer product demands via Big Data: the roles of online
   promotional marketing and online reviews
SO INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH
VL 55
IS 17
BP 5142
EP 5156
DI 10.1080/00207543.2015.1066519
DT Article
PD 2017
PY 2017
AB This study aims to investigate the contributions of online promotional
   marketing and online reviews as predictors of consumer product demands.
   Using electronic data from Amazon. com, we attempt to predict if online
   review variables such as valence and volume of reviews, the number of
   positive and negative reviews, and online promotional marketing
   variables such as discounts and free deliveries, can influence the
   demand of electronic products in Amazon. com. A Big Data architecture
   was developed and Node. JS agents were deployed for scraping the Amazon.
   com pages using asynchronous Input/Output calls. The completed Web
   crawling and scraping data-sets were then preprocessed for Neural
   Network analysis. Our results showed that variables from both online
   reviews and promotional marketing strategies are important predictors of
   product demands. Variables in online reviews in general were better
   predictors as compared to online marketing promotional variables. This
   study provides important implications for practitioners as they can
   better understand how online reviews and online promotional marketing
   can influence product demands. Our empirical contributions include the
   design of a Big Data architecture that incorporate Neural Network
   analysis which can used as a platform for future researchers to
   investigate how Big Data can be used to understand and predict online
   consumer product demands.
RI Ch'ng, Eugene/Q-8277-2019; Liu, Martin J/; Chong, Alain/ABD-6916-2021; Li, Boying/ACX-4757-2022
OI Ch'ng, Eugene/0000-0003-3992-8335; Liu, Martin J/0000-0001-7873-0554;
   Chong, Alain/0000-0002-0881-1612; 
ZR 0
TC 126
ZB 2
ZA 0
ZS 2
Z8 4
Z9 163
U1 12
U2 241
SN 0020-7543
EI 1366-588X
DA 2018-12-28
UT WOS:000404671500022
ER

PT J
AU Sawadogo, Pegdwende
   Darmont, Jerome
TI On data lake architectures and metadata management
SO JOURNAL OF INTELLIGENT INFORMATION SYSTEMS
VL 56
IS 1
BP 97
EP 120
DI 10.1007/s10844-020-00608-7
EA JUN 2020
DT Article
PD FEB 2021
PY 2021
AB Over the past two decades, we have witnessed an exponential increase of
   data production in the world. So-called big data generally come from
   transactional systems, and even more so from the Internet of Things and
   social media. They are mainly characterized by volume, velocity, variety
   and veracity issues. Big data-related issues strongly challenge
   traditional data management and analysis systems. The concept of data
   lake was introduced to address them. A data lake is a large, raw data
   repository that stores and manages all company data bearing any format.
   However, the data lake concept remains ambiguous or fuzzy for many
   researchers and practitioners, who often confuse it with the Hadoop
   technology. Thus, we provide in this paper a comprehensive state of the
   art of the different approaches to data lake design. We particularly
   focus on data lake architectures and metadata management, which are key
   issues in successful data lakes. We also discuss the pros and cons of
   data lakes and their design alternatives.
OI Sawadogo, Pegdwendé N/0000-0001-6180-5476
ZA 0
ZR 0
Z8 4
ZB 6
ZS 0
TC 122
Z9 162
U1 4
U2 119
SN 0925-9902
EI 1573-7675
DA 2020-07-08
UT WOS:000543267800001
ER

PT C
AU Miloslavskaya, Natalia
   Tolstoy, Alexander
BE Samsonovich, AV
   Klimov, VV
TI Big Data, Fast Data and Data Lake Concepts
SO 7TH ANNUAL INTERNATIONAL CONFERENCE ON BIOLOGICALLY INSPIRED COGNITIVE
   ARCHITECTURES, (BICA 2016)
SE Procedia Computer Science
VL 88
BP 300
EP 305
DI 10.1016/j.procs.2016.07.439
DT Proceedings Paper
PD 2016
PY 2016
AB Today we witness the appearance of two additional to Big Data concepts:
   data lakes and fast data. Are they simply the new marketing labels for
   the old Big Data IT or really new ones? Thus the key goal of the paper
   is to identify the relationship between these three concepts.
CT 7th Annual International Conference on Biologically Inspired Cognitive
   Architectures (BICA)
CY JUL 16-19, 2016
CL New York City, NY
RI Tolstoy, Alexander/; Miloslavskaya, Natalia/F-7562-2011
OI Tolstoy, Alexander/0000-0001-9265-1510; Miloslavskaya,
   Natalia/0000-0002-1231-1805
ZB 4
ZA 0
ZS 1
Z8 0
TC 119
ZR 0
Z9 162
U1 2
U2 42
SN 1877-0509
BN *****************
DA 2017-02-08
UT WOS:000391723200042
ER

PT J
AU Chong, Alain Yee Loong
   Li, Boying
   Ngai, Eric W. T.
   Ch'ng, Eugene
   Lee, Filbert
TI Predicting online product sales via online reviews, sentiments, and
   promotion strategies A big data architecture and neural network approach
SO INTERNATIONAL JOURNAL OF OPERATIONS & PRODUCTION MANAGEMENT
VL 36
IS 4
BP 358
EP +
DI 10.1108/IJOPM-03-2015-0151
DT Article
PD 2016
PY 2016
AB Purpose - The purpose of this paper is to investigate if online reviews
   (e.g. valence and volume), online promotional strategies (e.g. free
   delivery and discounts) and sentiments from user reviews can help
   predict product sales.
   Design/methodology/approach - The authors designed a big data
   architecture and deployed Node. js agents for scraping the Amazon.com
   pages using asynchronous input/output calls. The completed web crawling
   and scraping data sets were then preprocessed for sentimental and neural
   network analysis. The neural network was employed to examine which
   variables in the study are important predictors of product sales.
   Findings - This study found that although online reviews, online
   promotional strategies and online sentiments can all predict product
   sales, some variables are more important predictors than others. The
   authors found that the interplay effects of these variables become more
   important variables than the individual variables themselves. For
   example, online volume interactions with sentiments and discounts are
   more important than the individual predictors of discounts, sentiments
   or online volume.
   Originality/value - This study designed big data architecture, in
   combination with sentimental and neural network analysis that can
   facilitate future business research for predicting product sales in an
   online environment. This study also employed a predictive analytic
   approach (e.g. neural network) to examine the variables, and this
   approach is useful for future data analysis in a big data environment
   where prediction can have more practical implications than significance
   testing. This study also examined the interplay between online reviews,
   sentiments and promotional strategies, which up to now have mostly been
   examined individually in previous studies.
RI Ngai, Eric WT/ABC-2167-2020; NGAI, WT Eric/ABC-2167-2020; Li, Boying/ACX-4757-2022; Ch'ng, Eugene/Q-8277-2019; Chong, Alain/ABD-6916-2021
OI Ngai, Eric WT/0000-0002-7278-7434; NGAI, WT Eric/0000-0001-6891-6750;
   Ch'ng, Eugene/0000-0003-3992-8335; Chong, Alain/0000-0002-0881-1612
Z8 4
ZB 0
ZR 0
TC 118
ZS 0
ZA 0
Z9 149
U1 4
U2 188
SN 0144-3577
EI 1758-6593
DA 2016-01-01
UT WOS:000379677500001
ER

PT C
AU Fang, Huang
GP IEEE
TI Managing Data Lakes in Big Data Era What's a data lake and why has it
   became popular in data management ecosystem
SO 2015 IEEE INTERNATIONAL CONFERENCE ON CYBER TECHNOLOGY IN AUTOMATION,
   CONTROL, AND INTELLIGENT SYSTEMS (CYBER)
SE IEEE Annual International Conference on Cyber Technology in Automation
   Control and Intelligent Systems
BP 820
EP 824
DT Proceedings Paper
PD 2015
PY 2015
AB the concept of a data lake is emerging as a popular way to organize and
   build the next generation of systems to master new big data challenges,
   but there are lots of concerns and questions for large enterprises to
   implement data lakes. The paper discusses the concept of data lakes and
   shares the author's thoughts and practices of data lakes.
CT IEEE International Conference on Cyber Technology in Automation,
   Control, and Intelligent Systems (CYBER)
CY JUN 09-12, 2015
CL Shenyang, PEOPLES R CHINA
SP IEEE
ZB 4
Z8 2
ZS 1
ZR 0
ZA 0
TC 117
Z9 148
U1 0
U2 32
SN 2379-7711
BN 978-1-4799-8730-6
DA 2016-09-13
UT WOS:000380502300156
ER

PT J
AU Bilal, Muhammad
   Oyedele, Lukumon O.
   Akinade, Olugbenga O.
   Ajayi, Saheed O.
   Alaka, Hafiz A.
   Owolabi, Hakeem A.
   Qadir, Junaid
   Pasha, Maruf
   Bello, Sururah A.
TI Big data architecture for construction waste analytics (CWA): A
   conceptual framework
SO JOURNAL OF BUILDING ENGINEERING
VL 6
BP 144
EP 156
DI 10.1016/j.jobe.2016.03.002
DT Article
PD JUN 2016
PY 2016
AB In recent times, construction industry is enduring pressure to take
   drastic steps to minimise waste. Waste intelligence advocates
   retrospective measures to manage waste after it is produced. Existing
   waste intelligence based waste management software are fundamentally
   limited and cannot facilitate stakeholders in controlling wasteful
   activities. Paradoxically, despite a great amount of effort, the waste
   being produced by the construction industry is escalating. This
   undesirable situation motivates a radical change from waste intelligence
   to waste analytics (in which waste is propose to be tackle proactively
   right at design through sophisticated big data technologies). This paper
   highlight that waste minimisation at design (a.k.a. designing-out waste)
   is data-driven and computationally intensive challenge.
   The aim of this paper is to propose a Big Data architecture for
   construction waste analytics. To this end, existing literature on big
   data technologies is reviewed to identify the critical components of the
   proposed Big Data based waste analytics architecture. At the crux,
   graph-based components are used: in particular, a graph database (Neo41)
   is adopted to store highly voluminous and diverse datasets. To
   complement, Spark, a highly resilient graph processing system, is
   employed. Provision for extensions through Building Information
   Modelling (BIM) are also considered for synergy and greater adoption.
   This symbiotic integration of technologies enables a vibrant environment
   for design exploration and optimisation to tackle construction waste.
   The main contribution of this paper is that it presents, to the best of
   our knowledge, the first Big Data based architecture for construction
   waste analytics. The architecture is validated for exploratory analytics
   of 200,000 waste disposal records from 900 completed projects. It is
   revealed that existing waste management software classify the bulk of
   construction waste as mixed waste, which exposes poor waste data
   management. The findings of this paper will be of interest, more
   generally to researchers, who are seeking to develop big data based
   simulation tools in similar non-trivial applications. (C) 2016 Elsevier
   Ltd. All rights reserved.
RI Qadir, Junaid/Q-6329-2019; Akinade, Olugbenga/; Ajayi, Saheed/AAV-4209-2021; Pasha, Maruf/S-6758-2016
OI Qadir, Junaid/0000-0001-9466-2475; Akinade,
   Olugbenga/0000-0003-3950-3775; 
ZR 0
ZS 0
Z8 2
ZB 10
TC 119
ZA 0
Z9 141
U1 5
U2 147
SN 2352-7102
DA 2016-06-01
UT WOS:000397385700015
ER

PT J
AU Yi, Xiaomeng
   Liu, Fangming
   Liu, Jiangchuan
   Jin, Hai
TI Building a Network Highway for Big Data: Architecture and Challenges
SO IEEE NETWORK
VL 28
IS 4
SI SI
BP 5
EP 13
DI 10.1109/MNET.2014.6863125
DT Article
PD JUL-AUG 2014
PY 2014
AB Big data, with their promise to discover valuable insights for better
   decision making, have recently attracted significant interest from both
   academia and industry. Voluminous data are generated from a variety of
   users and devices, and are to be stored and processed in powerful data
   centers. As such, there is a strong demand for building an unimpeded
   network infrastructure to gather geologically distributed and rapidly
   generated data, and move them to data centers for effective knowledge
   discovery. The express network should also be seamlessly extended to
   interconnect multiple data centers as well as interconnect the server
   nodes within a data center. In this article, we take a close look at the
   unique challenges in building such a network infrastructure for big
   data. Our study covers each and every segment in this network highway:
   the access networks that connect data sources, the Internet backbone
   that bridges them to remote data centers, as well as the dedicated
   network among data centers and within a data center. We also present two
   case studies of real-world big data applications that are empowered by
   networking, highlighting interesting and promising future research
   directions.
RI Liu, Fangming/HLG-2050-2023
ZR 0
Z8 3
ZA 0
ZB 1
TC 119
ZS 0
Z9 137
U1 1
U2 51
SN 0890-8044
EI 1558-156X
DA 2014-07-01
UT WOS:000345579100003
ER

PT C
AU Bakshi, Kapil
GP IEEE
TI Considerations for Big Data: Architecture and Approach
SO 2012 IEEE AEROSPACE CONFERENCE
SE IEEE Aerospace Conference Proceedings
DT Proceedings Paper
PD 2012
PY 2012
AB The amount of data in our industry and the world is exploding. Data is
   being collected and stored at unprecedented rates. The challenge is not
   only to store and manage the vast volume of data ("big data"), but also
   to analyze and extract meaningful value from it. There are several
   approaches to collecting, storing, processing, and analyzing big data.
   The main focus of the paper is on unstructured data analysis.
   Unstructured data refers to information that either does not have a
   pre-defined data model or does not fit well into relational tables.
   Unstructured data is the fastest growing type of data, some example
   could be imagery, sensors, telemetry, video, documents, log files, and
   e-mail data files. There are several techniques to address this problem
   space of unstructured analytics. The techniques share a common character
   tics of scale-out, elasticity and high availability. MapReduce, in
   conjunction with the Hadoop Distributed File System (HDFS) and HBase
   database, as part of the Apache Hadoop project is a modern approach to
   analyze unstructured data. Hadoop clusters are an effective means of
   processing massive volumes of data, and can be improved with the right
   architectural approach.
CT IEEE Aerospace Conference
CY MAR 03-10, 2012
CL Big Sky, MT
SP IEEE; AIAA; Phmsoc; AESS
TC 96
ZR 0
ZS 1
ZA 0
Z8 0
ZB 3
Z9 121
U1 0
U2 14
SN 1095-323X
BN 978-1-4577-0557-1
DA 2012-11-28
UT WOS:000309105303077
ER

PT C
AU Su, Fei
   Peng, Yi
   Mao, Xu
   Cheng, Xinzhou
   Chen, Weiwei
GP IEEE
TI The Research of Big Data Architecture on Telecom Industry
SO 2016 16TH INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION
   TECHNOLOGIES (ISCIT)
BP 280
EP 284
DT Proceedings Paper
PD 2016
PY 2016
AB In Big Data era, telecom operators have massive data resources, such as
   user call data, user online data, user location data, network
   performance data, and so on. These data reaches PB level and is
   distributed in the NEs and interfaces. How to effectively carry out the
   collection, parsing, analysis for the amount of data to support the
   network construction, maintenance and optimization is a major
   opportunities and challenges that the telecom operators face. This paper
   proposes a big data platform (BDP) to solve the above problem for
   telecom industries. Through the collection of telecom network data, the
   BDP can realize data parsing, storage and analysis for MR(Measure
   Report), CDR(Call Detail Record), OMC(Operation and Maintenance Centre)
   data and etc. It can achieve a unified storage and management of all
   types of data. Based on BDP, operators can carry out big data analysis
   and data mining to realize the value of data. Finally, this paper builds
   a testing platform for BDP to test the performance of big data loading
   and big data analysis. The results of experiment show that the
   performance of data loading and analysis of BDP is better than
   traditional data warehouse. It can be applied by the telecom operators
   to be the foundational infrastructure to carry on the future application
   of big data.
CT 16th International Symposium on Communications and Information
   Technologies (ISCIT)
CY SEP 26-28, 2016
CL Qingdao, PEOPLES R CHINA
SP IEEE Circuits & Syst Soc; Chinese Assoc Artificial Intelligence; Engn
   Sci Soc; Inst Elect Informat & Commun Engineers; Engn Elect Comp
   Telecommunicat & Informat Technol Assoc; IEEE, CAS Soc; IEICE, ES Soc
RI Chen, Wanjing/GZL-2801-2022
ZB 3
ZR 0
TC 115
ZS 0
ZA 0
Z8 6
Z9 115
U1 0
U2 2
BN 978-1-5090-4099-5
DA 2017-02-08
UT WOS:000391872600057
ER

PT J
AU Bellini, Pierfrancesco
   Benigni, Monica
   Billero, Riccardo
   Nesi, Paolo
   Rauch, Nadia
TI Km4City ontology building vs data harvesting and cleaning for smart-city
   services
SO JOURNAL OF VISUAL LANGUAGES AND COMPUTING
VL 25
IS 6
SI SI
BP 827
EP 839
DI 10.1016/j.jvlc.2014.10.023
DT Article
PD DEC 2014
PY 2014
AB Presently, a very large number of public and private data sets are
   available from local governments. In most cases, they are not
   semantically interoperable and a huge human effort would be needed to
   create integrated ontologies and knowledge base for smart city. Smart
   City ontology is not yet standardized, and a lot of research work is
   needed to identify models that can easily support the data
   reconciliation, the management of the complexity, to allow the data
   reasoning. In this paper, a system for data ingestion and reconciliation
   of smart cities related aspects as road graph, services available on the
   roads, traffic sensors etc., is proposed. The system allows managing a
   big data volume of data coming from a variety of sources considering
   both static and dynamic data. These data are mapped to a smart-city
   ontology, called KM4City (Knowledge Model for City), and stored into an
   RDF-Store where they are available for applications via SPARQL queries
   to provide new services to the users via specific applications of public
   administration and enterprises. The paper presents the process adopted
   to produce the ontology and the big data architecture for the knowledge
   base feeding on the basis of open and private data, and the mechanisms
   adopted for the data verification, reconciliation and validation. Some
   examples about the possible usage of the coherent big data knowledge
   base produced are also offered and are accessible from the RDF-store and
   related services. The article also presented the work performed about
   reconciliation algorithms and their comparative assessment and
   selection. (C) 2014 The Authors. Published by Elsevier Ltd.
RI BILLERO, RICCARDO/; Bellini, Pierfrancesco/D-5923-2015
OI BILLERO, RICCARDO/0000-0002-6099-222X; Bellini,
   Pierfrancesco/0000-0002-8167-1003
ZS 0
Z8 1
ZA 0
ZR 0
TC 98
ZB 1
Z9 106
U1 0
U2 70
SN 1045-926X
EI 1095-8533
DA 2014-12-01
UT WOS:000346818000023
ER

PT C
AU Ramakrishnan, Raghu
   Sridharan, Baskar
   Douceur, John R.
   Kasturi, Pavan
   Krishnamachari-Sampath, Balaji
   Krishnamoorthy, Karthick
   Li, Peng
   Manu, Mitica
   Michaylov, Spiro
   Ramos, Rogerio
   Sharman, Neil
   Xu, Zee
   Barakat, Youssef
   Douglas, Chris
   Draves, Richard
   Naidu, Shrikant S.
   Shastry, Shankar
   Sikaria, Atul
   Sun, Simon
   Venkatesan, Ramarathnam
GP ACM SIGMOD
TI Azure Data Lake Store: A Hyperscale Distributed File Service for Big
   Data Analytics
SO SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON
   MANAGEMENT OF DATA
BP 51
EP 63
DI 10.1145/3035918.3056100
DT Proceedings Paper
PD 2017
PY 2017
AB Azure Data Lake Store (ADLS) is a fully-managed, elastic, scalable, and
   secure file system that supports Hadoop distributed file system (HDFS)
   and Cosmos semantics. It is specifically designed and optimized for a
   broad spectrum of Big Data analytics that depend on a very high degree
   of parallel reads and writes, as well as collocation of compute and data
   for high bandwidth and low-latency access. It brings together key
   components and features of Microsoft's Cosmos file system-long used
   internally at Microsoft as the warehouse for data and analytics-and
   HDFS, and is a unified file storage solution for analytics on Azure.
   Internal and external workloads run on this unified platform.
   Distinguishing aspects of ADLS include its support for multiple storage
   tiers, exabyte scale, and comprehensive security and data sharing. We
   discuss ADLS architecture, design points, and performance.
CT ACM International Conference on Management of Data
CY MAY 14-19, 2017
CL Chicago, IL
SP Assoc Comp Machinery; ACM SIGMOD; Oracle; Microsoft; Amazon; Facebook;
   Google; Huawei; IBM; MongoDB; Recruit; SAP; Vertica; Northwestern Univ,
   McCormick Sch Engn & Appl Sci; Cloudera; Intel; LogicBlox; MemSQL;
   Snowflake; Workday
RI Manu, Mohammed/GVT-8064-2022
OI Manu, Mohammed/0000-0002-8889-295X
ZA 0
ZB 0
ZR 0
TC 75
Z8 2
ZS 0
Z9 103
U1 0
U2 17
BN 978-1-4503-4197-4
DA 2018-12-28
UT WOS:000452550000006
ER

PT J
AU Nambiar, Athira
   Mundra, Divyansh
TI An Overview of Data Warehouse and Data Lake in Modern Enterprise Data
   Management
SO BIG DATA AND COGNITIVE COMPUTING
VL 6
IS 4
AR 132
DI 10.3390/bdcc6040132
DT Review
PD DEC 2022
PY 2022
AB Data is the lifeblood of any organization. In today's world,
   organizations recognize the vital role of data in modern business
   intelligence systems for making meaningful decisions and staying
   competitive in the field. Efficient and optimal data analytics provides
   a competitive edge to its performance and services. Major organizations
   generate, collect and process vast amounts of data, falling under the
   category of big data. Managing and analyzing the sheer volume and
   variety of big data is a cumbersome process. At the same time, proper
   utilization of the vast collection of an organization's information can
   generate meaningful insights into business tactics. In this regard, two
   of the popular data management systems in the area of big data analytics
   (i.e., data warehouse and data lake) act as platforms to accumulate the
   big data generated and used by organizations. Although seemingly
   similar, both of them differ in terms of their characteristics and
   applications. This article presents a detailed overview of the roles of
   data warehouses and data lakes in modern enterprise data management. We
   detail the definitions, characteristics and related works for the
   respective data management frameworks. Furthermore, we explain the
   architecture and design considerations of the current state of the art.
   Finally, we provide a perspective on the challenges and promising
   research directions for the future.
RI Nambiar, Athira/S-6243-2017
OI Nambiar, Athira/0000-0002-4957-5804
ZB 3
TC 55
ZR 0
ZA 0
ZS 1
Z8 0
Z9 90
U1 9
U2 66
EI 2504-2289
DA 2023-01-04
UT WOS:000900337700001
ER

PT J
AU Immonen, Anne
   Paakkonen, Pekka
   Ovaska, Eila
TI Evaluating the Quality of Social Media Data in Big Data Architecture
SO IEEE ACCESS
VL 3
BP 2028
EP 2043
DI 10.1109/ACCESS.2015.2490723
DT Article
PD 2015
PY 2015
AB The use of freely available online data is rapidly increasing, as
   companies have detected the possibilities and the value of these data in
   their businesses. In particular, data from social media are seen as
   interesting as they can, when properly treated, assist in achieving
   customer insight into business decision making. However, the
   unstructured and uncertain nature of this kind of big data presents a
   new kind of challenge: how to evaluate the quality of data and manage
   the value of data within a big data architecture? This paper contributes
   to addressing this challenge by introducing a new architectural solution
   to evaluate and manage the quality of social media data in each
   processing phase of the big data pipeline. The proposed solution
   improves business decision making by providing real-time, validated data
   for the user. The solution is validated with an industrial case example,
   in which the customer insight is extracted from social media data in
   order to determine the customer satisfaction regarding the quality of a
   product.
ZR 0
Z8 3
TC 62
ZA 0
ZB 1
ZS 2
Z9 81
U1 0
U2 52
SN 2169-3536
DA 2016-03-23
UT WOS:000371388200155
ER

PT J
AU Barbierato, Enrico
   Gribaudo, Marco
   Iacono, Mauro
TI Performance evaluation of NoSQL big-data applications using
   multi-formalism models
SO FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
VL 37
BP 345
EP 353
DI 10.1016/j.future.2013.12.036
DT Article
PD JUL 2014
PY 2014
AB Starting with the birth of Web 2.0, the quantity of data managed by
   large-scale web services has grown exponentially, posing new challenges
   and infrastructure requirements. This has led to new programming
   paradigms and architectural choices, such as map-reduce and NoSQL
   databases, which constitute two of the main peculiarities of the
   specialized massively distributed systems referred to as Big Data
   architectures. The underlying computer infrastructures usually face
   complexity requirements, resulting from the need for efficiency and
   speed in computing over huge evolving data sets. This is achieved by
   taking advantage from the features of new technologies, such as the
   automatic scaling and replica provisioning of Cloud environments.
   Although performances are a key issue for the considered applications,
   few performance evaluation results are currently available in this
   field. In this work we focus on investigating how a Big Data application
   designer can evaluate the performances of applications exploiting the
   Apache Hive query language for NoSQL databases, built over a Apache
   Hadoop map-reduce infrastructure.
   This paper presents a dedicated modeling language and an application,
   showing first how it is possible to ease the modeling process and second
   how the semantic gap between modeling logic and the domain can be
   reduced, by means of vertical multiformalism modeling. (C) 2014 Elsevier
   B.V. All rights reserved.
RI GRIBAUDO, MARCO/AAI-5402-2021; Iacono, Mauro/G-2772-2011; Barbierato, Enrico/AAC-6349-2021
OI GRIBAUDO, MARCO/0000-0002-1415-5287; Iacono, Mauro/0000-0002-2089-975X; 
ZR 0
ZS 0
Z8 3
TC 68
ZA 0
ZB 5
Z9 81
U1 0
U2 109
SN 0167-739X
EI 1872-7115
DA 2014-07-01
UT WOS:000337931200033
ER

PT C
AU Manogaran, Gunasekaran
   Thota, Chandu
   Kumar, M. Vijay
BE Devi, SP
   Manivannan, S
   Jeyaseelan, R
   Arivudainambi, D
TI MetaCloudDataStorage Architecture for Big Data Security in Cloud
   Computing
SO FOURTH INTERNATIONAL CONFERENCE ON RECENT TRENDS IN COMPUTER SCIENCE &
   ENGINEERING (ICRTCSE 2016)
SE Procedia Computer Science
VL 87
BP 128
EP 133
DI 10.1016/j.procs.2016.05.138
DT Proceedings Paper
PD 2016
PY 2016
AB The cloud is increasingly being used to store and process the big data.
   Many researchers have been trying to protect big data in cloud computing
   environment. Traditional security mechanisms using encryption are
   neither efficient nor suited to the task of protecting big data in the
   Cloud. In this paper, we first discuss about challenges and potential
   solutions for protecting big data in cloud computing. Second, we propose
   MetaCloudDataStorage Architecture for protecting Big Data in Cloud
   Computing Environment. This framework ensures efficient processing of
   big data in cloud computing environment and gains more business
   insights.
CT 4th International Conference on Recent Trends in Computer Science and
   Engineering (ICRTCSE)
CY APR 29-30, 2016
CL Apollo Engn Coll, Dept Comp Sci & Engn, Chennai, INDIA
HO Apollo Engn Coll, Dept Comp Sci & Engn
RI Manogaran, Gunasekaran/K-7621-2017; Thota, Chandu/GXW-1156-2022
OI Thota, Chandu/0009-0008-1757-420X
TC 65
ZR 0
ZB 0
ZS 0
Z8 4
ZA 0
Z9 79
U1 0
U2 30
SN 1877-0509
BN *****************
DA 2016-09-22
UT WOS:000381937700021
ER

PT J
AU O'Leary, Daniel E.
TI Embedding AI and Crowdsourcing in the Big Data Lake
SO IEEE INTELLIGENT SYSTEMS
VL 29
IS 5
BP 70
EP 73
DI 10.1109/MIS.2014.82
DT Editorial Material
PD SEP-OCT 2014
PY 2014
RI O'Leary, Daniel Edmund/B-6469-2008
OI O'Leary, Daniel Edmund/0000-0002-5240-9516
Z8 2
ZS 0
ZB 4
ZA 0
TC 67
ZR 0
Z9 76
U1 1
U2 43
SN 1541-1672
EI 1941-1294
DA 2014-09-01
UT WOS:000345088300010
ER

PT C
AU Marchal, Samuel
   Jiang, Xiuyan
   State, Radu
   Engel, Thomas
BE Kesselman, C
   Chen, P
   Jain, H
TI A Big Data Architecture for Large Scale Security Monitoring
SO 2014 IEEE INTERNATIONAL CONGRESS ON BIG DATA (BIGDATA CONGRESS)
SE IEEE International Congress on Big Data
BP 56
EP 63
DI 10.1109/BigData.Congress.2014.18
DT Proceedings Paper
PD 2014
PY 2014
AB Network traffic is a rich source of information for security monitoring.
   However the increasing volume of data to treat raises issues, rendering
   holistic analysis of network traffic difficult. In this paper we propose
   a solution to cope with the tremendous amount of data to analyse for
   security monitoring perspectives. We introduce an architecture dedicated
   to security monitoring of local enterprise networks. The application
   domain of such a system is mainly network intrusion detection and
   prevention, but can be used as well for forensic analysis. This
   architecture integrates two systems, one dedicated to scalable
   distributed data storage and management and the other dedicated to data
   exploitation. DNS data, NetFlow records, HTTP traffic and honeypot data
   are mined and correlated in a distributed system that leverages state of
   the art big data solution. Data correlation schemes are proposed and
   their performance are evaluated against several well-known big data
   framework including Hadoop and Spark.
CT 3rd IEEE International Congress on Big Data
CY JUN 27-JUL 02, 2014
CL Anchorage, AK
SP IEEE; IEEE Comp Soc; Serv Comp; Serv Soc; Cloud Comp; BIG DATA; HP; IBM;
   SAP; IBM Res; HUAWEI; Object Management Grp; Business Proc Integrat &
   Management; IT Profess; Int Journal Web Serv Res; Computing Now; IEEE
   Transact Serv Comp
OI Marchal, Samuel/0000-0002-8522-2707; State, Radu/0000-0002-4751-9577;
   Engel, Thomas/0000-0002-7374-3927
ZR 0
ZA 0
Z8 2
TC 68
ZB 1
ZS 0
Z9 75
U1 0
U2 33
SN 2379-7703
BN 978-1-4799-5057-7
DA 2015-03-25
UT WOS:000350154200008
ER

PT C
AU Ciavotta, Michele
   Alge, Marino
   Menato, Silvia
   Rovere, Diego
   Pedrazzoli, Paolo
BE Pellicciari, M
   Peruzzini, M
TI A microservice-based middleware for the digital factory
SO 27TH INTERNATIONAL CONFERENCE ON FLEXIBLE AUTOMATION AND INTELLIGENT
   MANUFACTURING, FAIM2017
SE Procedia Manufacturing
VL 11
BP 931
EP 938
DI 10.1016/j.promfg.2017.07.197
DT Proceedings Paper
PD 2017
PY 2017
AB In recent years a considerable effort has been spent by research and
   industrial communities in the digitalization of production environments
   with the main objective of achieving a new automation paradigm, more
   flexible, responsive to changes, and safe. This paper presents the
   architecture, and discusses the benefits, of a distributed middleware
   prototype supporting a new generation of smart-factory-enabled
   applications with special attention paid to simulation tools. Devised
   within the scope of MAYA EU project, the proposed platform aims at being
   the first solution capable of empowering shop-floor
   Cyber-Physical-Systems (CPSs), providing an environment for their
   Digital Twin along the whole plant life-cycle. The platform implements a
   microservice IoT-Big Data architecture supporting the distributed
   publication of multidisciplinary simulation models, managing in an
   optimized way streams of data coming from the shop-floor for
   real-digital synchronization, ensuring security and confidentiality of
   sensible data. (C) 2017 The Authors. Published by Elsevier B.V.
CT 27th International Conference on Flexible Automation and Intelligent
   Manufacturing (FAIM)
CY JUN 27-30, 2017
CL Modena, ITALY
RI Rovere, Diego/; Menato, Silvia/JEF-2464-2023; Ciavotta, Michele/AAH-5411-2021; Peazzoli, Paolo/HSF-7009-2023
OI Rovere, Diego/0000-0002-3028-1929; Menato, Silvia/0000-0002-5440-0726;
   Ciavotta, Michele/0000-0002-2480-966X; Peazzoli,
   Paolo/0000-0001-5353-9251
ZA 0
ZB 0
Z8 0
ZS 0
ZR 0
TC 59
Z9 69
U1 0
U2 27
SN 2351-9789
BN *****************
DA 2018-03-06
UT WOS:000419072100108
ER

PT J
AU Mezghani, Emna
   Exposito, Ernesto
   Drira, Khalil
   Da Silveira, Marcos
   Pruski, Cedric
TI A Semantic Big Data Platform for Integrating Heterogeneous Wearable Data
   in Healthcare
SO JOURNAL OF MEDICAL SYSTEMS
VL 39
IS 12
AR 185
DI 10.1007/s10916-015-0344-x
DT Article
PD DEC 2015
PY 2015
AB Advances supported by emerging wearable technologies in healthcare
   promise patients a provision of high quality of care. Wearable computing
   systems represent one of the most thrust areas used to transform
   traditional healthcare systems into active systems able to continuously
   monitor and control the patients' health in order to manage their care
   at an early stage. However, their proliferation creates challenges
   related to data management and integration. The diversity and variety of
   wearable data related to healthcare, their huge volume and their
   distribution make data processing and analytics more difficult. In this
   paper, we propose a generic semantic big data architecture based on the
   "Knowledge as a Service" approach to cope with heterogeneity and
   scalability challenges. Our main contribution focuses on enriching the
   NIST Big Data model with semantics in order to smartly understand the
   collected data, and generate more accurate and valuable information by
   correlating scattered medical data stemming from multiple wearable
   devices or/and from other distributed data sources. We have implemented
   and evaluated a Wearable KaaS platform to smartly manage heterogeneous
   data coming from wearable devices in order to assist the physicians in
   supervising the patient health evolution and keep the patient up-to-date
   about his/her status.
RI Pruski, Ceic/; Exposito, Ernesto/T-2513-2019; Da Silveira, Marcos/; Drira, Khalil/R-9913-2018
OI Pruski, Ceic/0000-0002-2103-0431; Exposito, Ernesto/0000-0002-3543-2909;
   Da Silveira, Marcos/0000-0002-2604-3645; 
Z8 1
ZR 0
ZB 6
TC 57
ZS 0
ZA 0
Z9 67
U1 1
U2 91
SN 0148-5598
EI 1573-689X
DA 2015-12-02
UT WOS:000364527200009
PM 26490143
ER

PT J
AU Yu, Wenjin
   Liu, Yuehua
   Dillon, Tharam
   Rahayu, Wenny
   Mostafa, Fahed
TI An Integrated Framework for Health State Monitoring in a Smart Factory
   Employing IoT and Big Data Techniques
SO IEEE INTERNET OF THINGS JOURNAL
VL 9
IS 3
BP 2443
EP 2454
DI 10.1109/JIOT.2021.3096637
DT Article
PD FEB 1 2022
PY 2022
AB With the rapid growth in the use of various smart digital sensors, the
   Internet of Things (IoT) is a swiftly growing technology, which has
   contributed significantly to Industry 4.0 and the promotion of IoT-based
   smart factories, which gives rise to the new challenges of big data
   analytics and the implementation of machine learning techniques. This
   article proposes a practical framework that combines IoT techniques, a
   data lake, data analysis, and cloud computing for manufacturing
   equipment health-state monitoring and diagnostics in smart
   manufacturing. It addresses all the required aspects in the realization
   of such a system and allows the seamless interchange of data and
   functionality. Due to the specific characteristics of IoT sensor data
   (low quality, redundant multisources, partial labeling), we not only
   provide a promising framework but also give detailed insights and pay
   considerable attention to data quality issues. In the proposed
   framework, an ingestion procedure is designed to manage data collection,
   data security, data transformation and data storage issues. To improve
   the quality of IoT big data, a high-noise feature filter is proposed for
   automated preliminary sensor selection to suppress noisy features,
   followed by a noisy data cleaning module to provide good quality data
   for unbiased diagnosis modeling. The proposed framework can achieve
   seamless integration between IoT big data ingestion from the physical
   factory and machine learning-based data analytics in the virtual
   systems. It is built on top of the Apache Spark processing engine, being
   capable of working in both big data and real-time environments. One case
   study has been conducted based on a four-stage syngas compressor from
   real industries, which won the Best Industry Application of IoT at the
   BigInsights Data & AI Innovation Awards. The experimental results
   demonstrate the effectiveness of both the proposed IoT-architecture and
   techniques to address the data quality issues.
RI Dillon, Tharam/; Yu, Wenjin/ABA-5516-2020; Rahayu, Wenny/ABA-5515-2020
OI Dillon, Tharam/0000-0002-7527-129X; Yu, Wenjin/0000-0002-1101-5588; 
ZA 0
Z8 2
ZR 0
ZS 1
ZB 1
TC 55
Z9 66
U1 6
U2 53
SN 2327-4662
DA 2022-02-06
UT WOS:000747462100066
ER

PT J
AU Debauche, Olivier
   Mahmoudi, Said
   Manneback, Pierre
   Lebeau, Frederic
TI Cloud and distributed architectures for data management in agriculture
   4.0: Review and future trends
SO JOURNAL OF KING SAUD UNIVERSITY COMPUTER AND INFORMATION SCIENCES
VL 34
IS 9
BP 7494
EP 7514
DI 10.1016/j.jksuci.2021.09.015
DT Article
PD OCT 2022
PY 2022
AB The Agriculture 4.0, also called Smart Agriculture or Smart Farming, is
   at the origin of the production of huge amount of data that must be
   collected, stored, and processed in a very short time. Processing this
   massive quantity of data needs to use specific infrastructure that use
   adapted IoT architectures. Our review offers a comparative panorama of
   Central Cloud, Distributed Cloud Architectures, Collaborative Computing
   Strategies, and new trends used in the context of Agriculture 4.0. In
   this review, we try to answer 4 research questions: (1) Which storage
   and processing architectures are best suited to Agriculture 4.0
   applications and respond to its peculiarities? (2) Can generic
   architectures meet the needs of Agriculture 4.0 application cases? (3)
   What are the horizontal development possibilities that allow the
   transition from research to industrialization? (4) What are the vertical
   valuations possibilities to move from algorithms trained in the cloud to
   embedded or stand-alone products? For this, we compare archi-tectures
   with 8 criteria (User Proximity, Latency & Jitter, Network stability,
   high throughput, Reliability, Scalability, Cost Effectiveness,
   Maintainability), and analyze the advantages and disadvantages of each
   of them.(c) 2021 The Authors. Published by Elsevier B.V. on behalf of
   King Saud University. This is an open access article under the CC
   BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
RI Debauche, Olivier/AAE-9242-2019; Mahmoudi, Saïd/AAC-3834-2020; Manneback, Pierre/AAC-6680-2020
OI Debauche, Olivier/0000-0003-4711-2694; Mahmoudi,
   Saïd/0000-0001-8272-9425; 
ZS 0
ZR 0
ZB 8
ZA 0
TC 51
Z8 3
Z9 64
U1 5
U2 40
SN 1319-1578
EI 2213-1248
DA 2022-11-02
UT WOS:000870513100012
ER

PT C
AU Alrehamy, Hassan
   Walker, Coral
BE Li, KQ
   Gaudiot, JL
   Kishigami, J
   Wu, HY
   Li, KC
   Wu, YW
TI Personal Data Lake With Data Gravity Pull
SO PROCEEDINGS 2015 IEEE FIFTH INTERNATIONAL CONFERENCE ON BIG DATA AND
   CLOUD COMPUTING BDCLOUD 2015
BP 160
EP 167
DT Proceedings Paper
PD 2015
PY 2015
AB This paper presents Personal Data Lake, a unified storage facility for
   storing, analyzing and querying personal data. A data lake stores data
   regardless of format and thus provides an intuitive way to store
   personal data fragments of any type. Metadata management is a central
   part of the lake architecture. For structured/semi-structured data
   fragments, metadata may contain information about the schema of the data
   so that the data can be transformed into queryable data objects when
   required. For unstructured data, enabling gravity pull means allowing
   third-party plugins so that the unstructured data can be analyzed and
   queried.
CT IEEE 5th International Conference on Big Data and Cloud Computing
   (BDCloud)
CY AUG 26-28, 2015
CL Dalian, PEOPLES R CHINA
SP IEEE; Dalian Univ Technol; IEEE Comp Soc; IEEE TCSC
RI alrehamy, hassan/I-1338-2018
OI alrehamy, hassan/0000-0002-2962-8892
ZS 0
Z8 3
ZA 0
ZB 1
TC 46
ZR 0
Z9 63
U1 0
U2 14
BN 978-1-4673-7183-4
DA 2016-09-20
UT WOS:000380444200026
ER

PT J
AU Abbas, Khizar
   Tawalbeh, Lo'Ai A.
   Rafiq, Ahsan
   Muthanna, Ammar
   Elgendy, Ibrahim A.
   Abd El-Latif, Ahmed A.
TI Convergence of Blockchain and IoT for Secure Transportation Systems in
   Smart Cities
SO SECURITY AND COMMUNICATION NETWORKS
VL 2021
DI 10.1155/2021/5597679
DT Article
PD APR 27 2021
PY 2021
AB Smart cities provide citizens with smart and advanced services to
   improve their quality of life. However, it has been observed that the
   collection, storage, processing, and analysis of heterogeneous data that
   are usually borne by citizens will bear certain difficulties. The
   development of the Internet of Things, cloud computing, social media,
   and other Industry 4.0 influencers pushed technology into a smart
   society's framework, bringing potential vulnerabilities to sensor data,
   services, and smart city applications. These vulnerabilities lead to
   data security problems. We propose a decentralized data management
   system for smart and secure transportation that uses blockchain and the
   Internet of Things in a sustainable smart city environment to solve the
   data vulnerability problem. A smart transportation mobility system
   demands creating an interconnected transit system to ensure flexibility
   and efficiency. This article introduces prior knowledge and then
   provides a Hyperledger Fabric-based data architecture that supports a
   secure, trusted, smart transportation system. The simulation results
   show the balance between the blockchain mining time and the number of
   blocks created. We also use the average transaction delay evaluation
   model to evaluate the model and to test the proposed system's
   performance. The system will address residents' and authorities'
   security challenges of the transportation system in smart, sustainable
   cities and lead to better governance.
RI Muthanna, Ammar/N-8984-2015; Abd El-Latif, Ahmed/GRO-1613-2022; Abbas, Khizar/AHA-6327-2022; Rafiq, Ahsan/LZE-6257-2025; Elgendy, Ibrahim A/J-7263-2018
OI Abd El-Latif, Ahmed/0000-0002-5068-2033; Abbas,
   Khizar/0000-0002-2432-1357; Rafiq, Ahsan/0000-0001-9526-1032; Elgendy,
   Ibrahim A/0000-0001-7154-2307
ZR 0
TC 57
ZB 1
ZA 0
Z8 0
ZS 0
Z9 61
U1 0
U2 23
SN 1939-0114
EI 1939-0122
DA 2021-07-09
UT WOS:000663349500003
ER

PT C
AU Farid, Mina
   Roatis, Alexandra
   Ilyas, Ihab F.
   Hoffmann, Hella-Franziska
   Chu, Xu
GP ACM SIGMOD
TI CLAMS: Bringing Quality to Data Lakes
SO SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON
   MANAGEMENT OF DATA
BP 2089
EP 2092
DI 10.1145/2882903.2899391
DT Proceedings Paper
PD 2016
PY 2016
AB With the increasing incentive of enterprises to ingest as much data as
   they can in what is commonly referred to as "data lakes", and with the
   recent development of multiple technologies to support this "load-first"
   paradigm, the new environment presents serious data management
   challenges. Among them, the assessment of data quality and cleaning
   large volumes of heterogeneous data sources become essential tasks in
   unveiling the value of big data.
   The coveted use of unstructured and semi-structured data in large
   volumes makes current data cleaning tools (primarily designed for
   relational data) not directly adoptable.
   We present CLAMS, a system to discover and enforce expressive integrity
   constraints from large amounts of lake data with very limited schema
   information (e.g., represented as RDF triples). This demonstration shows
   how CLAMS is able to discover the constraints and the schemas they are
   defined on simultaneously. CLAMS also introduces a scale-out solution to
   efficiently detect errors in the raw data. CLAMS interacts with human
   experts to both validate the discovered constraints and to suggest data
   repairs.
   CLAMS has been deployed in a real large-scale enterprise data lake and
   was experimented with a real data set of 1.2 billion triples. It has
   been able to spot multiple obscure data inconsistencies and errors early
   in the data processing stack, providing huge value to the enterprise.
CT ACM SIGMOD International Conference on Management of Data
CY JUN 26-JUL 01, 2016
CL San Francisco, CA
SP ACM SIGMOD; Assoc Comp Machinery; Microsoft; Oracle; Tableau; Alibaba
   com; AT & T; Facebook; Google; IBM Res; Infosys; Platfora; Recruit;
   Salesforce; SAP; Snowflake; Amazon Web Serv; Cloudera; Esgyn; HP; Intel;
   LinkedIn; LogicBlox; Memsql; Splice Machine; Visa Res; Natl Sci Fdn
ZA 0
Z8 1
ZR 0
ZB 3
ZS 0
TC 46
Z9 61
U1 0
U2 1
BN 978-1-4503-3531-7
DA 2016-01-01
UT WOS:000452538600143
ER

PT J
AU Hai, Rihan
   Koutras, Christos
   Quix, Christoph
   Jarke, Matthias
TI Data Lakes: A Survey of Functions and Systems
SO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING
VL 35
IS 12
BP 12571
EP 12590
DI 10.1109/TKDE.2023.3270101
DT Article
PD DEC 1 2023
PY 2023
AB Data lakes are becoming increasingly prevalent for Big Data management
   and data analytics. In contrast to traditional 'schema-on-write'
   approaches such as data warehouses, data lakes are repositories storing
   raw data in its original formats and providing a common access
   interface. Despite the strong interest raised from both academia and
   industry, there is a large body of ambiguity regarding the definition,
   functions and available technologies for data lakes. A complete,
   coherent picture of data lake challenges and solutions is still missing.
   This survey reviews the development, architectures, and systems of data
   lakes. We provide a comprehensive overview of research questions for
   designing and building data lakes. We classify the existing approaches
   and systems based on their provided functions for data lakes, which
   makes this survey a useful technical reference for designing,
   implementing and deploying data lakes. We hope that the thorough
   comparison of existing solutions and the discussion of open research
   challenges in this survey will motivate the future development of data
   lake research and practice.
OI Quix, Christoph/0000-0002-1698-4345; Jarke,
   Matthias/0000-0001-6169-2942; Hai, Rihan/0000-0002-3720-6585; Koutras,
   Christos/0000-0003-3015-154X
TC 47
ZA 0
ZS 0
Z8 0
ZB 0
ZR 0
Z9 59
U1 9
U2 38
SN 1041-4347
EI 1558-2191
DA 2024-03-13
UT WOS:001105152100023
ER

PT J
AU Imran, Sohail
   Mahmood, Tariq
   Morshed, Ahsan
   Sellis, Timos
TI Big Data Analytics in Healthcare - A Systematic Literature Review and
   Roadmap for Practical Implementation
SO IEEE-CAA JOURNAL OF AUTOMATICA SINICA
VL 8
IS 1
BP 1
EP 22
DI 10.1109/JAS.2020.1003384
DT Review
PD JAN 2021
PY 2021
AB The advent of healthcare information management systems (HIMSs)
   continues to produce large volumes of healthcare data for patient care
   and compliance and regulatory requirements at a global scale. Analysis
   of this big data allows for boundless potential outcomes for discovering
   knowledge. Big data analytics (BDA) in healthcare can, for instance,
   help determine causes of diseases, generate effective diagnoses, enhance
   QoS guarantees by increasing efficiency of the healthcare delivery and
   effectiveness and viability of treatments, generate accurate predictions
   of readmissions, enhance clinical care, and pinpoint opportunities for
   cost savings. However, BDA implementations in any domain are generally
   complicated and resource-intensive with a high failure rate and no
   roadmap or success strategies to guide the practitioners. In this paper,
   we present a comprehensive roadmap to derive insights from BDA in the
   healthcare (patient care) domain, based on the results of a systematic
   literature review. We initially determine big data characteristics for
   healthcare and then review BDA applications to healthcare in academic
   research focusing particularly on NoSQL databases. We also identify the
   limitations and challenges of these applications and justify the
   potential of NoSQL databases to address these challenges and further
   enhance BDA healthcare research. We then propose and describe a
   state-of-the-art BDA architecture called Med-BDA for healthcare domain
   which solves all current BDA challenges and is based on the latest zeta
   big data paradigm. We also present success strategies to ensure the
   working of Med-BDA along with outlining the major benefits of BDA
   applications to healthcare. Finally, we compare our work with other
   related literature reviews across twelve hallmark features to justify
   the novelty and importance of our work. The aforementioned contributions
   of our work are collectively unique and clearly present a roadmap for
   clinical administrators, practitioners and professionals to successfully
   implement BDA initiatives in their organizations.
RI Morshed, Ahsan/AAK-2685-2021; Sellis, Timos/A-8731-2016; IMRAN, Sohail/AGP-2583-2022
OI Sellis, Timos/0000-0002-9067-5639; IMRAN, Sohail/0000-0002-8569-5838
Z8 4
ZR 0
ZS 1
TC 37
ZA 0
ZB 0
Z9 59
U1 1
U2 72
SN 2329-9266
EI 2329-9274
DA 2020-12-29
UT WOS:000594739100001
ER

PT J
AU Shi, Qiang
   Chen, Weiya
   Huang, Siqi
   Wang, Yan
   Xue, Zhidong
TI Deep learning for mining protein data
SO BRIEFINGS IN BIOINFORMATICS
VL 22
IS 1
SI SI
BP 194
EP 218
DI 10.1093/bib/bbz156
DT Review
PD JAN 2021
PY 2021
AB The recent emergence of deep learning to characterize complex patterns
   of protein big data reveals its potential to address the classic
   challenges in the field of protein data mining. Much research has
   revealed the promise of deep learning as a powerful tool to transform
   protein big data into valuable knowledge, leading to scientific
   discoveries and practical solutions. In this review, we summarize recent
   publications on deep learning predictive approaches in the field of
   mining protein data. The application architectures of these methods
   include multilayer perceptrons, stacked autoencoders, deep belief
   networks, two- or three-dimensional convolutional neural networks,
   recurrent neural networks, graph neural networks, and complex neural
   networks and are described from five perspectives: residue-level
   prediction, sequence-level prediction, three-dimensional structural
   analysis, interaction prediction, and mass spectrometry data mining. The
   advantages and deficiencies of these architectures are presented in
   relation to various tasks in protein data mining. Additionally, some
   practical issues and their future directions are discussed, such as
   robust deep learning for protein noisy data, architecture optimization
   for specific tasks, efficient deep learning for limited protein data,
   multimodal deep learning for heterogeneous protein data, and
   interpretable deep learning for protein understanding. This review
   provides comprehensive perspectives on general deep learning techniques
   for protein data analysis.
RI Huang, Siqi/JUF-3769-2023; Xue, Zhidong/; Shi, Qiang/
OI Xue, Zhidong/0000-0003-3559-0867; Shi, Qiang/0000-0001-6655-3116
Z8 3
ZR 0
ZA 0
TC 48
ZB 26
ZS 0
Z9 57
U1 6
U2 145
SN 1467-5463
EI 1477-4054
DA 2021-05-11
UT WOS:000634950200017
PM 31867611
ER

PT J
AU Fagherazzi, Guy
TI Deep Digital Phenotyping and Digital Twins for Precision Health: Time to
   Dig Deeper
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 22
IS 3
AR e16770
DI 10.2196/16770
DT Article
PD MAR 3 2020
PY 2020
AB This viewpoint describes the urgent need for more large-scale, deep
   digital phenotyping to advance toward precision health. It describes why
   and how to combine real-world digital data with clinical data and omics
   features to identify someone's digital twin, and how to finally enter
   the era of patient-centered care and modify the way we view disease
   management and prevention.
RI Fagherazzi, Guy/ABB-2555-2020
OI Fagherazzi, Guy/0000-0001-5033-5966
ZB 11
ZR 0
Z8 0
ZA 0
ZS 0
TC 47
Z9 55
U1 3
U2 98
SN 1438-8871
DA 2020-03-12
UT WOS:000517925500001
PM 32130138
ER

PT J
AU Tchito Tchapga, Christian
   Mih, Thomas Attia
   Tchagna Kouanou, Aurelle
   Fozin Fonzin, Theophile
   Kuetche Fogang, Platini
   Mezatio, Brice Anicet
   Tchiotsop, Daniel
TI Biomedical Image Classification in a Big Data Architecture Using Machine
   Learning Algorithms
SO JOURNAL OF HEALTHCARE ENGINEERING
VL 2021
AR 9998819
DI 10.1155/2021/9998819
DT Review
PD MAY 31 2021
PY 2021
AB In modern-day medicine, medical imaging has undergone immense
   advancements and can capture several biomedical images from patients. In
   the wake of this, to assist medical specialists, these images can be
   used and trained in an intelligent system in order to aid the
   determination of the different diseases that can be identified from
   analyzing these images. Classification plays an important role in this
   regard; it enhances the grouping of these images into categories of
   diseases and optimizes the next step of a computer-aided diagnosis
   system. 'the concept of classification in machine learning deals with
   the problem of identifying to which set of categories a new population
   belongs. When category membership is known, the classification is done
   on the basis of a training set of data containing observations. The goal
   of this paper is to perform a survey of classification algorithms for
   biomedical images. The paper then describes how these algorithms can be
   applied to a big data architecture by using the Spark framework. This
   paper further proposes the classification workflow based on the observed
   optimal algorithms, Support Vector Machine and Deep Learning as drawn
   from the literature. The algorithm for the feature extraction step
   during the classification process is presented and can be customized in
   all other steps of the proposed classification workflow.
RI TCHAGNA KOUANOU, Aurelle/ACL-9091-2022; Theophile, FOZIN FONZIN/
OI TCHAGNA KOUANOU, Aurelle/0000-0002-9735-6016; Theophile, FOZIN
   FONZIN/0000-0001-7385-5462
ZA 0
TC 43
ZS 0
ZB 7
Z8 0
ZR 0
Z9 52
U1 0
U2 23
SN 2040-2295
EI 2040-2309
DA 2021-07-01
UT WOS:000664097300001
PM 34122785
ER

PT C
AU Santos, Maribel Yasmina
   Oliveira e Sa, Jorge
   Costa, Carlos
   Galvao, Joao
   Andrade, Carina
   Martinho, Bruno
   Lima, Francisca Vale
   Costa, Eduarda
BE Rocha, A
   Correia, AM
   Adeli, H
   Reis, LP
   Costanzo, S
TI A Big Data Analytics Architecture for Industry 4.0
SO RECENT ADVANCES IN INFORMATION SYSTEMS AND TECHNOLOGIES, VOL 2
SE Advances in Intelligent Systems and Computing
VL 570
BP 175
EP 184
DI 10.1007/978-3-319-56538-5_19
DT Proceedings Paper
PD 2017
PY 2017
AB In an era in which people, devices, infrastructures and sensors can
   constantly communicate exchanging data and, also, generating new data
   that traces many of these exchanges, vast volumes of data is generated
   giving the context for the emergence of the Big Data concept. In
   particular, recent developments in Information and Communications
   Technology (ICT) are pushing the fourth industrial revolution, Industry
   4.0, being data generated by several sources like machine controllers,
   sensors, manufacturing systems, among others. Joining the volume and
   variety of data, arriving at high velocity, with Industry 4.0, makes the
   opportunity to enhance sustainable innovation in the Factories of the
   future. In this, the collection, integration, storage, processing and
   analysis of data is a key challenge, being Big Data systems needed to
   link all the entities and data needs of the factory. In this context,
   this paper proposes a Big Data Analytics architecture that includes
   layers dedicated to deal with all data needs, from collection to
   analysis and distribution.
CT World Conference on Information Systems and Technologies (WorldCIST)
CY APR 11-13, 2017
CL Madeira, PORTUGAL
RI Costa, Carlos/P-3314-2019; Galvão, João/N-8673-2015; Santos, Maribel Yasmina/M-5214-2013; Anade, Carina/K-6539-2016; Oliveira e Sá, Jorge/B-7176-2012
OI Costa, Carlos/0000-0003-0011-6030; Galvão, João/0000-0003-4263-8726;
   Santos, Maribel Yasmina/0000-0002-3249-6229; Anade,
   Carina/0000-0001-8783-9412; Oliveira e Sá, Jorge/0000-0003-4095-3431
ZB 0
ZA 0
ZS 0
ZR 0
Z8 0
TC 44
Z9 48
U1 0
U2 36
SN 2194-5357
EI 2194-5365
BN 978-3-319-56538-5; 978-3-319-56537-8
DA 2018-03-09
UT WOS:000425541200019
ER

PT J
AU Ouafiq, El Mehdi
   Saadane, Rachid
   Chehri, Abdellah
   Jeon, Seunggil
TI AI-based modeling and data-driven evaluation for smart farming-oriented
   big data architecture using IoT with energy harvesting capabilities
SO SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS
VL 52
AR 102093
DI 10.1016/j.seta.2022.102093
EA FEB 2022
PN A
DT Article
PD AUG 2022
PY 2022
AB The use of Internet of Things (IoT) networks offers great advantages
   over wired networks, especially due to their simple installation, low
   maintenance costs, and automatic configuration. IoT facilitates the
   integration of sensing and communication for various industries,
   including smart farming and precision agriculture. For several years,
   many researchers have strived to find new sources of energy that are
   always "cleaner" and more environmentally friendly. Energy harvesting
   technology is one of the most promising environment-friendly solutions
   that extend the lifetime of these IoT devices. In this paper, the
   state-of-art of IoT energy harvesting capabilities and communication
   technologies in smart agriculture is presented. In addition, this work
   proposes a comprehensive architecture that includes big data
   technologies, IoT components, and knowledge-based systems for innovative
   farm architecture. The solution answers some of the biggest challenges
   the agriculture industry faces, especially when handling small files in
   a big data environment without impacting the computation performance.
   The so-lution is built on top of a pre-defined big data architecture
   that includes an abstraction layer of the data lake that handles data
   quality following a data migration strategy to ensure the data's
   insights. Furthermore, in this paper, we compared several machine
   learning algorithms to find the most suitable smart farming analytics
   tools in terms of forecasting and predictions.
RI rachid, saadane/J-4558-2019; Chehri, Abdellah/X-9516-2019; OUAFIQ, El Mehdi/
OI rachid, saadane/0000-0002-0197-8313; Chehri,
   Abdellah/0000-0002-4193-6062; OUAFIQ, El Mehdi/0000-0001-8205-5002
TC 36
Z8 1
ZA 0
ZS 0
ZR 0
ZB 3
Z9 47
U1 5
U2 44
SN 2213-1388
EI 2213-1396
DA 2022-05-29
UT WOS:000789644900002
ER

PT J
AU Anthony, Bokolo, Jr.
   Petersen, Sobah Abbas
   Ahlers, Dirk
   Krogstie, John
TI Big data driven multi-tier architecture for electric mobility as a
   service in smart cities A design science approach
SO INTERNATIONAL JOURNAL OF ENERGY SECTOR MANAGEMENT
VL 14
IS 5
BP 1023
EP 1047
DI 10.1108/IJESM-08-2019-0001
EA MAR 2020
DT Article
PD AUG 19 2020
PY 2020
AB Purpose
   Electric mobility as a service (eMaaS) is suggested as a possible
   solution to ease transportation and lessen environmental issues by
   providing a collaborative transport sharing infrastructure that is based
   on electric vehicles (EVs) such as electric cars, electric bicycles and
   so on. Accordingly, this study aims to propose a multi-tier architecture
   to support the collection, processing, analytics and usage of mobility
   data in providing eMaaS within smart cities. The architecture uses
   application programming interfaces to enable interoperability between
   different infrastructures required for eMaaS and allow multiple partners
   to exchange and share data for making decision regarding electric
   mobility services.
   Design/methodology/approach
   Design science methodology based on a case study by interview was used
   to collect data from an infrastructure company in Norway to verify the
   applicability of the proposed multi-tier architecture.
   Findings
   Findings suggest that the architecture offers an approach for
   collecting, aggregating, processing and provisioning of data originating
   from sources to improve electric mobility in smart cities. More
   importantly, findings from this study provide guidance for
   municipalities and policymakers in improving electric mobility services.
   Moreover, the author's findings provide a practical data-driven mobility
   use case that can be used by transport companies in deploying eMaaS in
   smart cities.
   Social implications
   This study proposes the deployment of electric mobility to address
   increased usage of vehicles, which contributes to pollution of the
   environment that has a serious effect on citizen's quality of life.
   Originality/value
   This study proposes a multi-tier architecture that stores, processes,
   analyze and provides data and related services to improve electric
   mobility within smart cities. The multi-tier architecture aims to
   support and increase eMaaS operation of EVs toward improving
   transportation services for city transport operators and citizens for
   sustainable transport and mobility system.
RI Krogstie, John/NBX-4350-2025; Bokolo, Anthony Jnr/K-9280-2016
OI Krogstie, John/0000-0003-4830-1876; 
ZS 0
TC 44
ZB 3
Z8 0
ZR 0
ZA 0
Z9 47
U1 0
U2 48
SN 1750-6220
EI 1750-6239
DA 2020-04-08
UT WOS:000521756000001
ER

PT C
AU Howard, Daniel Anthony
   Ma, Zheng
   Aaslyng, Jesper Mazanti
   Jorgensen, Bo Norregaard
GP IEEE
TI Data Architecture for Digital Twin of Commercial Greenhouse Production
SO 2020 RIVF INTERNATIONAL CONFERENCE ON COMPUTING & COMMUNICATION
   TECHNOLOGIES (RIVF 2020)
SE IEEE RIVF International Conference on Computing and Communication
   Technologies Research Innovation and Vision for the Future
BP 130
EP 136
DI 10.1109/rivf48685.2020.9140726
DT Proceedings Paper
PD 2020
PY 2020
AB There is an increasing demand for industry-specific solutions for
   optimizing production processes with the transitions towards Industry
   4.0. The commercial greenhouse sector relies heavily on optimal use of
   energy with multiple new concepts introduced in recent years e.g.
   vertical farming and urban agriculture. Digital twins allow utilizing
   the Internet of Things and big data to simulate the alternative
   operation strategies without compromising current operation. This paper
   aims to present the development of a digital twin of the commercial
   greenhouse production process as a part of the recently launched EUDP
   funded project Greenhouse Industry 4.0 in Denmark This digital twin
   allows using big data and the Internet of Things to optimize the
   greenhouse production process and communicate with other digital twins
   representing essential areas in the greenhouse (climate and energy).
   This digital twin can estimate future states of the greenhouse by using
   past and real-time data inputs from databases, sensors, and spot
   markets. This paper also introduces a Smart Industry Architecture Model
   Framework for the discussion of the required data architecture of the
   digital twin for the greenhouse production flow which ensures a correct
   data architecture for the data exchange across all entities in the
   system.
CT RIVF International Conference on Computing and Communication
   Technologies (RIVF)
CY OCT 14-15, 2020
CL RMIT University, Ho Chi Minh City, VIETNAM
HO RMIT University
SP IEEE Vietnam Sect
RI Jørgensen, Bo Nørregaard/O-9785-2018; Ma, Zheng Grace/O-9674-2018; Howard, Daniel Anthony/AAE-5271-2019
OI Jørgensen, Bo Nørregaard/0000-0001-5678-6602; Ma, Zheng
   Grace/0000-0002-9134-1032; Howard, Daniel Anthony/0000-0003-4556-0602
ZR 0
ZB 7
ZS 0
TC 38
Z8 1
ZA 0
Z9 47
U1 5
U2 36
SN 2162-786X
BN 978-1-7281-5377-3
DA 2021-06-04
UT WOS:000648838400023
ER

PT J
AU Hou, Fangfang
   Li, Boying
   Chong, Alain Yee-Loong
   Yannopoulou, Natalia
   Liu, Martin J.
TI Understanding and predicting what influence online product sales? A
   neural network approach
SO PRODUCTION PLANNING & CONTROL
VL 28
IS 11-12
SI SI
BP 964
EP 975
DI 10.1080/09537287.2017.1336791
DT Article
PD 2017
PY 2017
AB Understanding the factors that influence sales is important for online
   sellers to manage their supply chains. This study aims to examine the
   roles of online reviews and reviewer characteristics in predicting
   product sales. With Amazon.com data captured using our big data
   architecture, this study performs sentiment analysis to measure the
   sentiment strength and polarity of review content. The predicting powers
   of sentiment together with other variables are then examined using
   neural network analysis. The results indicate that all the proposed
   variables are important predictors of online sales, and among them
   helpful votes of reviewer and picture of reviewer are the most
   influential ones. The findings of this study can be helpful for online
   sellers to manage their businesses, and the big data architecture and
   methodology can be generalised into other research contexts.
RI Yannopoulou, Natalia/; Liu, Martin J/; HOU, FANGFANG/AAW-2306-2021; Chong, Alain/ABD-6916-2021; Li, Boying/ACX-4757-2022
OI Yannopoulou, Natalia/0000-0002-4243-0119; Liu, Martin
   J/0000-0001-7873-0554; HOU, FANGFANG/0000-0001-9647-6166; Chong,
   Alain/0000-0002-0881-1612; 
ZS 0
ZB 0
ZA 0
Z8 2
TC 42
ZR 0
Z9 46
U1 1
U2 83
SN 0953-7287
EI 1366-5871
DA 2017-07-25
UT WOS:000405416500008
ER

PT C
AU Michalik, Peter
   Stofa, Jan
   Zolotova, Iveta
GP IEEE
TI Concept Definition for Big Data Architecture in the Education System
SO 2014 IEEE 12TH INTERNATIONAL SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE
   AND INFORMATICS (SAMI)
BP 331
EP 334
DT Proceedings Paper
PD 2014
PY 2014
AB Big Data is a huge phenomenon of last days. Number of applications using
   this concept is still increasing and area of implementation is still
   wider and wider. There are different examples such as transport,
   health-care, industry or education. This work covers just the field of
   education or university's environment. Purpose of this work is to
   identify Big Data sources from university's environment and also design
   of procedure how to work with them. Paper also shows possible using
   concept of Big Data, but does not show what types of analyses should be
   performed to improve education process. It means that the main benefit
   of this work is design of architecture for this problem from the
   technical perspective. In addition, there are some specific software
   applications that can be deployed to solve this problem.
CT IEEE 12th International Symposium on Applied Machine Intelligence and
   Informatics (SAMI)
CY JAN 23-25, 2014
CL Herlany, SLOVAKIA
SP IEEE
RI Zolotova, Iveta/B-2875-2014; Michalik, Peter/F-5517-2013
OI Zolotova, Iveta/0000-0002-2816-2306; Michalik, Peter/0000-0001-8431-6050
ZR 0
Z8 0
TC 41
ZB 0
ZA 0
ZS 0
Z9 46
U1 0
U2 21
BN 978-1-4799-3442-3
DA 2015-07-05
UT WOS:000355915900063
ER

PT J
AU Manogaran, Gunasekaran
   Lopez, Daphne
TI Disease Surveillance System for Big Climate Data Processing and Dengue
   Transmission
SO INTERNATIONAL JOURNAL OF AMBIENT COMPUTING AND INTELLIGENCE
VL 8
IS 2
BP 88
EP 105
DI 10.4018/IJACI.2017040106
DT Article
PD APR-JUN 2017
PY 2017
AB Ambient intelligence is an emerging platform that provides advances in
   sensors and sensor networks, pervasive computing, and artificial
   intelligence to capture the real time climate data. This result
   continuously generates several exabytes of unstructured sensor data and
   so it is often called big climate data. Nowadays, researchers are trying
   to use big climate data to monitor and predict the climate change and
   possible diseases. Traditional data processing techniques and tools are
   not capable of handling such huge amount of climate data. Hence, there
   is a need to develop advanced big data architecture for processing the
   real time climate data. The purpose of this paper is to propose a big
   data based surveillance system that analyzes spatial climate big data
   and performs continuous monitoring of correlation between climate change
   and Dengue. Proposed disease surveillance system has been implemented
   with the help of Apache Hadoop MapReduce and its supporting tools.
RI Lopez, Daphne/K-7407-2017; Manogaran, Gunasekaran/K-7621-2017
OI Lopez, Daphne/0000-0003-1452-2144; 
ZA 0
TC 40
ZR 0
ZS 0
ZB 1
Z8 0
Z9 43
U1 0
U2 40
SN 1941-6237
EI 1941-6245
DA 2017-04-01
UT WOS:000396727500006
ER

PT J
AU Li, Boying
   Ch'ng, Eugene
   Chong, Alain Yee-Loong
   Bao, Haijun
TI Predicting online e-marketplace sales performances: A big data approach
SO COMPUTERS & INDUSTRIAL ENGINEERING
VL 101
BP 565
EP 571
DI 10.1016/j.cie.2016.08.009
DT Article
PD NOV 2016
PY 2016
AB To manage supply chain efficiently, e-business organizations need to
   understand their sales effectively. Previous research has shown that
   product review plays an important role in influencing sales performance,
   especially review volume and rating. However, limited attention has been
   paid to understand how other factors moderate the effect of product
   review on online sales. This study aims to confirm the importance of
   review volume and rating on improving sales performance, and further
   examine the moderating roles of product category, answered questions,
   discount and review usefulness in such relationships. By analyzing 2939
   records of data extracted from Amazon.com using a big data architecture,
   it is found that review volume and rating have stronger influence on
   sales rank for search product than for experience product. Also, review
   usefulness significantly moderates the effects of review volume and
   rating on product sales rank. In addition, the relationship between
   review volume and sales rank is significantly moderated by both answered
   questions and discount. However, answered questions and discount do not
   have significant moderation effect on the relationship between review
   rating and sales rank. The findings expand previous literature by
   confirming important interactions between customer review features and
   other factors, and the findings provide practical guidelines to manage
   e-businesses. This study also explains a big data architecture and
   illustrates the use of big data technologies in testing theoretical
   framework. (C) 2016 Elsevier Ltd. All rights reserved.
RI Chong, Alain/ABD-6916-2021; Li, Boying/ACX-4757-2022; Ch'ng, Eugene/Q-8277-2019
OI Chong, Alain/0000-0002-0881-1612; Ch'ng, Eugene/0000-0003-3992-8335
TC 35
ZB 0
ZA 0
ZS 1
ZR 0
Z8 0
Z9 42
U1 1
U2 90
SN 0360-8352
EI 1879-0550
DA 2017-01-18
UT WOS:000390497900046
ER

PT C
AU Ramaswamy, Lakshmish
   Lawson, Victor
   Gogineni, Siva Venkat
GP IEEE
TI Towards A Quality-Centric Big Data Architecture for Federated Sensor
   Services
SO 2013 IEEE INTERNATIONAL CONGRESS ON BIG DATA
SE IEEE International Congress on Big Data
BP 86
EP 93
DI 10.1109/BigData.Congress.2013.21
DT Proceedings Paper
PD 2013
PY 2013
AB As the Internet of Things (IoT) paradigm gains popularity, the next few
   years will likely witness 'servitization' of domain sensing
   functionalities. We envision a cloud-based eco-system in which high
   quality data from large numbers of independently-managed sensors is
   shared or even traded in real-time. Such an eco-system will necessarily
   have multiple stakeholders such as sensor data providers, domain
   applications that utilize sensor data (data consumers), and cloud
   infrastructure providers who may collaborate as well as compete. While
   there has been considerable research on wireless sensor networks, the
   challenges involved in building cloud-based platforms for hosting sensor
   services are largely unexplored.
   In this paper, we present our vision for data quality (DQ)-centric big
   data infrastructure for federated sensor service clouds. We first
   motivate our work by providing real-world examples. We outline the key
   features that federated sensor service clouds need to possess. This
   paper proposes a big data architecture in which DQ is pervasive
   throughout the platform. Our architecture includes a markup language
   called SDQ-ML for describing sensor services as well as for domain
   applications to express their sensor feed requirements. The paper
   explores the advantages and limitations of current big data technologies
   in building various components of the platform. We also outline our
   initial ideas towards addressing the limitations.
CT IEEE International Congress on Big Data
CY JUN 27-JUL 02, 2013
CL Santa Clara, CA
SP IEEE; IEEE Comp Soc
ZR 0
ZB 0
TC 39
ZA 0
Z8 0
ZS 0
Z9 41
U1 0
U2 17
SN 2379-7703
BN 978-0-7695-5006-0
DA 2013-01-01
UT WOS:000332528300012
ER

PT J
AU Santos, Maribel Yasmina
   Martinho, Bruno
   Costa, Carlos
TI Modelling and implementing big data warehouses for decision support
SO JOURNAL OF MANAGEMENT ANALYTICS
VL 4
IS 2
BP 111
EP 129
DI 10.1080/23270012.2017.1304292
DT Article
PD 2017
PY 2017
AB In the era of Big Data, many NoSQL databases emerged for the storage and
   later processing of vast volumes of data, using data structures that can
   follow columnar, key-value, document or graph formats. For analytical
   contexts, requiring a Big Data Warehouse, Hive is used as the driving
   force, allowing the analysis of vast amounts of data. Data models in
   Hive are usually defined taking into consideration the queries that need
   to be answered. In this work, a set of rules is presented for the
   transformation of multidimensional data models into Hive tables, making
   available data at different levels of detail. These several levels are
   suited for answering different queries, depending on the analytical
   needs. After the identification of the Hive tables, this paper
   summarizes a demonstration case in which the implementation of a
   specific Big Data architecture shows how the evolution from a
   traditional Data Warehouse to a Big Data Warehouse is possible.
RI Costa, Carlos/P-3314-2019; Santos, Maribel Yasmina/M-5214-2013
OI Costa, Carlos/0000-0003-0011-6030; Santos, Maribel
   Yasmina/0000-0002-3249-6229
ZR 0
Z8 1
ZB 1
TC 31
ZS 0
ZA 0
Z9 39
U1 0
U2 14
SN 2327-0012
EI 2327-0039
DA 2017-09-14
UT WOS:000408859700001
ER

PT J
AU Brennan, Niall
   Oelschlaeger, Allison
   Cox, Christine
   Tavenner, Marilyn
TI Leveraging The Big-Data Revolution: CMS Is Expanding Capabilities To
   Spur Health System Transformation
SO HEALTH AFFAIRS
VL 33
IS 7
BP 1195
EP 1202
DI 10.1377/hlthaff.2014.0130
DT Article
PD JUL 2014
PY 2014
AB As the largest single payer for health care in the United States, the
   Centers for Medicare and Medicaid Services (CMS) generates enormous
   amounts of data. Historically, CMS has faced technological challenges in
   storing, analyzing, and disseminating this information because of its
   volume and privacy concerns. However, rapid progress in the fields of
   data architecture, storage, and analysis-the big-data revolution-over
   the past several years has given CMS the capabilities to use data in new
   and innovative ways. We describe the different types of CMS data being
   used both internally and externally, and we highlight a selection of
   innovative ways in which big-data techniques are being used to generate
   actionable information from CMS data more effectively. These include the
   use of real-time analytics for program monitoring and detecting fraud
   and abuse and the increased provision of data to providers, researchers,
   beneficiaries, and other stakeholders.
ZA 0
ZS 0
ZR 0
Z8 0
ZB 5
TC 27
Z9 36
U1 0
U2 34
SN 0278-2715
DA 2014-09-10
UT WOS:000340469700013
PM 25006146
ER

PT J
AU Anthony Jnr, Bokolo
TI Smart city data architecture for energy prosumption in municipalities:
   concepts, requirements, and future directions
SO INTERNATIONAL JOURNAL OF GREEN ENERGY
VL 17
IS 13
BP 827
EP 845
DI 10.1080/15435075.2020.1791878
EA AUG 2020
DT Article
PD OCT 20 2020
PY 2020
AB Big data is gaining visibility and importance, and its use is attaining
   higher levels of influence within municipalities. Due to this
   proliferation smart cities are posed to deploy architectures toward
   managing energy for Electric Vehicles (EV) and orchestrate the
   production, consumption, and distributing of energy from renewable
   sources such as solar, wind etc. in communities also known as
   prosumption. In smart city domain, Enterprise Architecture (EA) can be
   employed to facilitate alignment between municipality goals and the
   direction of the city in relation to Information Technology (IT) that
   supports stakeholders within the city. Hence, the alignment between IT
   and goals of the city is a critical process to support the continued
   growth and improvement of city services and energy sustainability.
   However, despite several research effort focused on data architecture in
   smart city, there have been few studies aimed at exploring how EA can be
   applied in smart cities to support residential buildings and EV for
   energy prosumption in municipalities. Therefore, this study conducts an
   extensive review and develops an architecture that can be employed in
   smart city domain based on big data management for energy prosumption in
   residential buildings and EV. Furthermore, secondary data was employed
   to present a case study to show the applications of the developed
   architecture in promoting energy prosumption. Findings suggest that the
   architecture provides interoperable open real-time, online, and
   historical data in facilitating energy prosumption. Respectively, this
   study offers exchange of data for sharing energy resources and provides
   insights to improve energy prosumption services.
RI Anthony Jnr, Bokolo/K-9280-2016
OI Anthony Jnr, Bokolo/0000-0002-7276-0258
ZR 0
ZA 0
ZS 1
TC 32
ZB 0
Z8 0
Z9 35
U1 1
U2 21
SN 1543-5075
EI 1543-5083
DA 2020-09-07
UT WOS:000562403700001
ER

PT J
AU Stefanowski, Jerzy
   Krawiec, Krzysztof
   Wrembel, Robert
TI EXPLORING COMPLEX AND BIG DATA
SO INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE
VL 27
IS 4
BP 669
EP 679
DI 10.1515/amcs-2017-0046
DT Article
PD DEC 2017
PY 2017
AB This paper shows how big data analysis opens a range of research and
   technological problems and calls for new approaches. We start with
   defining the essential properties of big data and discussing the main
   types of data involved. We then survey the dedicated solutions for
   storing and processing big data, including a data lake, virtual
   integration, and a polystore architecture. Difficulties in managing data
   quality and provenance are also highlighted. The characteristics of big
   data imply also specific requirements and challenges for data mining
   algorithms, which we address as well. The links with related areas,
   including data streams and deep learning, are discussed. The common
   theme that naturally emerges from this characterization is complexity.
   All in all, we consider it to be the truly defining feature of big data
   (posing particular research and technological challenges), which
   ultimately seems to be of greater importance than the sheer data volume.
RI Wrembel, Robert/F-7482-2014; Krawiec, Krzysztof/L-9390-2014; Stefanowski, Jerzy/L-9359-2014
OI Wrembel, Robert/0000-0001-6037-5718; Krawiec,
   Krzysztof/0000-0001-5439-3231; Stefanowski, Jerzy/0000-0002-4949-8271
ZA 0
ZR 0
Z8 1
TC 31
ZB 1
ZS 0
Z9 35
U1 0
U2 13
SN 1641-876X
EI 2083-8492
DA 2018-01-23
UT WOS:000419814400001
ER

PT C
AU Wang, YiChuan
   Harbert, Raymond J.
   Kung, LeeAnn
   Harbert, Raymond J.
   Ting, Chaochi
   Byrd, Terry Anthony
   Harbert, Raymond J.
BE Bui, TX
   Sprague, RH
TI Beyond a Technical Perspective: Understanding Big Data Capabilities in
   Health Care
SO 2015 48TH HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES (HICSS)
SE Proceedings of the Annual Hawaii International Conference on System
   Sciences
BP 3044
EP 3053
DI 10.1109/HICSS.2015.368
DT Proceedings Paper
PD 2015
PY 2015
AB To date, the health care industry has paid little attention to the
   potential benefits to be gained from big data. While most pioneering big
   data studies have adopted technological perspectives, a better
   understanding of the strategic implications of big data is urgently
   needed. To address this lack, this study examines the development,
   architecture and component functionalities of big data, and identifies
   its capabilities, including traceability, the analysis of unstructured
   data and patterns of care, and its predictive capacity to support
   healthcare managers seeking to formulate more effective big-data-based
   strategies. Our findings will help healthcare organizations respond
   strategically to the challenges they face in today's highly competitive
   healthcare market.
CT 48th Annual Hawaii International Conference on System Sciences (HICSS)
CY JAN 05-08, 2015
CL Kauai, HI
SP IEEE Comp Soc; Univ Hawaii, Shidler Coll Business; Univ Hawaii, Dept EE;
   Univ Hawaii, Informat Sci Program; ONR; AFOSR; Natl Sci Fdn; IEEE Syst
   Sci & Cybernet Soc; ACM; SIAM; IEEE Hawaii Sect; IEEE Control Syst Soc;
   IEEE Grp Informat Theory; IEEE Grp Automat Control; ARO; Reg Med Program
   Hawaii; Univ Hawaii, Coll Business Adm; Nasdaq
RI Kung, LeeAnn/; Wang, Yichuan/N-2391-2019
OI Kung, LeeAnn/0000-0002-0568-980X; Wang, Yichuan/0000-0003-1575-0245
ZS 1
Z8 0
ZB 0
ZA 0
ZR 0
TC 28
Z9 35
U1 1
U2 27
SN 1060-3425
BN 978-1-4799-7367-5
DA 2016-01-20
UT WOS:000366264103020
ER

PT J
AU Al-Ali, A. R.
   Gupta, Ragini
   Zualkernan, Imran
   Das, Sajal K.
TI Role of IoT technologies in big data management systems: A review and
   Smart Grid case study
SO PERVASIVE AND MOBILE COMPUTING
VL 100
AR 101905
DI 10.1016/j.pmcj.2024.101905
EA MAR 2024
DT Article
PD MAY 2024
PY 2024
AB Empowered by Internet of Things (IoT) and cloud computing platforms, the
   concept of smart cities is making a transition from conceptual models to
   development and implementation phases. Multiple smart city initiatives
   and services such as Smart Grid and Smart Meters have emerged that have
   led to the accumulation of massive amounts of energy big data. Big data
   is typically characterized by five distinct features namely, volume,
   velocity, variety, veracity, and value. To gain insights and to monetize
   big data, data has to be collected, stored, processed, analyzed, mined,
   and visualized. This paper identifies the primary layers of a big data
   architecture with start -of -the -art communication, storage, and
   processing technologies that can be utilized to gain meaningful insights
   and intelligence from big data. In addition, this paper gives an
   in-depth overview for research and development who intend to explore the
   various techniques and technologies that can be implemented for
   harnessing big data value utilizing the recent big data specific
   processing and visualization tools. Finally, a use case model utilizing
   the above mentioned technologies for Smart Grid is presented to
   demonstrate the energy big data road map from generation to
   monetization. Our key findings highlight the significance of selecting
   the appropriate big data tools and technologies for each layer of big
   data architecture, detailing their advantages and disadvantages. We
   pinpoint the critical shortcomings of existing works, particularly the
   lack of a unified framework that effectively integrates these layers for
   smart city applications. This gap presents both a challenge and an
   opportunity for future research, suggesting a need for more holistic and
   interoperable solutions in big data management and utilization.
RI Das, Sajal/AAE-1671-2019; Zualkernan, Imran/ABA-7171-2020; Al-Ali, A R/
OI Al-Ali, A R/0000-0002-6902-5092
ZB 1
ZR 0
ZA 0
ZS 0
Z8 0
TC 27
Z9 34
U1 30
U2 43
SN 1574-1192
EI 1873-1589
DA 2024-05-03
UT WOS:001207751900001
ER

PT J
AU Ouafiq, El Mehdi
   Saadane, Rachid
   Chehri, Abdellah
TI Data Management and Integration of Low Power Consumption Embedded
   Devices IoT for Transforming Smart Agriculture into Actionable Knowledge
SO AGRICULTURE-BASEL
VL 12
IS 3
AR 329
DI 10.3390/agriculture12030329
DT Article
PD MAR 2022
PY 2022
AB Smart agriculture today uses a wide range of wireless communication
   technologies. Low Power Consumption Embedded Devices (LPCED), such as
   the Internet of Things (IoT) and Wireless Sensor Networks, make it
   possible to work over great distances at a reduced cost but with limited
   transferable data volumes. However, data management (DM) in intelligent
   agriculture is still not well understood due to the fact that there are
   not enough scientific publications available on this. Though data
   management (DM) benefits are factual and substantial, many challenges
   must be addressed in order to fully realize the DM's potential. The main
   difficulties are data integration complexities, the lack of skilled
   personnel and sufficient resources, inadequate infrastructure, and
   insignificant data warehouse architecture. This work proposes a
   comprehensive architecture that includes big data technologies, IoT
   components, and knowledge-based systems. We proposed an AI-based
   architecture for smart farming. This architecture called, Smart Farming
   Oriented Big-Data Architecture (SFOBA), is designed to guarantee the
   system's durability and the data modeling in order to transform the
   business needs for smart farming into analytics. Furthermore, the
   proposed solution is built on a pre-defined big data architecture that
   includes an abstraction layer of the data lake that handles data
   quality, following a data migration strategy in order to ensure the
   data's insights.
RI OUAFIQ, El Mehdi/; rachid, saadane/J-4558-2019; Chehri, Abdellah/X-9516-2019
OI OUAFIQ, El Mehdi/0000-0001-8205-5002; rachid,
   saadane/0000-0002-0197-8313; Chehri, Abdellah/0000-0002-4193-6062
ZS 0
Z8 0
ZA 0
TC 30
ZB 5
ZR 0
Z9 33
U1 17
U2 51
EI 2077-0472
DA 2022-04-07
UT WOS:000775495500001
ER

PT J
AU Sarabia-Jacome, David
   Palau, Carlos E.
   Esteve, Manuel
   Boronat, Fernando
TI Seaport Data Space for Improving Logistic Maritime Operations
SO IEEE ACCESS
VL 8
BP 4372
EP 4382
DI 10.1109/ACCESS.2019.2963283
DT Article
PD 2020
PY 2020
AB The maritime industry expects several improvements to efficiently manage
   the operation processes by introducing Industry 4.0 enabling
   technologies. Seaports are the most critical point in the maritime
   logistics chain because of its multimodal and complex nature.
   Consequently, coordinated communication among any seaport stakeholders
   is vital to improving their operations. Currently, Electronic Data
   Interchange (EDI) and Port Community Systems (PCS), as primary enablers
   of digital seaports, have demonstrated their limitations to interchange
   information on time, accurately, efficiently, and securely, causing high
   operation costs, low resource management, and low performance. For these
   reasons, this contribution presents the Seaport Data Space (SDS) based
   on the Industrial Data Space (IDS) reference architecture model to
   enable a secure data sharing space and promote an intelligent transport
   multimodal terminal. Each seaport stakeholders implements the IDS
   connector to take part in the SDS and share their data. On top of SDS, a
   Big Data architecture is integrated to manage the massive data shared in
   the SDS and extract useful information to improve the decision-making.
   The architecture has been evaluated by enabling a port authority and a
   container terminal to share its data with a shipping company. As a
   result, several Key Performance Indicators (KPIs) have been developed by
   using the Big Data architecture functionalities. The KPIs have been
   shown in a dashboard to allow easy interpretability of results for
   planning vessel operations. The SDS environment may improve the
   communication between stakeholders by reducing the transaction costs,
   enhancing the quality of information, and exhibiting effectiveness.
RI Palau, Carlos E/HCH-5674-2022; Esteve, Manuel/; Sarabia-Jácome, David/AAG-5233-2019; Sarabia, David/AAG-5233-2019; Boronat, Fernando/A-3234-2011
OI Palau, Carlos E/0000-0002-3795-5404; Esteve, Manuel/0000-0002-7985-3270;
   Sarabia, David/0000-0003-4930-9677; Boronat,
   Fernando/0000-0001-5525-3441
ZA 0
TC 27
ZR 0
ZS 0
ZB 1
Z8 1
Z9 33
U1 7
U2 78
SN 2169-3536
DA 2020-07-28
UT WOS:000549773400004
ER

PT C
AU Cuzzocrea, Alfredo
BE Unger, H
   Kim, J
   Kang, U
   SoIn, C
   Du, J
   Saad, W
   Ha, YG
   Wagner, C
   Bourgeois, J
   Sathitwiriyawong, C
   Kwon, HY
   Leung, C
TI Big Data Lakes: Models, Frameworks, and Techniques
SO 2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA AND SMART COMPUTING
   (BIGCOMP 2021)
SE International Conference on Big Data and Smart Computing
BP 1
EP 4
DI 10.1109/BigComp51126.2021.00010
DT Proceedings Paper
PD 2021
PY 2021
AB Nowadays, big data lakes are prominent components of emerging big data
   architectures. Basically, big data lakes are the natural evolution of
   data warehousing systems in the big data context, and deal with several
   requirements deriving from the well-known 3 V nature of big data. Along
   with the emerging of big data lake research initiative, several issues
   appeared, such as: (i) big data lake models; (ii) big data lake
   frameworks; (iii) big data lake techniques. In line with this exciting
   research perspective, this paper proposes an overview of
   state-of-the-art approaches that are at the foundations of big data lake
   research, and innovative open problems and issues, which drive future
   research directions, on advancing the big data lake research trend.
CT IEEE International Conference on Big Data and Smart Computing (BigComp)
CY JAN 17-20, 2021
CL SOUTH KOREA
SP IEEE; IEEE Comp Soc; Korean Inst Informat Scientists & Engineers
RI Cuzzocrea, Alfredo/B-6374-2015
ZB 0
Z8 0
ZA 0
TC 24
ZS 0
ZR 0
Z9 32
U1 28
U2 915
SN 2375-933X
BN 978-1-7281-8924-6
DA 2021-07-08
UT WOS:000662199000001
ER

PT J
AU Petrillo, Alberto
   Picariello, Antonio
   Santini, Stefania
   Scarciello, Biagio
   Sperli, Giancarlo
TI Model-based vehicular prognostics framework using Big Data architecture
SO COMPUTERS IN INDUSTRY
VL 115
AR 103177
DI 10.1016/j.compind.2019.103177
DT Article
PD FEB 2020
PY 2020
AB Nowadays, the continuous technological advances allow designing novel
   Integrated Vehicle Health Management (IVHM) systems to deal with strict
   safety regulations in the automotive field with the aim at improving
   efficiency and reliability of automotive components. However,
   challenging issue, which arises in this domain, is handling a huge
   amount of data that are useful for prognostic. To this aim, in this
   paper we propose a cloud-based infrastructure, namely Automotive
   predicTOr Maintenance In Cloud (ATOMIC), for prognostic analysis that
   leverages Big Data technologies and mathematical models of both nominal
   and faulty behaviour of the automotive components to estimate on-line
   the End-Of-Life (EOL) and Remaining Useful Life (EUL) indicators for the
   automotive systems under investigation. A case study based on the Delphi
   DFG1596 fuel pump has been presented to evaluate the proposed prognostic
   method. Finally, we perform a benchmark analysis of the deployment
   configurations of ATOMIC architecture in terms of scalability and cost.
   (C) 2019 Elsevier B.V. All rights reserved.
RI Picariello, Antonio/L-6820-2015; Petrillo, Alberto/ITU-6226-2023
OI Petrillo, Alberto/0000-0003-4630-6673
Z8 0
ZR 0
TC 30
ZB 0
ZS 0
ZA 0
Z9 32
U1 1
U2 18
SN 0166-3615
EI 1872-6194
DA 2020-03-10
UT WOS:000515211100014
ER

PT J
AU Shrouf, Fadi
   Gong, Bing
   Ordieres-Mere, Joaquin
TI Multi-level awareness of energy used in production processes
SO JOURNAL OF CLEANER PRODUCTION
VL 142
BP 2570
EP 2585
DI 10.1016/j.jclepro.2016.11.019
PN 4
DT Article
PD JAN 20 2017
PY 2017
AB To ensure green manufacturing, the energy consumption of production
   processes should be transparent and minimized. Also, to achieve the
   desired level of energy consumption awareness and efficiency
   improvements, energy use should be measured in more detail and linked to
   production data. In this scenario, real-time monitoring of energy
   consumption represents an essential step to increasing energy awareness,
   efficiency and the support of energy-aware production processes. This
   paper seeks to provide a way to achieve multi-level awareness of the
   energy used during production processes. The multi-level awareness of
   energy consumption means identifying the amount of energy used, CO2
   emitted, and the cost of the energy used at operation, product, and
   order level. This multi-level awareness is achieved by integrating
   energy usage data with production data at the operational level.
   Furthermore, energy sources need to be considered to define the amount
   of CO2 that is emitted from the production process for each product. A
   pilot study was carried out to integrate electrical energy data,
   production data and scheduling data in real time to achieve the
   multi-level awareness of energy used in production. The results show
   that integrating energy with production data enables factories to
   provide specific energy consumption information for decision makers at
   the factory level, as well as for the consumers and the regulators. This
   integration of energy and production data is achieved efficiently when
   there is a high level of standardization of production processes and the
   availability of detailed energy usage data. (C) 2016 Elsevier Ltd. All
   rights reserved.
RI Ordieres-Meré, Joaquín/B-9677-2011; Gong, Bing/ADD-1408-2022
OI Ordieres-Meré, Joaquín/0000-0002-9677-6764; Gong,
   Bing/0000-0001-7770-2738
Z8 0
ZR 0
ZA 0
ZB 1
ZS 0
TC 29
Z9 32
U1 0
U2 36
SN 0959-6526
EI 1879-1786
DA 2017-02-08
UT WOS:000391516300016
ER

PT J
AU Daniel, Alfred
   Subburathinam, Karthik
   Paul, Anand
   Rajkumar, Newlin
   Rho, Seungmin
TI Big autonomous vehicular data classifications: Towards procuring
   intelligence in ITS
SO VEHICULAR COMMUNICATIONS
VL 9
BP 306
EP 312
DI 10.1016/j.vehcom.2017.03.002
DT Article
PD JUL 2017
PY 2017
AB For effectively utilization of acquired resources in Autonomous Vehicle
   (AV), big data analysis in real time will be a reliable way to produce
   valuable information from sensor data. With the combined ability of
   telematics and real-time analysis, big data analytics forming the key
   drivers of autonomous cars. To emphasize the significant of data fusion
   or knowledge discovery, an efficient architecture has been proposed for
   real-time big data analysis in an autonomous vehicle, which indeed will
   keep pace with the latest trends and development with respect to
   emerging big data paradigm. The proposed architecture comprises
   distributed data storage mechanism for a streaming process for real-time
   analysis and the vehicular cloud server tool for batch processing the
   offline data. Furthermore, a workflow model has also been designed for
   big data architecture to examine streaming data in near real time
   process. Furthermore, an algorithm is developed for data classification
   in distributed storage unit, and mathematical modeling is carried to
   analysis the data classification functionality in AV. The proposed
   system model using Hadoop framework which is for the optimal utilization
   of the massive data set, meant for data classification in distributed
   environment for streaming data in real time, which is intended for
   intelligent transportation of the autonomous vehicle. (C) 2017 Elsevier
   Inc. All rights reserved.
RI Daniel, Alfred/O-1875-2017; Rho, Seungmin/HTP-6683-2023; Paul, Anand/V-6724-2017
OI Daniel, Alfred/0000-0003-0602-3425; 
TC 31
ZS 0
ZR 0
ZA 0
Z8 1
ZB 1
Z9 31
U1 1
U2 14
SN 2214-2096
DA 2017-10-16
UT WOS:000411126300031
ER

PT C
AU Matsebula, Fezile
   Mnkandla, Ernest
BE Cornish, DR
TI A BIG DATA ARCHITECTURE FOR LEARNING ANALYTICS IN HIGHER EDUCATION
SO 2017 IEEE AFRICON
SE Africon
BP 951
EP 956
DT Proceedings Paper
PD 2017
PY 2017
AB Data with high volume, velocity, variety and veracity brings the new
   experience curve of analytics. Big data in higher education comes from
   different sources that include blogs, social networks, student
   information systems, learning management systems, research, and other
   machine-generated data. Once the data is analysed it promises better
   student placement processes; more accurate enrolment forecasts, and
   early warning systems that identify and assist students at-risk of
   failing or dropping out. Big data is becoming a key to creating
   competitive advantages in higher education. Like with any organization,
   traditional data processing and analysis of structured and unstructured
   data using RDBMS and data warehousing no longer satisfy big data
   challenges. The lack of adequate conceptual architectures for big data
   tailored for institutions of higher education has led to many failures
   to produce meaningful, accessible, and timely information for decision
   making. Therefore, this calls for the development of conceptual
   architectures for big data in higher education. This paper presents an
   architecture for big data analytics in higher education.
CT IEEE AFRICON Conference - Science, Technology and Innovation for Africa
CY SEP 18-20, 2017
CL Cape Town, SOUTH AFRICA
SP IEEE; mlab; IEEE Reg 8; IEEE S Africa Sect; IES; Univ Pretoria; SAiEE;
   IBM; Altair; CST
RI Mnkandla, Ernest/G-5235-2012; Matsebula, Fezile/JDD-6998-2023; Matsebula, Fezile/
OI Matsebula, Fezile/0000-0001-7646-6243
ZB 0
ZS 0
TC 16
ZA 0
Z8 1
ZR 0
Z9 29
U1 0
U2 17
SN 2153-0025
BN 978-1-5386-2775-4
DA 2017-01-01
UT WOS:000424741600162
ER

PT C
AU Suriarachchi, Isuru
   Plale, Beth
GP IEEE
TI Crossing Analytics Systems: A Case for Integrated Provenance in Data
   Lakes
SO PROCEEDINGS OF THE 2016 IEEE 12TH INTERNATIONAL CONFERENCE ON E-SCIENCE
   (E-SCIENCE)
SE Proceeding IEEE International Conference on e-Science (e-Science)
BP 349
EP 354
DT Proceedings Paper
PD 2016
PY 2016
AB The volumes of data in Big Data, their variety and unstructured nature,
   have had researchers looking beyond the data warehouse. The data
   warehouse, among other features, requires mapping data to a schema upon
   ingest, an approach seen as inflexible for the massive variety of Big
   Data. The Data Lake is emerging as an alternate solution for storing
   data of widely divergent types and scales. Designed for high
   flexibility, the Data Lake follows a schema-on-read philosophy and data
   transformations are assumed to be performed within the Data Lake. During
   its lifecycle in a Data Lake, a data product may undergo numerous
   transformations performed by any number of Big Data processing engines
   leading to questions of traceability. In this paper we argue that
   provenance contributes to easier data management and traceability within
   a Data Lake infrastructure. We discuss the challenges in provenance
   integration in a Data Lake and propose a reference architecture to
   overcome the challenges. We evaluate our architecture through a
   prototype implementation built using our distributed provenance
   collection tools.
CT 12th IEEE International Conference on e-Science (e-Science)
CY OCT 23-27, 2016
CL Baltimore, MD
SP IEEE; Inst Date Intens Engn & Sci; Microsoft; Gordon & Betty Moore Fdn;
   Alfred P Sloan Fdn
RI Plale, Beth/F-8803-2011
OI Plale, Beth/0000-0003-2164-8132
ZS 0
ZR 0
ZB 2
TC 21
ZA 0
Z8 1
Z9 29
U1 0
U2 5
SN 2325-372X
BN 978-1-5090-4273-9
DA 2017-08-04
UT WOS:000405564400040
ER

PT J
AU Errami, Soukaina Ait
   Hajji, Hicham
   Kadi, Kenza Ait El
   Badir, Hassan
TI Spatial big data architecture: From Data Warehouses and Data Lakes to
   the LakeHouse
SO JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING
VL 176
BP 70
EP 79
DI 10.1016/j.jpdc.2023.02.007
EA MAR 2023
DT Article
PD JUN 2023
PY 2023
AB The construction of systems supporting spatial data has experienced
   great enthusiasm in the past, due to the richness of this type of data
   and their semantics, which can be used in the decision-making process in
   various fields. Thus, the problem of integrating spatial data into
   existing databases and information systems has been addressed by
   creating spatial extensions to relational tables or by creating spatial
   data warehouses, while arranging data structures and query languages by
   making them more spatiallyaware. With the advent of Big Data, these
   conventional storage and spatial representation structures are becoming
   increasingly outdated, and required a new organization of spatial data.
   Approaches based on distributed storage and data lakes have been
   proposed, to integrate the complexity of spatial data, with operational
   and analytical systems which unfortunately quickly showed their limits.
   Recently the concept of lakehouse was introduced in order to integrate,
   among other things, the notion of reliability and ACID properties to the
   volume of data to be managed. This new data architecture is a
   combination of governed and reliable Data Warehouses and flexible,
   scalable and cost-effective Data Lakes.In this paper, we present how
   traditional approaches of spatial data management in the context of
   spatial big data have quickly shown their limits. We present a
   literature overview of these approaches, and how they led to the Data
   LakeHouse. We detail how the Lakehouse paradigm can be used and extended
   for managing spatial big data, by giving the different components and
   best practices for building a spatial data LakeHouse architecture
   optimized for the storage and computing over spatial big data.(c) 2023
   Elsevier Inc. All rights reserved.
RI aitelkadi, aitelkadi/; Hassan, BADIR/R-6226-2019
OI aitelkadi, aitelkadi/0000-0002-4233-1292; 
ZS 0
TC 21
ZR 0
ZB 0
Z8 0
ZA 0
Z9 28
U1 4
U2 45
SN 0743-7315
EI 1096-0848
DA 2023-04-10
UT WOS:000956881700001
ER

PT J
AU Kastouni, Mohamed Zouheir
   Lahcen, Ayoub Ait
TI Big data analytics in telecommunications: Governance, architecture and
   use cases
SO JOURNAL OF KING SAUD UNIVERSITY-COMPUTER AND INFORMATION SCIENCES
VL 34
IS 6
BP 2758
EP 2770
DI 10.1016/j.jksuci.2020.11.024
PN A
DT Article
PD JUN 2022
PY 2022
AB With the upsurge of data traffic due to the change in customer behavior
   towards the use of telecommu-nications services, fostered by the current
   global health situation (mainly due to Covid-19), the
   telecom-munications operators have a golden opportunity to create new
   sources of revenues using Big Data Analytics (BDA) solutions. Looking to
   setting up a BDA project, we faced several challenges, notably, in terms
   of choice of the technical solution from the plethora of the existing
   tools, and the choice of the gov-ernance methodologies for governing the
   project and the data. The majority of research documents related to the
   telecommunications industry have not addressed BDA project
   implementation from start to finish. The purpose of this study focuses
   on a BDA telecommunications project, namely, Project's Governance,
   Architecture, Data Governance and the BDA Project's Team. The last part
   of this study pre-sents useful BDA use cases, in terms of applications
   enabling revenue creation and cost optimization. It appears that this
   work will facilitate the implementation of BDA projects, and enable
   telecommunica-tions operators to have a better understanding about the
   fundamental aspects to be focused on. It is therefore, a study that will
   contribute positively toward such goal.(c) 2020 The Authors. Published
   by Elsevier B.V. on behalf of King Saud University. This is an open
   access article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).
RI Ait Lahcen, Ayoub/AAF-5823-2021
OI Ait Lahcen, Ayoub/0000-0001-8739-3369
ZA 0
TC 21
ZS 0
Z8 1
ZB 0
ZR 0
Z9 28
U1 4
U2 34
SN 1319-1578
EI 2213-1248
DA 2022-08-11
UT WOS:000835043700015
ER

PT J
AU Damiani, A.
   Masciocchi, C.
   Lenkowicz, J.
   Capocchiano, N. D.
   Boldrini, L.
   Tagliaferri, L.
   Cesario, A.
   Sergi, P.
   Marchetti, A.
   Luraschi, A.
   Patarnello, S.
   Valentini, V.
TI Building an Artificial Intelligence Laboratory Based on Real World Data:
   The Experience of Gemelli Generator
SO FRONTIERS IN COMPUTER SCIENCE
VL 3
AR 768266
DI 10.3389/fcomp.2021.768266
DT Article
PD DEC 7 2021
PY 2021
AB The problem of transforming Real World Data into Real World Evidence is
   becoming increasingly important in the frameworks of Digital Health and
   Personalized Medicine, especially with the availability of modern
   algorithms of Artificial Intelligence high computing power, and large
   storage facilities.Even where Real World Data are well maintained in a
   hospital data warehouse and are made available for research purposes,
   many aspects need to be addressed to build an effective architecture
   enabling researchers to extract knowledge from data.We describe the
   first year of activity at Gemelli Generator RWD, the challenges we faced
   and the solutions we put in place to build a Real World Data laboratory
   at the service of patients and health researchers. Three classes of
   services are available today: retrospective analysis of existing patient
   data for descriptive and clustering purposes; automation of knowledge
   extraction, ranging from text mining, patient selection for trials, to
   generation of new research hypotheses; and finally the creation of
   Decision Support Systems, with the integration of data from the hospital
   data warehouse, apps, and Internet of Things.
RI Lenkowicz, Jacopo/AAT-8218-2020; Luraschi, Alice/; CESARIO, Alfredo/O-4215-2015; MARCHETTI, ANTONIO/W-2226-2018; Capocchiano, Nikola Dino/AAA-9318-2022
OI Lenkowicz, Jacopo/0000-0002-8366-1474; Luraschi,
   Alice/0000-0001-7400-7182; MARCHETTI, ANTONIO/0009-0008-7589-3196;
   Capocchiano, Nikola Dino/0000-0003-3556-9232
ZA 0
ZS 0
ZB 3
TC 27
Z8 0
ZR 0
Z9 28
U1 0
U2 4
EI 2624-9898
DA 2021-12-30
UT WOS:000733593100001
ER

PT C
AU Alserafi, Ayman
   Abello, Alberto
   Romero, Oscar
   Calders, Toon
BE Domeniconi, C
   Gullo, F
   Bonchi, F
   DomingoFerrer, J
   BaezaYates, R
   Zhou, ZH
   Wu, X
TI Towards Information Profiling: Data Lake Content Metadata Management
SO 2016 IEEE 16TH INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOPS (ICDMW)
SE International Conference on Data Mining Workshops
BP 178
EP 185
DI 10.1109/ICDMW.2016.87
DT Proceedings Paper
PD 2016
PY 2016
AB There is currently a burst of Big Data (BD) processed and stored in huge
   raw data repositories, commonly called Data Lakes (DL). These BD require
   new techniques of data integration and schema alignment in order to make
   the data usable by its consumers and to discover the relationships
   linking their content. This can be provided by metadata services which
   discover and describe their content. However, there is currently a lack
   of a systematic approach for such kind of metadata discovery and
   management. Thus, we propose a framework for the profiling of
   informational content stored in the DL, which we call information
   profiling. The profiles are stored as metadata to support data analysis.
   We formally define a metadata management process which identifies the
   key activities required to effectively handle this. We demonstrate the
   alternative techniques and performance of our process using a prototype
   implementation handling a real-life case-study from the OpenML DL, which
   showcases the value and feasibility of our approach.
CT 16th IEEE International Conference on Data Mining (ICDM)
CY DEC 12-15, 2016
CL Barcelona, SPAIN
SP IEEE; IEEE Comp Soc; Natl Sci Fdn; Pinnacle Lab
RI Calders, Toon/S-6315-2018; Romero, Oscar/D-5504-2012; Abello, Alberto/H-5357-2016
OI Romero, Oscar/0000-0001-6350-8328; 
ZS 0
ZA 0
TC 20
Z8 0
ZB 0
ZR 0
Z9 28
U1 0
U2 18
SN 2375-9232
BN 978-1-5090-5910-2
DA 2017-06-12
UT WOS:000401906900025
ER

PT J
AU Brous, Paul
   Janssen, Marijn
TI Trusted Decision-Making: Data Governance for Creating Trust in Data
   Science Decision Outcomes
SO ADMINISTRATIVE SCIENCES
VL 10
IS 4
AR 81
DI 10.3390/admsci10040081
DT Article
PD DEC 2020
PY 2020
AB Organizations are increasingly introducing data science initiatives to
   support decision-making. However, the decision outcomes of data science
   initiatives are not always used or adopted by decision-makers, often due
   to uncertainty about the quality of data input. It is, therefore, not
   surprising that organizations are increasingly turning to data
   governance as a means to improve the acceptance of data science decision
   outcomes. In this paper, propositions will be developed to understand
   the role of data governance in creating trust in data science decision
   outcomes. Two explanatory case studies in the asset management domain
   are analyzed to derive boundary conditions. The first case study is a
   data science project designed to improve the efficiency of road
   management through predictive maintenance, and the second case study is
   a data science project designed to detect fraudulent usage of
   electricity in medium and low voltage electrical grids without
   infringing privacy regulations. The duality of technology is used as our
   theoretical lens to understand the interactions between the
   organization, decision-makers, and technology. The results show that
   data science decision outcomes are more likely to be accepted if the
   organization has an established data governance capability. Data
   governance is also needed to ensure that organizational conditions of
   data science are met, and that incurred organizational changes are
   managed efficiently. These results imply that a mature data governance
   capability is required before sufficient trust can be placed in data
   science decision outcomes for decision-making.
RI Brous, Paul/; Janssen, Marijn/H-6223-2013
OI Brous, Paul/0000-0002-0593-1168; Janssen, Marijn/0000-0001-6211-8790
ZS 0
ZA 0
TC 21
ZR 0
Z8 0
ZB 0
Z9 27
U1 3
U2 84
EI 2076-3387
DA 2021-01-06
UT WOS:000601534200001
ER

PT J
AU Panicucci, Simone
   Nikolakis, Nikolaos
   Cerquitelli, Tania
   Ventura, Francesco
   Proto, Stefano
   Macii, Enrico
   Makris, Sotiris
   Bowden, David
   Becker, Paul
   O'Mahony, Niamh
   Morabito, Lucrezia
   Napione, Chiara
   Marguglio, Angelo
   Coppo, Guido
   Andolina, Salvatore
TI A Cloud-to-Edge Approach to Support Predictive Analytics in Robotics
   Industry
SO ELECTRONICS
VL 9
IS 3
AR 492
DI 10.3390/electronics9030492
DT Article
PD MAR 2020
PY 2020
AB Data management and processing to enable predictive analytics in cyber
   physical systems holds the promise of creating insight over underlying
   processes, discovering anomalous behaviours and predicting imminent
   failures threatening a normal and smooth production process. In this
   context, proactive strategies can be adopted, as enabled by predictive
   analytics. Predictive analytics in turn can make a shift in traditional
   maintenance approaches to more effective optimising their cost and
   transforming maintenance from a necessary evil to a strategic business
   factor. Empowered by the aforementioned points, this paper discusses a
   novel methodology for remaining useful life (RUL) estimation enabling
   predictive maintenance of industrial equipment using partial knowledge
   over its degradation function and the parameters that are affecting it.
   Moreover, the design and prototype implementation of a plug-n-play
   end-to-end cloud architecture, supporting predictive maintenance of
   industrial equipment is presented integrating the aforementioned concept
   as a service. This is achieved by integrating edge gateways, data stores
   at both the edge and the cloud, and various applications, such as
   predictive analytics, visualization and scheduling, integrated as
   services in the cloud system. The proposed approach has been implemented
   into a prototype and tested in an industrial use case related to the
   maintenance of a robotic arm. Obtained results show the effectiveness
   and the efficiency of the proposed methodology in supporting predictive
   analytics in the era of Industry 4.0.
RI Proto, Stefano/; Ventura, Francesco/GPX-6074-2022; Napione, Chiara/; Cerquitelli, Tania/; Makris, Sotiris/ADB-7760-2022; Morabito, Lucrezia/; Panicucci, Simone/; Marguglio, Angelo/; Nikolakis, Nikolaos/ACG-5403-2022; MACII, Enrico/
OI Proto, Stefano/0000-0002-8143-3611; Ventura,
   Francesco/0000-0003-3398-8265; Napione, Chiara/0000-0003-3315-3399;
   Cerquitelli, Tania/0000-0002-9039-6226; Makris,
   Sotiris/0000-0001-9687-5925; Morabito, Lucrezia/0000-0002-7131-5339;
   Panicucci, Simone/0000-0002-0670-4485; Marguglio,
   Angelo/0000-0003-4726-8832; MACII, Enrico/0000-0001-9046-5618
ZS 0
Z8 0
TC 26
ZR 0
ZA 0
ZB 1
Z9 27
U1 0
U2 20
EI 2079-9292
DA 2020-04-21
UT WOS:000524079100113
ER

PT J
AU Sarramia, David
   Claude, Alexandre
   Ogereau, Francis
   Mezhoud, Jeremy
   Mailhot, Gilles
TI CEBA: A Data Lake for Data Sharing and Environmental Monitoring
SO SENSORS
VL 22
IS 7
AR 2733
DI 10.3390/s22072733
DT Article
PD APR 2022
PY 2022
AB This article presents a platform for environmental data named
   "Environmental Cloud for the Benefit of Agriculture" (CEBA). The CEBA
   should fill the gap of a regional institutional platform to share,
   search, store and visualize heterogeneous scientific data related to the
   environment and agricultural researches. One of the main features of
   this tool is its ease of use and the accessibility of all types of data.
   To answer the question of data description, a scientific consensus has
   been established around the qualification of data with at least the
   information "when" (time), "where" (geographical coordinates) and "what"
   (metadata). The development of an on-premise solution using the data
   lake concept to provide a cloud service for end-users with institutional
   authentication and for open data access has been completed. Compared to
   other platforms, CEBA fully supports the management of geographic
   coordinates at every stage of data management. A comprehensive
   JavaScript Objet Notation (JSON) architecture has been designed, among
   other things, to facilitate multi-stage data enrichment. Data from the
   wireless network are queried and accessed in near real-time, using a
   distributed JSON-based search engine.
RI MEZHOUD, Jeremy/; Mailhot, Gilles/ABC-7180-2020; David, SARRAMIA/
OI MEZHOUD, Jeremy/0009-0005-7162-7665; Mailhot,
   Gilles/0000-0002-8179-8880; David, SARRAMIA/0000-0002-7062-9707
ZA 0
ZR 0
ZB 3
TC 22
ZS 0
Z8 0
Z9 26
U1 0
U2 20
EI 1424-8220
DA 2022-04-24
UT WOS:000781144200001
PM 35408347
ER

PT J
AU Asaithambi, Suriya Priya R.
   Venkatraman, Ramanathan
   Venkatraman, Sitalakshmi
TI MOBDA: Microservice-Oriented Big Data Architecture for Smart City
   Transport Systems
SO BIG DATA AND COGNITIVE COMPUTING
VL 4
IS 3
AR 17
DI 10.3390/bdcc4030017
DT Article
PD SEP 2020
PY 2020
AB Highly populated cities depend highly on intelligent transportation
   systems (ITSs) for reliable and efficient resource utilization and
   traffic management. Current transportation systems struggle to meet
   different stakeholder expectations while trying their best to optimize
   resources in providing various transport services. This paper proposes a
   Microservice-Oriented Big Data Architecture (MOBDA) incorporating data
   processing techniques, such as predictive modelling for achieving smart
   transportation and analytics microservices required towards smart cities
   of the future. We postulate key transportation metrics applied on
   various sources of transportation data to serve this objective. A novel
   hybrid architecture is proposed to combine stream processing and batch
   processing of big data for a smart computation of microservice-oriented
   transportation metrics that can serve the different needs of
   stakeholders. Development of such an architecture for smart
   transportation and analytics will improve the predictability of
   transport supply for transport providers and transport authority as well
   as enhance consumer satisfaction during peak periods.
RI Venkatraman, Sitalakshmi/R-3130-2018
OI Venkatraman, Sitalakshmi/0000-0002-2772-133X
ZS 0
Z8 0
TC 22
ZR 0
ZA 0
ZB 0
Z9 25
U1 0
U2 17
EI 2504-2289
DA 2021-10-02
UT WOS:000697677900002
ER

PT C
AU Ahmadov, Ahmad
   Thiele, Maik
   Eberius, Julian
   Lehner, Wolfgang
   Wrembel, Robert
BE Raicu, I
   Rana, O
   Buyya, R
TI Towards a Hybrid Imputation Approach Using Web Tables
SO 2015 IEEE/ACM 2ND INTERNATIONAL SYMPOSIUM ON BIG DATA COMPUTING (BDC)
BP 21
EP 30
DI 10.1109/BDC.2015.38
DT Proceedings Paper
PD 2015
PY 2015
AB Data completeness is one of the most important data quality dimensions
   and an essential premise in data analytics. With new emerging Big Data
   trends such as the data lake concept, which provides a low cost data
   preparation repository instead of moving curated data into a data
   warehouse, the problem of data completeness is additionally reinforced.
   While traditionally the process of filling in missing values is
   addressed by the data imputation community using statistical techniques,
   we complement these approaches by using external data sources from the
   data lake or even the Web to lookup missing values. In this paper we
   propose a novel hybrid data imputation strategy that, takes into account
   the characteristics of an incomplete dataset and based on that chooses
   the best imputation approach, i.e. either a statistical approach such as
   regression analysis or a Web-based lookup or a combination of both. We
   formalize and implement both imputation approaches, including a Web
   table retrieval and matching system and evaluate them extensively using
   a corpus with 125M Web tables. We show that applying statistical
   techniques in conjunction with external data sources will lead to a
   imputation system which is robust, accurate, and has high coverage at
   the same time.
CT International Symposium on Big Data Computing
CY DEC 07-15, 2015
CL Limassol, CYPRUS
SP CPS; IEEE Comp Soc; IEEE; ACM; TCSC; Cyprus heart; fuseami; sighpc;
   Passion Success; Austrian; CPS Online
RI Thiele, Maik/AAX-6115-2021; Wrembel, Robert/F-7482-2014; Lehner, Wolfgang/V-6799-2019
OI Thiele, Maik/0000-0002-1665-977X; Wrembel, Robert/0000-0001-6037-5718;
   Lehner, Wolfgang/0000-0001-8107-2775
ZR 0
Z8 0
ZB 0
ZA 0
ZS 0
TC 21
Z9 25
U1 0
U2 2
BN 978-0-7695-5696-3
DA 2016-09-11
UT WOS:000380459200003
ER

PT J
AU Golov, Nikolay
   Ronnback, Lars
TI Big Data normalization for massively parallel processing databases
SO COMPUTER STANDARDS & INTERFACES
VL 54
SI SI
BP 86
EP 93
DI 10.1016/j.csi.2017.01.009
PN 2
DT Article
PD NOV 2017
PY 2017
AB High performance querying and ad-hoc querying are commonly viewed as
   mutually exclusive goals in massively parallel processing databases.
   Furthermore, there is a contradiction between ease of extending the data
   model and ease of analysis. The modern 'Data Lake' approach, promises
   extreme ease of adding new data to a data model, however it is prone to
   eventually becoming a Data Swamp - unstructured, ungoverned, and out of
   control Data Lake where due to a lack of process, standards and
   governance, data is hard to find, hard to use and is consumed out of
   context. This paper introduces a novel technique, highly normalized Big
   Data using Anchor modeling, that provides a very efficient way to store
   information and utilize resources, thereby providing ad-hoc querying
   with high performance for the first time in massively parallel
   processing databases. This technique is almost as convenient for
   expanding data model as a Data Lake, while it is internally protected
   from transforming to Data Swamp. A case study of how this approach is
   used for a Data Warehouse at Avito over a three-year period, with
   estimates for and results of real data experiments carried out in HP
   Vertica, an MPP RDBMS, is also presented. This paper is an extension of
   theses from The 34th International Conference on Conceptual Modeling (ER
   2015) (Golov and Ronnback 2015) [1], it is complemented with numerical
   results about key operating areas of highly normalized big data
   warehouse, collected over several (1-3) years of commercial operation.
   Also, the limitations, imposed by using a single MPP database cluster,
   are described, and cluster fragmentation approach is proposed.
OI Rönnbäck, Lars/0000-0002-8810-7587
ZS 0
ZR 0
TC 19
ZB 1
ZA 0
Z8 0
Z9 24
U1 0
U2 76
SN 0920-5489
EI 1872-7018
DA 2017-06-15
UT WOS:000401888800004
ER

PT C
AU Gupta, Maanak
   Patwa, Farhan
   Sandhu, Ravi
BE Livraga, G
   Zhu, S
TI Object-Tagged RBAC Model for the Hadoop Ecosystem
SO DATA AND APPLICATIONS SECURITY AND PRIVACY XXXI, DBSEC 2017
SE Lecture Notes in Computer Science
VL 10359
BP 63
EP 81
DI 10.1007/978-3-319-61176-1_4
DT Proceedings Paper
PD 2017
PY 2017
AB Hadoop ecosystem provides a highly scalable, fault-tolerant and
   cost-effective platform for storing and analyzing variety of data
   formats. Apache Ranger and Apache Sentry are two predominant frameworks
   used to provide authorization capabilities in Hadoop ecosystem. In this
   paper we present a formal multi-layer access control model (called HeAC)
   for Hadoop ecosystem, as an academic-style abstraction of Ranger, Sentry
   and native Apache Hadoop access-control capabilities. We further extend
   HeAC base model to provide a cohesive object-tagged role-based access
   control (OT-RBAC) model, consistent with generally accepted academic
   concepts of RBAC. Besides inheriting advantages of RBAC, OT-RBAC offers
   a novel method for combining RBAC with attributes (beyond NIST proposed
   strategies). Additionally, a proposed implementation approach for
   OT-RBAC in Apache Ranger, is presented. We further outline
   attribute-based extensions to OT-RBAC.
CT 31st Annual IFIP WG 11.3 Conference on Data and Applications Security
   and Privacy (DBSec)
CY JUL 19-21, 2017
CL Philadelphia, PA
SP IFIP WG 11 3
RI Gupta, Maanak/ABD-4037-2020
OI Gupta, Maanak/0000-0001-9189-2478
ZS 0
TC 18
ZA 0
ZB 0
ZR 0
Z8 2
Z9 24
U1 0
U2 3
SN 0302-9743
EI 1611-3349
BN 978-3-319-61176-1; 978-3-319-61175-4
DA 2017-01-01
UT WOS:000463615900004
ER

PT J
AU Calyam, Prasad
   Mishra, Anup
   Antequera, Ronny Bazan
   Chemodanov, Dmitrii
   Berryman, Alex
   Zhu, Kunpeng
   Abbott, Carmen
   Skubic, Marjorie
TI Synchronous Big Data analytics for personalized and remote physical
   therapy
SO PERVASIVE AND MOBILE COMPUTING
VL 28
SI SI
BP 3
EP 20
DI 10.1016/j.pmcj.2015.09.004
DT Article
PD JUN 2016
PY 2016
AB With gigabit networking becoming economically feasible and widely
   installed at homes, there are new opportunities to revisit in-home,
   personalized telehealth services. In this paper, we describe a novel
   telehealth eldercare service that we developed viz., "Physical
   Therapy-as-a-Service'' (PTaaS) that connects a remote physical therapist
   at a clinic to a senior at home. The service leverages a high-speed,
   low-latency network connection through an interactive interface built on
   top of Microsoft Kinect motion sensing capabilities. The interface that
   is built using user-centered design principles for wellness coaching
   exercises is essentially a 'Synchronous Big Data' application due to
   its: (i) high data-in-motion velocity (i.e., peak data rate is
   approximate to 400 Mbps), (ii) considerable variety (i.e., measurements
   include 3D sensing, network health, user opinion surveys and video clips
   of RGB, skeletal and depth data), and (iii) large volume (i.e., several
   GB of measurement data for a simple exercise activity). The successful
   PTaaS delivery through this interface is dependent on the veracity
   analytics needed for correlation of the real-time Big Data streams
   within a session, in order to assess exercise balance of the senior
   without any bias due to network quality effects. Our experiments with
   PTaaS in an actual testbed involving senior homes in Kansas City with
   Google Fiber connections and our university clinic demonstrate the
   network configuration and time synchronization related challenges in
   order to perform online analytics. Our findings provide insights on how
   to: (a) enable suitable resource calibration and perform network
   troubleshooting for high user experience for both the therapist and the
   senior, and (b) realize a Big Data architecture for PTaaS and other
   similar personalized healthcare services to be remotely delivered at a
   large-scale in a reliable, secure and cost-effective manner. (C) 2015
   Elsevier B.V. All rights reserved.
RI Mishra, Anup Kumar/GLS-1825-2022
OI Mishra, Anup Kumar/0000-0001-8489-0087
ZA 0
ZR 0
ZB 2
Z8 0
ZS 0
TC 17
Z9 24
U1 1
U2 48
SN 1574-1192
EI 1873-1589
DA 2016-06-01
UT WOS:000376438200002
ER

PT C
AU Miloslavskaya, Natalia
   Tolstoy, Alexander
BE Younas, M
   Awan, I
   ElHaddad, J
TI Application of Big Data, Fast Data and Data Lake Concepts to Information
   Security Issues
SO 2016 IEEE 4TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND
   CLOUD WORKSHOPS (FICLOUDW)
BP 148
EP 153
DI 10.1109/W-FiCloud.2016.41
DT Proceedings Paper
PD 2016
PY 2016
AB Today we witness the appearance of some additional to Big Data concepts:
   data lakes and fast data. Are they simply the new marketing labels for
   the old Big Data IT or really new ones? Thus the key goal of the paper
   is to identify the relationship between these three concepts, giving
   special attention to their application to information security (IS)
   issues. The reason lies in the fact that volumes of IS-related
   information is one thing, but the real problem for securing enterprises'
   IT infrastructure assets is the speed with which things related to IS
   happen.
CT IEEE 4th International Conference on Future Internet of Things and Cloud
   Workshops (FiCloudW)
CY AUG 22-24, 2016
CL Vienna, AUSTRIA
SP IEEE; IEEE Comp Soc; Univ Bradford; Oxford Brooks Univ; Univ Wien
RI Miloslavskaya, Natalia/F-7562-2011; Tolstoy, Alexander/
OI Miloslavskaya, Natalia/0000-0002-1231-1805; Tolstoy,
   Alexander/0000-0001-9265-1510
ZR 0
ZB 0
ZA 0
TC 16
ZS 0
Z8 1
Z9 24
U1 0
U2 12
BN 978-1-5090-3946-3
DA 2016-11-23
UT WOS:000386667700023
ER

PT J
AU Zhang, Yong
   Sheng, Ming
   Liu, Xingyue
   Wang, Ruoyu
   Lin, Weihang
   Ren, Peng
   Wang, Xia
   Zhao, Enlai
   Song, Wenchao
TI A heterogeneous multi-modal medical data fusion framework supporting
   hybrid data exploration
SO HEALTH INFORMATION SCIENCE AND SYSTEMS
VL 10
IS 1
AR 22
DI 10.1007/s13755-022-00183-x
DT Article
PD AUG 26 2022
PY 2022
AB Industry 4.0 era has witnessed that more and more high-tech and precise
   devices are applied into medical field to provide better services.
   Besides EMRs, medical data include a large amount of unstructured data
   such as X-rays, MRI scans, CT scans and PET scans, which is still
   continually increasing. These massive, heterogeneous multi-modal data
   bring the big challenge to finding valuable data sets for healthcare
   researchers and other users. The traditional data warehouses are able to
   integrate the data and support interactive data exploration through ETL
   process. However, they have high cost and are not real-time.
   Furthermore, they lack of the ability to deal with multi-modal data in
   two phases-data fusion and data exploration. In the data fusion phase,
   it is difficult to unify the multi-modal data under one data model. In
   the data exploration phase, it is challenging to explore the multi-modal
   data at the same time, which impedes the process of extracting the
   diverse information underlying multi-modal data. Therefore, in order to
   solve these problems, we propose a highly efficient data fusion
   framework supporting data exploration for heterogeneous multi-modal
   medical data based on data lake. This framework provides a novel and
   efficient method to fuse the fragmented multi-modal medical data and
   store their metadata in the data lake. It offers a user-friendly
   interface supporting hybrid graph queries to explore multi-modal data.
   Indexes are created to accelerate the hybrid data exploration. One
   prototype has been implemented and tested in a hospital, which
   demonstrates the effectiveness of our framework.
RI Lin, Weihang/JPY-1060-2023
ZB 2
ZA 0
ZR 0
ZS 0
Z8 1
TC 18
Z9 23
U1 3
U2 52
SN 2047-2501
DA 2022-09-04
UT WOS:000844978000001
PM 36039096
ER

PT J
AU Havard, Vincent
   Sahnoun, M'hammed
   Bettayeb, Belgacem
   Duval, Fabrice
   Baudry, David
TI Data architecture and model design for Industry 4.0 components
   integration in cyber-physical production systems
SO PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART B-JOURNAL OF
   ENGINEERING MANUFACTURE
VL 235
IS 14
SI SI
BP 2338
EP 2349
AR 0954405420979463
DI 10.1177/0954405420979463
EA DEC 2020
DT Article
PD DEC 2021
PY 2021
AB In the context of Industry 4.0, Cyber-Physical Production Systems (CPPS)
   and digital twins are key technologies for the management of huge amount
   of data generated by Industrial Internet of things (IIoT) devices.
   However, the interoperability and flexibility of different components is
   still an important challenge so as to integrate them in the process and
   fit all industrial specific needs. Thus, the main contribution of this
   paper is to propose a database architecture and a data model associated
   allowing multiple agents to work collaboratively and synchronously to
   perform high-level tasks. Therefore, it fulfils requirements and needs
   of Industry 4.0: interoperability, scalability, flexibility and
   resilience. The proposed architecture and model are implemented on a
   cyber-physical production system (CPPS) which is used in order to show
   and discuss several use cases examples.
RI Bauy, David/ABG-2242-2020; Sahnoun, M'hammed/AAD-4021-2019; DUVAL, Fabrice/; Bettayeb, Belgacem/GRS-9595-2022; Havard, Vincent/HPD-8308-2023
OI Bauy, David/0000-0002-4386-4496; Sahnoun, M'hammed/0000-0003-3515-8118;
   DUVAL, Fabrice/0000-0001-8825-9321; Bettayeb,
   Belgacem/0000-0003-0997-9529; Havard, Vincent/0000-0001-8248-3496
ZB 0
ZS 0
ZA 0
Z8 0
TC 22
ZR 0
Z9 23
U1 2
U2 26
SN 0954-4054
EI 2041-2975
DA 2021-08-15
UT WOS:000681099900001
ER

PT C
AU Brous, Paul
   Janssen, Marijn
   Krans, Rutger
BE Hattingh, M
   Matthee, M
   Smuts, H
   Pappas, I
   Dwivedi, YK
   Mantymaki, M
TI Data Governance as Success Factor for Data Science
SO RESPONSIBLE DESIGN, IMPLEMENTATION AND USE OF INFORMATION AND
   COMMUNICATION TECHNOLOGY, I3E 2020, PT I
SE Lecture Notes in Computer Science
VL 12066
BP 431
EP 442
DI 10.1007/978-3-030-44999-5_36
DT Proceedings Paper
PD 2020
PY 2020
AB More and more, asset management organizations are introducing data
   science initiatives to support predictive maintenance and anomaly
   detection. Asset management organizations are by nature data intensive
   to manage their assets like bridges, dykes, railways and roads. For
   this, they often implement data lakes using a variety of architectures
   and technologies to store big data and facilitate data science
   initiatives. However, the decision-outcomes of data science models are
   often highly reliant on the quality of the data. The data in the data
   lake therefore has to be of sufficient quality to develop trust by
   decision-makers. Not surprisingly, organizations are increasingly
   adopting data governance as a means to ensure that the quality of data
   entering the data lake is and remains of sufficient quality, and to
   ensure the organization remains legally compliant. The objective of the
   case study is to understand the role of data governance as success
   factor for data science. For this, a case study regarding the governance
   of data in a data lake in the asset management domain is analyzed to
   test three propositions contributing to the success of using data
   science. The results show that unambiguous ownership of the data,
   monitoring the quality of the data entering the data lake, and a
   controlled overview of standard and specific compliance requirements are
   important factors for maintaining data quality and compliance and
   building trust in data science products.
CT 19th IFIP WG 6.11 Conference on e-Business, e-Services, and e-Society
   (I3E)
CY APR 06-08, 2020
CL Univ Pretoria, Dep Informat, Skukuza, SOUTH AFRICA
HO Univ Pretoria, Dep Informat
SP Int Federat Automat Control
RI Janssen, Marijn/H-6223-2013; Brous, Paul/
OI Janssen, Marijn/0000-0001-6211-8790; Brous, Paul/0000-0002-0593-1168
ZR 0
ZA 0
ZB 0
TC 17
ZS 0
Z8 0
Z9 23
U1 4
U2 9
SN 0302-9743
EI 1611-3349
BN 978-3-030-44998-8; 978-3-030-44999-5
DA 2020-01-01
UT WOS:001352251200036
ER

PT C
AU Giebler, Corinna
   Groger, Christoph
   Hoos, Eva
   Schwarz, Holger
   Mitschang, Bernhard
GP IEEE
TI A Zone Reference Model for Enterprise-Grade Data Lake Management
SO 2020 IEEE 24TH INTERNATIONAL ENTERPRISE DISTRIBUTED OBJECT COMPUTING
   CONFERENCE (EDOC 2020)
SE IEEE International Enterprise Distributed Object Computing
   Conference-EDOC
BP 57
EP 66
DI 10.1109/EDOC49727.2020.00017
DT Proceedings Paper
PD 2020
PY 2020
AB Data lakes are on the rise as data platforms for any kind of analytics,
   from data exploration to machine learning. They achieve the required
   flexibility by storing heterogeneous data in their raw format, and by
   avoiding the need for pre-defined use cases. However, storing only raw
   data is inefficient, as for many applications, the same data processing
   has to be applied repeatedly. To foster the reuse of processing steps,
   literature proposes to store data in different degrees of processing in
   addition to their raw format. To this end, data lakes are typically
   structured in zones. There exists various zone models, but they are
   varied, vague, and no assessments are given. It is unclear which of
   these zone models is applicable in a practical data lake implementation
   in enterprises. In this work, we assess existing zone models using
   requirements derived from multiple representative data analytics use
   cases of a real-world industry case. We identify the shortcomings of
   existing work and develop a zone reference model for enterprise-grade
   data lake management in a detailed manner. We assess the reference
   model's applicability through a prototypical implementation for a
   real-world enterprise data lake use case. This assessment shows that the
   zone reference model meets the requirements relevant in practice and is
   ready for industry use.
CT 24th IEEE International Enterprise Distributed Object Computing
   Conference (IEEE EDOC)
CY OCT 05-08, 2020
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc
RI Schwarz, Holger/AAP-1719-2020
TC 18
Z8 1
ZR 0
ZB 0
ZA 0
ZS 0
Z9 23
U1 0
U2 3
SN 2325-6354
BN 978-1-7281-6473-1
DA 2021-04-20
UT WOS:000630246800007
ER

PT C
AU Georgiou, Kyriakos
   Constambeys, Timotheos
   Laoudias, Christos
   Petrou, Lambros
   Chatzimilioudis, Georgios
   Zeinalipour-Yazti, Demetrios
GP IEEE
TI Anyplace: A Crowdsourced Indoor Information Service
SO 2015 16TH IEEE INTERNATIONAL CONFERENCE ON MOBILE DATA MANAGEMENT, VOL 1
SE IEEE International Conference on Mobile Data Management
BP 291
EP 294
DI 10.1109/MDM.2015.80
DT Proceedings Paper
PD 2015
PY 2015
AB People do most of their activities, business, commerce, entertainment
   and socializing indoors. As all of these are increasingly aided by
   online services and indoor spaces are becoming bigger and more complex,
   there is a growing need for cost-effective indoor localization, mapping,
   navigation and information services. In this paper, we present a
   complete Indoor Information Service, coined Anyplace 1, which has an
   open, modular, extensible and scalable architecture, making it ideal for
   a wide range of applications. Our service features three highly
   desirable properties, namely crowdsourcing, scalability and accuracy.
   Anyplace implements a set of crowdsourcing-supportive mechanisms to
   handle the enormous amount of crowd-sensed data, filter incorrect user
   contributions and exploit Wi-Fi data from heterogeneous mobile devices.
   Moreover, it uses a big-data architecture for efficient storage and
   retrieval of localization and mapping data. Finally, our service relies
   on the abundance of sensory data on smartphones (e.g., Wi-Fi signal
   strength and inertial measurements) to deliver reliable indoor
   geolocation information that received several international awards.
CT IEEE 16th International Conference on Mobile Data Management MDM
CY JUN 15-18, 2015
CL Pittsburgh, PA
SP IEEE Comp Soc; IEEE Tech Comm Data Engn; Univ of Pittsburgh, USA; Natl
   Sci Fdn, USA; Aalborg Univ, Denmark; Hewlett Packard; Hewlett Packard
   Vertica; Conf Publishing Serv
RI Laoudias, Christos/W-4881-2019; Zeinalipour-Yazti, Demetrios/O-2301-2016
OI Laoudias, Christos/0000-0002-2907-7488; Zeinalipour-Yazti,
   Demetrios/0000-0002-7239-2387
Z8 0
ZS 0
ZR 0
ZA 0
TC 18
ZB 0
Z9 23
U1 1
U2 1
SN 1551-6245
BN 978-1-4799-9972-9
DA 2016-10-19
UT WOS:000380404900037
ER

PT J
AU Wieder, Philipp
   Nolte, Hendrik
TI Toward data lakes as central building blocks for data management and
   analysis
SO FRONTIERS IN BIG DATA
VL 5
AR 945720
DI 10.3389/fdata.2022.945720
DT Review
PD AUG 19 2022
PY 2022
AB Data lakes are a fundamental building block for many industrial data
   analysis solutions and becoming increasingly popular in research. Often
   associated with big data use cases, data lakes are, for example, used as
   central data management systems of research institutions or as the core
   entity of machine learning pipelines. The basic underlying idea of
   retaining data in its native format within a data lake facilitates a
   large range of use cases and improves data reusability, especially when
   compared to the schema-on-write approach applied in data warehouses,
   where data is transformed prior to the actual storage to fit a
   predefined schema. Storing such massive amounts of raw data, however,
   has its very own challenges, spanning from the general data modeling,
   and indexing for concise querying to the integration of suitable and
   scalable compute capabilities. In this contribution, influential papers
   of the last decade have been selected to provide a comprehensive
   overview of developments and obtained results. The papers are analyzed
   with regard to the applicability of their input to data lakes that serve
   as central data management systems of research institutions. To achieve
   this, contributions to data lake architectures, metadata models, data
   provenance, workflow support, and FAIR principles are investigated.
   Last, but not least, these capabilities are mapped onto the requirements
   of two common research personae to identify open challenges. With that,
   potential research topics are determined, which have to be tackled
   toward the applicability of data lakes as central building blocks for
   research data management.
OI Wieder, Philipp/0000-0002-6992-1866
ZB 2
ZS 1
Z8 1
TC 14
ZR 0
ZA 0
Z9 22
U1 5
U2 59
EI 2624-909X
DA 2022-10-01
UT WOS:000859506800001
PM 36072823
ER

PT J
AU Tamym, Lahcen
   Benyoucef, Lyes
   Moh, Ahmed Nait Sidi
   El Ouadghiri, Moulay Driss
TI A big data based architecture for collaborative networks: Supply chains
   mixed-network
SO COMPUTER COMMUNICATIONS
VL 175
BP 102
EP 111
DI 10.1016/j.comcom.2021.05.008
EA MAY 2021
DT Article
PD JUL 1 2021
PY 2021
AB Nowadays, the world knows a high-speed development and evolution of
   technologies, vulnerable economic environments, market changes, and
   personalised consumer trends. The issue and challenge related to
   enterprises networks design are more and more critical. These networks
   are often designed for short terms since their strategies must be
   competitive and better adapted to the environment, social and economical
   changes. As a solution, to design a flexible and robust network, it is
   necessary to deal with the trade-off between conflicting qualitative and
   quantitative criteria such as cost, quality, delivery time, and
   competition, etc. To this end, using Big Data (BD) as emerging
   technology will enhance the real performances of these kinds of
   networks. Moreover, even if the literature is rich with BD models and
   frameworks developed for a single supply chain network (SCN), there is a
   real need to scale and extend these BD models to networked supply chains
   (NSCs). To do so, this paper proposes a BD architecture to drive a
   mixed-network of SCs that collaborate in serial and parallel fashions.
   The collaboration is set up by sharing their resources, capabilities,
   competencies, and information to imitate a unique organisation. The
   objective is to increase internal value to their shareholders (where
   value is seen as wealth) and deliver better external value to the
   end-customer (where value represents customer satisfaction). Within a
   mixed-network of SCs, both values are formally calculated considering
   both serial and parallel networks configurations. Besides, some
   performance factors of the proposed BD architecture such as security,
   flexibility, robustness and resilience are discussed.
RI Tamym, Lahcen/; EL OUADGHIRI, Moulay iss/; Moh, Ahmed/ABF-5081-2020
OI Tamym, Lahcen/0000-0002-5083-958X; EL OUADGHIRI, Moulay
   iss/0000-0002-1480-083X; 
ZA 0
ZR 0
ZB 0
ZS 0
TC 20
Z8 0
Z9 22
U1 0
U2 34
SN 0140-3664
EI 1873-703X
DA 2021-08-08
UT WOS:000678421800010
ER

PT J
AU Davila Delgado, Juan Manuel
   Oyedele, Lukumon
   Bilal, Muhammad
   Ajayi, Anuoluwapo
   Akanbi, Lukman
   Akinade, Olugbenga
TI Big Data Analytics System for Costing Power Transmission Projects
SO JOURNAL OF CONSTRUCTION ENGINEERING AND MANAGEMENT
VL 146
IS 1
AR 05019017
DI 10.1061/(ASCE)CO.1943-7862.0001745
DT Article
PD JAN 1 2020
PY 2020
AB Inaccurate cost estimates have significant impacts on the final cost of
   power transmission projects and erode profits. Methods for cost
   estimation have been investigated thoroughly, but they are not used
   widely in practice. The purpose of this study is to leverage a big data
   architecture, to manage the large and diverse data required for
   predictive analytics. This paper presents a predictive analytics and
   modeling system (PAMS) that facilitates the use of different data-driven
   cost prediction methods. A 2.75-million-point dataset of power
   transmission projects has been used as a case study. The proposed big
   data architecture fits this purpose. It can handle the diverse datasets
   used in the construction sector. The three most prevalent cost
   estimation models were implemented (linear regression, support vector
   regression, and artificial neural networks). All models performed better
   than the estimated human-level performance. The primary contribution of
   this study to the body of knowledge is an empirical indication that
   data-driven methods analysed in this study are on average 13.5% better
   than manual methods for cost estimation of power transmission projects.
   Additionally, the paper presents a big data architecture that can manage
   and process large varied datasets and seamless scalability.
RI Akanbi, Lukman/HMW-1378-2023; Davila Delgado, Juan Manuel/
OI Akanbi, Lukman/0000-0003-1258-9142; Davila Delgado, Juan
   Manuel/0000-0001-8242-7339
ZS 0
Z8 1
ZB 0
TC 21
ZR 0
ZA 0
Z9 22
U1 2
U2 105
SN 0733-9364
EI 1943-7862
DA 2019-12-06
UT WOS:000497672300005
ER

PT C
AU Costa, Carlos
   Santos, Maribel Yasmina
GP IEEE
TI BASIS: A Big Data Architecture for Smart Cities
SO PROCEEDINGS OF THE 2016 SAI COMPUTING CONFERENCE (SAI)
BP 1247
EP 1256
DT Proceedings Paper
PD 2016
PY 2016
AB Nowadays, cities are the common choice for living, representing a
   complex system where governments need to perform adequately, despite
   current restrictions, in order to satisfy the needs of the citizens and
   overcome economic, social and environmental sustainability challenges.
   The Smart City term emerges to conceptualize the need to understand
   citizens, namely their services demand and their relevance in a
   participatory government. Smart Cities are known for their human
   dynamics, which makes recurrent use of permanently connected devices,
   frequently known as Internet of Things (IoT). Consequently, since these
   new cities generate a vast volume of data with significant variety and
   velocity, they have the potential to be one of the richest and
   challenging systems to generate Big Data and to benefit from its
   adequate storage, processing, analysis and public availability. This
   paper presents a Big Data architecture for Smart Cities, entitled BASIS,
   whose specification pays particular attention to the creation of
   multiple abstraction layers, from the most conceptual to the most
   technological, fulfilling the lack of technological detail often
   observed in the literature. BASIS also pays particular attention to the
   public availability of data. Tested in a demonstration case, the
   obtained results reveal adequate capability to store, process, analyse
   and make available Big Data in the context of Smart Cities.
CT SAI Computing Conference (SAI)
CY JUL 13-15, 2016
CL London, ENGLAND
SP IEEE; Inst Engn & Technol; Usenix; Deutsche Telekom; Deep ER; iMinds;
   Cancer Res UK; BCS; Sci & Informat Org
RI Santos, Maribel Yasmina/M-5214-2013; Costa, Carlos/P-3314-2019
OI Santos, Maribel Yasmina/0000-0002-3249-6229; Costa,
   Carlos/0000-0003-0011-6030
ZR 0
TC 17
ZS 0
Z8 0
ZA 0
ZB 0
Z9 22
U1 0
U2 11
BN 978-1-4673-8460-5
DA 2016-01-01
UT WOS:000389451900181
ER

PT C
AU Petrou, Lambros
   Larkou, George
   Laoudias, Christos
   Zeinalipour-Yazti, Demetrios
   Panayiotou, Christos G.
GP IEEE
TI Demonstration Abstract: Crowdsourced Indoor Localization and Navigation
   with Anyplace
SO PROCEEDINGS OF THE 13TH INTERNATIONAL SYMPOSIUM ON INFORMATION
   PROCESSING IN SENSOR NETWORKS (IPSN' 14)
BP 331
EP +
DT Proceedings Paper
PD 2014
PY 2014
AB In this demonstration paper, we present the Anyplace system that relies
   on the abundance of sensory data on smartphones (e. g., WiFi signal
   strength and inertial measurements) to deliver reliable indoor
   geolocation information. Our system features two highly desirable
   properties, namely crowdsourcing and scalability. Anyplace implements a
   set of crowdsourcing-supportive mechanisms to handle the enormous amount
   of crowdsensed data, filter incorrect user contributions and exploit
   WiFi data from heterogeneous mobile devices. Moreover, Anyplace follows
   a big-data architecture for efficient and scalable storage and retrieval
   of localization and mapping data.
CT 13th IEEE/ACM International Symposium on Information Processing in
   Sensor Networks (IPSN)
CY APR 15-17, 2014
CL Berlin, GERMANY
SP Assoc Comp Machinery; IEEE; ACM SIGBED; IEEE Comp Soc; Carl Ossietzky
   Univ Oldenburg
RI Zeinalipour-Yazti, Demetrios/O-2301-2016; Panayiotou, Christos/; Laoudias, Christos/W-4881-2019
OI Zeinalipour-Yazti, Demetrios/0000-0002-7239-2387; Panayiotou,
   Christos/0000-0002-6476-9025; Laoudias, Christos/0000-0002-2907-7488
Z8 0
ZS 0
ZA 0
ZR 0
TC 19
ZB 0
Z9 22
U1 0
U2 1
BN 978-1-4799-3146-0
DA 2014-11-19
UT WOS:000343592200053
ER

PT J
AU Tripathi, Aakash
   Waqas, Asim
   Venkatesan, Kavya
   Yilmaz, Yasin
   Rasool, Ghulam
TI Building Flexible, Scalable, and Machine Learning-Ready Multimodal
   Oncology Datasets
SO SENSORS
VL 24
IS 5
AR 1634
DI 10.3390/s24051634
DT Article
PD MAR 2024
PY 2024
AB The advancements in data acquisition, storage, and processing techniques
   have resulted in the rapid growth of heterogeneous medical data.
   Integrating radiological scans, histopathology images, and molecular
   information with clinical data is essential for developing a holistic
   understanding of the disease and optimizing treatment. The need for
   integrating data from multiple sources is further pronounced in complex
   diseases such as cancer for enabling precision medicine and personalized
   treatments. This work proposes Multimodal Integration of Oncology Data
   System (MINDS)-a flexible, scalable, and cost-effective metadata
   framework for efficiently fusing disparate data from public sources such
   as the Cancer Research Data Commons (CRDC) into an interconnected,
   patient-centric framework. MINDS consolidates over 41,000 cases from
   across repositories while achieving a high compression ratio relative to
   the 3.78 PB source data size. It offers sub-5-s query response times for
   interactive exploration. MINDS offers an interface for exploring
   relationships across data types and building cohorts for developing
   large-scale multimodal machine learning models. By harmonizing
   multimodal data, MINDS aims to potentially empower researchers with
   greater analytical ability to uncover diagnostic and prognostic insights
   and enable evidence-based personalized care. MINDS tracks granular
   end-to-end data provenance, ensuring reproducibility and transparency.
   The cloud-native architecture of MINDS can handle exponential data
   growth in a secure, cost-optimized manner while ensuring substantial
   storage optimization, replication avoidance, and dynamic access
   capabilities. Auto-scaling, access controls, and other mechanisms
   guarantee pipelines' scalability and security. MINDS overcomes the
   limitations of existing biomedical data silos via an interoperable
   metadata-driven approach that represents a pivotal step toward the
   future of oncology data integration.
RI Tripathi, Aakash/; Venkatesan, Kavya/; Yilmaz, Yasin/LDG-3986-2024; Waqas, Asim/KEH-8791-2024; Rasool, Ghulam/T-7960-2019
OI Tripathi, Aakash/0000-0001-7231-0487; Venkatesan,
   Kavya/0009-0004-9069-5500; Waqas, Asim/0000-0002-6834-4710; Rasool,
   Ghulam/0000-0001-8551-0090
ZS 0
Z8 0
ZR 0
ZA 0
TC 14
ZB 3
Z9 21
U1 5
U2 12
EI 1424-8220
DA 2024-04-06
UT WOS:001183030500001
PM 38475170
ER

PT J
AU Yang, Chao-Tung
   Chen, Tzu-Yang
   Kristiani, Endah
   Wu, Shyhtsun Felix
TI The implementation of data storage and analytics platform for big data
   lake of electricity usage with spark
SO JOURNAL OF SUPERCOMPUTING
VL 77
IS 6
BP 5934
EP 5959
DI 10.1007/s11227-020-03505-6
EA NOV 2020
DT Article
PD JUN 2021
PY 2021
AB Electricity data could generate a large number of records from smart
   meter day by day. The traditional architecture might not properly handle
   the increasingly dynamic data that need flexibility. For effective
   storing and analytics, efficient architecture is needed to provide much
   greater data volumes and varieties. In this paper, we proposed the
   architecture of data storage and analytic in the big data lake of
   electricity usage using Spark. Apache Sqoop was used to migrate
   historical data to Apache Hive for processing from an existing system.
   Apache Kafka was used as the input source for Spark to stream data to
   Apache HBase to ensure the integrity of the streaming data. In order to
   integrate the data, we use the Hive and HBase principle of Data Lake as
   search engines for Hive and HBase. Apache Impala and Apache Phoenix are
   used separately. This work also analyzes electricity usage and power
   failure with Apache Spark. All of the visualizations of this project are
   presented in Apache Superset. Moreover, the usage prediction comparison
   is presented using HoltWinters algorithm.
RI Kristiani, Endah/AAA-9579-2020; Wu, Shyhtsun/KYR-1843-2024; Yang, Chao-Tung/B-4562-2009
OI Kristiani, Endah/0000-0003-2925-2992; Yang,
   Chao-Tung/0000-0002-9579-4426
TC 17
Z8 0
ZS 0
ZR 0
ZA 0
ZB 0
Z9 21
U1 3
U2 41
SN 0920-8542
EI 1573-0484
DA 2020-11-30
UT WOS:000589461400002
ER

PT J
AU Diannantini, Claudia
   Lo Giudice, Paolo
   Potena, Domenico
   Storti, Emanuele
   Ursino, Domenico
TI An Approach to Extracting Topic-guided Views from the Sources of a Data
   Lake
SO INFORMATION SYSTEMS FRONTIERS
VL 23
IS 1
SI SI
BP 243
EP 262
DI 10.1007/s10796-020-10010-x
EA MAY 2020
DT Article
PD FEB 2021
PY 2021
AB In the last years, data lakes are emerging as an effective and an
   efficient support for information and knowledge extraction from a huge
   amount of highly heterogeneous and quickly changing data sources. Data
   lake management requires the definition of new techniques, very
   different from the ones adopted for data warehouses in the past. In this
   scenario, one of the most challenging issues to address consists in the
   extraction of topic-guided (i.e., thematic) views from the (very
   heterogeneous and often unstructured) sources of a data lake. In this
   paper, we propose a new network-based model to uniformly represent
   structured, semi-structured and unstructured sources of a data lake.
   Then, we present a new approach to, at least partially, "structuring"
   unstructured data. Finally, we define a technique to extract
   topic-guided views from the sources of a data lake, based on similarity
   and other semantic relationships among source metadata.
RI Potena, Domenico/J-8653-2013; Storti, Emanuele/
OI Storti, Emanuele/0000-0001-5966-6921
ZR 0
TC 17
ZS 0
Z8 2
ZA 0
ZB 0
Z9 21
U1 2
U2 25
SN 1387-3326
EI 1572-9419
DA 2020-06-04
UT WOS:000535166500001
ER

PT J
AU Schatz, Bruce R.
TI National Surveys of Population Health: Big Data Analytics for Mobile
   Health Monitors
SO BIG DATA
VL 3
IS 4
SI SI
BP 219
EP 229
DI 10.1089/big.2015.0021
DT Review
PD DEC 1 2015
PY 2015
AB At the core of the healthcare crisis is fundamental lack of actionable
   data. Such data could stratify individuals within populations to predict
   which persons have which outcomes. If baselines existed for all
   variations of all conditions, then managing health could be improved by
   matching the measuring of individuals to their cohort in the population.
   The scale required for complete baselines involves effective National
   Surveys of Population Health (NSPH). Traditionally, these have been
   focused upon acute medicine, measuring people to contain the spread of
   epidemics. In recent decades, the focus has moved to chronic conditions
   as well, which require smaller measures over longer times. NSPH have
   long utilized quality of life questionnaires. Mobile Health Monitors,
   where computing technologies eliminate manual administration, provide
   richer data sets for health measurement. Older technologies of telephone
   interviews will be replaced by newer technologies of smartphone sensors
   to provide deeper individual measures at more frequent timings across
   larger-sized populations. Such continuous data can provide personal
   health records, supporting treatment guidelines specialized for
   population cohorts. Evidence-based medicine will become feasible by
   leveraging hundreds of millions of persons carrying mobile devices
   interacting with Internet-scale services for Big Data Analytics.
ZR 0
TC 17
Z8 0
ZB 7
ZS 0
ZA 0
Z9 21
U1 1
U2 59
SN 2167-6461
EI 2167-647X
DA 2015-12-01
UT WOS:000367993300003
PM 26858915
ER

PT J
AU Pau, Marco
   Kapsalis, Panagiotis
   Pan, Zhiyu
   Korbakis, George
   Pellegrino, Dario
   Monti, Antonello
TI MATRYCS-A Big Data Architecture for Advanced Services in the Building
   Domain
SO ENERGIES
VL 15
IS 7
AR 2568
DI 10.3390/en15072568
DT Article
PD APR 2022
PY 2022
AB The building sector is undergoing a deep transformation to contribute to
   meeting the climate neutrality goals set by policymakers worldwide. This
   process entails the transition towards smart energy-aware buildings that
   have lower consumptions and better efficiency performance.
   Digitalization is a key part of this process. A huge amount of data is
   currently generated by sensors, smart meters and a multitude of other
   devices and data sources, and this trend is expected to exponentially
   increase in the near future. Exploiting these data for different use
   cases spanning multiple application scenarios is of utmost importance to
   capture their full value and build smart and innovative building
   services. In this context, this paper presents a high-level architecture
   for big data management in the building domain which aims to foster data
   sharing, interoperability and the seamless integration of advanced
   services based on data-driven techniques. This work focuses on the
   functional description of the architecture, underlining the requirements
   and specifications to be addressed as well as the design principles to
   be followed. Moreover, a concrete example of the instantiation of such
   an architecture, based on open source software technologies, is
   presented and discussed.
RI Kormpakis, Georgios/; Monti, Antonello/ABF-6760-2021; Pan, Zhiyu/KHV-3195-2024; Pellegrino, Dario/; Pau, Marco/ABE-1750-2020; Kapsalis, Panagiotis/
OI Kormpakis, Georgios/0000-0003-4052-4549; Monti,
   Antonello/0000-0003-1914-9801; Pan, Zhiyu/0000-0003-4949-2782;
   Pellegrino, Dario/0000-0001-8154-8710; Pau, Marco/0000-0002-4681-2317;
   Kapsalis, Panagiotis/0000-0002-5571-820X
Z8 0
ZB 0
ZA 0
ZR 0
TC 20
ZS 1
Z9 20
U1 0
U2 6
EI 1996-1073
DA 2022-04-24
UT WOS:000781280600001
ER

PT J
AU Morales-Botello, Maria Luz
   Gachet, Diego
   de Buenaga, Manuel
   Aparicio, Fernando
   Busto, Maria J.
   Ascanio, Juan Ramon
TI Chronic patient remote monitoring through the application of big data
   and internet of things
SO HEALTH INFORMATICS JOURNAL
VL 27
IS 3
AR 14604582211030956
DI 10.1177/14604582211030956
DT Article
PD JUL 2021
PY 2021
AB Chronic patients could benefit from the technological advances, but the
   clinical approaches for this kind of patients are still limited. This
   paper describes a system for chronic patients monitoring both, in home
   and external environments. For this purpose, we used novel technologies
   as big data, cloud computing and internet of things (IoT). Additionally,
   the system has been validated for three use cases: cardiovascular
   disease (CVD), hypertension (HPN) and chronic obstructive pulmonary
   disease (COPD), which were selected for their incidence in the
   population. This system is innovative within e-health, mainly due to the
   use of a big data architecture based on open-source components, also it
   provides a scalable and distributed environment for storage and
   processing of biomedical sensor data. The proposed system enables the
   incorporation of non-medical data sources in order to improve the
   self-management of chronic diseases and to develop better strategies for
   health interventions for chronic and dependents patients.
RI Gachet Páez, Diego/AGJ-3099-2022; Aparicio, Fernando/M-2822-2014; Buenaga, Manuel/AFI-6155-2022
OI Gachet Páez, Diego/0000-0001-6578-2275; 
ZA 0
ZS 0
ZB 1
ZR 0
TC 18
Z8 0
Z9 20
U1 1
U2 58
SN 1460-4582
EI 1741-2811
DA 2021-09-07
UT WOS:000691403000001
PM 34256646
ER

PT C
AU Begoli, Edmon
   Goethert, Ian
   Knight, Kathryn
BE Chen, Y
   Ludwig, H
   Tu, Y
   Fayyad, U
   Zhu, X
   Hu, X
   Byna, S
   Liu, X
   Zhang, J
   Pan, S
   Papalexakis, V
   Wang, J
   Cuzzocrea, A
   Ordonez, C
TI A Lakehouse Architecture for the Management and Analysis of
   Heterogeneous Data for Biomedical Research and Mega-biobanks
SO 2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
BP 4643
EP 4651
DI 10.1109/BigData52589.2021.9671534
DT Proceedings Paper
PD 2021
PY 2021
AB Data Lakehouse is a new paradigm in data architectures that embodies and
   integrates already established concepts for the systematic management of
   disparate, large-scale data - a data lake for heterogeneous data
   management, use of open standards for high-performance querying, and
   systematic maintenance of the data "freshness". In addition to being a
   new concept, the data lakehouse is also still a conceptual construct.
   Many projects that use the lakehouse require maturing, empirical
   studies, and specific implementations. In this paper, we present our
   implementation of the data lakehouse concept in a biomedical research
   and health data analytics domain, and we discuss the implementation of
   some unique and novel features such as support for specialized access
   controls in support of HIPAA regulation and IRB protocols, and support
   for the FAIR standard.
CT 9th IEEE International Conference on Big Data (IEEE BigData)
CY DEC 15-18, 2021
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc; Ankura; Lyve Cloud, Seagate; NSF
RI Begoli, Edmon/KLY-9238-2024; Knight, Kathryn/AAP-1459-2021
OI Knight, Kathryn/0000-0003-2976-0049
TC 14
ZA 0
ZB 2
Z8 0
ZR 0
ZS 0
Z9 20
U1 2
U2 10
SN 2639-1589
BN 978-1-6654-3902-2
DA 2022-06-29
UT WOS:000800559504110
ER

PT J
AU Munshi, Amr A.
   Alhindi, Ahmad
TI Big Data Platform for Educational Analytics
SO IEEE ACCESS
VL 9
BP 52883
EP 52890
DI 10.1109/ACCESS.2021.3070737
DT Article
PD 2021
PY 2021
AB Huge amounts of educational data are being produced, and a common
   challenge that many educational organizations confront, is finding an
   effective method to harness and analyze this data for continuously
   delivering enhanced education. Nowadays, the educational data is
   evolving and has become large in volume, wide in variety and high in
   velocity. This produced data needs to be handled in an efficient manner
   to extract value and make informed decisions. For that, this paper
   confronts such data as a big data challenge and presents a comprehensive
   platform tailored to perform educational big data analytical
   applications. Further, present an effective environment for non-data
   scientists and people in the educational sector to apply their demanding
   educational big data applications. The implementation stages of the
   educational big data platform on a cloud computing platform and the
   organization of educational data in a data lake architecture are
   highlighted. Furthermore, two analytical applications are performed to
   test the feasibility of the presented platform in discovering knowledge
   that potentially promotes the educational institutions.
RI Munshi, Amr/AHB-7543-2022; Alhindi, Ahmad/U-5347-2019
OI Munshi, Amr/0000-0002-4002-3755; Alhindi, Ahmad/0000-0002-0516-7868
ZA 0
TC 16
Z8 1
ZS 0
ZB 0
ZR 0
Z9 20
U1 4
U2 118
SN 2169-3536
DA 2021-04-23
UT WOS:000639865600001
ER

PT J
AU Akhbar, Farzaneh
   Chang, Victor
   Yao, Yulin
   Mendez Munoz, Victor
TI Outlook on moving of computing services towards the data sources
SO INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
VL 36
IS 4
BP 645
EP 652
DI 10.1016/j.ijinfomgt.2016.03.014
DT Article
PD AUG 2016
PY 2016
AB The internet of things (IoT) is potentially interconnecting
   unprecedented amounts of raw data, opening countless possibilities by
   two main logical layers: become data into information, then turn
   information into knowledge. The former is about filtering the
   significance in the appropriate format, while the latter provides
   emerging categories of the whole domain. This path of the data is a
   bottom-up flow. On the other hand, the path of the process is a top-down
   flow, starting at the strategic level of business and scientific
   institutions. Today, the path of the process treasures a sizeable amount
   of well-known methods, architectures and technologies: the so called Big
   Data. On the top, Big Data analytics aims variable association
   (e-commerce), data mining (predictive behaviour) or clustering
   (marketing segmentation). Digging the Big Data architecture there are a
   myriad of enabling technologies for data taking, storage and management.
   However the strategic aim is to enhance knowledge with the appropriate
   information, which does need of data, but not vice versa. In the way,
   the magnitude of upcoming data from the IoT will disrupt the data
   centres. To cope with the extreme scale is a matter of moving the
   computing services towards the data sources. This paper explores the
   possibilities of providing many of the IoT services which are currently
   hosted in monolithic cloud centres, moving these computing services into
   nano data centres (NaDa). Particularly, data-information processes,
   which usually are performing at sub-problem domains. NaDa distributes
   computing power over the already present machines of the IP provides,
   like gateways or wireless routers to overcome latency, storage cost and
   alleviate transmissions. Large scale questionnaires have been taken for
   300 IT professionals to validate the points of view for IoT adoption.
   Considering IoT is by definition connected to the Internet, NaDa may be
   used to implement the logical low layer architecture of the services.
   Obviously, such distributed NaDa send results on a logical high layer in
   charge of the information-knowledge turn. This layer requires the whole
   picture of the domain to enable those processes of Big Data analytics on
   the top. (C) 2016 Elsevier Ltd. All rights reserved.
RI Chang, Victor/AAC-7582-2019; Méndez Muñoz, Víctor/M-1762-2014
OI Chang, Victor/0000-0002-8012-5852; Méndez Muñoz,
   Víctor/0000-0002-9044-1189
ZA 0
TC 17
ZB 0
ZS 0
ZR 0
Z8 0
Z9 20
U1 0
U2 61
SN 0268-4012
EI 1873-4707
DA 2016-06-15
UT WOS:000376449000015
ER

PT J
AU Fadler, Martin
   Legner, Christine
TI Data ownership revisited: clarifying data accountabilities in times of
   big data and analytics
SO JOURNAL OF BUSINESS ANALYTICS
VL 5
IS 1
SI SI
BP 123
EP 139
DI 10.1080/2573234X.2021.1945961
DT Article
PD JAN 2 2022
PY 2022
AB Today, a myriad of data is generated via connected devices and digital
   applications. In order to benefit from these data, companies have to
   develop their capabilities related to big data and analytics (BDA). A
   critical factor that is often cited concerning the "soft" aspects of BDA
   is data ownership, i.e., clarifying the fundamental rights and
   responsibilities for data. IS research has investigated data ownership
   for operational systems and data warehouses, where the purpose of data
   processing is known. In the BDA context, defining accountabilities for
   data is more challenging because data are stored in data lakes and used
   for previously unknown purposes. Based on four case studies, we identify
   ownership principles and three distinct types: data, data platform, and
   data product ownership. Our research answers fundamental questions about
   how data management changes with BDA and lays the foundation for future
   research on data and analytics governance.
RI Legner, Christine/ABF-8429-2020
OI Legner, Christine/0000-0001-8891-3813
ZA 0
ZS 0
ZB 0
ZR 0
TC 15
Z8 0
Z9 19
U1 3
U2 29
SN 2573-234X
EI 2573-2358
DA 2022-01-02
UT WOS:000923484800007
ER

PT C
AU Costa, Constantinos
   Chatzimilioudis, Georgios
   Zeinalipour-Yazti, Demetrios
   Mokbel, Mohamed F.
GP ACM
TI Towards Real-Time Road Traffiic Analytics using Telco Big Data
SO PROCEEDINGS OF THE ELEVENTH INTERNATIONAL WORKSHOP ON REAL-TIME BUSINESS
   INTELLIGENCE AND ANALYTICS
DI 10.1145/3129292.3129296
DT Proceedings Paper
PD 2017
PY 2017
AB Atelecommunication company (telco) is traditionally only perceived as
   the entity that provides telecommunication services, such as telephony
   and data communication access to users. However, the IP backbone
   infrastructure of such entities spanning densely urban spaces and widely
   rural areas, provides nowadays a unique opportunity to collect immense
   amounts of mobility data that can provide valuable insights for road
   traffic management and avoidance. In this paper we outline the
   components of the Traffic-TBD (Traffic Telco Big Data) architecture,
   which aims to become an innovative road traffic analytic and prediction
   system with the following desiderata: i) provide micro-level traffic
   modeling and prediction that goes beyond the current state provided by
   Internet-based navigation enterprises utilizing crowdsourcing; ii)
   retain the location privacy boundaries of users inside their mobile
   network operator, to avoid the risks of exposing location data to
   third-party mobile applications; and iii) be available with minimal
   costs and using existing infrastructure (i.e., cell towers and TBD data
   streams are readily available inside a telco). Road traffic
   understanding, management and analytics can minimize the number of road
   accidents, optimize fuel and energy consumption, avoid unexpected
   delays, contribute to a macroscopic spatio-temporal understanding of
   traffic in cities but also to "smart" societies through applications in
   city planning, public transportation, logistics and fleet management for
   enterprises, startups and governmental bodies.
CT 11th International Workshop on Real-Time Business Intelligence and
   Analytics (BIRTE)
CY AUG 28, 2017
CL Munich, GERMANY
SP U S Natl Sci Fdn; Google Inc
RI Zeinalipour-Yazti, Demetrios/O-2301-2016; Costa, Constantinos/; Mokbel, Mohamed/AAX-6146-2021
OI Costa, Constantinos/0000-0003-1471-2167; 
ZR 0
Z8 0
TC 17
ZS 0
ZB 0
ZA 0
Z9 19
U1 0
U2 11
BN 978-1-4503-5425-7
DA 2018-12-28
UT WOS:000426583400005
ER

PT J
AU O'Leary, Daniel E. E.
TI Digitization, digitalization, and digital transformation in accounting,
   electronic commerce, and supply chains
SO INTELLIGENT SYSTEMS IN ACCOUNTING FINANCE & MANAGEMENT
VL 30
IS 2
BP 101
EP 110
DI 10.1002/isaf.1524
EA DEC 2022
DT Editorial Material
PD APR 2023
PY 2023
AB This paper provides some basic definitions associated with digital
   transformation in organizations and applies those definitions to
   accounting, electronic commerce, and supply chains. I also drill down on
   the dimensions associated with digital transformation, including digital
   everywhere, integration (across applications and with customers and
   partners), and the need to reengineer processes. I examine several
   examples of processes ranging from digitization to digital
   transformation. I also examine the role of people in digitally
   transformed organizations and some technologies that are important to
   continued evolution of digitally transformed organizations. Further, we
   explore a number of scenarios of digital transformation. Finally, these
   investigations result in the determination of a number of emerging
   research issues.
RI O'Leary, Daniel Edmund/B-6469-2008
OI O'Leary, Daniel Edmund/0000-0002-5240-9516
ZB 0
Z8 0
ZS 0
TC 14
ZR 0
ZA 0
Z9 18
U1 16
U2 109
SN 1055-615X
EI 1099-1174
DA 2023-01-01
UT WOS:000899324100001
ER

PT J
AU Hinojosa-Palafox, Eduardo A.
   Rodriguez-Elias, Oscar M.
   Hoyo-Montano, Jose A.
   Pacheco-Ramirez, Jesus H.
   Nieto-Jalil, Jose M.
TI An Analytics Environment Architecture for Industrial Cyber-Physical
   Systems Big Data Solutions
SO SENSORS
VL 21
IS 13
AR 4282
DI 10.3390/s21134282
DT Article
PD JUL 2021
PY 2021
AB The architecture design of industrial data analytics system addresses
   industrial process challenges and the design phase of the industrial Big
   Data management drivers that consider the novel paradigm in integrating
   Big Data technologies into industrial cyber-physical systems (iCPS). The
   goal of this paper is to support the design of analytics Big Data
   solutions for iCPS for the modeling of data elements, predictive
   analysis, inference of the key performance indicators, and real-time
   analytics, through the proposal of an architecture that will support the
   integration from IIoT environment, communications, and the cloud in the
   iCPS. An attribute driven design (ADD) approach has been adopted for
   architectural design gathering requirements from smart production
   planning, manufacturing process monitoring, and active preventive
   maintenance, repair, and overhaul (MRO) scenarios. Data management
   drivers presented consider new Big Data modeling analytics techniques
   that show data is an invaluable asset in iCPS. An architectural design
   reference for a Big Data analytics architecture is proposed. The
   before-mentioned architecture supports the Industrial Internet of Things
   (IIoT) environment, communications, and the cloud in the iCPS context. A
   fault diagnosis case study illustrates how the reference architecture is
   applied to meet the functional and quality requirements for Big Data
   analytics in iCPS.
RI Hinojosa Palafox, Eduardo Antonio/; Hoyo-Montano, Jose/N-8619-2019; Roiguez-Elias, Oscar Mario/A-7496-2008; Pacheco, Jesus/
OI Hinojosa Palafox, Eduardo Antonio/0000-0002-4881-9221; Hoyo-Montano,
   Jose/0000-0002-3669-3895; Roiguez-Elias, Oscar
   Mario/0000-0002-3213-7808; Pacheco, Jesus/0000-0002-8636-5902
Z8 0
ZB 0
ZA 0
ZR 0
TC 16
ZS 0
Z9 18
U1 2
U2 60
EI 1424-8220
DA 2021-07-19
UT WOS:000670932500001
PM 34201541
ER

PT J
AU Salas, Daniel
   Liang, Xu
   Navarro, Miguel
   Liang, Yao
   Luna, Daniel
TI An open-data open-model framework for hydrological models integration,
   evaluation and application
SO ENVIRONMENTAL MODELLING & SOFTWARE
VL 126
AR 104622
DI 10.1016/j.envsoft.2020.104622
DT Article
PD APR 2020
PY 2020
AB To tackle fundamental scientific questions regarding health, resilience
   and sustainability of water resources which encompass multiple
   disciplines, researchers need to be able to easily access diverse data
   sources and to also effectively incorporate these data into
   heterogeneous models. To address these cyberinfrastructure challenges, a
   new sustainable and easy-to-use Open Data and Open Modeling framework
   called Meta-Scientific-Modeling (MSM) is developed. MSM addresses the
   challenges of accessing heterogeneous data sources via the Open Data
   architecture which facilitates integration of various external data
   sources. Data Agents are used to handle remote data access protocols,
   metadata standards, and source-specific implementations. The Open
   Modeling architecture allows different models to be easily integrated
   into MSM via Model Agents, enabling direct heterogeneous model coupling.
   MSM adopts a graphical scientific workflow system (VisTrails) and does
   not require re-compiling or adding interface codes for any diverse model
   integration. A study case is presented to illustrate the merit of MSM.
RI Liang, Xu/KOD-4016-2024; Luna, Daniel/PJB-9424-2026; Liang, Yao/; Luna, Daniel/
OI Liang, Xu/0000-0001-7397-2490; Liang, Yao/0000-0002-7353-7242; Luna,
   Daniel/0000-0002-2866-9668
ZS 0
ZR 0
Z8 0
ZB 1
ZA 0
TC 16
Z9 18
U1 0
U2 14
SN 1364-8152
EI 1873-6726
DA 2020-04-16
UT WOS:000522639600001
ER

PT J
AU Zelenkauskaite, Asta
TI Remediation, convergence, and big data: Conceptual limits of
   cross-platform social media
SO CONVERGENCE-THE INTERNATIONAL JOURNAL OF RESEARCH INTO NEW MEDIA
   TECHNOLOGIES
VL 23
IS 5
BP 512
EP 527
DI 10.1177/1354856516631519
DT Article
PD OCT 2017
PY 2017
AB The era of multiplatform media and big data provide new opportunities to
   reconsider data access by media companies. Outlined here is the
   discussion surrounding data access from media institutional logic and
   user-centric perspectives in the contexts of digitalization and big
   data. The discussion includes technological affordances that can be
   geared toward users or that merely reinforce media companies'
   prominence. However, limitations of information architecture lie in its
   structure and the inability to facilitate navigation by users across
   multiple content streams. Media companies concentrate access around
   their own cross-platform content. Despite technological feasibility,
   media companies continue to choose cross-platform architecture that is
   structurally limiting to users. Cross-platform conceptual limits are
   discussed within the context of the broader socioeconomic landscape of
   mass media digitalization and big data.
OI Zelenkauskaite, Asta/0000-0001-5762-4605
ZS 1
Z8 0
ZB 0
ZR 0
TC 14
ZA 1
Z9 18
U1 2
U2 48
SN 1354-8565
EI 1748-7382
DA 2017-10-01
UT WOS:000418513000004
ER

PT C
AU Rao, A. Ravishankar
   Clarke, Daniel
GP IEEE
TI A fully integrated open-source toolkit for mining healthcare big-data:
   architecture and applications
SO 2016 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI)
BP 255
EP 261
DI 10.1109/ICHI.2016.35
DT Proceedings Paper
PD 2016
PY 2016
AB We create an analytics toolkit based on open-source modules that
   facilitate the exploration of healthcare-related datasets. We illustrate
   our framework by providing a detailed analysis of physician and hospital
   ratings data. Our technique should prove valuable to software
   developers, big-data architects, hospital administrators, policy makers
   and patients.
   As an illustration of the capabilities of our toolkit, we examine a
   controversial issue in the medical field regarding the relationship
   between seniority of medical professionals and clinical outcomes. We use
   a publicly available dataset of national hospital ratings in the USA to
   suggest that there is no significant association between experience of
   medical professionals and hospital ratings as defined by the US
   government.
CT IEEE International Conference on Healthcare Informatics (ICHI)
CY OCT 04-07, 2016
CL Chicago, IL
SP IEEE; GE Digital; IEEE Comp Soc; NSF; Computers; Univ Illinois
OI Clarke, Daniel/0000-0003-3471-7416
ZA 0
TC 13
ZR 0
Z8 0
ZB 1
ZS 0
Z9 18
U1 0
U2 7
BN 978-1-5090-6117-4
DA 2016-01-01
UT WOS:000391422100034
ER

PT C
AU Diaz-Aviles, Ernesto
   Pinelli, Fabio
   Lynch, Karol
   Nabi, Zubair
   Gkoufas, Yiannis
   Bouillet, Eric
   Calabrese, Francesco
   Coughlan, Eoin
   Holland, Peter
   Salzwedel, Jason
BE Ho, H
   Ooi, BC
   Zaki, MJ
   Hu, XH
   Haas, L
   Kumar, V
   Rachuri, S
   Yu, SP
   Hsiao, MHI
   Li, J
   Luo, F
   Pyne, S
   Ogan, K
TI Towards Real-time Customer Experience Prediction for Telecommunication
   Operators
SO PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIG DATA
BP 1063
EP 1072
DT Proceedings Paper
PD 2015
PY 2015
AB Telecommunications operators (telcos) traditional sources of income,
   voice and SMS, are shrinking due to customers using over-the-top (OTT)
   applications such as WhatsApp or Viber. In this challenging environment
   it is critical for telcos to maintain or grow their market share, by
   providing users with as good an experience as possible on their network.
   But the task of extracting customer insights from the vast amounts of
   data collected by telcos is growing in complexity and scale everey day.
   How can we measure and predict the quality of a user's experience on a
   telco network in real-time? That is the problem that we address in this
   paper. We present an approach to capture, in (near) real-time, the
   mobile customer experience in order to assess which conditions lead the
   user to place a call to a telco's customer care center. To this end, we
   follow a supervised learning approach for prediction and train our
   Restricted Random Forest model using, as a proxy for bad experience, the
   observed customer transactions in the telco data feed before the user
   places a call to a customer care center. We evaluate our approach using
   a rich dataset provided by a major African telecommunication's company
   and a novel big data architecture for both the training and scoring of
   predictive models. Our empirical study shows our solution to be
   effective at predicting user experience by inferring if a customer will
   place a call based on his current context.
   These promising results open new possibilities for improved customer
   service, which will help telcos to reduce churn rates and improve
   customer experience, both factors that directly impact their revenue
   growth.
CT IEEE International Conference on Big Data
CY OCT 29-NOV 01, 2015
CL Santa Clara, CA
SP IEEE; IEEE Comp Soc; Natl Sci Fdn; CCF; HUAWEI; Springer; ELSEVIER;
   CISCO; Intel
RI PINELLI, Fabio/V-3090-2019
OI PINELLI, Fabio/0000-0003-1058-6917
ZB 0
ZR 0
ZA 0
Z8 0
ZS 0
TC 13
Z9 18
U1 0
U2 14
BN 978-1-4799-9925-5
DA 2016-09-08
UT WOS:000380404600129
ER

PT C
AU Rabelo, Thomas
   Lama, Manuel
   Amorim, Ricardo R.
   Vidal, Juan C.
GP IEEE
TI SmartLAK: A Big Data Architecture for Supporting Learning Analytics
   Services
SO FRONTIERS IN EDUCATION CONFERENCE (FIE), 2015
SE Frontiers in Education Conference
BP 781
EP 785
DT Proceedings Paper
PD 2015
PY 2015
AB In this paper, we present a big data software architecture that uses an
   ontology, based on the Experience API specification, to semantically
   represent the data streams generated by the learners when they undertake
   the learning activities of a course, e.g., in a course. These data are
   stored in a RDF database to provide a high performance access so
   learning analytics services can process the large amount of data
   generated in a virtual learning environment. These services provide
   valuable information to teachers and instructors such as predict the
   learner's performance, discover the real learning paths, extract the
   learner's behavior patterns and so on. The proposed architecture has
   been validated in the Educational Technology undergraduate course of the
   Degree in Pedagogy at the Faculty of Education of the University of
   Santiago de Compostela.
CT 45th Annual Frontiers in Education Conference (FIE)
CY OCT 21-24, 2015
CL El Paso, TX
SP IEEE Educ Soc; IEEE Comp Soc; ASEE Educ Res & Methods Div; New Mexico
   State Univ; Univ Texas El Paso; Hewlett Packard; VentureWell; Markkula
   Ctr Appl Eth; IEEE
RI Vidal, Juan C/L-7375-2014; Amorim, Ricardo/M-8120-2019; Lama, Manuel/AAE-6880-2019
OI Vidal, Juan C/0000-0002-8682-6772; Lama, Manuel/0000-0001-7195-6155
ZB 0
ZR 0
Z8 0
TC 16
ZA 0
ZS 1
Z9 18
U1 0
U2 6
SN 0190-5848
BN 978-1-4799-8454-1
DA 2016-04-06
UT WOS:000371705200132
ER

PT J
AU Ataei, Pouya
   Staegemann, Daniel
TI Application of microservices patterns to big data systems
SO JOURNAL OF BIG DATA
VL 10
IS 1
AR 56
DI 10.1186/s40537-023-00733-4
DT Article
PD MAY 4 2023
PY 2023
AB The panorama of data is ever evolving, and big data has emerged to
   become one of the most hyped terms in the industry. Today, users are the
   perpetual producers of data that if gleaned and crunched, have the
   potential to reveal game-changing patterns. This has introduced an
   important shift regarding the role of data in organizations and many
   strive to harness to power of this new material. Howbeit,
   institutionalizing data is not an easy task and requires the absorption
   of a great deal of complexity. According to the literature, it is
   estimated that only 13% of organizations succeeded in delivering on
   their data strategy. Among the root challenges, big data system
   development and data architecture are prominent. To this end, this study
   aims to facilitate data architecture and big data system development by
   applying well-established patterns of microservices architecture to big
   data systems. This objective is achieved by two systematic literature
   reviews, and infusion of results through thematic synthesis. The result
   of this work is a series of theories that explicates how microservices
   patterns could be useful for big data systems. These theories are then
   validated through expert opinion gathering with 7 experts from the
   industry. The findings emerged from this study indicates that big data
   architectures can benefit from many principles and patterns of
   microservices architecture.
RI Ataei, Pouya/; Staegemann, Daniel/MWO-8533-2025
OI Ataei, Pouya/0000-0002-0993-3574; 
ZR 0
ZA 0
ZB 0
TC 15
ZS 0
Z8 1
Z9 17
U1 2
U2 32
EI 2196-1115
DA 2023-05-24
UT WOS:000980542100002
ER

PT C
AU Olawoyin, Anifat M.
   Leung, Carson K.
   Cuzzocrea, Alfredo
BE Chen, Y
   Ludwig, H
   Tu, Y
   Fayyad, U
   Zhu, X
   Hu, X
   Byna, S
   Liu, X
   Zhang, J
   Pan, S
   Papalexakis, V
   Wang, J
   Cuzzocrea, A
   Ordonez, C
TI Open Data Lake to Support Machine Learning on Arctic Big Data
SO 2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
BP 5215
EP 5224
DI 10.1109/BigData52589.2021.9671453
DT Proceedings Paper
PD 2021
PY 2021
AB The era of big data is evolving with the introduction of the data lake
   concept. While a data warehouse provides a well-structured model to
   manage big data, a data lake accepts data of any types and formats with
   or without schema and provides access to the data for diverse
   communities of users. A data lake provides flexible, agile, and scalable
   solution to manage the ever-increasing volume of big data we are
   witnessing in the world today, including many siloed data collected over
   the years by researchers through Arctic expeditions. In this paper, we
   present our conceptual model of a data lake for integrating the diverse
   huge amount of data collected by researchers during Arctic expedition.
   We also design a baseline metadata using a data-driven approach to
   manage the disparately huge structured, semi-structured, and
   unstructured data collected from the Arctic region. The resulting open
   data lake not only effectively manages big Arctic data but also supports
   machine learning on these big data.
CT 9th IEEE International Conference on Big Data (IEEE BigData)
CY DEC 15-18, 2021
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc; Ankura; Lyve Cloud, Seagate; NSF
RI Olawoyin, Anifat/; Leung, Carson/M-8682-2013; Cuzzocrea, Alfredo/B-6374-2015
OI Olawoyin, Anifat/0009-0007-6119-0062; Leung, Carson/0000-0002-7541-9127;
   
Z8 0
ZB 0
ZR 0
ZS 0
TC 14
ZA 0
Z9 17
U1 0
U2 9
SN 2639-1589
BN 978-1-6654-3902-2
DA 2022-06-29
UT WOS:000800559505040
ER

PT J
AU Villegas-Ch, William
   Roman-Canizares, Milton
   Jaramillo-Alcazar, Angel
   Palacios-Pacheco, Xavier
TI Data Analysis as a Tool for the Application of Adaptive Learning in a
   University Environment
SO APPLIED SCIENCES-BASEL
VL 10
IS 20
AR 7016
DI 10.3390/app10207016
DT Article
PD OCT 2020
PY 2020
AB Currently, data are a very valuable resource for organizations. Through
   analysis, it is possible to profile people or obtain knowledge about an
   event or environment and make decisions that help improve their quality
   of life. This concept takes on greater value in the current pandemic,
   due to coronavirus disease 2019 (COVID-19), that affects society. This
   emergency has changed the way people live. As a result, the majority of
   activities are carried out using the internet, virtually or online.
   Education is not far behind and has seen the web as the most successful
   option to continue with its activities. The use of any computer
   application generates a large volume of data that can be analyzed by a
   big data architecture in order to obtain knowledge from its students and
   use it to improve educational processes. The big data, when included as
   a tool for adaptive learning, allow the analysis of a large volume of
   data to offer an educational model based on personalized education. In
   this work, the analysis of educational data through a big data
   architecture is proposed to generate learning based on meeting the needs
   of students.
OI Villegas Chiliquinga, William Eduardo/0000-0002-5421-7710;
   Jaramillo-Alcázar, Angel/0000-0003-4143-2515; ROMAN,
   MILTON/0000-0002-2861-1722
Z8 0
ZS 0
ZA 0
ZB 0
ZR 0
TC 12
Z9 17
U1 0
U2 29
EI 2076-3417
DA 2020-11-19
UT WOS:000586885500001
ER

PT J
AU Deligiannis, Kimon
   Raftopoulou, Paraskevi
   Tryfonopoulos, Christos
   Platis, Nikos
   Vassilakis, Costas
TI Hydria: An Online Data Lake for Multi-Faceted Analytics in the Cultural
   Heritage Domain
SO BIG DATA AND COGNITIVE COMPUTING
VL 4
IS 2
AR 7
DI 10.3390/bdcc4020007
DT Article
PD JUN 2020
PY 2020
AB Advancements in cultural informatics have significantly influenced the
   way we perceive, analyze, communicate and understand culture. New data
   sources, such as social media, digitized cultural content, and Internet
   of Things (IoT) devices, have allowed us to enrich and customize the
   cultural experience, but at the same time have created an avalanche of
   new data that needs to be stored and appropriately managed in order to
   be of value. Although data management plays a central role in driving
   forward the cultural heritage domain, the solutions applied so far are
   fragmented, physically distributed, require specialized IT knowledge to
   deploy, and entail significant IT experience to operate even for trivial
   tasks. In this work, we present Hydria, an online data lake that allows
   users without any IT background to harvest, store, organize, analyze and
   share heterogeneous, multi-faceted cultural heritage data. Hydria
   provides a zero-administration, zero-cost, integrated framework that
   enables researchers, museum curators and other stakeholders within the
   cultural heritage domain to easily (i) deploy data acquisition services
   (like social media scrapers, focused web crawlers, dataset imports,
   questionnaire forms), (ii) design and manage versatile customizable data
   stores, (iii) share whole datasets or horizontal/vertical data shards
   with other stakeholders, (iv) search, filter and analyze data via an
   expressive yet simple-to-use graphical query engine and visualization
   tools, and (v) perform user management and access control operations on
   the stored data. To the best of our knowledge, this is the first
   solution in the literature that focuses on collecting, managing,
   analyzing, and sharing diverse, multi-faceted data in the cultural
   heritage domain and targets users without an IT background.
RI Tryfonopoulos, Christos/HKW-5651-2023; Vassilakis, Costas/AAH-5948-2019; Raftopoulou, Paraskevi/HKO-5623-2023
OI Tryfonopoulos, Christos/0000-0003-0640-9088; Vassilakis,
   Costas/0000-0001-9940-1821; Raftopoulou, Paraskevi/0000-0003-0663-4322
ZR 0
ZS 0
ZA 0
TC 14
Z8 0
ZB 0
Z9 17
U1 0
U2 13
EI 2504-2289
DA 2020-06-01
UT WOS:000697676000004
ER

PT J
AU Ruiz, M.
   German, M.
   Contreras, L. M.
   Velasco, L.
TI Big Data-backed video distribution in the telecom cloud
SO COMPUTER COMMUNICATIONS
VL 84
BP 1
EP 11
DI 10.1016/j.comcom.2016.03.026
DT Article
PD JUN 15 2016
PY 2016
AB Telecom operators are starting the deployment of Content Delivery
   Networks (CDN) to better control and manage video contents injected into
   the network. Cache nodes placed close to end users can manage contents
   and adapt them to users' devices, while reducing video traffic in the
   core. By adopting the standardized MPEG-DASH technique, video contents
   can be delivered over HTTP. Thus, HTTP servers can be used to serve
   contents, while packagers running as software can prepare live contents.
   This paves the way for virtualizing the CDN function. In this paper, a
   CDN manager is proposed to adapt the virtualized CDN function to current
   and future demand. A Big Data architecture, fulfilling the ETSI NFV
   guide lines, allows controlling virtualized components while collecting
   and pre-processing data. Optimization problems minimize CDN costs while
   ensuring the highest quality. Re-optimization is triggered based on
   threshold violations; data stream mining sketches transform collected
   into modeled data and statistical linear regression and machine learning
   techniques are proposed to produce estimation of future scenarios.
   Exhaustive simulation over a realistic scenario reveals remarkable costs
   reduction by dynamically reconfiguring the CDN. (C) 2016 Elsevier B.V.
   All rights reserved.
RI Velasco, Luis/L-2335-2014; Ruiz, Marc/K-1780-2014
ZA 0
Z8 0
ZB 0
TC 16
ZR 0
ZS 0
Z9 17
U1 0
U2 32
SN 0140-3664
EI 1873-703X
DA 2016-06-15
UT WOS:000376552400001
ER

PT C
AU Demchenko, Yuri
   Gruengard, Emanuel
   Klous, Sander
GP IEEE
TI Instructional Model for Building effective Big Data Curricula for Online
   and Campus Education
SO 2014 IEEE 6TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING TECHNOLOGY AND
   SCIENCE (CLOUDCOM)
SE International Conference on Cloud Computing Technology and Science
BP 935
EP 941
DI 10.1109/CloudCom.2014.162
DT Proceedings Paper
PD 2014
PY 2014
AB This paper presents current results and ongoing work to develop
   effective educational courses on the Big Data (BD) and Data Intensive
   Science and Technologies (DIST) that is been done at the University of
   Amsterdam in cooperation with KPMG and by the Laureate Online Education
   (online partner of the University of Liverpool). The paper introduces
   the main Big Data concepts: multicomponent Big Data definition and Big
   Data Architecture Framework that provide the basis for defining the
   course structure and Common Body of Knowledge for Data Science and Big
   Data technology domains. The paper presents details on approach,
   learning model, and course content for two courses at the Laureate
   Online Education/University of Liverpool and at the University of
   Amsterdam. The paper also provides background information about existing
   initiatives and activities related to information exchange and
   coordination on developing educational materials and programs on Big
   Data, Data Science, and Research Data Management.
CT 6th IEEE International Conference on Cloud Computing Technology and
   Science (CloudCom)
CY DEC 15-18, 2014
CL Singapore, SINGAPORE
SP IEEE; IEEE Comp Soc; Nanyang Technol Univ; A STAR Inst High Performance
   Comp; STC Cloud Comp; Cloud Comp Assoc; IEEE Cloud Comp; TCBIS; hp;
   FUJITSU; IEEE Cloud Comp Initiat; IEEE Comp Soc Cloud Comp Special Tech
   Comm; EEE Comp Soc Tech Comm Scalable Comp; IEEE Comp Soc Tech Comm
   Business Informat & Syst
OI Demchenko, Yuri/0000-0001-7474-9506
ZA 0
TC 16
ZR 0
ZB 0
Z8 0
ZS 0
Z9 17
U1 1
U2 19
SN 2330-2194
BN 978-1-4799-4093-6
DA 2014-01-01
UT WOS:000392947000147
ER

PT J
AU Nadal, Sergi
   Jovanovic, Petar
   Bilalli, Besim
   Romero, Oscar
TI Operationalizing and automating Data Governance
SO JOURNAL OF BIG DATA
VL 9
IS 1
AR 117
DI 10.1186/s40537-022-00673-5
DT Article
PD DEC 10 2022
PY 2022
AB The ability to cross data from multiple sources represents a competitive
   advantage for organizations. Yet, the governance of the data lifecycle,
   from the data sources into valuable insights, is largely performed in an
   ad-hoc or manual manner. This is specifically concerning in scenarios
   where tens or hundreds of continuously evolving data sources produce
   semi-structured data. To overcome this challenge, we develop a framework
   for operationalizing and automating data governance. For the first, we
   propose a zoned data lake architecture and a set of data governance
   processes that allow the systematic ingestion, transformation and
   integration of data from heterogeneous sources, in order to make them
   readily available for business users. For the second, we propose a set
   of metadata artifacts that allow the automatic execution of data
   governance processes, addressing a wide range of data management
   challenges. We showcase the usefulness of the proposed approach using a
   real world use case, stemming from the collaborative project with the
   World Health Organization for the management and analysis of data about
   Neglected Tropical Diseases. Overall, this work contributes on
   facilitating organizations the adoption of data-driven strategies into a
   cohesive framework operationalizing and automating data governance.
RI Romero, Oscar/D-5504-2012; Nadal, Sergi/AAB-7223-2021; Bilalli, Besim/AAC-4818-2022
OI Romero, Oscar/0000-0001-6350-8328; Nadal, Sergi/0000-0002-8565-952X;
   Bilalli, Besim/0000-0002-0575-2389
ZS 0
TC 13
ZR 0
ZB 0
Z8 0
ZA 0
Z9 16
U1 12
U2 107
EI 2196-1115
DA 2022-12-29
UT WOS:000897485100001
PM 36532842
ER

PT J
AU Francia, Matteo
   Gallinucci, Enrico
   Golfarelli, Matteo
   Leoni, Anna Giulia
   Rizzi, Stefano
   Santolini, Nicola
TI Making data platforms smarter with MOSES
SO FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
VL 125
BP 299
EP 313
DI 10.1016/j.future.2021.06.031
EA JUL 2021
DT Article
PD DEC 2021
PY 2021
AB The rise of data platforms has enabled the collection and processing of
   huge volumes of data, but has opened to the risk of losing their
   control. Collecting proper metadata about raw data and transformations
   can significantly reduce this risk. In this paper we propose MOSES, a
   technology-agnostic, extensible, and customizable framework for metadata
   handling in big data platforms. The framework hinges on a metadata
   repository that stores information about the objects in the big data
   platform and the processes that transform them. MOSES provides a wide
   range of functionalities to different types of users of the platform.
   Differently from previous high-level proposals, MOSES is fully
   implemented and it was not conceived for a specific technology. Besides
   discussing the rationale and the features of MOSES, in this paper we
   describe its implementation and we test it on a real case study. The
   ultimate goal is to take a significant step forward towards proving that
   metadata handling in big data platforms is feasible and beneficial. (C)
   2021 Elsevier B.V. All rights reserved.
RI Rizzi, Stefano/IUO-7212-2023; GALLINUCCI, ENRICO/AAU-2553-2021; Golfarelli, Matteo/; FRANCIA, MATTEO/GRX-7073-2022
OI Rizzi, Stefano/0000-0002-4617-217X; Golfarelli,
   Matteo/0000-0002-0437-0725; FRANCIA, MATTEO/0000-0002-0805-1051
TC 14
ZS 0
ZA 0
ZR 0
Z8 0
ZB 1
Z9 16
U1 1
U2 8
SN 0167-739X
EI 1872-7115
DA 2021-09-01
UT WOS:000687981400003
ER

PT C
AU Zagan, Elisabeta
   Danubianu, Mirela
GP IEEE
TI Data Lake Approaches: A Survey
SO 2020 15TH INTERNATIONAL CONFERENCE ON DEVELOPMENT AND APPLICATION
   SYSTEMS (DAS)
BP 189
EP 193
DI 10.1109/das49615.2020.9108912
DT Proceedings Paper
PD 2020
PY 2020
AB The explosion of new data: social media, commercial, industrial, health,
   school, etc. appeared in recent years, has led to the emergence and
   development of new technologies and techniques of data management. The
   old technologies of data storage and data processing are beginning to be
   overwhelmed by the large volume of data and their variety. Data Lake is
   one of the latest technologies that seem to be in the spotlight in the
   last period. In this article, we analyze some of the recent approaches
   and architectures using Data Lake, approaches that have tried to cover
   several shortcomings encountered with the advent of these new
   technologies.
CT 15th International Conference on Development and Application Systems
   (DAS)
CY MAY 21-23, 2020
CL Suceava, ROMANIA
SP Stefan Cel Mare Univ Suceava, Fac Elect Engn & Comp Sci; IEEE Ind
   Applicat Soc
RI Danubianu, Mirela/O-3620-2014; Zagan, Elisabeta/NGQ-7564-2025
OI Danubianu, Mirela/0000-0002-5470-1406; 
ZB 0
Z8 1
ZA 0
ZS 0
TC 11
ZR 0
Z9 16
U1 0
U2 9
BN 978-1-7281-6870-8
DA 2020-12-16
UT WOS:000589776100035
ER

PT C
AU Li, Xiaoquan
   Zhang, Fujiang
   Wang, Yongliang
GP IEEE
TI Research on Big Data Architecture, Key Technologies and its Measures
SO 2013 IEEE 11TH INTERNATIONAL CONFERENCE ON DEPENDABLE, AUTONOMIC AND
   SECURE COMPUTING (DASC)
BP 1
EP 4
DI 10.1109/DASC.2013.28
DT Proceedings Paper
PD 2013
PY 2013
AB Big data require exceptional technologies to efficiently process large
   quantities of data within tolerable elapsed times, such as capture,
   curation, storage, search, sharing, transfer, analysis and
   visualization. Concept, features, construction importance, architecture,
   run mode, and its key technologies of big data are analyzed in this
   paper. Information sharing and data security under big data constructin
   are studied, at last, four measures for building big data are
   putforward, which can provide good decision-making for big data
   construction.
CT 11th IEEE International Conference on Dependable, Autonomic and Secure
   Computing (DASC)
CY DEC 21-22, 2013
CL Chengdu, PEOPLES R CHINA
SP IEEE; NSFC; IEEE Comp Soc; Unive Elect Sci & Technol China; StFX Univ;
   Ubiquitous Media Communicat Lab; IEEE Tech Comm Scalable Comp
ZB 5
ZR 0
ZS 0
ZA 0
TC 15
Z8 0
Z9 16
U1 0
U2 9
BN 978-1-4799-3381-5
DA 2013-01-01
UT WOS:000360991500001
ER

PT J
AU Hoseini, Sayed
   Theissen-Lipp, Johannes
   Quix, Christoph
TI A survey on semantic data management as intersection of ontology-based
   data access, semantic modeling and data lakes
SO JOURNAL OF WEB SEMANTICS
VL 81
AR 100819
DI 10.1016/j.websem.2024.100819
EA APR 2024
DT Article
PD JUL 2024
PY 2024
AB In recent years, data lakes emerged as a way to manage large amounts of
   heterogeneous data for modern data analytics. One way to prevent data
   lakes from turning into inoperable data swamps is semantic data
   management. Such approaches propose the linkage of metadata to knowledge
   graphs based on the Linked Data principles to provide more meaning and
   semantics to the data in the lake. Such a semantic layer may be utilized
   not only for data management but also to tackle the problem of data
   integration from heterogeneous sources, in order to make data access
   more expressive and interoperable. In this survey, we review recent
   approaches with a specific focus on the application within data lake
   systems and scalability to Big Data. We classify the approaches into (i)
   basic semantic data management, (ii) semantic modeling approaches for
   enriching metadata in data lakes, and (iii) methods for ontology -based
   data access. In each category, we cover the main techniques and their
   background, and compare latest research. Finally, we point out
   challenges for future work in this research area, which needs a closer
   integration of Big Data and Semantic Web technologies.
RI Theissen-Lipp, Johannes/AAZ-4722-2020; Quix, Christoph/; Hoseini, Sayed/
OI Theissen-Lipp, Johannes/0000-0002-2639-1949; Quix,
   Christoph/0000-0002-1698-4345; Hoseini, Sayed/0000-0002-4489-9025
Z8 0
ZR 0
ZS 0
ZB 0
ZA 0
TC 14
Z9 14
U1 6
U2 12
SN 1570-8268
EI 1873-7749
DA 2024-06-07
UT WOS:001237052300001
ER

PT J
AU Ruiz, M. Dolores
   Gomez-Romero, Juan
   Fernandez-Basso, Carlos
   Martin-Bautista, Maria J.
TI Big Data Architecture for Building Energy Management Systems
SO IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS
VL 18
IS 9
BP 5738
EP 5747
DI 10.1109/TII.2021.3130052
DT Article
PD SEP 2022
PY 2022
AB The enormous quantity of data handled by building management systems are
   key to develop more efficient energy operational systems. However, the
   inability of current systems to take benefit from the generated data may
   waste good opportunities of improving building performance. Big Data
   appears as a suitable framework to sustain the management system and
   conduct future prospective analysis. In this article, we present a Big
   Data-based architecture for the efficient management of buildings. The
   different Big Data components are involved not only in the data
   acquisition phase, but also in the implementation of algorithms capable
   of analyzing massive data collected from very heterogeneous sources.
   They also enable fast computations that can help the generation of
   optimal operational plan generations to improve the building
   functioning. The proposed architecture has been effectively introduced
   in four different-purpose buildings, demonstrating that Big Data can
   help during the energy cycle of the building.
RI Ruiz, M.Dolores/P-4357-2019; Martin-Bautista, Maria J/H-7754-2015; Ruiz Jiménez, María Dolores/I-4734-2012; Gómez Romero, Juan/F-7550-2011; Fernandez-Basso, Carlos/C-7599-2017
OI Martin-Bautista, Maria J/0000-0002-6973-477X; Ruiz Jiménez, María
   Dolores/0000-0003-1077-3173; Gómez Romero, Juan/0000-0003-0439-3692;
   Fernandez-Basso, Carlos/0000-0002-8809-8676
TC 11
ZS 1
Z8 0
ZB 1
ZA 0
ZR 0
Z9 14
U1 4
U2 62
SN 1551-3203
EI 1941-0050
DA 2022-06-24
UT WOS:000811603400006
ER

PT J
AU Caceres, Laura
   Ignacio Merino, Jose
   Diaz-Diaz, Norberto
TI A Computational Intelligence Approach to Predict Energy Demand Using
   Random Forest in a Cloudera Cluster
SO APPLIED SCIENCES-BASEL
VL 11
IS 18
AR 8635
DI 10.3390/app11188635
DT Article
PD SEP 2021
PY 2021
AB Society's energy consumption has shot up in recent years, making the
   prediction of its demand a current challenge to ensure an efficient and
   responsible use. Artificial intelligence techniques have proven to be
   potential tools in handling tedious tasks and making sense of
   large-scale data to make better business decisions in different areas of
   knowledge. In this article, the use of random forests algorithms in a
   Big Data environment is proposed for household energy demand
   forecasting. The predictions are based on the use of information from
   different sources, confirming a fundamental role of socioeconomic data
   in consumer's behaviours. On the other hand, the use of Big Data
   architectures is proposed to perform horizontal and vertical scaling of
   the solution to be used in real environments. Finally, a tool for
   high-resolution predictions with great efficiency is introduced, which
   enables energy management in a very accurate way.
RI Diaz-Diaz, Norberto/G-3309-2013; Cáceres, Laura/; , José Ignacio/
OI Diaz-Diaz, Norberto/0000-0001-8100-8781; Cáceres,
   Laura/0009-0006-6677-6161; , José Ignacio/0000-0001-7822-9815
ZB 0
ZR 0
TC 13
ZS 1
ZA 0
Z8 0
Z9 14
U1 0
U2 8
EI 2076-3417
DA 2021-10-02
UT WOS:000699449100001
ER

PT J
AU Semlali, Badr-Eddine Boudriki
   El Amrani, Chaker
   Ortiz, Guadalupe
TI Hadoop Paradigm for Satellite Environmental Big Data Processing
SO INTERNATIONAL JOURNAL OF AGRICULTURAL AND ENVIRONMENTAL INFORMATION
   SYSTEMS
VL 11
IS 1
BP 23
EP 47
DI 10.4018/IJAEIS.2020010102
DT Article
PD JAN-MAR 2020
PY 2020
AB The important growth of industrial, transport, and agriculture
   activities, has not led only to the air quality and climate changes
   issues, but also to the increase of the potential natural disasters. The
   emission of harmful gases, particularly: the Vertical Column Density
   (VCD) of CO, SO2 and NOx, is one of the major factors causing the
   aforementioned environmental problems. Our research aims to contribute
   finding solution to this hazardous phenomenon, by using remote sensing
   (RS) techniques to monitor air quality which may help decision makers.
   However, RS data is not easy to manage, because of their huge amount,
   high complexity, variety, and velocity, Thus, our manuscript explains
   the different aspects of the used satellite data. Furthermore, this
   article has proven that RS data could be regarded as big data.
   Accordingly, we have adopted the Hadoop big data architecture and
   explained how to process efficiently RS environmental data.
RI Bouiki Semlali, Badr-Eddine/A-7390-2019; Ortiz, Guadalupe/K-8601-2014
OI Bouiki Semlali, Badr-Eddine/0000-0003-0671-4808; Ortiz,
   Guadalupe/0000-0002-5121-6341
ZA 0
TC 13
ZR 0
ZS 0
Z8 0
ZB 0
Z9 14
U1 0
U2 9
SN 1947-3192
EI 1947-3206
DA 2020-02-04
UT WOS:000508359100002
ER

PT J
AU Tardio, Roberto
   Mate, Alejandro
   Trujillo, Juan
TI An Iterative Methodology for Defining Big Data Analytics Architectures
SO IEEE ACCESS
VL 8
BP 210597
EP 210616
DI 10.1109/ACCESS.2020.3039455
DT Article
PD 2020
PY 2020
AB Thanks to the advances achieved in the last decade, the lack of adequate
   technologies to deal with Big Data characteristics such as Data Volume
   is no longer an issue. Instead, recent studies highlight that one of the
   main Big Data issues is the lack of expertise to select adequate
   technologies and build the correct Big Data architecture for the problem
   at hand. In order to tackle this problem, we present our methodology for
   the generation of Big Data pipelines based on several requirements
   derived from Big Data features that are critical for the selection of
   the most appropriate tools and techniques. Thus, thanks to our approach
   we reduce the required know-how to select and build Big Data
   architectures by providing a step-by-step methodology that leads Big
   Data architects into creating their Big Data Pipelines for the case at
   hand. Our methodology has been tested in two use cases.
RI Trujillo, Juan/L-7079-2014; Tardío, Roberto/ABD-6563-2020; Maté, Alejano/H-5963-2015
OI Tardío, Roberto/0000-0002-7913-1534; Maté, Alejano/0000-0001-7770-3693
ZR 0
Z8 0
ZS 0
ZA 0
TC 12
ZB 0
Z9 14
U1 3
U2 159
SN 2169-3536
DA 2020-12-18
UT WOS:000596372800001
ER

PT C
AU Gupta, Maanak
   Patwa, Farhan
   Benson, James
   Sandhu, Ravi
GP ACM
TI Multi-Layer Authorization Framework for a Representative Hadoop
   Ecosystem Deployment
SO PROCEEDINGS OF THE 22ND ACM SYMPOSIUM ON ACCESS CONTROL MODELS AND
   TECHNOLOGIES (SACMAT'17)
BP 183
EP 190
DI 10.1145/3078861.3084173
DT Proceedings Paper
PD 2017
PY 2017
AB Apache Hadoop is a predominant software framework to store and process
   vast amount of data, produced in varied formats. Data stored in Hadoop
   multi-tenant data lake often includes sensitive data such as social
   security numbers, intelligence sources and medical particulars, which
   should only be accessed by legitimate users. Apache Ranger and Apache
   Sentry are important authorization systems providing fine-grained access
   control across several Hadoop ecosystem services. In this paper, we
   provide a comprehensive explanation for the authorization framework
   offered by Hadoop ecosystem, incorporating core Hadoop 2.x native access
   control features and capabilities offered by Apache Ranger, with prime
   focus on data services including Apache Hive and Hadoop 2.x core
   services. A multi-layer authorization system is discussed and
   demonstrated, reflecting access control for services, data, applications
   and infrastructure resources inside a representative Hadoop ecosystem
   instance. A concrete use case is discussed to underline the application
   of aforementioned access control points. We use Hortonworks Hadoop
   distribution HDP 2.5 to exhibit this multi-layer access control
   framework.
CT 22nd ACM Symposium on Access Control Models and Technologies (SACMAT)
CY JUN 21-23, 2017
CL Indianapolis, IN
SP Assoc Comp Machinery; ACM SIGSAC
RI Gupta, Maanak/ABD-4037-2020; Benson, James/
OI Gupta, Maanak/0000-0001-9189-2478; Benson, James/0000-0001-7209-2344
ZA 0
ZR 0
TC 10
Z8 1
ZS 0
ZB 1
Z9 14
U1 0
U2 3
BN 978-1-4503-4702-0
DA 2017-01-01
UT WOS:000614039400021
ER

PT C
AU Lytra, Ioanna
   Vidal, Maria-Esther
   Orlandi, Fabrizio
   Attard, Judie
BE JardimGoncalves, R
   Mendonca, JP
   Pallot, M
   Zarli, A
   Martins, J
   Marques, M
TI A Big Data Architecture for Managing Oceans of Data and Maritime
   Applications
SO 2017 INTERNATIONAL CONFERENCE ON ENGINEERING, TECHNOLOGY AND INNOVATION
   (ICE/ITMC)
SE International ICE Conference on Engineering Technology and Innovation
BP 1216
EP 1226
DT Proceedings Paper
PD 2017
PY 2017
AB Data in the maritime domain is growing at an unprecedented rate, e.g.,
   terabytes of oceanographic data are collected every month, and petabytes
   of data are already publicly available. Big data from heterogeneous
   sources such as sensors, buoys, vessels, and satellites could
   potentially fuel a large number of interesting applications for
   environmental protection, security, fault prediction, shipping routes
   optimization, and energy production. However, because of several
   challenges related to big data and the high heterogeneity of the data
   sources, such applications are still underdeveloped and fragmented. In
   this paper, we analyze challenges and requirements related to big
   maritime data applications and propose a scalable data management
   solution. A big data architecture meeting these requirements is
   described, and examples of its implementation in concrete scenarios are
   provided. The related data value chain and use cases in the context of a
   European project, BigDataOcean, are also described.
CT 23rd International Conference on Engineering, Technology and Innovation
   (ICE/ITMC)
CY JUN 27-29, 2017
CL PORTUGAL
SP Univ Nova Lisboa, Fac Ciencias Tecnologia; UNINOVA; GRIS; Univ Minho
RI Orlandi, Fabrizio/; Vidal, Maria-Esther/AAU-8163-2020
OI Orlandi, Fabrizio/0000-0001-9561-4635; Vidal,
   Maria-Esther/0000-0003-1160-8727
ZB 0
ZS 0
Z8 0
ZR 0
TC 14
ZA 0
Z9 14
U1 0
U2 13
SN 2334-315X
BN 978-1-5386-0774-9
DA 2017-01-01
UT WOS:000464318300159
ER

PT C
AU Wibowo, Merlinda
   Sulaiman, Sarina
   Shamsuddin, Siti Mariyam
BA Shi, Y
BE Tan, Y
   Takagi, H
TI Machine Learning in Data Lake for Combining Data Silos
SO DATA MINING AND BIG DATA, DMBD 2017
SE Lecture Notes in Computer Science
VL 10387
BP 294
EP 306
DI 10.1007/978-3-319-61845-6_30
DT Proceedings Paper
PD 2017
PY 2017
AB Data silo can grow to be a large-scale data for years, overlapping and
   has an indefinite quality. It allows an organization to develop their
   own analytical capabilities. Data lake has the ability to solve this
   problem efficiently with the data analysis by using statistical and
   predictive modeling techniques which can be applied to enhance and
   support an organization's business strategy. This study provides an
   overview of the process of decision-making, operational efficiency, and
   creating the solution for an organization. Machine Learning can
   distribute the architecture of data model and integrate the data silo
   with other organizations data to optimize the operational business
   processes within an organization in order to improve data quality and
   efficiency. Testing is done by utilizing the data from the Malaysia's
   and Singapore's Government Open Data on the Air Pollutant Index to
   determine the condition of air pollution levels for the health and
   safety of the population.
CT 2nd International Conference on Data Mining and Big Data (DMBD)
CY JUL 27-AUG 01, 2017
CL Fukuoka, JAPAN
SP Peking Univ, Computat Intelligence Lab; Kyushu Univ, Res Ctr Appl
   Perceptual Sci; IEEE Computat Intelligence Soc; IEEE Syst, Man &
   Cybernet Soc, Japan Chapter
RI Wibowo, Merlinda/AAD-1609-2021; Sulaiman, Sarina/A-1704-2013
Z8 0
TC 8
ZA 0
ZB 0
ZS 0
ZR 0
Z9 14
U1 0
U2 40
SN 0302-9743
EI 1611-3349
BN 978-3-319-61845-6; 978-3-319-61844-9
DA 2018-08-15
UT WOS:000440465200030
ER

PT J
AU Harby, Ahmed A.
   Zulkernine, Farhana
TI Data Lakehouse: A survey and experimental study
SO INFORMATION SYSTEMS
VL 127
AR 102460
DI 10.1016/j.is.2024.102460
EA SEP 2024
DT Article
PD JAN 2025
PY 2025
AB Efficient big data management is a dire necessity to manage the
   exponential growth in data generated by digital information systems to
   produce usable knowledge. Structured databases, data lakes, and
   warehouses have each provided a solution with varying degrees of
   success. However, a new and superior solution, the data Lakehouse, has
   emerged to extract actionable insights from unstructured data ingested
   from distributed sources. By combining the strengths of data warehouses
   and data lakes, the data Lakehouse can process and merge data quickly
   while ingesting and storing high-speed unstructured data with
   post-storage transformation and analytics capabilities. The Lakehouse
   architecture offers the necessary features for optimal functionality and
   has gained significant attention in the big data management research
   community. In this paper, we compare data lake, warehouse, and lakehouse
   systems, highlight their strengths and shortcomings, identify the
   desired features to handle the evolving challenges in big data
   management and analysis and propose an advanced data Lakehouse
   architecture. We also demonstrate the performance of three
   state-of-the-art data management systems namely HDFS data lake, Hive
   data warehouse, and Delta lakehouse in managing data for analytical
   query responses through an experimental study.
OI harby, ahmed/0000-0002-4672-0957
Z8 0
ZS 0
ZR 0
ZB 0
TC 11
ZA 0
Z9 13
U1 15
U2 37
SN 0306-4379
EI 1873-6076
DA 2024-10-13
UT WOS:001329248200001
ER

PT J
AU Ataei, Pouya
   Litchfield, Alan
TI The State of Big Data Reference Architectures: A Systematic Literature
   Review
SO IEEE ACCESS
VL 10
BP 113789
EP 113807
DI 10.1109/ACCESS.2022.3217557
DT Review
PD 2022
PY 2022
AB Big Data (BD) is a nascent term emerged to describe large amount of data
   that comes in different forms from various channels. In modern world,
   users are the ceaseless generators of structured, semi-structured, and
   unstructured data that if gleaned and crunched precisely, will reveal
   game-changing patterns. While the opportunities exist with BD, the
   unprecedented amount of data has brought traditional approaches to a
   bottleneck, and the growth of data is outpacing technological and
   scientific advances in data analytics. It is estimated that
   approximately 75% of the BD projects have failed within the last decade
   according to multiple sources. Among the challenges, system development
   and data architecture are prominent. This paper aims to facilitate BD
   system development and architecture by conducting a systematic
   literature review on BD reference architectures (RA). The primary goal
   is to highlight the state of BD RAs and how they can be helpful for BD
   system development. The secondary goal is to find all BD RAs, describe
   the challenges of creating these RA, discuss the common architectural
   components of these RA and the limitations of these RA. As a result of
   this work, firstly major concepts about RA are discussed and their
   applicability to BD system development is depicted. Secondly, 22 BD
   reference architecture is assessed from academia and practice and their
   commonalities, challenges, and limitations are identified. The findings
   gained emerges the understanding that RAs can be an effective artefact
   to tackle complex BD system development.
RI Ataei, Pouya/; Litchfield, Alan/L-5949-2019
OI Ataei, Pouya/0000-0002-0993-3574; 
Z8 0
ZR 0
TC 11
ZA 0
ZB 0
ZS 0
Z9 13
U1 3
U2 15
SN 2169-3536
DA 2022-11-21
UT WOS:000880591400001
ER

PT C
AU Megdiche, Imen
   Ravat, Franck
   Zhao, Yan
BE Bures, T
   Dondi, R
   Gamper, J
   Guerrini, G
   Jurdzinski, T
   Pahl, C
   Sikora, F
   Wong, PWH
TI Metadata Management on Data Processing in Data Lakes
SO SOFSEM 2021: THEORY AND PRACTICE OF COMPUTER SCIENCE
SE Lecture Notes in Computer Science
VL 12607
BP 553
EP 562
DI 10.1007/978-3-030-67731-2_40
DT Proceedings Paper
PD 2021
PY 2021
AB Data Lake (DL) is known as a Big Data analysis solution. A data lake
   stores not only data but also the processes that were carried out on
   these data. It is commonly agreed that data preparation/transformation
   takes most of the data analyst's time. To improve the efficiency of data
   processing in a DL, we propose a framework which includes a metadata
   model and algebraic transformation operations. The metadata model
   ensures the findability, accessibility, interoperability and reusability
   of data processes as well as data lineage of processes. Moreover, each
   process is described through a set of coarse-grained data transforming
   operations which can be applied to different types of datasets. We
   illustrate and validate our proposal with a real medical use case
   implementation.
CT 47th International Conference on Current Trends in Theory and Practice
   of Computer Science (SOFSEM)
CY JAN 25-29, 2021
CL Bolzano-Bozen, ITALY
RI Megdiche, Imen/JTS-6315-2023; RAVAT, Franck/AAG-7714-2019; ZHAO, Yan/
OI RAVAT, Franck/0000-0003-4820-841X; ZHAO, Yan/0000-0002-6624-322X
Z8 0
ZA 0
ZB 0
ZS 0
TC 9
ZR 0
Z9 13
U1 1
U2 7
SN 0302-9743
EI 1611-3349
BN 978-3-030-67730-5; 978-3-030-67731-2
DA 2021-01-01
UT WOS:000927597000040
ER

PT J
AU Lopez, Ivan Dario
   Grass, Jose Fernando
   Figueroa, Apolinar
   Corrales, Juan Carlos
TI A proposal for a multi-domain data fusion strategy in a climate-smart
   agriculture context
SO INTERNATIONAL TRANSACTIONS IN OPERATIONAL RESEARCH
VL 30
IS 4
SI SI
BP 2049
EP 2070
DI 10.1111/itor.12899
EA OCT 2020
DT Article
PD JUL 2023
PY 2023
AB Agriculture provides food, raw materials, and employment opportunities
   for a significant percentage of the world's population. Climate,
   economic, political, social, and other conditions affect decision making
   in agricultural processes. In many cases, these conditions imply the
   loss of suitability of many areas for some traditional crops. In
   contrast, these areas can produce new crops by taking advantage of
   changing conditions. In this sense, having reliable tools and
   information for decision making is essential in adapting to new
   agricultural productivity scenarios. The above implies having sufficient
   and relevant data sources to reduce the uncertainty in the
   decision-making processes. However, data by nature tend to be diverse in
   structure, storage formats, and access protocols. Data fusion tasks have
   been immersed in a multitude of applications and have been approached
   from different points of view when implementing a suitable solution. We
   propose a multi-domain data fusion strategy to support data analysis
   tasks in agricultural contexts. We also describe all the data sources
   collected, which are the main input to the proposed strategy. The
   combined data sources were also evaluated through a preliminary
   exploratory analysis in a multi-label learning approach. Finally, the
   data fusion strategy is explained through an example in agricultural
   crop production.
RI Figueroa Casas, Apolinar/AAC-3182-2019; Lopez Gomez, Ivan Dario/; lopez, ivan/GSM-8495-2022
OI Figueroa Casas, Apolinar/0000-0003-3586-8187; Lopez Gomez, Ivan
   Dario/0000-0002-9781-6094; 
ZS 0
Z8 0
TC 11
ZR 0
ZA 0
ZB 3
Z9 13
U1 0
U2 25
SN 0969-6016
EI 1475-3995
DA 2020-11-19
UT WOS:000587020700001
ER

PT J
AU Derakhshannia, Marzieh
   Gervet, Carmen
   Hajj-Hassan, Hicham
   Laurent, Anne
   Martin, Arnaud
TI Data Lake Governance: Towards a Systemic and Natural Ecosystem Analogy
SO FUTURE INTERNET
VL 12
IS 8
AR 126
DI 10.3390/fi12080126
DT Article
PD AUG 2020
PY 2020
AB The realm of big data has brought new venues for knowledge acquisition,
   but also major challenges including data interoperability and effective
   management. The great volume of miscellaneous data renders the
   generation of new knowledge a complex data analysis process. Presently,
   big data technologies provide multiple solutions and tools towards the
   semantic analysis of heterogeneous data, including their accessibility
   and reusability. However, in addition to learning from data, we are
   faced with the issue of data storage and management in a cost-effective
   and reliable manner. This is the core topic of this paper. A data lake,
   inspired by the natural lake, is a centralized data repository that
   stores all kinds of data in any format and structure. This allows any
   type of data to be ingested into the data lake without any restriction
   or normalization. This could lead to a critical problem known as data
   swamp, which can contain invalid or incoherent data that adds no values
   for further knowledge acquisition. To deal with the potential avalanche
   of data, some legislation is required to turn such heterogeneous
   datasets into manageable data. In this article, we address this problem
   and propose some solutions concerning innovative methods, derived from a
   multidisciplinary science perspective to manage data lake. The proposed
   methods imitate the supply chain management and natural lake principles
   with an emphasis on the importance of the data life cycle, to implement
   responsible data governance for the data lake.
RI Hajj-Hassan, Hicham/; Martin, Arnaud/ABC-4371-2020; Gervet, Carmen/; Laurent, Anne/
OI Hajj-Hassan, Hicham/0000-0001-9917-4606; Martin,
   Arnaud/0000-0001-6323-0135; Gervet, Carmen/0000-0002-8062-2808; Laurent,
   Anne/0000-0003-3708-6429
ZB 0
TC 10
Z8 0
ZR 0
ZS 0
ZA 0
Z9 13
U1 1
U2 32
SN 1999-5903
DA 2020-09-15
UT WOS:000564878800001
ER

PT C
AU Gupta, Maanak
   Patwa, Farhan
   Sandhu, Ravi
GP ACM
TI POSTER: Access Control Model for the Hadoop Ecosystem
SO PROCEEDINGS OF THE 22ND ACM SYMPOSIUM ON ACCESS CONTROL MODELS AND
   TECHNOLOGIES (SACMAT'17)
BP 125
EP 127
DI 10.1145/3078861.3084164
DT Proceedings Paper
PD 2017
PY 2017
AB Apache Hadoop is an important framework for fault-tolerant and
   distributed storage and processing of Big Data. Hadoop core platform
   along with other open-source tools such as Apache Hive, Storm, HBase
   offer an ecosystem to enable users to fully harness Big Data potential.
   Apache Ranger and Apache Sentry provide access control capabilities to
   several ecosystem components by offering centralized policy
   administration and enforcement through plugins. In this work we discuss
   the access control model for Hadoop ecosystem (referred as HeAC) used by
   Apache Ranger (release 0.6) and Sentry (release 1.7.0) along with Hadoop
   2.x native authorization capabilities. This multi-layer model provides
   several access enforcement points to restrict unauthorized users to
   cluster resources. We further outline some preliminary approaches to
   extend the HeAC model consistent with widely accepted access control
   models.
CT 22nd ACM Symposium on Access Control Models and Technologies (SACMAT)
CY JUN 21-23, 2017
CL Indianapolis, IN
SP Assoc Comp Machinery; ACM SIGSAC
RI Gupta, Maanak/ABD-4037-2020
OI Gupta, Maanak/0000-0001-9189-2478
Z8 1
ZR 0
TC 10
ZB 0
ZA 0
ZS 0
Z9 13
U1 0
U2 2
BN 978-1-4503-4702-0
DA 2017-01-01
UT WOS:000614039400014
ER

PT C
AU Li, Sujie
   Zhang, Guigang
   Wang, Jian
GP IEEE
TI Civil Aircraft Health Management Research based on Big Data and Deep
   Learning Technologies
SO 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT
   (ICPHM)
BP 154
EP 159
DT Proceedings Paper
PD 2017
PY 2017
AB the coupling and correlation degree between aircraft systems is higher,
   and the diagnosis and prognosis of aircraft are more complex. Building a
   platform for storing and analyzing the aviation big data becomes an
   important task for civil aviation. This paper proposes a civil aircraft
   health management big data architecture. The civil aircraft health
   management system includes airborne PHM, ground PHM, remote diagnosis
   system, portable maintenance assistant system, maintenance center,
   automatic test equipment, special test equipment. Airborne PHM collects
   data from multiple types of data sources. Ground PHM provides decision
   making support for civil aircrafts including real-time alarm, health
   management, maintenance plan, spare parts. The paper introduces deep
   learning algorithm and aircraft fault diagnosis and prognosis
   implementation.
CT IEEE International Conference on Prognostics and Health Management
   (ICPHM)
CY JUN 19-21, 2017
CL Dallas, TX
SP IEEE; IEEE Reliabil Soc
ZS 0
TC 9
ZB 0
Z8 2
ZR 0
ZA 0
Z9 13
U1 3
U2 18
BN 978-1-5090-5710-8
DA 2017-01-01
UT WOS:000452639100025
ER

PT C
AU Spangenberg, Norman
   Wilke, Moritz
   Franczyk, Bogdan
BE Shakshuki, E
TI A Big Data architecture for intra-surgical remaining time predictions
SO 8TH INTERNATIONAL CONFERENCE ON EMERGING UBIQUITOUS SYSTEMS AND
   PERVASIVE NETWORKS (EUSPN 2017) / 7TH INTERNATIONAL CONFERENCE ON
   CURRENT AND FUTURE TRENDS OF INFORMATION AND COMMUNICATION TECHNOLOGIES
   IN HEALTHCARE (ICTH-2017) / AFFILIATED WORKSHOPS
SE Procedia Computer Science
VL 113
BP 310
EP 317
DI 10.1016/j.procs.2017.08.332
DT Proceedings Paper
PD 2017
PY 2017
AB The operating room area is still one of the most expensive sections in
   the hospital due to the high resource requirements and the diverse
   uncertainties. However there are few solutions that support monitoring
   and decision-making in operating room management. But with new data
   sources and analytical methods of big data research more improvements
   could be achieved. In this work we utilize surgical phase events
   recognized in surgical device data to learn prediction models and
   trigger online predictions for remaining intervention times in operating
   rooms. To identify the best algorithm for prediction model computation
   with the existing data, we evaluate a set of regression algorithms.
   Based on this methods we propose an architecture approach for the
   integrated processing of real-time data and historic learning data. The
   evaluation and comparison with related work shows that our prototype is
   competitive regarding prediction accuracy. (C) 2017 The Authors.
   Published by Elsevier B.V.
CT 8th International Conference on Emerging Ubiquitous Systems and
   Pervasive Networks (EUSPN) / 7th International Conference on Current and
   Future Trends of Information and Communication Technologies in
   Healthcare (ICTH)
CY SEP 18-20, 2017
CL Lund, SWEDEN
ZB 0
TC 9
Z8 0
ZR 0
ZS 0
ZA 0
Z9 13
U1 0
U2 9
SN 1877-0509
BN *****************
DA 2018-02-01
UT WOS:000419236500040
ER

PT C
AU Farrugia, Ashley
   Claxton, Rob
   Thompson, Simon
BE Kumar, R
   Caverlee, J
   Tong, H
TI Towards Social Network Analytics for Understanding and Managing
   Enterprise Data Lakes
SO PROCEEDINGS OF THE 2016 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN
   SOCIAL NETWORKS ANALYSIS AND MINING ASONAM 2016
BP 1213
EP 1220
DT Proceedings Paper
PD 2016
PY 2016
AB We have built a tool for inspecting and managing data lakes. The
   motivations for creating this tool are 1) schema discovery (determining
   links pertinent to solving a data analysis problem), 2) discovering high
   risk links in data schemas that give rise to Information Security
   problems and 3) discovering high value relationships enabling data asset
   curation. The tool works by extracting metadata from the Hive database
   on a shared-tenancy instance of Hadoop, which contained a multi-terabyte
   real-world data asset. We use this metadata to calculate a graph of the
   relationships between the entities based on column matching. This allows
   us to apply Social Network Analysis (SNA) techniques in order to
   discover meaningful properties of the accumulated data. For example to
   extract previously unknown relationships between data entities. The
   challenges and the agenda for future research are also provided.
CT 8th IEEE/ACM International Conference on Advances in Social Networks
   Analysis and Mining (ASONAM)
CY AUG 18-21, 2016
CL San Francisco, CA
SP IEEE; Assoc Comp Machinery; ACM SIGMOD; IEEE Comp Soc; IEEE TCDE;
   Springer; VEEPIO
ZS 0
ZA 0
Z8 0
ZR 0
TC 9
ZB 1
Z9 13
U1 0
U2 4
BN 978-1-5090-2846-7
DA 2017-01-18
UT WOS:000390760100188
ER

PT J
AU Ausiello, Dennis
   Lipnick, Scott
TI Real-Time Assessment of Wellness and Disease in Daily Life
SO BIG DATA
VL 3
IS 3
BP 203
EP 208
DI 10.1089/big.2015.0016
DT Article
PD SEP 1 2015
PY 2015
AB The next frontier in medicine involves better quantifying human traits,
   known as "phenotypes." Biological markers have been directly associated
   with disease risks, but poor measurement of behaviors such as diet and
   exercise limits our understanding of preventive measures. By joining
   together an uncommonly wide range of disciplines and expertise, the
   Kavli HUMAN Project will advance measurement of behavioral phenotypes,
   as well as environmental factors that impact behavior. By following the
   same individuals over time, KHP will liberate new understanding of
   dynamic links between behavioral phenotypes, disease, and the broader
   environment. As KHP advances understanding of the bio-behavioral
   complex, it will seed new approaches to the diagnosis, prevention, and
   treatment of human disease.
ZR 0
ZB 4
Z8 0
ZS 0
ZA 0
TC 10
Z9 13
U1 0
U2 13
SN 2167-6461
EI 2167-647X
DA 2015-09-01
UT WOS:000361364900010
PM 26487991
ER

PT C
AU Agarwal, Sonali
   Prasad, Bakshi Rohit
GP IEEE
TI High Speed Streaming Data Analysis of Web Generated Log Streams
SO 2015 IEEE 10TH INTERNATIONAL CONFERENCE ON INDUSTRIAL AND INFORMATION
   SYSTEMS (ICIIS)
SE International Conference on Industrial and Information Systems
BP 413
EP 418
DT Proceedings Paper
PD 2015
PY 2015
AB Web logs provide useful insight of large scale web based applications
   and helpful in deriving web usage patterns. Since, web usage patterns
   are available at a high rate and a high volume and also continuously
   updating in a real time environment, must be handled through modern big
   data architectures supported by powerful real time big data processing
   tools. Web generated log streams have most significant impact when it is
   feasible to analyze them at a time when they are emitted. In proposed
   research work, an advanced stream analytics framework especially for web
   generated log streams has been proposed by using the dataset of web
   access logs representing HTTP requests received by NASA Kennedy Space
   Center Server. The proposed framework can resourcefully handle the
   challenging issues associated to manage multiple web based log streams
   that are distributed across a fleet of web based applications and
   present a summarized view of statistical profile of web based
   applications which may be useful for web usage mining.
CT 10th IEEE International Conference on Industrial and Information Systems
   (ICIIS)
CY DEC 17-20, 2015
CL SRI LANKA
SP Univ Peradeniya, Fac Engn, Dept Elect & Elect Engn; Sri Lanka Telecom
   Mobitel; Univ Peradeniya, Int Res Ctr; LTL Holdings Pvt Ltd; Lanka Elect
   Co Ltd
RI Prasad, B/R-1432-2019; Agarwal, Sonali/AAT-1740-2020
OI Agarwal, Sonali/0000-0001-9083-5033
TC 12
Z8 0
ZS 0
ZR 0
ZA 0
ZB 0
Z9 13
U1 0
U2 6
SN 2164-7011
BN 978-1-4799-1876-8
DA 2015-01-01
UT WOS:000412187900072
ER

PT C
AU Munar, A.
   Chiner, E.
   Sales, I.
GP IEEE
TI A Big Data Financial Information Management Architecture for Global
   Banking
SO 2014 INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD
   (FICLOUD)
BP 385
EP 388
DI 10.1109/FiCloud.2014.68
DT Proceedings Paper
PD 2014
PY 2014
AB Global investment banks and financial institutions are facing growing
   data processing demands. These originate not only from increasing
   regulatory requirements and an expanding variety and disparity of data
   sources, but also from ongoing pressures in cost reduction without
   compromising system scalability and flexibility. In this context, the
   ability to apply promising state-of-the-art big data technologies to
   extract the maximum value from the vast amounts of the data generated is
   generating a lot of interest in the financial services industry. In this
   paper we present a Big Data architecture system design, based in open
   distributed computing paradigms like Hadoop map-reduce, offering
   horizontal scalability and no-SQL flexibility while at the same time
   meeting the stringent quality and resilience requirements of the banking
   software standards. The proposed architecture is able to consolidate,
   validate, enrich and process with different Big Data analytics
   techniques the data gathered from the different source systems as
   encountered in the banking practice, while at the same time supporting
   the different data integration, transmission and process orchestration
   requirements traditionally encountered in a global financial
   institution.
CT 2nd International Conference on Future Internet of Things and Cloud
   (FiCloud)
CY AUG 27-29, 2014
CL Barcelona, SPAIN
SP IEEE Comp Soc, Tech Comm Internet; IEEE Comp Soc
Z8 0
ZS 0
ZR 0
TC 9
ZA 0
ZB 0
Z9 13
U1 0
U2 22
BN 978-1-4799-4357-9
DA 2014-01-01
UT WOS:000378641000058
ER

PT J
AU Silvestri, Stefano
   Tricomi, Giuseppe
   Bassolillo, Salvatore Rosario
   De Benedictis, Riccardo
   Ciampi, Mario
TI An Urban Intelligence Architecture for Heterogeneous Data and
   Application Integration, Deployment and Orchestration
SO SENSORS
VL 24
IS 7
AR 2376
DI 10.3390/s24072376
DT Article
PD APR 2024
PY 2024
AB This paper describes a novel architecture that aims to create a template
   for the implementation of an IT platform, supporting the deployment and
   integration of the different digital twin subsystems that compose a
   complex urban intelligence system. In more detail, the proposed Smart
   City IT architecture has the following main purposes: (i) facilitating
   the deployment of the subsystems in a cloud environment; (ii)
   effectively storing, integrating, managing, and sharing the huge amount
   of heterogeneous data acquired and produced by each subsystem, using a
   data lake; (iii) supporting data exchange and sharing; (iv) managing and
   executing workflows, to automatically coordinate and run processes; and
   (v) to provide and visualize the required information. A prototype of
   the proposed IT solution was implemented leveraging open-source
   frameworks and technologies, to test its functionalities and
   performance. The results of the tests performed in real-world settings
   confirmed that the proposed architecture could efficiently and easily
   support the deployment and integration of heterogeneous subsystems,
   allowing them to share and integrate their data and to select, extract,
   and visualize the information required by a user, as well as promoting
   the integration with other external systems, and defining and executing
   workflows to orchestrate the various subsystems involved in complex
   analyses and processes.
RI Silvestri, Stefano/IUP-0829-2023; Tricomi, Giuseppe/S-6029-2019; Ciampi, Mario/B-3874-2015; Bassolillo, Salvatore Rosario/IUN-2349-2023; De Benedictis, Riccardo/AAU-3550-2020
OI Silvestri, Stefano/0000-0002-9890-8409; Tricomi,
   Giuseppe/0000-0003-3837-8730; Ciampi, Mario/0000-0002-7286-6212;
   Bassolillo, Salvatore Rosario/0000-0002-0411-3729; De Benedictis,
   Riccardo/0000-0003-2344-4088
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
TC 11
Z9 12
U1 2
U2 7
EI 1424-8220
DA 2024-04-17
UT WOS:001201137000001
PM 38610587
ER

PT J
AU Mounica, B.
   Lavanya, K.
TI Feature selection method on twitter dataset with part-of-speech (PoS)
   pattern applied to traffic analysis
SO INTERNATIONAL JOURNAL OF SYSTEM ASSURANCE ENGINEERING AND MANAGEMENT
VL 15
IS 1
SI SI
BP 110
EP 123
DI 10.1007/s13198-022-01677-3
EA MAY 2022
DT Article
PD JAN 2024
PY 2024
AB In day-to-day life transportation plays a major role in cities. Present
   day traffic management is a complex task for transportation agencies
   through traditional approaches, hence Intelligent Transportation systems
   is applied to give traffic management solutions like parking, E-toll
   charge and traffic control by analyzing data from related sources. Data
   is collected from various sources for analyzing transportation need's,
   yet transportation issues remain one of the major tribulations in
   cities. Unstructureddata gives enormous information load for big data
   analytics, but the unstructured content processing is a challenge in
   industry. Passive data like social media data is a major data sources
   for Intelligent Systems, social media applications such as Twitter,
   Facebook where user can share live comments based on their interaction
   with the world is a rich source for passive data. Social media data
   helps in analyzing traffic issues like traffic jam, accident locations,
   road condition etc. Major issue with social media data is processing and
   analysis of data is very complex because of volume and data format. Big
   data architecture helps in extracting, processing, loading in database
   and analyzing this unstructured data. To identify thesentimentalanalysis
   is majorly classified based onpositive, negative and neutral tweets. As
   the polarity of neutral tweets is zero it cannot be used for Opinion
   mining. So, this paper is focused on Neutral tweets classification based
   on feature selection. Part of Speech (PoS) tagging is used for labeling
   the words of the text in the tweets to find nouns example location, date
   and time are compared with the other attribute values for improving the
   classification of neutral tweets. Research work shown in this paper has
   taken social media speech data (Tweets) from twitter as input and
   preprocessing techniques are applied on the data collected, Methods such
   as feature selection are then used to extract the features related to
   tweets for classifying neutral tweets for better understanding on road
   condition, identification of traffic patterns and finally traffic
   behavior is analyzed by using Ensemble machine learning algorithm. In
   the proposed model to measure the sentimental analysis a new approach is
   provided based on feature selection. The findings disclose with
   SentiWordNetopinion lexicon approach gives 56% accuracy of positive or
   negative opinion using twitter dataset, the results of feature
   selection-based opinion mining proposed model increased substantially
   with 88% accuracy.
RI , lavanya/AGH-9235-2022; K, Lavanya/
OI , lavanya/0000-0003-0213-2528; K, Lavanya/0000-0002-2961-321X
ZA 0
ZR 0
Z8 0
ZS 0
ZB 2
TC 11
Z9 12
U1 0
U2 6
SN 0975-6809
EI 0976-4348
DA 2022-06-07
UT WOS:000801074200001
ER

PT J
AU Yu, Han
   Cai, Hongming
   Liu, Zhiyuan
   Xu, Boyi
   Jiang, Lihong
TI An Automated Metadata Generation Method for Data Lake of Industrial WoT
   Applications
SO IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS
VL 52
IS 8
BP 5235
EP 5248
DI 10.1109/TSMC.2021.3119871
EA OCT 2021
DT Article
PD AUG 2022
PY 2022
AB Recent trends in the Web of Things (WoT) have led to data explosion.
   Data lake (DL), as a flexible on-demand heterogeneous data management
   architecture, has become a feasible solution in data management.
   Metadata modeling for DLs is the key basis for smart analysis and
   processing. However, the varieties in structures and semantics of
   industrial WoT data hinder metadata modeling and maintenance. Moreover,
   the lack of textual descriptions and the semantics hidden in value
   streams make it hard to automatically construct semantic metadata. The
   dynamic nature of WoT requires on-time evolution on metadata. To
   overcome these challenges, we propose an automated bottom-up metadata
   generation approach for DL of WoT applications. Applying a data-driven
   framework, raw data are notated as linked data and self-organizing
   map-based online clustering is applied to real timely extract data
   characteristics. To recognize entities, concepts and relations,
   semantics-based entity discovery approach from short texts is proposed
   according to the feature of WoT data. The numerical analysis is
   performed to find the hidden relations from raw values. Full-dimensional
   metadata with rich semantic knowledge are finally built. Experiments on
   a real-world dataset are conducted to verify the effectiveness of
   methods and a case study on an energy WoT system is provided to
   demonstrate the feasibility of the approach.
RI Yu, Han/GXW-2253-2022; Cai, Hongming/
OI Yu, Han/0000-0001-8070-1293; Cai, Hongming/0000-0003-0190-6907
TC 9
ZA 0
ZS 0
Z8 2
ZR 0
ZB 0
Z9 12
U1 0
U2 35
SN 2168-2216
EI 2168-2232
DA 2021-12-28
UT WOS:000732305700001
ER

PT J
AU Woo, Jongwook
   Mishra, Monika
TI Predicting the ratings of Amazon products using Big Data
SO WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY
VL 11
IS 3
AR e1400
DI 10.1002/widm.1400
EA DEC 2020
DT Review
PD MAY 2021
PY 2021
AB This paper aims to apply several machine learning (ML) models to the
   massive dataset present in the area of e-commerce from Amazon to analyze
   and predict ratings and to recommend products. For this purpose, we have
   used both traditional and Big Data algorithms. As the Amazon product
   review dataset is large, we present Big Data architecture suitable
   massive dataset for storing and computation, which is not possible with
   the traditional architecture. Furthermore, the dataset contains 15
   attributes and has about 7 million records. With the dataset, we develop
   several models in Oracle Big Data and Azure Cloud Computing services to
   predict the review rating and recommendation for the items at Amazon. We
   present a comparative conclusion in terms of the accuracy as well as the
   efficiency with Spark ML-the Big Data architecture, and Azure ML-the
   traditional architecture.
   This article is categorized under:
   Fundamental Concepts of Data and Knowledge > Big Data Mining
   Technologies > Machine Learning
   Technologies > Prediction
OI Mishra, Monika/0000-0003-2696-3262
ZA 0
ZB 0
TC 9
ZR 0
Z8 0
ZS 0
Z9 12
U1 4
U2 52
SN 1942-4787
EI 1942-4795
DA 2020-12-24
UT WOS:000597676400001
ER

PT J
AU Kachaoui, Jabrane
   Larioui, Jihane
   Belangour, Abdessamad
TI Towards an Ontology Proposal Model in Data Lake for Real-time COVID-19
   Cases Prevention
SO INTERNATIONAL JOURNAL OF ONLINE AND BIOMEDICAL ENGINEERING
VL 16
IS 9
BP 123
EP 136
DI 10.3991/ijoe.v16i09.15325
DT Article
PD 2020
PY 2020
AB Globally, the coronavirus epidemic has now hit lives of millions and
   thousands of people around the world. The growing threat of this virus
   continues rising as new cases appear every day. Yet, affected countries
   by coronavirus are currently taking important measures to remedy it by
   using Artificial Intelligence (AI) and Big Data technologies. According
   to the World Health Organization (WHO), AI and Big Data have performed
   an important role in China's response to COVID-19, the genetic mutation
   name for coronavirus. Predicting an epidemic emergence, from the
   coronavirus appearance to a person's predisposition to develop it, is
   fundamental to combating it. In this battle, Big Data is on the front
   line. However, Big Data cannot provide all of the expected insights and
   derive value from manipulated data. This is why we propose a semantic
   approach to facilitate the use of these data. In this paper, we present
   a novel approach that combines between the Semantic Web Services (SWS)
   and the Big Data characteristics in order to extract a significant
   information from multiple data sources that can be exploitable for
   generating real-time statistics and reports.
RI Belangour, Abdessamad/KAL-6712-2024
TC 8
ZS 0
ZB 2
Z8 0
ZA 0
ZR 0
Z9 12
U1 0
U2 10
EI 2626-8493
DA 2020-09-01
UT WOS:000560666400008
ER

PT C
AU Cheng, Guangming
   Li, Yao
   Gao, Zhiwei
   Liu, Xiaoyin
BE Wenzheng, L
   Babu, MSP
   Xiaohui, L
TI Cloud Data Governance Maturity Model
SO PROCEEDINGS OF 2017 8TH IEEE INTERNATIONAL CONFERENCE ON SOFTWARE
   ENGINEERING AND SERVICE SCIENCE (ICSESS 2017)
SE International Conference on Software Engineering and Service Science
BP 517
EP 520
DT Proceedings Paper
PD 2017
PY 2017
AB Data governance is essential to ensure the accuracy, moderate sharing
   and protection of data, and to support data to play an important role in
   various decisions. However, with the development of cloud computing and
   big data technology, the characteristics of data are undergoing many
   changes, so traditional data governance theories cannot fully meet the
   governance requirements of data in era of cloud computing and big data.
   Learning from traditional data governance theories and intensively
   analyzing data characteristics in cloud computing, we put forward the
   cloud data governance maturity model (CDGM) innovatively. The CDGM
   includes lots of policies focusing on data strategy, data management,
   data optimization, data operations, data architecture and data security
   & privacy protection, and a continuous improvement loop governance
   system of planning, implementation, evaluation and optimization. At the
   same time, this paper also defines the cloud data governance maturity
   levels and explains their main features, and at last presents the cloud
   data governance maturity assessment methods. So, the CDGM can provide an
   overall guidance for an organization to manage their cloud data.
CT 8th IEEE International Conference on Software Engineering and Service
   Science (ICSESS)
CY NOV 24-26, 2017
CL Beijing, PEOPLES R CHINA
SP Inst Elect & Elect Engineers; IEEE Beijing Sect
TC 11
ZS 0
ZR 0
ZB 0
ZA 0
Z8 0
Z9 12
U1 2
U2 33
SN 2327-0594
BN 978-1-5386-0497-7
DA 2018-06-29
UT WOS:000434977800118
ER

PT C
AU Gargiulo, Francesco
   Silvestri, Stefano
   Ciampi, Mario
GP IEEE
TI A Big Data Architecture for Knowledge Discovery in Pubmed Articles
SO 2017 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS (ISCC)
SE IEEE Symposium on Computers and Communications ISCC
BP 82
EP 87
DT Proceedings Paper
PD 2017
PY 2017
AB The need of smart information retrieval systems is in contrast with the
   difficulties to deal with huge amount of data. In this paper we present
   a Big Data Analytics architecture used to implement a semantic
   similarity search tool for natural language texts in biomedical domain.
   The implemented methodology is based on Word Embeddings (WEs) models
   obtained using the word2vec algorithm. The system has been assessed with
   documents extracted from the whole PubMed library. It will be also
   presented a user friendly web front-end able to assess the methodology
   on a real context.
CT IEEE Symposium on Computers and Communications (ISCC)
CY JUL 03-07, 2017
CL Heraklion, GREECE
RI Silvestri, Stefano/IUP-0829-2023; Ciampi, Mario/B-3874-2015; Gargiulo, Francesco/Q-5204-2018
OI Silvestri, Stefano/0000-0002-9890-8409; Ciampi,
   Mario/0000-0002-7286-6212; Gargiulo, Francesco/0000-0003-0400-3332
TC 12
ZR 0
ZS 0
ZA 0
ZB 3
Z8 0
Z9 12
U1 0
U2 4
SN 1530-1346
BN 978-1-5386-1629-1
DA 2018-03-28
UT WOS:000426895800015
ER

PT C
AU Begoli, Edmon
   Kistler, Derek
   Bates, Jack
BE Joshi, J
   Karypis, G
   Liu, L
   Hu, X
   Ak, R
   Xia, Y
   Xu, W
   Sato, AH
   Rachuri, S
   Ungar, L
   Yu, PS
   Govindaraju, R
   Suzumura, T
TI Towards a Heterogeneous, Polystore-like Data Architecture for the
   US Department of Veteran Affairs (VA) Enterprise Analytics
SO 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
BP 2550
EP 2554
DT Proceedings Paper
PD 2016
PY 2016
AB The Polystore architecture revisits the federated approach to access and
   querying the standalone, independent databases in the uniform and
   optimized fashion, but this time in the context of heterogeneous data
   and specialized analyses. In light of this architectural philosophy, and
   in the light of the major data architecture development efforts at the
   US Department of Veterans Administration (VA), we discuss the need for
   the heterogeneous data store consisting of large relational data
   warehouse, an image and text datastore, and a peta-scale genomic
   repository. The VA's heterogeneous datastore would, to a larger or
   smaller degree, follow the architectural blueprint proposed by the
   polystore architecture. To this end, we discuss the current state of the
   data architecture at VA, architectural alternatives for development of
   the heterogeneous datastore, some relevant use cases, the anticipated
   challenges, and the drawbacks and benefits of adopting the polystore
   architecture.
CT 4th IEEE International Conference on Big Data (Big Data)
CY DEC 05-08, 2016
CL Washington, DC
SP IEEE; IEEE Comp Soc; Natl Sci Fdn; Cisco; Huawei; Elsevier; Navigant;
   Johns Hopkins Whiting Sch Engn
OI Begoli, Edmon/0000-0002-2173-3663
ZB 2
TC 12
ZR 0
ZA 0
ZS 0
Z8 0
Z9 12
U1 0
U2 3
BN 978-1-4673-9005-7
DA 2017-06-13
UT WOS:000399115002080
ER

PT C
AU Las-Casas, Pedro H. B.
   Dias, Vinicius Santos
   Meira, Wagner, Jr.
   Guedes, Dorgival
BE Qiu, MK
TI A Big Data architecture for security data and its application to
   phishing characterization
SO 2016 IEEE 2ND INTERNATIONAL CONFERENCE ON BIG DATA SECURITY ON CLOUD
   (BIGDATASECURITY), IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE AND
   SMART COMPUTING (HPSC), AND IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT
   DATA AND SECURITY (IDS)
BP 36
EP 41
DI 10.1109/BigDataSecurity-HPSC-IDS.2016.44
DT Proceedings Paper
PD 2016
PY 2016
AB As the Internet grows, cybersecurity problems also arise. Different
   types of malicious activities have been explored by attackers. However,
   the existent defense mechanisms are not able to completely end the
   malicious threats, perpetuating this continuous arms race. The
   development of applications to mitigate those threats presents some
   complicating factors such as the growth in the amount of data, and the
   variety of data, that can come from different sources. In this paper we
   present an architecture built on top of Big Data frameworks that aims to
   mitigate cybersecurity problems such as spam and phishing and we show
   how it is being used to study spam and phishing collected using a global
   honeynet.
CT 2nd IEEE International Conference on Big Data Security on Cloud
   (BigDataSecurity)
CY APR 08-10, 2016
CL New York, NY
SP IEEE; IEEE Comp Soc; IEEE TCSC; PACE Univ; Columbia Univ; N Amer Chinese
   Talents Assoc; Longxian High Tech; Stony Brook Univ; IEEE Big Data Secur
   HPSC IDS Comm; Shanghai Jiao Tong Univ; IEEE Transact Comp; IEEE
   Transact Cloud Comp
RI Dias, Vinícius/HJA-1778-2022; Guedes, Dorgival/; Meira, Wagner/AAT-9286-2020
OI Dias, Vinícius/0000-0002-8324-8487; Guedes,
   Dorgival/0000-0003-0865-1417; 
ZR 0
ZS 0
ZB 0
TC 11
ZA 0
Z8 0
Z9 12
U1 0
U2 1
BN 978-1-5090-2403-2
DA 2017-01-11
UT WOS:000390194300006
ER

PT C
AU Hughes, John S.
   Crichton, Daniel
   Hardman, Sean
   Law, Emily
   Joyner, Ronald
   Ramirez, Paul
GP IEEE
TI PDS4: A Model-Driven Planetary Science Data Architecture for Long-Term
   Preservation
SO 2014 IEEE 30TH INTERNATIONAL CONFERENCE ON DATA ENGINEERING WORKSHOPS
   (ICDEW)
BP 134
EP 141
DT Proceedings Paper
PD 2014
PY 2014
AB The goal of the Planetary Data System (PDS) is the digital preservation
   of scientific data for long-term use by the scientific research
   community. After two decades of successful operation, the PDS found
   itself in a new era of big data, international cooperation, distributed
   nodes, and multiple ways of analysing and interpreting data. A project
   was formed to develop a disciplined architectural approach that would
   drive the design and implementation of a scalable data system that could
   evolve to meet the demands of this new era. PDS4, the next generation
   system, uses an explicit model-driven architectural approach coupled
   with modern information technologies and standards to meet these
   challenges in order to ensure the planetary data assets can be mined for
   scientific knowledge for years to come.
CT IEEE 30th International Conference on Data Engineering (ICDE)
CY MAR 31-APR 04, 2014
CL Chicago, IL
SP IEEE; Microsoft; Qatar Comp Res Inst; HERE Nokia; Purdue Univ, Cyber
   Ctr; NW Univ, McCormick Sch Engn; Google
OI Hughes, John/0000-0003-4851-293X
ZB 2
Z8 1
ZR 0
ZA 0
TC 11
ZS 0
Z9 12
U1 0
U2 5
BN 978-1-4799-3481-2
DA 2015-12-30
UT WOS:000366168000021
ER

PT J
AU Gagliardelli, Luca
   Zecchini, Luca
   Ferretti, Luca
   Beneventano, Domenico
   Simonini, Giovanni
   Bergamaschi, Sonia
   Orsini, Mirko
   Magnotta, Luca
   Mescoli, Emma
   Livaldi, Andrea
   Gessa, Nicola
   De Sabbata, Piero
   D'Agosta, Gianluca
   Paolucci, Fabrizio
   Moretti, Fabio
TI A big data platform exploiting auditable tokenization to promote good
   practices inside local energy communities
SO FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
VL 141
BP 595
EP 610
DI 10.1016/j.future.2022.12.007
EA DEC 2022
DT Article
PD APR 2023
PY 2023
AB The Energy Community Platform (ECP) is a modular system conceived to
   promote a conscious use of energy by the users inside local energy
   communities. It is composed of two integrated subsystems: the Energy
   Community Data Platform (ECDP), a middleware platform designed to
   support the collection and the analysis of big data about the energy
   consumption inside local energy communities, and the Energy Community
   Tokenization Platform (ECTP), which focuses on tokenizing processed
   source data to enable incentives through smart contracts hosted on a
   decentralized infrastructure possibly governed by multiple authorities.
   We illustrate the overall design of our system, conceived considering
   some real-world projects (dealing with different types of local energy
   community, different amounts and nature of incoming data, and different
   types of users), analyzing in detail the key aspects of the two
   subsystems. In particular, the ECDP acquires data of a different nature
   in a heterogeneous format from multiple sources and supports a data
   integration workflow and a data lake workflow, designed for different
   uses of the data. We motivate our technological choices and present the
   alternatives taken into account, both in terms of software and of
   architectural design. On the other hand, the ECTP operates a
   tokenization process via smart contracts to promote good behaviors of
   users within the local energy community. The peculiarity of this
   platform is to allow external parties to audit the correct behavior of
   the whole tokenization process while protecting the confidentiality of
   the data and the performance of the platform. The main strengths of the
   presented system are flexibility and scalability (guaranteed by its
   modular architecture), which allow its applicability to any type of
   local energy community.(c) 2022 Elsevier B.V. All rights reserved.
RI Zecchini, Luca/; Ferretti, Luca/D-6339-2016; Moretti, Fabio/; Paolucci, Federico/ABD-7622-2020; D'Agosta, Gianluca/; Livaldi, Anea/; Beneventano, Domenico/K-9087-2015; BERGAMASCHI, Sonia/AAN-1712-2021
OI Zecchini, Luca/0000-0002-4856-0838; Moretti, Fabio/0000-0003-4561-7533;
   D'Agosta, Gianluca/0009-0004-3824-8113; Livaldi,
   Anea/0000-0001-7792-1842; 
Z8 0
ZA 0
ZR 0
ZB 1
TC 9
ZS 0
Z9 11
U1 3
U2 10
SN 0167-739X
EI 1872-7115
DA 2023-01-26
UT WOS:000910727200001
ER

PT J
AU Mustapha, S. M. F. D. Syed
TI The UAE Employees' Perceptions towards Factors for Sustaining Big Data
   Implementation and Continuous Impact on Their Organization's Performance
SO SUSTAINABILITY
VL 14
IS 22
AR 15271
DI 10.3390/su142215271
DT Article
PD NOV 2022
PY 2022
AB The UAE has officially launched the Big Data initiative in the year
   2022; however, the interest in and adoption of Big Data technologies and
   strategies had started much earlier in the private and public sectors.
   This research aims to explore the perceptions of the UAE employees on
   factors needed to implement sustainable Big Data and the continuous
   impact on their organizational performance. A total of 257 employees
   were randomly selected for an online survey, and data were collected
   using a Likert-style five-point scale that was tested for validity and
   reliability. The findings indicate that employees believe that Big Data
   Sustainable Implementation leads to Business Performance. Additionally,
   employees consider factors such as Big Data Architecture Quality, Human
   Cognitive Factors, and Organizational Readiness to significantly impact
   on Sustainable Implementation. Further, a moderating impact of Human
   Cognitive Factors was found on the relationship between Big Data
   Architecture Quality and Sustainable Implementation. The study provides
   managerial insights and recommendations for policymaking.
RI Syed Mustapha, SMFD/F-4496-2016
OI Syed Mustapha, SMFD/0000-0001-7026-7198
ZB 0
ZA 0
Z8 0
ZR 0
ZS 1
TC 8
Z9 11
U1 0
U2 10
EI 2071-1050
DA 2022-12-05
UT WOS:000887553300001
ER

PT C
AU Cherradi, Mohamed
   EL Haddadi, Anass
BE BenAhmed, M
   Boudhir, AA
   Karas, IR
   Jain, V
   Mellouli, S
TI Data Lakes: A Survey Paper
SO 6TH INTERNATIONAL CONFERENCE ON SMART CITY APPLICATIONS
SE Lecture Notes in Networks and Systems
VL 393
BP 823
EP 835
DI 10.1007/978-3-030-94191-8_66
DT Proceedings Paper
PD 2022
PY 2022
AB In the last decades, the amount of data produced every day is absolutely
   horrible. So-called big data, that refers to the exponential growth of
   massive data and the difficulties that appear with it. In this context,
   data lakes have been proposed to address this issue. It allows storing
   heterogeneous data in a central repository without any predefined
   schema. However, existing literature on data lakes is fairly fuzzy and
   obscure, and the different contributions that have been proposed do not
   incorporate all aspects of data lakes, still inconsistent. Thereby, we
   present in this paper a clear and comprehensible overview of data lake
   definitions, architectures, and technologies. Indeed, we will classify
   the different scientific contributions on the data lake according to
   each layer of their architecture. This allows the different young
   researchers to identify the research problem on which we want to work,
   which forms the key point to the publication engineering. Also, we
   discuss the capital importance of some major functionality that promotes
   the creation of data lakes so that they don't turn into a data swamp.
CT 6th International Conference on Smart City Applications
CY OCT 27-29, 2021
CL Safranbolu, TURKEY
RI EL HADDADI, Anass/ABD-8465-2021
ZR 0
ZB 0
Z8 0
ZA 0
ZS 0
TC 5
Z9 11
U1 0
U2 14
SN 2367-3370
EI 2367-3389
BN 978-3-030-94191-8; 978-3-030-94190-1
DA 2023-03-09
UT WOS:000928840400066
ER

PT J
AU Chen, Wenyu
   Yang, Xue
   Zhang, Haikuo
   Xu, Yanzhi
   Pang, Zihan
TI Big Data Architecture for Scalable and Trustful DNS based on Sharded DAG
   Blockchain
SO JOURNAL OF SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO
   TECHNOLOGY
VL 93
IS 7
BP 753
EP 768
DI 10.1007/s11265-021-01645-3
EA APR 2021
DT Article
PD JUL 2021
PY 2021
AB Internet applications remain exposed to pervasive Domain Name System
   (DNS)-based threats. Blockchain technologies provide a new way for
   tackling DNS vulnerability issues, and have been highlighted recently.
   However, traditional blockchain is still not well suited for big data
   applications such as DNS, because the performance of blockchain
   consensus greatly limits its practical adoption. In this paper, we
   present DagGridLedger, a sharded directed acyclic graph (DAG) blockchain
   that provides scalable big data architecture for trustful DNS
   management. To achieve this goal, DagGridLedger proposes a radical new
   architecture that combines blockchain sharding and DAG techniques on the
   DNS resolver side, thereby making it a promising solution to enhance the
   security and stability of large-scale DNS system. To be specific,
   DagGridLedger provides a blockchain structure targeting DNS application,
   which employs a high-performance DAG consensus algorithm named DagGrid.
   DagGrid consensus realizes a multi-DNS negotiation mechanism through
   block sharding in generating a block. With an improved asynchronous
   leaderless Byzantine consensus, DagGrid implements total order
   determination, which guarantees the trustful DNS management. Further
   experiments verified the performance of DagGridLedger as well as the
   applicability of the proposed blockchain architecture in traditional
   DNS. To this end, DagGridLedger consistently achieves a big data
   architecture for secure DNS record management, with a novel shared DAG
   consensus designed for high throughput. This makes DagGridLedger a
   promising architecture for highly secure and efficient DNS solution.
RI Chen, Wenyu/IQV-0732-2023; Zhang, Haikuo/
OI Zhang, Haikuo/0000-0001-6875-7155
ZA 0
ZB 0
ZS 0
ZR 0
Z8 4
TC 7
Z9 11
U1 0
U2 24
SN 1939-8018
EI 1939-8115
DA 2021-04-24
UT WOS:000636131700001
ER

PT J
AU He, Xiaoming
   Wang, Kun
   Lu, Haodong
   Xu, Wenyao
   Guo, Song
TI Edge QoE: Intelligent Big Data Caching via Deep Reinforcement Learning
SO IEEE NETWORK
VL 34
IS 4
BP 8
EP 13
DI 10.1109/MNET.011.1900393
DT Article
PD JUL-AUG 2020
PY 2020
AB In mobile edge networks (MENs), big data caching services are expected
   to provide mobile users with better quality of experience (QoE) than
   normal scenarios. However, the increasing types of sensors and devices
   are producing an explosion of big data. Extracting valuable contents for
   caching is becoming a vital issue for the satisfaction of QoE.
   Therefore, it is urgent to propose some rational strategies to improve
   QoE, which is the major challenge for content-centric caching. This
   article introduces a novel big data architecture consisting of data
   management units for content extraction and caching decision, improving
   quality of service and ensuring QoE. Then a caching strategy is proposed
   to improve QoE, including three parts: (1) the caching location
   decision, which means the method of deploying caching nodes to make them
   closer to users; (2) caching capacity assessment, which aims to seek
   suitable contents to match the capacity of caching nodes; and (3)
   caching priority choice, which leads to contents being cached according
   to their priority to meet user demands. With this architecture and
   strategy, we particularly use a caching algorithm based on deep
   reinforcement learning to achieve lower cost for intelligent caching.
   Experimental results indicate that our schemes achieve higher QoE than
   existing algorithms.
RI Xu, Wenyao/C-4043-2011; Guo, Song/AAZ-4542-2020
ZB 0
TC 11
ZA 0
ZR 0
Z8 0
ZS 0
Z9 11
U1 0
U2 20
SN 0890-8044
EI 1558-156X
DA 2020-08-05
UT WOS:000552273900002
ER

PT J
AU Yang, Chao-Tung
   Chen, Shuo-Tsung
   Yan, Yin-Zhen
TI The implementation of a cloud city traffic state assessment system using
   a novel big data architecture
SO CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS
VL 20
IS 2
SI SI
BP 1101
EP 1121
DI 10.1007/s10586-017-0846-z
DT Article
PD JUN 2017
PY 2017
AB In order to store and analyze the increasing data in recent years, big
   data techniques are applied to many fields such as healthcare,
   manufacturing, telecommunications, retail, energy, transportation,
   automotive, security, environment, etc. This work implements a city
   traffic state assessment system in cloud using a novel big data
   architecture. The proposed system provides the real-time busses location
   and real-time traffic state, especially the real-time traffic state
   nearby, through open data, cloud computing, bid data technology,
   clustering methods, and irregular moving average. With the
   high-scalability cloud technologies, Hadoop and Spark, the proposed
   system architecture is first implemented successfully and efficiently.
   Next, we utilize irregular moving average and clustering methods to find
   the area of traffic jam. Finally, three important experiments are
   performed. The first experiment indicates that the computing ability of
   Spark is better than that of Hadoop. The second experiment applies Spark
   to process bus location data under different number of executors. In the
   last experiment, we apply irregular moving average and clustering
   methods to efficiently find the area of traffic jam in Taiwan Boulevard
   which is the main road in Taichung city. Based on these experimental
   results, the provided system services are present via an advanced web
   technology.
RI Yang, Chao-Tung/B-4562-2009; Chen, Shuo-Tsung/LMP-9707-2024
OI Yang, Chao-Tung/0000-0002-9579-4426; 
ZR 0
Z8 1
ZB 1
ZA 0
ZS 0
TC 10
Z9 11
U1 0
U2 34
SN 1386-7857
EI 1573-7543
DA 2017-06-01
UT WOS:000403457100015
ER

PT C
AU Goncalves, Andre
   Portela, Filipe
   Santos, Manuel Filipe
   Rua, Fernando
BE Shakshuki, E
TI Towards of a Real-time Big Data Architecture to Intensive Care
SO 8TH INTERNATIONAL CONFERENCE ON EMERGING UBIQUITOUS SYSTEMS AND
   PERVASIVE NETWORKS (EUSPN 2017) / 7TH INTERNATIONAL CONFERENCE ON
   CURRENT AND FUTURE TRENDS OF INFORMATION AND COMMUNICATION TECHNOLOGIES
   IN HEALTHCARE (ICTH-2017) / AFFILIATED WORKSHOPS
SE Procedia Computer Science
VL 113
BP 585
EP 590
DI 10.1016/j.procs.2017.08.294
DT Proceedings Paper
PD 2017
PY 2017
AB These days the exponential increase in the volume and variety of data
   stored by companies and organizations of various sectors of activity,
   has required to organizations the search for new solutions to improve
   their services and/or products, taking advantage of technological
   evolution. As a response to the inability of organizations to process
   large quantities and varieties of data, in the technological market,
   arise the Big Data. This emerging concept defined mainly by the volume,
   velocity and variety has evolved greatly in part by its ability to
   generate value for organizations in decision making. Currently, the
   health care sector is one of the five sectors of activity where the
   potential of Big Data growth most stands out. However, the way to go is
   still long and in fact there are few organizations, related to health
   care, that are taking advantage of the true potential of Big Data. The
   main target of this research is to produce a real-time Big Data
   architecture to the INTCare system, of the Centro Hospitalar do Porto,
   using the main open source big data solution, the Apache Hadoop. As a
   result of the first phase of this research we obtained a generic
   architecture who can be adopted by other Intensive Care Units. (c) 2017
   The Authors. Published by Elsevier B.V.
CT 8th International Conference on Emerging Ubiquitous Systems and
   Pervasive Networks (EUSPN) / 7th International Conference on Current and
   Future Trends of Information and Communication Technologies in
   Healthcare (ICTH)
CY SEP 18-20, 2017
CL Lund, SWEDEN
RI Gomes, André/KXQ-9620-2024; Santos, Manuel/ABD-3467-2020; Portela, Filipe/G-5324-2012
OI Santos, Manuel/0000-0002-5441-3316; Portela, Filipe/0000-0003-2181-6837
TC 5
ZB 0
Z8 1
ZR 0
ZA 0
ZS 1
Z9 11
U1 0
U2 4
SN 1877-0509
BN *****************
DA 2018-02-01
UT WOS:000419236500080
ER

PT C
AU Martinez-Prieto, Miguel A.
   Bregon, Anibal
   Garcia-Miranda, Ivan
   Alvarez-Esteban, Pedro C.
   Diaz, Fernando
   Scarlatti, David
GP IEEE
TI Integrating Flight-related Information into a (Big) Data Lake
SO 2017 IEEE/AIAA 36TH DIGITAL AVIONICS SYSTEMS CONFERENCE (DASC)
SE IEEE-AIAA Digital Avionics Systems Conference
DT Proceedings Paper
PD 2017
PY 2017
AB Flight cancellations, departure delays, congestion in taxi times and
   airborne holding delays are increasingly frequent problems that
   negatively impact the performance, fuel burn, emissions rate and
   customer satisfaction at major airports in the world. However, this is
   just a brushstroke of the future to come. The dramatic growth in the air
   traffic levels has become a problem of paramount importance, leading
   into an increased interest for enhancing the current Air Traffic
   Management (ATM) systems. The main objective is to being able to cope
   with the sustained air traffic growth under safe, economic, efficient
   and environmental friendly working conditions. The ADSB (Automatic
   Dependent Surveillance - Broadcast) technology plays a major role in the
   new ATM systems, since it provides more accurate real-time positioning
   information than secondary radars, in spite of using a cheaper
   infrastructure. However, the main flaw in the use of ADS-B technology is
   the generation of large volumes of data, that, when merged with other
   flight-related information, faces important scalability issues. In this
   work, we start off from a previously developed data lake for the support
   of the full ADS-B data life-cycle in a scalable and cost-effective way,
   and propose a data architecture to integrate data from different
   providers and reconstruct flight trajectories that can ultimately be
   used to improve the efficiency in flight operations. This data
   architecture is also evaluated using a 2-week testbed which reports some
   interesting figures about its effectiveness.
CT 36th IEEE/AIAA Digital Avionics Systems Conference (DASC)
CY SEP 17-21, 2017
CL St Petersburg, FL
SP IEEE; AIAA; IEEE Aerosp Elect Syst Soc; AIAA Digital Avion Tech Comm;
   Boeing; MITRE Corp; Mark III Syst; Aldec; Cray; Rockwell Collins;
   Embry-Riddle Aeronaut Univ, Dept Elect Comp Software & Syst Engn
RI Bregon, Anibal/R-6700-2018; Diaz, Fernando/Y-6802-2019; Scarlatti, David/KVC-1552-2024; Martínez-Prieto, Miguel A/A-4891-2011; Esteban, Pedro/A-4446-2017
OI Martínez-Prieto, Miguel A/0000-0003-4418-561X; 
ZA 0
ZR 0
ZS 0
TC 9
ZB 0
Z8 0
Z9 11
U1 0
U2 2
SN 2155-7195
BN 978-1-5386-0365-9
DA 2017-01-01
UT WOS:000417412000041
ER

PT C
AU Tan, Yi-Fei
   Lam, Hai-Shuan
   Azlan, Asyraf
   Soo, Wooi-King
BE MizeraPietraszko, J
   Chung, YL
   Pichappan, P
TI Sentiment Analysis for Telco Popularity on Twitter Big Data Using a
   Novel Malaysian Dictionary
SO ADVANCES IN DIGITAL TECHNOLOGIES
SE Frontiers in Artificial Intelligence and Applications
VL 282
BP 112
EP 125
DI 10.3233/978-1-61499-637-8-112
DT Proceedings Paper
PD 2016
PY 2016
AB Malaysians are actively expressing feelings and opinions on social
   networks such as Twitter. These expressions can be harvested for
   studying the customer sentiments towards certain brands and preferences
   of customers. As business analytics becoming more important, sentiment
   analysis may provide crucial information in making customer-driven
   decisions. Therefore, accuracy is critical in determining the
   reliability and integrity of the analysis. Although, processing massive
   messages on social media is a huge challenge, it is now made easier by
   the advancement of the big data architecture. There are many techniques
   in interpreting these messages. However, Malaysians consists of people
   from very diversified backgrounds with a multitude of cultures and
   languages in daily use. Therefore, it is very common to find messages on
   social media with mixture of various local languages and slangs. The
   slangs used are mostly dialects expressed with alphabets. This project
   explores the techniques on analyzing the popularity of 5
   telecommunication companies in Malaysia and addresses the shortfalls of
   using the existing English sentiment dictionary. With the accomplishment
   of this project, a new localized dictionary is developed by compiling
   various mixtures of English, Malay localized sentiwords and slangs into
   the dictionary. The new dictionary is proven to capture and analyze 30%
   extra keywords on the Malaysian tweets sampled. These additional matches
   will improve the accuracy compared to existing dictionaries.
CT 7th International Conference on Applications of Digital Information and
   Web Technologies (ICADIWT)
CY MAR 29-31, 2016
CL Natl Taiwan Ocean Univ, Keelung, TAIWAN
HO Natl Taiwan Ocean Univ
SP Digital Informat Res Lab
RI Tan, Yi-Fei/GQR-1872-2022; Soo, Wooi King/AGW-6496-2022; AZIZ, AHMAD AZLAN/AAK-6182-2020
OI Tan, Yi-Fei/0000-0002-9030-1434; Soo, Wooi King/0000-0002-0295-230X; 
ZA 0
ZR 0
ZS 0
ZB 0
TC 11
Z8 0
Z9 11
U1 0
U2 15
SN 0922-6389
EI 1879-8314
BN 978-1-61499-637-8; 978-1-61499-636-1
DA 2016-11-09
UT WOS:000385790500013
ER

PT J
AU Park, Sun
   Yang, Chan-Su
   Kim, JongWon
TI Design of Vessel Data Lakehouse with Big Data and AI Analysis Technology
   for Vessel Monitoring System
SO ELECTRONICS
VL 12
IS 8
AR 1943
DI 10.3390/electronics12081943
DT Article
PD APR 2023
PY 2023
AB The amount of data in the maritime domain is rapidly increasing due to
   the increase in devices that can collect marine information, such as
   sensors, buoys, ships, and satellites. Maritime data is growing at an
   unprecedented rate, with terabytes of marine data being collected every
   month and petabytes of data already being made public. Heterogeneous
   marine data collected through various devices can be used in various
   fields such as environmental protection, defect prediction,
   transportation route optimization, and energy efficiency. However, it is
   difficult to manage vessel related data due to high heterogeneity of
   such marine big data. Additionally, due to the high heterogeneity of
   these data sources and some of the challenges associated with big data,
   such applications are still underdeveloped and fragmented. In this
   paper, we propose the Vessel Data Lakehouse architecture consisting of
   the Vessel Data Lake layer that can handle marine big data, the Vessel
   Data Warehouse layer that supports marine big data processing and AI,
   and the Vessel Application Services layer that supports marine
   application services. Our proposed a Vessel Data Lakehouse that can
   efficiently manage heterogeneous vessel related data. It can be
   integrated and managed at low cost by structuring various types of
   heterogeneous data using an open source-based big data framework. In
   addition, various types of vessel big data stored in the Data Lakehouse
   can be directly utilized in various types of vessel analysis services.
   In this paper, we present an actual use case of a vessel analysis
   service in a Vessel Data Lakehouse by using AIS data in Busan area.
OI PARK, SUN/0000-0002-7371-2661; Yang, Chan-Su/0000-0002-6882-7325
TC 7
ZS 0
Z8 0
ZR 0
ZA 0
ZB 0
Z9 10
U1 1
U2 30
EI 2079-9292
DA 2023-05-15
UT WOS:000978958700001
ER

PT C
AU Belcao, Matteo
   Falzone, Emanuele
   Bionda, Enea
   Della Valle, Emanuele
BE Hotho, A
   Blomqvist, E
   Dietze, S
   Fokoue, A
   Ding, Y
   Barnaghi, P
   Haller, A
   Dragoni, M
   Alani, H
TI Chimera: A Bridge Between Big Data Analytics and Semantic Technologies
SO SEMANTIC WEB - ISWC 2021
SE Lecture Notes in Computer Science
VL 12922
BP 463
EP 479
DI 10.1007/978-3-030-88361-4_27
DT Proceedings Paper
PD 2021
PY 2021
AB In the last decades, Knowledge Graph (KG) empowered analytics have been
   used to extract advanced insights from data. Several companies
   integrated legacy relational databases with semantic technologies using
   Ontology-Based Data Access (OBDA). In practice, this approach enables
   the analysts to write SPARQL queries both over KGs and SQL relational
   data sources by making transparent most of the implementation details.
   However, the volume of data is continuously increasing, and a growing
   number of companies are adopting distributed storage platforms and
   distributed computing engines. There is a gap between big data and
   semantic technologies. Ontop, one of the reference OBDA systems, is
   limited to legacy relational databases, and the compatibility with the
   big data analytics engine Apache Spark is still missing. This paper
   introduces Chimera, an open-source software suite that aims at filling
   such a gap. Chimera enables a new type of round-tripping data science
   pipelines. Data Scientists can query data stored in a data lake using
   SPARQL through Ontop and SparkSQL while saving the semantic results of
   such analysis back in the data lake. This new type of pipelines
   semantically enriches data from Spark before saving them back.
CT 20th International Semantic Web Conference (ISWC)
CY OCT 24-28, 2021
CL ELECTR NETWORK
SP IBM; Google; Metaphacts; Oracle; Franz Inc; ebay; Internet & Data Lab;
   imec
OI Belcao, Matteo/0009-0004-4771-1931; Bionda, Enea/0000-0001-7016-7269
TC 10
Z8 0
ZS 0
ZR 0
ZA 0
ZB 0
Z9 10
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-030-88361-4; 978-3-030-88360-7
DA 2021-10-29
UT WOS:000706991800027
ER

PT C
AU Priebe, Torsten
   Neumaier, Sebastian
   Markus, Stefan
BE Chen, Y
   Ludwig, H
   Tu, Y
   Fayyad, U
   Zhu, X
   Hu, X
   Byna, S
   Liu, X
   Zhang, J
   Pan, S
   Papalexakis, V
   Wang, J
   Cuzzocrea, A
   Ordonez, C
TI Finding Your Way Through the Jungle of Big Data Architectures
SO 2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
BP 5994
EP 5996
DI 10.1109/BigData52589.2021.9671862
DT Proceedings Paper
PD 2021
PY 2021
AB This paper presents a systematic review of common analytical data
   architectures based on DAMA-DMBOK and ArchiMate. The paper is work in
   progress and provides a first view on Gartner's Logical Data Warehouse
   paradigm, Data Fabric and Dehghani's Data Mesh proposal as well as their
   interdependencies. It furthermore sketches the way forward how this work
   can be extended by covering more architecture paradigms (incl. classic
   Data Warehouse, Data Vault, Data Lake, Lambda and Kappa architectures)
   and introducing a template with among others "context", "problem" and
   "solution" descriptions, leading ultimately to a pattern system
   providing guidance for choosing the right architecture paradigm for the
   right situation
CT 9th IEEE International Conference on Big Data (IEEE BigData)
CY DEC 15-18, 2021
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc; Ankura; Lyve Cloud, Seagate; NSF
OI Neumaier, Sebastian/0000-0002-9804-4882
ZS 0
ZA 0
ZR 0
Z8 0
TC 9
ZB 0
Z9 10
U1 1
U2 33
SN 2639-1589
BN 978-1-6654-3902-2
DA 2022-06-29
UT WOS:000800559506042
ER

PT C
AU Xu, Yong
   Yao, Wenjian
   Qu, Yuan
   Ma, Chao
GP ASSOC COMP MACHINERY
TI Design of Integrated Simulation Test System for Ship Platform
SO PROCEEDINGS OF 2021 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL
   INTELLIGENCE AND INFORMATION SYSTEMS (ICAIIS '21)
DI 10.1145/3469213.3470411
DT Proceedings Paper
PD 2021
PY 2021
AB Aiming at the problems of low expansion performance, slow data
   processing speed, and long post-processing time of the traditional
   equipment test appraisal simulation test platform, an integrated
   simulation test platform is designed based on the distributed big data
   architecture of the end-network-cloud integration. Through index system
   construction, test task management, data modeling and simulation and
   other technologies, an integrated simulation test platform is developed
   to support the testing of various devices, interfaces, and buses, and to
   provide real-time generation and collection and processing capabilities
   of massive test data. The actual application of the test platform in a
   certain type of equipment shows that the platform has good versatility,
   fast processing speed, provides support for more than 200 million test
   points, and real-time processing performance reaches the microsecond
   level.
CT 2nd International Conference on Artificial Intelligence and Information
   Systems (ICAIIS )
CY MAY 28-30, 2021
CL Chongqing, PEOPLES R CHINA
RI Ma, Chao/C-3124-2011
ZS 0
ZA 0
Z8 0
ZR 0
ZB 0
TC 10
Z9 10
U1 0
U2 3
BN 978-1-4503-9020-0
DA 2022-04-15
UT WOS:000770803700204
ER

PT C
AU Suriarachchi, Isuru
   Plale, Beth
BE Mattoso, M
   Glavic, B
TI Provenance as Essential Infrastructure for Data Lakes
SO PROVENANCE AND ANNOTATION OF DATA AND PROCESSES, IPAW 2016
SE Lecture Notes in Computer Science
VL 9672
BP 178
EP 182
DI 10.1007/978-3-319-40593-3_16
DT Proceedings Paper
PD 2016
PY 2016
AB The Data Lake is emerging as a Big Data storage and management solution
   which can store any type of data at scale and execute data
   transformations for analysis. Higher flexibility in storage increases
   the risk of Data Lakes becoming data swamps. In this paper we show how
   provenance contributes to data management within a Data Lake
   infrastructure. We study provenance integration challenges and propose a
   reference architecture for provenance usage in a Data Lake. Finally we
   discuss the applicability of our tools in the proposed architecture.
CT 6th International Provenance and Annotation Workshop (IPAW)
CY JUN 07-08, 2016
CL McLean, VA
SP MITRE Corp
RI Plale, Beth/F-8803-2011
OI Plale, Beth/0000-0003-2164-8132
Z8 0
ZB 0
ZR 0
TC 6
ZA 0
ZS 0
Z9 10
U1 0
U2 6
SN 0302-9743
EI 1611-3349
BN 978-3-319-40593-3; 978-3-319-40592-6
DA 2016-12-28
UT WOS:000389496000016
ER

PT C
AU Gollapudi, Sunila
BE Kankanhalli, MS
   Li, T
   Wang, W
TI Aggregating Financial Services Data without Assumptions A Semantic Data
   Reference Architecture
SO 2015 IEEE 9TH INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC)
SE IEEE International Conference on Semantic Computing
BP 312
EP 315
DT Proceedings Paper
PD 2015
PY 2015
AB We are seeing a sea change down the pike in terms of financial
   information aggregation and consumption; this could potentially be a
   game changer in financial services space with focus on ability to
   commoditize data. Financial Services Industry deals with a tremendous
   amount of data that varies in its structure, volume and purpose. The
   data is generated in the ecosystem (its customers, its own accounts,
   partner trades, securities transactions etc.), is handled by many
   systems - each having its own perspective. Front-office systems handle
   transactional behavior of the data, middle office systems which
   typically work with a drop-copy of the data subject it to intense
   processing, business logic, computations (such as inventory positions,
   fee calculations, commissions) and the back office systems deal with
   reconciliation, cleansing, exception management etc. Then there are the
   analytic systems which are concerned with auditing, compliance reporting
   as well as business analytics. Data that flows through this ecosystem
   gets aggregated, transformed, and transported time and again.
   Traditional approaches to managing such data leverage
   Extract-Transform-Load (ETL) technologies to set up data marts where
   each data mart serves a specific purpose (such as reconciliation or
   analytics). The result is proliferation of transformations and marts in
   the Organization. The need is to have architectures and IT systems that
   can aggregate data from many such sources without making any assumptions
   on HOW, WHERE or WHEN this data will be used. The incoming data is
   semantically annotated and stored in the triple store within storage
   tier and offers the ability to store, query and draw inferences using
   the ontology. There is a probable need for a Big Data Solution here that
   helps ease data liberation and co-location.
   This paper is a summary of one such business case of the Financial
   Services Industry where traditional ETL silos was broken to support the
   structurally dynamic, ever expanding and changing data usage needs
   employing Ontology and Semantic techniques like RDF/RDFS, SPARQL, OWL
   and related stack.
CT IEEE 9th International Conference on Semantic Computing
CY FEB 07-09, 2015
CL IEEE Computer Society, california, UNITED STATES
HO IEEE Computer Society
SP IEEE Systems; IEEE Computer Society
ZA 0
ZS 0
ZR 0
TC 7
Z8 0
ZB 0
Z9 10
U1 0
U2 3
SN 2325-6516
BN 978-1-4799-7935-6
DA 2016-09-13
UT WOS:000380479200055
ER

PT J
AU Pastor-Galindo, Javier
   Sandlin, Hong-An
   Marmol, Felix Gomez
   Bovet, Gerome
   Perez, Gregorio Martinez
TI A Big Data architecture for early identification and categorization of
   dark web sites
SO FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
VL 157
BP 67
EP 81
DI 10.1016/j.future.2024.03.025
EA MAR 2024
DT Article
PD AUG 2024
PY 2024
AB The dark web has become notorious for its association with illicit
   activities and there is a growing need for systems to automate the
   monitoring of this space. This paper proposes an end -to -end scalable
   architecture for the continuous early identification of new Tor sites
   and the daily analysis of their content. The solution is built using an
   Open Source Big Data stack for data serving with Kubernetes, Kafka,
   Kubeflow, and MinIO, continuously discovering onion addresses in
   different sources (threat intelligence, code repositories, web -Tor
   gateways, and Tor repositories), downloading the HTML from Tor and
   deduplicating the content using MinHash LSH, and categorizing with the
   BERTopic modeling (SBERT embedding, UMAP dimensionality reduction,
   HDBSCAN document clustering and c-TF-IDF topic keywords). In 93 days,
   the system identified 80,049 onion services and characterized 90% of
   them, addressing the challenge of Tor volatility. A disproportionate
   amount of repeated content is found, with only 6.1% unique sites. From
   the HTML files of the dark sites, 31 different low -topics are
   extracted, manually labeled, and grouped into 11 high-level topics. The
   five most popular included sexual and violent content, repositories and
   search engines, carding, cryptocurrencies, and marketplaces. During the
   experiments, we identified 14 sites with 13,946 clones that shared a
   suspiciously similar mirroring rate per day, suggesting an extensive
   common phishing network. Among the related works, this study is the most
   representative characterization of onion services based on topics to
   date.
RI Perez, Gregorio/I-7620-2013; Mármol, Félix/A-7505-2016
ZB 1
TC 8
ZS 0
ZA 0
ZR 0
Z8 0
Z9 9
U1 6
U2 19
SN 0167-739X
EI 1872-7115
DA 2024-05-19
UT WOS:001219503100001
ER

PT C
AU Levandoski, Justin
   Casto, Garrett
   Deng, Mingge
   Desai, Rushabh
   Edara, Pavan
   Hottelier, Thibaud
   Hormati, Amir
   Johnson, Anoop
   Johnson, Jeff
   Kurzyniec, Dawid
   McVeety, Sam
   Ramanathan, Prem
   Saxena, Gaurav
   Shanmugam, Vidya
   Volobuev, Yuri
GP ACM
TI BigLake: BigQuery's Evolution toward a Multi-Cloud Lakehouse
SO COMPANION OF THE 2024 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA,
   SIGMOD-COMPANION 2024
SE International Conference on Management of Data
BP 334
EP 346
DI 10.1145/3626246.3653388
DT Proceedings Paper
PD 2024
PY 2024
AB BigQuery's cloud-native disaggregated architecture has allowed Google
   Cloud to evolve the system to meet several customer needs across the
   analytics and AI/ML workload spectrum. A key customer requirement for
   BigQuery centers around the unification of data lake and enterprise data
   warehousing workloads. This approach combines: (1) the need for core
   data management primitives, e.g., security, governance, common runtime
   metadata, performance acceleration, ACID transactions, provided by an
   enterprise data warehouses coupled with (2) harnessing the flexibility
   of the open source format and analytics ecosystem along with new
   workload types such as AI/ML over unstructured data on object storage.
   In addition, there is a strong requirement to support BigQuery as a
   multi-cloud offering given cloud customers are opting for a multi-cloud
   footprint by default.
   This paper describes BigLake, an evolution of BigQuery toward a
   multi-cloud lakehouse to address these customer requirements in novel
   ways. We describe three main innovations in this space. We first present
   BigLake tables, making open-source table formats (e.g., Apache Parquet,
   Iceberg) first class citizens, providing fine-grained governance
   enforcement and performance acceleration over these formats to BigQuery
   and other open-source analytics engines. Next, we cover the design and
   implementation of BigLake Object tables that allow BigQuery to integrate
   AI/ML for inferencing and processing over unstructured data. Finally, we
   present Omni, a platform for deploying BigQuery on non-GCP clouds,
   focusing on the infrastructure and operational innovations we made to
   provide an enterprise lakehouse product regardless of the cloud provider
   hosting the data.
CT ACM International Conference on Management of Data (SIGMOD)
CY JUN 09-15, 2024
CL Santiago, CHILE
SP Assoc Comp Machinery; ACM SIGMOD
RI , Justin Levandoski/; Edara, Pavan/; Casto, Garrett/; Deng, Mingge/; , Anoop Johnson/; Hormati, Amir/; Desai, Rushabh/; Saxena, Gaurav/MHR-3400-2025; Johnson, Jeff Jeffrey/
OI , Justin Levandoski/0009-0005-7033-0528; Edara,
   Pavan/0009-0001-8943-140X; Casto, Garrett/0009-0008-8808-1779; Deng,
   Mingge/0009-0005-0497-8203; , Anoop Johnson/0009-0006-0891-0166;
   Hormati, Amir/0009-0002-5786-3301; Desai, Rushabh/0009-0001-7406-8433;
   Johnson, Jeff Jeffrey/0009-0009-9657-3121
Z8 0
ZA 0
ZR 0
ZS 0
ZB 0
TC 6
Z9 9
U1 3
U2 6
SN 0730-8078
BN 979-8-4007-0422-2
DA 2024-08-29
UT WOS:001267334100029
ER

PT J
AU Paolucci, Francesco
   Sgambelluri, Andrea
   Silva, Moises Felipe
   Pacini, Alessandro
   Castoldi, Piero
   Valcarenghi, Luca
   Cugini, Filippo
TI Peer-to-peer disaggregated telemetry for autonomic
   machine-learning-driven transceiver operation
SO JOURNAL OF OPTICAL COMMUNICATIONS AND NETWORKING
VL 14
IS 8
BP 606
EP 620
DI 10.1364/JOCN.456666
DT Article
PD AUG 2022
PY 2022
AB Autonomic networking and monitoring will drive the evolution of next
   generation software defined networking (SDN) optical networks towards
   the zero touch networking paradigm. Optical telemetry services will play
   a key role to enable advanced network awareness at device and component
   granularity. Optical disaggregation is pushing the adoption of open
   models, enabling multi-vendor interoperability, including telemetry.
   Moreover, due to whitebox programmability and the adoption of open
   source micro services, it is becoming feasible to monitor data streams
   from optical devices related to quality of transmission key performance
   indicators. Finally, due to mature big data analytics platforms,
   including machine learning and artificial intelligence, the telemetry
   data lake is processed to effectively detect network anomalies. However,
   current centralized telemetry architectures are prone to scalability
   issues, suboptimal soft failure recovery due to operational mode
   limitations, and/or the inability of the SDN controller of tuning finer
   or proprietary transmission parameters. Conversely, a number of soft
   failures might be detected and recovered directly at the optical card
   transmitter, often in a hitless fashion, also relying on optimized
   vendor-proprietary configurations. The paper proposes what we believe to
   be a novel peer-to-peer telemetry (P2PT) service ready for next
   generation digital coherent optics cards, for local processing and soft
   failure recovery at the transceiver agent level. The P2PT architecture,
   workflow, and subscription extensions are conceived to enable direct and
   fast recovery at the transceiver level, resorting to optical signal
   retuning and adaptations. Experimental evaluations, including
   lightweight machine learning detection at the card agent, are provided
   in a multi-vendor disaggregated optical network testbed to assess
   different soft failure use cases and P2PT service scalability. (C) 2022
   Optica Publishing Group
RI Cugini, Filippo/; Valcarenghi, Luca/ABG-4647-2020; Sgambelluri, Anea/; Pacini, Alessano/; Paolucci, Francesco/AAA-3284-2022; Castoldi, Piero/A-2169-2012
OI Cugini, Filippo/0000-0002-9840-0365; Sgambelluri,
   Anea/0000-0001-7435-0458; Pacini, Alessano/0000-0001-5092-6061;
   Paolucci, Francesco/0000-0003-4821-5193; 
ZR 0
ZB 0
Z8 0
ZA 0
ZS 0
TC 9
Z9 9
U1 0
U2 9
SN 1943-0620
EI 1943-0639
DA 2022-07-13
UT WOS:000821580200001
ER

PT J
AU Che, Haoyang
   Duan, Yucong
TI On the Logical Design of a Prototypical Data Lake System for Biological
   Resources
SO FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY
VL 8
AR 553904
DI 10.3389/fbioe.2020.553904
DT Article
PD SEP 29 2020
PY 2020
AB Biological resources are multifarious encompassing organisms, genetic
   materials, populations, or any other biotic components of ecosystems,
   and fine-grained data management and processing of these diverse types
   of resources proposes a tremendous challenge for both researchers and
   practitioners. Before the conceptualization of data lakes, former big
   data management platforms in the research fields of computational
   biology and biomedicine could not deal with many practical data
   management tasks very well. As an effective complement to those previous
   systems, data lakes were devised to store voluminous, varied, and
   diversely structured or unstructured data in their native formats, for
   the sake of various analyses like reporting, modeling, data exploration,
   knowledge discovery, data visualization, advanced analysis, and machine
   learning. Due to their intrinsic traits, data lakes are thought to be
   ideal technologies for processing of hybrid biological resources in the
   format of text, image, audio, video, and structured tabular data. This
   paper proposes a method for constructing a practical data lake system
   for processing multimodal biological data using a prototype system named
   ProtoDLS, especially from the explainability point of view, which is
   indispensable to the rigor, transparency, persuasiveness, and
   trustworthiness of the applications in the field. ProtoDLS adopts a
   horizontal pipeline to ensure the intra-component explainability factors
   from data acquisition to data presentation, and a vertical pipeline to
   ensure the inner-component explainability factors including mathematics,
   algorithm, execution time, memory consumption, network latency,
   security, and sampling size. The dual mechanism can ensure the
   explainability guarantees on the entirety of the data lake system.
   ProtoDLS proves that a single point of explainability cannot thoroughly
   expound the cause and effect of the matter from an overall perspective,
   and adopting a systematic, dynamic, and multisided way of thinking and a
   system-oriented analysis method is critical when designing a data
   processing system for biological resources.
RI che, haoyang/KIK-8325-2024
ZR 0
Z8 0
ZA 0
TC 7
ZB 3
ZS 0
Z9 9
U1 0
U2 15
SN 2296-4185
DA 2020-10-28
UT WOS:000578726100001
PM 33117777
ER

PT C
AU Salierno, Giulio
   Morvillo, Sabatino
   Leonardi, Letizia
   Cabri, Giacomo
BE DupuyChessa, S
   Proper, HA
TI An Architecture for Predictive Maintenance of Railway Points Based on
   Big Data Analytics
SO ADVANCED INFORMATION SYSTEMS ENGINEERING WORKSHOPS
SE Lecture Notes in Business Information Processing
VL 382
BP 29
EP 40
DI 10.1007/978-3-030-49165-9_3
DT Proceedings Paper
PD 2020
PY 2020
AB Massive amounts of data produced by railway systems are a valuable
   resource to enable Big Data analytics. Despite its richness, several
   challenges arise when dealing with the deployment of a big data
   architecture into a railway system. In this paper, we propose a
   four-layers big data architecture with the goal of establishing a data
   management policy to manage massive amounts of data produced by railway
   switch points and perform analytical tasks efficiently. An
   implementation of the architecture is given along with the realization
   of a Long Short-Term Memory prediction model for detecting failures on
   the Italian Railway Line of Milano - Monza - Chiasso.
CT 32nd International Conference on Advanced Information Systems
   Engineering (CAiSE)
CY JUN 08-12, 2020
CL ELECTR NETWORK
RI Salierno, Giulio/; Leonardi, Letizia/L-9722-2015; Cabri, Giacomo/M-6723-2015
OI Salierno, Giulio/0000-0002-9617-4448; Leonardi,
   Letizia/0000-0003-4035-8560; Cabri, Giacomo/0000-0002-4942-2453
ZS 0
ZB 0
ZR 0
ZA 0
TC 8
Z8 0
Z9 9
U1 1
U2 11
SN 1865-1348
EI 1865-1356
BN 978-3-030-49165-9; 978-3-030-49164-2
DA 2021-12-10
UT WOS:000724144400003
ER

PT J
AU Thorogood, Adrian
TI Policy-aware data lakes: a flexible approach to achieve legal
   interoperability for global research collaborations
SO JOURNAL OF LAW AND THE BIOSCIENCES
VL 7
IS 1
AR lsaa065
DI 10.1093/jlb/lsaa065
DT Article
PD JAN-JUN 2020
PY 2020
AB A popular model for global scientific repositories is the data commons,
   which pools or connectsmany datasets alongside supporting
   infrastructure. A data commons must establish legally interoperability
   between datasets to ensure researchers can aggregate and reuse them.
   This is usually achieved by establishing a shared governance structure.
   Unfortunately, governance often takes years to negotiate and involves a
   trade-off between data inclusion and data availability. It can also be
   difficult for repositories tomodify governance structures in response to
   changing scientific priorities, data sharing practices, or legal
   frameworks. This problem has been laid bare by the sudden shock of
   theCOVID-19 pandemic. This paper proposes a rapid and flexible strategy
   for scientific repositories to achieve legal interoperability: the
   policy-aware data lake. This strategy draws on technical concepts of
   modularity, metadata, and data lakes. Datasets are treated as
   independent modules, which can be subject to distinctive legal
   requirements. Each module must, however, be described using standard
   legal metadata. This allows legally compatible datasets to be rapidly
   combined and made available on a just-in-time basis to certain
   researchers for certain purposes. Global scientific repositories
   increasingly need such flexibility tomanage scientific, organizational,
   and legal complexity, and to improve their responsiveness to global
   pandemics.
OI Thorogood, Aian/0000-0001-5078-8164
TC 8
ZB 4
ZS 0
ZR 0
Z8 0
ZA 0
Z9 9
U1 0
U2 15
SN 2053-9711
DA 2021-02-25
UT WOS:000612916400044
PM 33005429
ER

PT J
AU Wang, Yanan
TI A comprehensive evaluation system of teaching quality based on big data
   architecture
SO INTERNATIONAL JOURNAL OF CONTINUING ENGINEERING EDUCATION AND LIFE-LONG
   LEARNING
VL 30
IS 2
SI SI
BP 176
EP 189
DT Article
PD 2020
PY 2020
AB Aiming at the problems of long response time and high error rate in
   traditional teaching quality comprehensive evaluation system, a teaching
   quality comprehensive evaluation system based on big data structure is
   proposed. Firstly, different evaluation indicators are designed, the
   principle of teaching quality comprehensive evaluation is designed by
   using data dictionary, the requirement analysis of teaching quality
   comprehensive evaluation system is realised by using UML use case model,
   then the overall framework of the system is designed, and the
   comprehensive evaluation system of teaching quality is developed and
   studied by using Asp.net related technology. The experimental results
   show that the response time of the designed system is shorter than that
   of the traditional teaching quality comprehensive evaluation system, and
   the fitting degree between the evaluation results and the actual
   situation is higher, which fully proves the superiority of the designed
   system.
ZB 2
ZR 0
Z8 0
ZS 0
ZA 0
TC 7
Z9 9
U1 0
U2 9
SN 1560-4624
EI 1741-5055
DA 2020-04-17
UT WOS:000523695600007
ER

PT C
AU Carnein, Matthias
   Heuchert, Markus
   Homann, Leschek
   Trautmann, Heike
   Vossen, Gottfried
   Becker, Jorg
   Kraume, Karsten
BE DeCesare, S
   Frank, U
TI Towards Efficient and Informative Omni-Channel Customer Relationship
   Management
SO ADVANCES IN CONCEPTUAL MODELING, ER 2017
SE Lecture Notes in Computer Science
VL 10651
BP 69
EP 78
DI 10.1007/978-3-319-70625-2_7
DT Proceedings Paper
PD 2017
PY 2017
AB Nowadays customers expect a seamless interaction with companies
   throughout all available communication channels. However, many companies
   rely on different software solutions to handle each channel, which leads
   to heterogeneous IT infrastructures and isolated data sources.
   Omni-Channel CRM is a holistic approach towards a unified view on the
   customer across all channels. This paper introduces three case studies
   which demonstrate challenges of omni-channel CRM and the value it can
   provide. The first case study shows how to integrate and visualise data
   from different sources which can support operational and strategic
   decision. In the second case study, a social media analysis approach is
   discussed which provides benefits by offering reports of service
   performance across channels. The third case study applies customer
   segmentation to an online fashion retailer in order to identify customer
   profiles.
CT 36th International Conference on Conceptual Modeling (ER)
CY NOV 06-09, 2017
CL Valencia, SPAIN
SP Generalitat Valencia; Project Performance Int; Springer; Univ
   Politecnica Valencia; PROS
RI Becker, Jörg/AAC-7913-2019; Trautmann, Heike/
OI Becker, Jörg/0000-0001-5690-439X; Trautmann, Heike/0000-0002-9788-8282
ZA 0
TC 8
ZB 0
ZR 0
Z8 0
ZS 0
Z9 9
U1 0
U2 4
SN 0302-9743
EI 1611-3349
BN 978-3-319-70625-2; 978-3-319-70624-5
DA 2017-01-01
UT WOS:000537752500007
ER

PT C
AU Haroun, Amir
   Mostefaoui, Ahmed
   Dessables, Francois
GP IEEE
TI A Big Data Architecture for Automotive Applications: PSA Group
   Deployment Experience
SO 2017 17TH IEEE/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER, CLOUD AND GRID
   COMPUTING (CCGRID)
SE IEEE-ACM International Symposium on Cluster Cloud and Grid Computing
BP 921
EP 928
DI 10.1109/CCGRID.2017.107
DT Proceedings Paper
PD 2017
PY 2017
AB Vehicles have become moving sensor platforms collecting huge volumes of
   data from their various embedded sensors. This data has a great value
   for automotive manufacturers and vehicles owners. Indeed, connected
   vehicles data can be used in a large broad of automotive services
   ranging from safety services to well-being services (e.g. fatigue
   detection). However, vehicle fleets send big volumes of data that
   traditional computing and storage approaches are not able to manage
   efficiently. In this paper, we present the experience of the PSA
   Group(1) on leveraging big data in automotive context. We describe in
   depth the big data architecture deployed within the PSA Group and the
   underlaying technologies/products used in each component.
CT 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid
   Computing (CCGRID)
CY MAY 14-17, 2017
CL Madrid, SPAIN
SP IEEE; Assoc Comp Machinery; IEEE Comp Soc; Mellanox Technologies; Univ
   Carlos III Madrid; ARCOS; IEEE TCSC
ZA 0
ZS 0
Z8 0
TC 9
ZB 0
ZR 0
Z9 9
U1 0
U2 7
SN 2376-4414
BN 978-1-5090-6611-7
DA 2018-03-27
UT WOS:000426912900116
ER

PT C
AU Petalas, Yannis G.
   Ammari, Ahmad
   Georgakis, Panos
   Nwagboso, Chris
BE Sellis, T
   Oikonomou, K
TI A Big Data Architecture for Traffic Forecasting Using Multi-Source
   Information
SO ALGORITHMIC ASPECTS OF CLOUD COMPUTING, ALGOCLOUD 2016
SE Lecture Notes in Computer Science
VL 10230
BP 65
EP 83
DI 10.1007/978-3-319-57045-7_5
DT Proceedings Paper
PD 2017
PY 2017
AB An important strand of predictive analytics for transport related
   applications is traffic forecasting. Accurate approximations of the
   state of transport networks in short, medium or long-term future
   horizons can be used for supporting traveller information, or traffic
   management systems. Traffic forecasting has been the focus of many
   researchers over the last two decades. Most of the existing works, focus
   on single point, corridor, or intersection based predictions with
   limited efforts to solutions that cover large metropolitan areas. In
   this work, an open big-data architecture for road traffic prediction in
   large metropolitan areas is proposed. The functional characteristics of
   the architecture, that allows processing of data from various sources,
   such as urban and inter-urban traffic data streams and social media, is
   investigated. Furthermore, its conceptual design using state-of-the-art
   computing technologies is realised.
CT 2nd International Workshop on Algorithmic Aspects of Cloud Computing
   (ALGOCLOUD)
CY AUG 22, 2016
CL Aarhus Univ, Aarhus, DENMARK
HO Aarhus Univ
SP Danish Natl Res Fdn Ctr MADALGO
RI Ammari, Ahmed Chiheb/I-6169-2013; Georgakis, Panagiotis/
OI Ammari, Ahmed Chiheb/0000-0002-9939-1624; Georgakis,
   Panagiotis/0000-0002-3200-997X
ZA 0
ZS 0
ZB 0
TC 8
Z8 0
ZR 0
Z9 9
U1 0
U2 12
SN 0302-9743
EI 1611-3349
BN 978-3-319-57045-7; 978-3-319-57044-0
DA 2018-12-28
UT WOS:000418851500005
ER

PT C
AU Udupi, Prakash Kumar
   Sharma, Nisha
   Jha, S. K.
BE Shukla, B
   Khatri, SK
   Kapur, PK
TI Educational Data Mining and Big Data Framework for e-Learning
   Environment
SO 2016 5TH INTERNATIONAL CONFERENCE ON RELIABILITY, INFOCOM TECHNOLOGIES
   AND OPTIMIZATION (TRENDS AND FUTURE DIRECTIONS) (ICRITO)
SE International Conference on Reliability Infocom Technologies and
   Optimization Trends and Future Directions
BP 258
EP 261
DT Proceedings Paper
PD 2016
PY 2016
AB E-learning data consists of large volume of educational data and
   available with complex and hybrid data architecture. Capturing of
   student performances, student evaluation and student's interaction
   information are one of the challenges faced by the e-learning software
   users at the time of analysis. Integrating student data along with
   educational data for analysis needs complex system design framework. New
   innovations in e-learning also facilitates augmented learning, adaptive
   learning, web based learning, activity based learning, and project based
   learning. Education technology interventions using learning management
   system, content management system, advanced distributed learning;
   sharable content object reference models and application program
   interfaces enhanced and extended the e-learning frameworks to a greater
   horizon. Present technology also ensures transformations of e-learning
   information without any geographical barriers. These educational and
   student or user data combined together forms big data architecture under
   e-learning environment and mining these big data for various
   requirements or knowledge discoveries needs innovative approaches. This
   paper identifies and evaluates various e-learning models and associated
   education technology paradigm. The research further explores and
   proposes a new framework for big data integrations. The paper also
   discusses the scope of future research on data mining and role of big
   data in e-learning environment.
CT 5th International Conference on Reliability, Infocom Technologies and
   Optimization (Trends and Future Directions) (ICRITO)
CY SEP 07-09, 2016
CL Amity Univ, Noida, INDIA
HO Amity Univ
SP Amity Univ, Amity Inst Informat Technol; IEEE UP Sect
RI Jha, Shambhu Kumar/AAA-5166-2022; JHA, SHAMBHU/AAA-5166-2022
OI Jha, Shambhu Kumar/0000-0003-0811-9666; JHA, SHAMBHU/0000-0001-8480-5647
ZS 0
ZB 0
TC 8
Z8 0
ZR 0
ZA 0
Z9 9
U1 0
U2 15
SN 2469-875X
BN 978-1-5090-1489-7
DA 2016-01-01
UT WOS:000391546800052
ER

PT J
AU Jan, Mian Ahmad
   Adil, Muhammad
   Brik, Bouziane
   Harous, Saad
   Abbas, Sohail
TI Making Sense of Big Data in Intelligent Transportation Systems: Current
   Trends, Challenges and Future Directions
SO ACM COMPUTING SURVEYS
VL 57
IS 8
AR 197
DI 10.1145/3716371
DT Article
PD AUG 2025
PY 2025
AB Intelligent Transportation Systems (ITS) generate massive amounts of Big
   Data through both sensory and non-sensory platforms. The data support
   batch processing as well as stream processing, which are essential for
   reliable operations on the roads and connected vehicles in ITS. Despite
   the immense potential of Big Data intelligence in ITS, autonomous
   vehicles are largely confined to testing and trial phases. The research
   community is working tirelessly to improve the reliability of ITS by
   designing new protocols, standards, and connectivity paradigms. In the
   recent past, several surveys have been conducted that focus on Big Data
   Intelligence for ITS, yet none of them have comprehensively addressed
   the fundamental challenges hindering the widespread adoption of
   autonomous vehicles on the roads. Our survey aims to help readers better
   understand the technological advancements by delving deep into Big Data
   architecture, focusing on data acquisition, data storage, and data
   visualization. We reviewed sensory and non-sensory platforms for data
   acquisition, data storage repositories for archival and retrieval of
   large datasets, and data visualization for presenting the processed data
   in an interactive and comprehensible format. To this end, we discussed
   the current research progress by comprehensively covering the literature
   and highlighting challenges that urgently require the attention of the
   research community. Based on the concluding remarks, we argued that
   these challenges hinder the widespread presence of autonomous vehicles
   on the roads. Understanding these challenges is important for a more
   informed discussion on the future of self-driven technology. Moreover,
   we acknowledge that these challenges not only affect individual layers
   but also impact the functionality of subsequent layers. Finally, we
   outline our future work that explores how resolving these challenges
   could enable the realization of innovations such as smart charging
   systems on the roads and data centers on wheels.
RI Abbas, Sohail/HJY-5016-2023; Jan, Mian Ahmad/AAG-8261-2019; , Muhammad Adil/AAB-5891-2020; BRIK, Bouziane/AAC-9626-2019; Harous, Saad/AAU-6859-2020
OI Abbas, Sohail/0000-0002-2287-3801; Jan, Mian Ahmad/0000-0002-5298-1328;
   , Muhammad Adil/0000-0003-4494-8576; Harous, Saad/0000-0001-6524-7352
Z8 0
ZS 0
ZB 0
TC 8
ZA 0
ZR 0
Z9 8
U1 8
U2 14
SN 0360-0300
EI 1557-7341
DA 2025-06-12
UT WOS:001503903000004
ER

PT J
AU Azzabi, Sarah
   Alfughi, Zakiya
   Ouda, Abdelkader
TI Data Lakes: A Survey of Concepts and Architectures
SO COMPUTERS
VL 13
IS 7
AR 183
DI 10.3390/computers13070183
DT Review
PD JUL 2024
PY 2024
AB This paper presents a comprehensive literature review on the evolution
   of data-lake technology, with a particular focus on data-lake
   architectures. By systematically examining the existing body of
   research, we identify and classify the major types of data-lake
   architectures that have been proposed and implemented over time. The
   review highlights key trends in the development of data-lake
   architectures, identifies the primary challenges faced in their
   implementation, and discusses future directions for research and
   practice in this rapidly evolving field. We have developed diagrammatic
   representations to highlight the evolution of various architectures.
   These diagrams use consistent notations across all architectures to
   further enhance the comparative analysis of the different architectural
   components. We also explore the differences between data warehouses and
   data lakes. Our findings provide valuable insights for researchers and
   practitioners seeking to understand the current state of data-lake
   technology and its potential future trajectory.
OI Alfughi, Zakiya/0009-0008-7083-9692; Ouda,
   Abdelkader/0000-0003-3882-1763; Azzabi, Sarah/0009-0005-6421-5582
ZS 0
ZB 0
TC 7
ZR 0
Z8 1
ZA 0
Z9 8
U1 3
U2 18
SN 2073-431X
DA 2024-08-02
UT WOS:001276702400001
ER

PT J
AU Han, Shuyang
   Zhang, Yichi
   Wang, Jiajun
   Tong, Dawei
   Lyu, Mingming
TI Graph neural network-based topological relationships automatic
   identification of geological boundaries
SO COMPUTERS & GEOSCIENCES
VL 188
AR 105621
DI 10.1016/j.cageo.2024.105621
EA MAY 2024
DT Article
PD JUN 2024
PY 2024
AB Topological relationship identification of geological boundaries greatly
   affects 3D geological modeling efficiency. However, due to the complex
   spatial distribution and unstructured data architecture of geological
   boundaries, the current topological identification still mainly depends
   on the manual operation and interpretation, which is time-consuming and
   labor-intensive. And geologists' judgments are based on knowledge and
   experience that are challenging to replicate through conventional
   algorithmic approaches. To address these challenges, a graph neural
   network-based geological boundary topological relationships automatic
   identification method is proposed. Based on graph theory, we
   conceptualize geological boundaries and their topological relationships
   as a graph transforming the identification task into an edge prediction
   problem within this topological graph. We employ the variational graph
   auto-encoder (VGAE) model to predict unknown edges in the topological
   graph, enabling automatic identification of geological boundary
   topological relationships. A geological information transformation
   method is proposed to extract and encode the geological boundaries'
   features as inputs for VGAE. The proposed method is verified in an
   engineering case study. The results reveal that this method can
   efficiently and accurately analyze the geological boundary topological
   relationships. Moreover, it can greatly reduce the manual workload,
   requiring only 30% of the manual effort to obtain the geological
   boundary topological relationships, while achieving an average
   prediction accuracy of approximately 95%.
RI Han, Shuyang/A-5239-2019; Staubach, Asher/
OI Staubach, Asher/0009-0005-9348-5522
ZR 0
ZA 0
TC 8
Z8 0
ZB 0
ZS 0
Z9 8
U1 6
U2 60
SN 0098-3004
EI 1873-7803
DA 2024-06-17
UT WOS:001243995900001
ER

PT J
AU Eltabakh, Mohamed Y.
   Kunjir, Mayuresh
   Elmagarmid, Ahmed K.
   Ahmad, Mohammad Shahmeer
TI Cross Modal Data Discovery over Structured and Unstructured Data Lakes
SO PROCEEDINGS OF THE VLDB ENDOWMENT
VL 16
IS 11
BP 3377
EP 3390
DI 10.14778/3611479.3611533
DT Article; Proceedings Paper
PD JUL 2023
PY 2023
AB Organizations are collecting increasingly large amounts of data for
   data-driven decision making. These data are often dumped into a
   centralized repository, e.g., a data lake, consisting of thousands of
   structured and unstructured datasets. Perversely, such mixture makes the
   problem of discovering tables or documents that are relevant to a user's
   query very challenging. Despite the recent efforts in data discovery,
   the problem remains widely open especially in the two fronts of (1)
   discovering relationships and relatedness across structured and
   unstructured datasets-where existing techniques suffer from either
   scalability, being customized for a specific problem type (e.g., entity
   matching or data integration), or demolishing the structural properties
   on its way, and (2) developing a holistic system for integrating various
   similarity measurements and sketches in an effective way to boost the
   discovery accuracy.
   In this paper, we propose a new data discovery system, named CMDL, for
   addressing these two limitations. CMDL supports the data discovery
   process over both structured and unstructured data while retaining the
   structural properties of tables. As a result, CMDL is the only system to
   date that empowers end-users to seamlessly pipeline the discovery tasks
   across the two modalities. We propose a novel multi-modal embedding
   representation that captures the similarities between text documents and
   tabular columns. The model training relies on labeled datasets generated
   though weak supervision, and thus the system is domain agnostic and
   easily generalizable. We evaluate CMDL on three real-world data lakes
   with diverse applications and show that our system is significantly more
   effective for cross-modality discovery compared to the search-based
   baseline techniques. Moreover, CMDL is more accurate and robust to
   different data types and distributions compared to the state-of-the-art
   systems that are limited to only the structured datasets.
CT 49th International Conference on Very Large Data Bases (VLDB)
CY AUG 28-SEP 01, 2023
CL Vancouver, CANADA
TC 6
Z8 0
ZB 0
ZS 0
ZA 0
ZR 0
Z9 8
U1 0
U2 7
SN 2150-8097
DA 2023-10-12
UT WOS:001059181900055
ER

PT J
AU Mostefaoui, Ahmed
   Merzoug, Mohammed Amine
   Haroun, Amir
   Nassar, Anthony
   Dessables, Francois
TI Big data architecture for connected vehicles: Feedback and application
   examples from an automotive group
SO FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
VL 134
BP 374
EP 387
DI 10.1016/j.future.2022.04.020
EA MAY 2022
DT Article
PD SEP 2022
PY 2022
AB Nowadays, using their onboard built-in sensors and communication
   devices, connected vehicles (CVs) can perform numerous measurements
   (speed, temperature, fuel consumption, etc.) and transmit them, in a
   real-time fashion, to dedicated infrastructure, usually via 4G/5G
   wireless communications. This raises many opportunities to develop new
   innovative telematics services, including, among others, driver safety,
   customer experience, location-based services and infotainment. Indeed,
   it is expected that there will be roughly 2 billion connected cars by
   the end of 2025 on the world's roadways, where each of which can produce
   up to 30 terabytes of data per day. Managing this big automotive data,
   in real and batch modes, imposes tight constraints on the underlying
   data management platform. To contribute to this research area, in this
   paper, we report on a real, in-production automotive big data platform;
   specifically, the one deployed by Groupe PSA (a French car manufacturer
   known also as Peugeot-Citroen). In particular, we present the
   technologies and open-source products used within the different
   components of this CV platform to gather, store, process, and leverage
   big automotive data. The proposed architecture is then assessed through
   realistic experiments, and the obtained results are reported and
   analyzed. Finally, we also provide examples of deployed automotive
   applications and reveal the implementation details of one of them (an
   eco-driving service). (c) 2022 Elsevier B.V. All rights reserved.
RI Mostefaoui, Ahmed/; Merzoug, Mohammed Amine/AAI-1281-2020
OI Mostefaoui, Ahmed/0000-0001-8863-4080; Merzoug, Mohammed
   Amine/0000-0002-5316-6456
ZA 0
ZR 0
TC 7
ZB 0
Z8 0
ZS 1
Z9 8
U1 2
U2 21
SN 0167-739X
EI 1872-7115
DA 2022-06-18
UT WOS:000808123100001
ER

PT J
AU Barradas, Adrian
   Tejeda-Gil, Acela
   Canton-Croda, Rosa-Maria
TI Real-Time Big Data Architecture for Processing Cryptocurrency and Social
   Media Data: A Clustering Approach Based on k-Means
SO ALGORITHMS
VL 15
IS 5
AR 140
DI 10.3390/a15050140
DT Article
PD MAY 2022
PY 2022
AB Cryptocurrencies have recently emerged as financial assets that allow
   their users to execute transactions in a decentralized manner. Their
   popularity has led to the generation of huge amounts of data,
   specifically on social media networks such as Twitter. In this study, we
   propose an iterative kappa architecture that collects, processes, and
   temporarily stores data regarding transactions and tweets of two of the
   major cryptocurrencies according to their market capitalization: Bitcoin
   (BTC) and Ethereum (ETH). We applied a k-means clustering approach to
   group data according to their principal characteristics. Data are
   categorized into three groups: BTC typical data, ETH typical data, BTC
   and ETH atypical data. Findings show that activity on Twitter correlates
   to activity regarding the transactions of cryptocurrencies. It was also
   found that around 14% of data relate to extraordinary behaviors
   regarding cryptocurrencies. These data contain higher transaction
   volumes of both cryptocurrencies, and about 9.5% more social media
   publications in comparison with the rest of the data. The main
   advantages of the proposed architecture are its flexibility and its
   ability to relate data from various datasets.
RI Cantón Croda, Rosa María/Y-4885-2019; Barradas, Aian/; Tejeda, Acela/
OI Cantón Croda, Rosa María/0000-0002-5469-8964; Barradas,
   Aian/0000-0002-5477-2976; Tejeda, Acela/0000-0002-8972-1642
ZS 1
ZB 0
ZR 0
Z8 0
ZA 0
TC 6
Z9 8
U1 1
U2 7
EI 1999-4893
DA 2022-06-08
UT WOS:000802593100001
ER

PT J
AU Groeger, Christoph
TI Industrial analytics - An overview
SO IT-INFORMATION TECHNOLOGY
VL 64
IS 1-2
BP 55
EP 65
DI 10.1515/itit-2021-0066
EA JAN 2022
DT Article
PD APR 26 2022
PY 2022
AB The digital transformation generates huge amounts of heterogeneous data
   across the industrial value chain, from simulation data in engineering,
   over sensor data in manufacturing to telemetry data on product use.
   Extracting insights from these data constitutes a critical success
   factor for industrial enterprises, e. g., to optimize processes and
   enhance product features. This is referred to as industrial analytics,
   i. e., data analytics for industrial value creation. Industrial
   analytics is an interdisciplinary subject area between data science and
   industrial engineering and is at the core of Industry 4.0. Yet, existing
   literature on industrial analytics is fragmented and specialized. To
   address this issue, this paper presents a holistic overview of the field
   of industrial analytics integrating both current research as well as
   industry experiences on real-world industrial analytics projects. We
   define key terms, describe typical use cases and discuss characteristics
   of industrial analytics. Moreover, we present a conceptual framework for
   industrial analytics that structures essential elements, e. g., data
   platforms and data roles. Finally, we conclude and highlight future
   research directions.
ZA 0
Z8 0
ZS 0
ZR 0
ZB 0
TC 5
Z9 8
U1 3
U2 21
SN 1611-2776
EI 2196-7032
DA 2022-03-01
UT WOS:000753899000001
ER

PT C
AU Vestues, Kathrine
   Hanssen, Geir Kjetil
   Mikalsen, Marius
   Buan, Thor Aleksander
   Conboy, Kieran
BE Stray, V
   Stol, KJ
   Paasivaara, M
   Kruchten, P
TI Agile Data Management in NAV: A Case Study
SO AGILE PROCESSES IN SOFTWARE ENGINEERING AND EXTREME PROGRAMMING, XP 2022
SE Lecture Notes in Business Information Processing
VL 445
BP 220
EP 235
DI 10.1007/978-3-031-08169-9_14
DT Proceedings Paper
PD 2022
PY 2022
AB To satisfy the need for analytical data in the development of digital
   services, many organizations use data warehouse, and, more recently,
   data lake architectures. These architectures have traditionally been
   accompanied by centralized organizational models, where a single team or
   department has been responsible for gathering, transforming, and giving
   access to analytical data. However, such centralized models presuppose
   stability and are incompatible with agile software development where
   applications and databases are continuously updated. To achieve more
   agile forms of data management, some organizations have therefore begun
   to experiment with distributed data management models such as "data
   meshes". Research on this topic is however limited. In this paper, we
   report findings from a case study of a public sector organization in
   Norway that has begun the transition from centralized to distributed
   data management, outlining both the benefits and challenges of a
   distributed approach.
CT 23rd International Conference on Agile Software Development (XP)
CY JUN 13-17, 2022
CL IT Univ Copenhagen, Copenhagen, DENMARK
HO IT Univ Copenhagen
SP Agile Alliance
RI Mikalsen, Marius/AAR-1025-2020
TC 8
ZR 0
ZA 0
ZB 0
Z8 0
ZS 0
Z9 8
U1 1
U2 23
SN 1865-1348
EI 1865-1356
BN 978-3-031-08169-9; 978-3-031-08168-2
DA 2022-11-09
UT WOS:000873597400014
ER

PT J
AU Jurado Perez, Luis
   Salvachua, Joaquin
TI An Approach to Build e-Health IoT Reactive Multi-Services Based on
   Technologies around Cloud Computing for Elderly Care in Smart City Homes
SO APPLIED SCIENCES-BASEL
VL 11
IS 11
AR 5172
DI 10.3390/app11115172
DT Article
PD JUN 2021
PY 2021
AB Although there are e-health systems for the care of elderly people, the
   reactive characteristics to enhance scalability and extensibility, and
   the use of this type of system in smart cities, have been little
   explored. To date, some studies have presented healthcare systems for
   specific purposes without an explicit approach for the development of
   health services. Moreover, software engineering is hindered by agile
   management challenges regarding development and deployment processes of
   new applications. This paper presents an approach to develop health
   Internet of Things (IoT) reactive applications that can be widely used
   in smart cities for the care of elderly individuals. The proposed
   approach is based on the Rozanski and Woods's iterative architectural
   design process, the use of architectural patterns, and the Reactive
   Manifesto Principles. Furthermore, domain-driven design and the
   characteristics of the emerging fast data architecture are used to adapt
   the functionalities of services around the IoT, big data, and cloud
   computing paradigms. In addition, development and deployment processes
   are proposed as a set of tasks through DevOps techniques. The approach
   validation was carried out through the implementation of several
   e-health services, and various workload experiments were performed to
   measure scalability and performance in certain parts of the
   architecture. The system obtained is flexible, scalable, and capable of
   handling the data flow in near real time. Such features are useful for
   users who work collaboratively in the care of elderly people. With the
   accomplishment of these results, one can envision using this approach
   for building other e-health services.
RI JURADO PÉREZ, LUIS ALBERTO/; Salvachúa, Joaquín/O-1445-2015
OI JURADO PÉREZ, LUIS ALBERTO/0000-0003-3536-0383; Salvachúa,
   Joaquín/0000-0002-7269-8079
ZS 0
ZR 0
TC 6
ZA 0
ZB 1
Z8 0
Z9 8
U1 6
U2 39
EI 2076-3417
DA 2021-06-17
UT WOS:000659643600001
ER

PT C
AU Berno, Michele
   Canil, Marco
   Chiarello, Nicola
   Piazzon, Luca
   Berti, Fabio
   Ferrari, Francesca
   Zaupa, Alessandro
   Ferro, Nicola
   Rossi, Michele
   Susto, Gian Antonio
GP IEEE
TI A Machine Learning-based Approach for Advanced Monitoring of Automated
   Equipment for the Entertainment Industry
SO 2021 IEEE INTERNATIONAL WORKSHOP ON METROLOGY FOR INDUSTRY 4.0 & IOT
   (IEEE METROIND4.0 & IOT)
BP 386
EP 391
DI 10.1109/METROIND4.0IOT51437.2021.9488481
DT Proceedings Paper
PD 2021
PY 2021
AB Machine Learning-based approaches are revolutionizing the way in which
   complex systems and machines are monitored and controlled. In this work,
   we present a smart monitoring system that combines a big data
   architecture with an unsupervised anomaly detection technique, targeting
   the automated equipment in the entertainment industry. Anomaly detection
   uses state-of-the-art univariate and multivariate algorithms, as well as
   recently proposed techniques in the field of explainable artificial
   intelligence, to achieve enhanced monitoring capabilities and optimize
   service operations. The monitoring system is here presented and tested
   on a real world case study, i.e., an amusement park ride.
CT IEEE International Workshop on Metrology for Industry 4.0 & IoT (IEEE
   MetroInd4.0 and IoT)
CY JUN 07-09, 2021
CL ELECTR NETWORK
SP IEEE; Univ Campus Bio Medico Roma; SantAnna Sch Adv Studies, Biorobot
   Inst; Univ Brescia; Univ Sannio
RI ROSSI, MICHELE/; SUSTO, GIAN ANTONIO/U-3488-2019; Ferro, Nicola/L-5292-2015; Canil, Marco/GPF-4863-2022
OI ROSSI, MICHELE/0000-0003-1121-324X; SUSTO, GIAN
   ANTONIO/0000-0001-5739-9639; Ferro, Nicola/0000-0001-9219-6239; Canil,
   Marco/0000-0001-8037-7497
Z8 0
ZA 0
ZR 0
ZB 0
ZS 0
TC 4
Z9 8
U1 0
U2 8
BN 978-1-6654-1980-2
DA 2021-12-20
UT WOS:000709093600071
ER

PT C
AU Dang, Vincent-Nam
   Zhao, Yan
   Megdiche, Imen
   Ravat, Franck
BE Desai, BC
TI A Zone-Based Data Lake Architecture for IoT, Small and Big Data
SO IDEAS 2021: 25TH INTERNATIONAL DATABASE ENGINEERING & APPLICATIONS
   SYMPOSIUM
SE International Database Engineering and Applications Symposium -
   Proceedings
BP 94
EP 102
DI 10.1145/3472163.3472185
DT Proceedings Paper
PD 2021
PY 2021
AB Data lakes are supposed to enable analysts to perform more efficient and
   efficacious data analysis by crossing multiple existing data sources,
   processes and analyses. However, it is impossible to achieve that when a
   data lake does not have a metadata governance system that progressively
   capitalizes on all the performed analysis experiments. The objective of
   this paper is to have an easily accessible, reusable data lake that
   capitalizes on all user experiences. To meet this need, we propose an
   analysis-oriented metadata model for data lakes. This model includes the
   descriptive information of datasets and their attributes, as well as all
   metadata related to the machine learning analyzes performed on these
   datasets. To illustrate our metadata solution, we implemented a web
   application of data lake metadata management. This application allows
   users to find and use existing data, processes and analyses by searching
   relevant metadata stored in a NoSQL data store within the data lake. To
   demonstrate how to easily discover metadata with the application, we
   present two use cases, with real data, including datasets similarity
   detection and machine learning guidance.
CT 25th International Database Applications and Engineering Symposium
   (IDAES)
CY JUL 14-16, 2021
CL Concordia Univ, ELECTR NETWORK
HO Concordia Univ
RI RAVAT, Franck/AAG-7714-2019; ZHAO, Yan/; Megdiche, Imen/GLQ-7260-2022
OI RAVAT, Franck/0000-0003-4820-841X; ZHAO, Yan/0000-0002-6624-322X; 
ZB 0
ZR 0
ZS 0
Z8 0
ZA 0
TC 7
Z9 8
U1 0
U2 13
SN 1098-8068
BN 978-1-4503-8991-4
DA 2022-02-11
UT WOS:000749525000011
ER

PT J
AU Ma, Tian J.
   Garcia, Rudy J.
   Danford, Forest
   Patrizi, Laura
   Galasso, Jennifer
   Loyd, Jason
TI Big data actionable intelligence architecture
SO JOURNAL OF BIG DATA
VL 7
IS 1
AR 103
DI 10.1186/s40537-020-00378-7
DT Article
PD NOV 23 2020
PY 2020
AB The amount of data produced by sensors, social and digital media, and
   Internet of Things (IoTs) are rapidly increasing each day. Decision
   makers often need to sift through a sea of Big Data to utilize
   information from a variety of sources in order to determine a course of
   action. This can be a very difficult and time-consuming task. For each
   data source encountered, the information can be redundant, conflicting,
   and/or incomplete. For near-real-time application, there is insufficient
   time for a human to interpret all the information from different
   sources. In this project, we have developed a near-real-time,
   data-agnostic, software architecture that is capable of using several
   disparate sources to autonomously generate Actionable Intelligence with
   a human in the loop. We demonstrated our solution through a traffic
   prediction exemplar problem.
RI Ma, Tianji/HGC-7336-2022
Z8 0
ZA 0
ZR 0
TC 7
ZS 0
ZB 2
Z9 8
U1 0
U2 14
EI 2196-1115
DA 2020-12-22
UT WOS:000596130300001
ER

PT C
AU Molnar, Balint
   Pisoni, Galena
   Tarcsi, Adam
BE Vansinderen, M
   Obaidat, M
   Benothman, J
TI Data Lakes for Insurance Industry: Exploring Challenges and
   Opportunities for Customer Behaviour Analytics, Risk Assessment, and
   Industry Adoption
SO ICE-B: PROCEEDINGS OF THE 17TH INTERNATIONAL JOINT CONFERENCE ON
   E-BUSINESS AND TELECOMMUNICATIONS, VOL 3: ICE-B
BP 127
EP 134
DI 10.5220/0009972301270134
DT Proceedings Paper
PD 2020
PY 2020
AB The proliferation of the big data movement has led to volumes of data.
   The data explosion has surpassed enterprises' ability to consume the
   various data types that may exist. This paper discusses the
   opportunities and challenges associated with implementing data lakes, a
   potential strategy for leveraging data as a strategic asset for
   enterprise decision-making The paper analyzes an information ecosystem
   of an Insurance Company environment. There are two types of data
   sources, information systems based on a transactional databases for
   recording claims, as the basis of financial administration and systems
   policies. There exists neither Data Warehouse solutions nor any other
   data collection solutions dedicated to utilizing by Data Science methods
   and tools. The emerging technologies provide opportunities for synergy
   between the traditional Data Warehouse and the most recent Data Lake
   approaches. Therefore, it seems feasible and reasonable to integrate
   these two architecture approaches to support data analytics on several
   aspects of insurance, financial activities, risk analysis, prediction
   and forecasting.
CT 17th International Joint Conference on e-Business and Telecommunications
   (ICE-B)
CY JUL 08-10, 2020
CL Paris, FRANCE
RI Molnár, Bálint/AAA-7271-2021; Pisoni, Galena/O-4598-2019
ZR 0
ZB 0
ZA 0
ZS 0
Z8 0
TC 6
Z9 8
U1 1
U2 8
BN 978-989-758-447-3
DA 2020-01-01
UT WOS:000836152300010
ER

PT C
AU Ziegler, Julian
   Reimann, Peter
   Keller, Florian
   Mitschang, Bernhard
BE Gao, RX
   Ehmann, K
TI A Graph-based Approach to Manage CAE Data in a Data Lake
SO 53RD CIRP CONFERENCE ON MANUFACTURING SYSTEMS, CMS 2020
SE Procedia CIRP
VL 93
BP 496
EP 501
DI 10.1016/j.procir.2020.04.155
DT Proceedings Paper
PD 2020
PY 2020
AB Computer-aided engineering (CAE) applications generate vast quantities
   of heterogeneous data. Domain experts often fail to explore and analyze
   these data, because they are not integrated across different
   applications. Existing data management solutions are rather tailored to
   scientific applications. In our approach, we tackle this issue by
   combining a data lake solution with graph-based metadata management.
   This provides a holistic view of all CAE data and of the data-generating
   applications in one interconnected structure. Based on a prototypical
   implementation, we discuss how this eases the task of domain experts to
   explore and extract data for further analyses.
   (c) 2020 The Authors. Published by Elsevier B.V. This is an open access
   article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under
   responsibility of the scientific committee of the 53rd CIRP Conference
   on Manufacturing Systems
CT 53rd CIRP Conference on Manufacturing Systems-CIRP CMS
CY JUL 01-03, 2020
CL Chicago, IL
SP Northwestern University
ZA 0
ZR 0
Z8 0
ZS 0
ZB 0
TC 6
Z9 8
U1 0
U2 0
SN 2212-8271
BN *****************
DA 2020-01-01
UT WOS:001491480300083
ER

PT C
AU Demchenko, Yuri
   Turkmen, Fatih
   de laat, Cees
   Blanchet, Christophe
BE Smari, WW
TI Cloud Based Big Data Infrastructure: Architectural Components and.
   Automated Provisioning
SO 2016 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING & SIMULATION
   (HPCS 2016)
BP 628
EP 636
DT Proceedings Paper
PD 2016
PY 2016
AB This paper describes the general architecture and functional components
   of the cloud based Big Data Infrastructure (BDI). The proposed BDI
   architecture is based on the analysis of the emerging Big Data and data
   intensive technologies and supported by the definition of the Big Data
   Architecture Framework (BDAF) that defines the following components of
   the Big Data technologies: Big Data definition, Data Management
   including data lifecycle and data structures, Big Data Infrastructure
   (generically cloud based), Data Analytics technologies and platforms,
   and Big Data security, compliance and privacy. The paper provides
   example of requirements analysis and implementation of two
   bioinformatics use cases on cloud and using SlipStream based cloud
   applications deployment and management automation platform being
   developed in the CYCLONE project. The paper also refers to importance of
   standardisation of all components of BDAF and BDI and provides short
   overview of the NIST Big Data Interopera Nifty Framework (BDIF). The
   paper discusses importance of automation of all stages of the Big Data
   applications developments, deployment and management and refers to
   existing cloud automation tools and new developments in the SlipStream
   cloud automation platform that allows multi-cloud applications
   deployment and management.
CT 14th International Conference on High Performance Computing & Simulation
   (HPCS)
CY JUL 18-22, 2016
CL Innsbruck, AUSTRIA
SP Assoc Comp Machinery; IEEE; Int Federation Informat Proc; ACM SIGACT;
   ACM SIGAPP; ACM SIGARCH; ACM SIGMICRO; ACM SIGMOD; ACM SIGSIM; IEEE
   Austria Sect; IEEE Dayton Sect; Altair PBS Works; E4 Comp Engn; European
   Grid Infrastructure; HUAWEI Technologies Co Ltd; IBM; Intel Corp; Nice
   Software; Nvidia; PRACE
RI Turkmen, Fatih/KAL-7426-2024
OI Turkmen, Fatih/0000-0002-6262-4869
ZR 0
ZS 0
ZA 0
ZB 0
Z8 0
TC 7
Z9 8
U1 1
U2 12
BN 978-1-5090-2088-1
DA 2016-01-01
UT WOS:000389590600086
ER

PT C
AU Psiuk-Maksymowicz, Krzysztof
   Placzek, Aleksander
   Jaksik, Roman
   Student, Sebastian
   Borys, Damian
   Mrozek, Dariusz
   Fujarewicz, Krzysztof
   Swierniak, Andrzej
BE Kozielski, S
   Mrozek, D
   Kasprowski, P
   Malysiak-Mrozek, B
   Kostrzewa, D
TI A Holistic Approach to Testing Biomedical Hypotheses and Analysis of
   Biomedical Data
SO BEYOND DATABASES, ARCHITECTURES AND STRUCTURES, BDAS 2016
SE Communications in Computer and Information Science
VL 613
BP 449
EP 462
DI 10.1007/978-3-319-34099-9_34
DT Proceedings Paper
PD 2016
PY 2016
AB Testing biomedical hypotheses is performed based on advanced and usually
   many-step analysis of biomedical data. This requires sophisticated
   analytical methods and data structures that allow to store intermediate
   results, which are needed in the subsequent steps. However, biomedical
   data, especially reference data, often change in time and new analytical
   methods are created every year. This causes the necessity to repeat the
   iterative analyses with new methods and new reference data sets, which
   in turn causes frequent changes of the underlying data structures. Such
   instability of data structures can be mitigated by the use of the idea
   of data lake, instead of traditional database systems.
   The aim of this paper is to show system for researchers dealing with
   various types of biomedical data. Such a system provides a functionality
   of data analysis and testing different biomedical hypotheses. We treat a
   problem in a holistic way giving a researcher freedom in configuration
   his own multi-step analysis. This is possible by using a multiversion
   dynamic-schema data warehouse, performing parallel calculations on the
   virtualized computational environment, and delivering data in
   MapReduce-based ETL processes.
CT 12th International Scientific Conference on Beyond Databases,
   Architectures and Structures (BDAS)
CY MAY 31-JUN 03, 2016
CL Ustron, POLAND
SP IEEE Poland Sect
RI Swierniak, Anzej/; Student, Sebastian/H-7895-2013; Mrozek, Dariusz/C-4149-2013; Płaczek, Aleksander/; Jaksik, Roman/ABA-7023-2021; Fujarewicz, Krzysztof/B-9667-2013; , Sebastian/H-7895-2013; Psiuk-Maksymowicz, Krzysztof/B-3151-2016; Borys, Damian/P-6652-2015
OI Swierniak, Anzej/0000-0002-5698-5721; Mrozek,
   Dariusz/0000-0001-6764-6656; Płaczek, Aleksander/0000-0002-2555-1058;
   Jaksik, Roman/0000-0003-1866-6380; Fujarewicz,
   Krzysztof/0000-0002-1837-6466; , Sebastian/0000-0003-1812-5496;
   Psiuk-Maksymowicz, Krzysztof/0000-0001-5180-5253; Borys,
   Damian/0000-0003-0229-2601
ZA 0
ZS 0
ZR 0
TC 7
ZB 1
Z8 0
Z9 8
U1 0
U2 10
SN 1865-0929
EI 1865-0937
BN 978-3-319-34099-9
DA 2016-01-01
UT WOS:000375769000034
ER

PT C
AU Fernandez, Alberto
   Casado, Ruben
   Usamentiaga, Ruben
BA Awan, I
   Younas, M
   Mecella, M
TI A Real-Time Big Data Architecture For Glasses Detection Using Computer
   Vision Techniques
SO 2015 3RD INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD
   (FICLOUD) AND INTERNATIONAL CONFERENCE ON OPEN AND BIG (OBD)
BP 591
EP 596
DI 10.1109/FiCloud.2015.78
DT Proceedings Paper
PD 2015
PY 2015
AB Automatic glasses detection is a hot topic withing the large-scale face
   images classification domain, which has impact on face recognition or
   soft biometrics for person identification. In many practical video
   surveillance applications, the faces acquired by cameras are low
   resolution. Therefore, this type of applications requires processing of
   a large number of relatively small-sized images. However, continuous
   stream of image and video data processing is a big data challenge. This
   need fits with the goals of Big Data streaming processing systems. In
   this paper, we propose a real-time Big Data architecture in order to
   collect, maintain and analyze massive volumes of images related with the
   problem of automatic glasses detection. This architecture can be used as
   an automatic image tagging related with glasses detection on face
   images.
CT 3rd International Conference on Future Internet of Things and Cloud
   (FiCloud) / International Conference on Open and Big Data (OBD)
CY AUG 24-26, 2015
CL Rome, ITALY
SP IEEE Comp Soc, Tech Comm Internet; IEEE Comp Soc
RI Usamentiaga, Ruben/D-9649-2013; Fernandez Hilario, Alberto/IZD-7676-2023
OI Usamentiaga, Ruben/0000-0003-0551-3203; Fernandez Hilario,
   Alberto/0000-0002-6480-8434
Z8 0
TC 7
ZA 0
ZB 0
ZR 0
ZS 0
Z9 8
U1 0
U2 11
BN 978-1-4673-8103-1
DA 2016-07-20
UT WOS:000378639200086
ER

PT C
AU Holley, Kerrie
   Sivakumar, Gandhi
   Kannan, KalaPriya
BE Kesselman, C
   Chen, P
   Jain, H
TI Enrichment Patterns for Big Data
SO 2014 IEEE INTERNATIONAL CONGRESS ON BIG DATA (BIGDATA CONGRESS)
SE IEEE International Congress on Big Data
BP 796
EP 799
DI 10.1109/BigData.Congress.2014.127
DT Proceedings Paper
PD 2014
PY 2014
AB Importance of "Big Data" in terms of business value is very well
   understood across different sectors such as telecom, banking, insurance
   etc for targeted campaigns or real time performance actions. "Big Data"
   emphasizes the following characteristics, Velocity, Volume, Variety, and
   Veracity. Business adopts one or more of the above properties to cater
   to the requirements of the clients. Data being crucial in this case has
   different facets. The sources of data being different and consumption
   across different businesses makes the data modeling a tougher problem.
   Data schema evolves with new sources of data, changes due to change in
   data sources, etc Thus enrichment of data constantly triggers the needs
   to device methods to adopt the models to the new patterns. When the
   enrichment patterns are understood, modeling the Big Data and Management
   becomes easy. We highlight the list of such identified patterns based
   upon our real world implementations. In this work, we propose a method
   to evolve the data models from its initially defined schema such that
   data models can easily adapt to changes. We show through cases studies
   from real world example that our model can adopt to evolve data from
   different sources.
CT 3rd IEEE International Congress on Big Data
CY JUN 27-JUL 02, 2014
CL Anchorage, AK
SP IEEE; IEEE Comp Soc; Serv Comp; Serv Soc; Cloud Comp; BIG DATA; HP; IBM;
   SAP; IBM Res; HUAWEI; Object Management Grp; Business Proc Integrat &
   Management; IT Profess; Int Journal Web Serv Res; Computing Now; IEEE
   Transact Serv Comp
TC 8
ZR 0
ZA 0
ZB 0
ZS 0
Z8 0
Z9 8
U1 0
U2 8
SN 2379-7703
BN 978-1-4799-5057-7
DA 2015-03-25
UT WOS:000350154200118
ER

PT J
AU Ionescu, Sergiu-Alexandru
   Diaconita, Vlad
   Radu, Andreea-Oana
TI Engineering Sustainable Data Architectures for Modern Financial
   Institutions
SO ELECTRONICS
VL 14
IS 8
AR 1650
DI 10.3390/electronics14081650
DT Article
PD APR 19 2025
PY 2025
AB Modern financial institutions now manage increasingly advanced
   data-related activities and place a growing emphasis on environmental
   and energy impacts. In financial modeling, relational databases, big
   data systems, and the cloud are integrated, taking into consideration
   resource optimization and sustainable computing. We suggest a four-layer
   architecture to address financial data processing issues. The layers of
   our design are for data sources, data integration, processing, and
   storage. Data ingestion processes market feeds, transaction records, and
   customer data. Real-time data are captured by Kafka and transformed by
   Extract-Transform-Load (ETL) pipelines. The processing layer is composed
   of Apache Spark for real-time data analysis, Hadoop for batch
   processing, and an Machine Learning (ML) infrastructure that supports
   predictive modeling. In order to optimize access patterns, the storage
   layer includes various data layer components. The test results indicate
   that the processing of market data in real-time, compliance reporting,
   risk evaluations, and customer analyses can be conducted in fulfillment
   of environmental sustainability goals. The metrics from the test
   deployment support the implementation strategies and technical
   specifications of the architectural components. We also looked at
   integration models and data flow improvements, with applications in
   finance. This study aims to enhance enterprise data architecture in the
   financial context and includes guidance on modernizing data
   infrastructure.
RI IONESCU, Sergiu Alexanu/; Diaconita, Vlad/D-6882-2015; RADU, ANEEA-OANA/
OI IONESCU, Sergiu Alexanu/0009-0005-9021-3716; Diaconita,
   Vlad/0000-0002-5169-9232; RADU, ANEEA-OANA/0009-0001-5858-4999
TC 5
Z8 0
ZB 0
ZS 0
ZA 0
ZR 0
Z9 7
U1 1
U2 6
SN 2079-9292
DA 2025-04-29
UT WOS:001474864100001
ER

PT J
AU Bianchini, Devis
   De Antonellis, Valeria
   Garda, Massimiliano
TI A semantics-enabled approach for personalised Data Lake exploration
SO KNOWLEDGE AND INFORMATION SYSTEMS
VL 66
IS 2
BP 1469
EP 1502
DI 10.1007/s10115-023-02014-1
EA DEC 2023
DT Article
PD FEB 2024
PY 2024
AB The increasing availability of Big Data is changing the way data
   exploration for Business Intelligence is performed, due to the volume,
   velocity and uncontrolled variety of data on which exploration relies.
   In particular, data exploration is required in Data Lakes that have been
   proposed to host heterogeneous data sources, given their flexibility to
   cope with cumbersome properties of Big Data. However, as data grows, new
   methods and techniques are required for extracting value and knowledge
   from data stored within Data Lakes, aggregating data into indicators
   according to multiple analysis dimensions, to enable a large number of
   users with different roles and competencies to capitalise on available
   information. In this paper, we propose PERSEUS (PERSonalised Exploration
   by User Support), a computer-aided approach for data exploration on top
   of a Data Lake, structured over three phases: (1) the construction of a
   semantic metadata catalog on top of the Data Lake, leveraging tools and
   metrics to ease the annotation of the Data Lake metadata; (2) modelling
   of indicators and analysis dimensions, guided by an openly available
   Multi-Dimensional Ontology to enable conformance checking of indicators
   and let users explore Data Lake contents; (3) enrichment of the
   definition of indicators with personalisation aspects, based on users'
   profiles and preferences, to make easier and more usable the exploration
   of data for a large number of users. Results of an experimental
   evaluation in the Smart City domain are presented with the aim of
   demonstrating the feasibility of the approach.
OI Garda, Massimiliano/0009-0006-5823-6595
ZS 0
TC 6
Z8 0
ZB 0
ZA 0
ZR 0
Z9 7
U1 2
U2 7
SN 0219-1377
EI 0219-3116
DA 2023-12-16
UT WOS:001115560300001
ER

PT J
AU Dolores, M.
   Fernandez-Basso, Carlos
   Gomez-Romero, Juan
   Martin-Bautista, Maria J.
TI A big data association rule mining based approach for energy building
   behaviour analysis in an IoT environment
SO SCIENTIFIC REPORTS
VL 13
IS 1
AR 19810
DI 10.1038/s41598-023-47056-1
DT Article
PD NOV 13 2023
PY 2023
AB The enormous amount of data generated by sensors and other data sources
   in modern grid management systems requires new infrastructures, such as
   IoT (Internet of Things) and Big Data architectures. This, in
   combination with Data Mining techniques, allows the management and
   processing of all these heterogeneous massive data in order to discover
   new insights that can help to reduce the energy consumption of the
   building. In this paper, we describe a developed methodology for an
   Internet of Things (IoT) system based on a robust big data architecture.
   This innovative approach, combined with the power of Spark algorithms,
   has been proven to uncover rules representing hidden connections and
   patterns in the data extracted from a building in Bucharest. These
   uncovered patterns were essential for improving the building's energy
   efficiency.
RI Fernandez-Basso, Carlos/C-7599-2017; Martin-Bautista, Maria/H-7754-2015; Gomez Romero, Juan/F-7550-2011
ZB 0
TC 5
Z8 0
ZA 0
ZS 0
ZR 0
Z9 7
U1 1
U2 7
SN 2045-2322
DA 2024-01-13
UT WOS:001104940600090
PM 37957251
ER

PT J
AU Cherradi, Mohamed
   Bouhafer, Fadwa
   EL Haddadi, Anass
TI Data lake governance using IBM-Watson knowledge catalog
SO SCIENTIFIC AFRICAN
VL 21
AR e01854
DI 10.1016/j.sciaf.2023.e01854
EA AUG 2023
DT Article
PD SEP 2023
PY 2023
AB The strategic importance of data in decision-making is increasingly
   recognized, demanding efficient solutions such as data catalogs to
   ensure data governance and emphasize data interoperability, in
   accordance with the FAIR (Findable, Accessible, Interoperable, and
   Reusable) principles. However, the usage of FAIR-compliant data catalogs
   lacks empirical studies due to its novelty. This study aims to promote
   the practical adoption of data catalogs as a means to manage the
   expanding data landscape. We differentiate our contribution by providing
   an empirical evaluation and comparison of IBM Watson Knowledge Catalog
   (IBM-WKC), a leading data cataloging solution, with two other prominent
   alternatives, Open-Metadata and Data-Galaxy, for extracting relevant
   information from data lakes containing heterogeneous data sources in
   their native formats. Our proposed methodology utilizes an innovative
   tool built on IBM-WKC for annotating collected documents. To evaluate
   our approach, we conducted experiments on a dataset of 100 documents
   sourced from scientific databases. Moreover, to assess our proposal, we
   compare the retrieved text to the appropriate interventions that use the
   original checklist. The results demonstrate the superiority of IBM-WKC
   over its competitors, showcasing its enhanced performance in addressing
   data cataloging challenges. Notably, the tested queries achieved an
   impressive accuracy, precision, and recall value of 96%. These findings
   highlight the reliability and alignment of IBM-WKC with the FAIR
   principles.
RI EL HADDADI, Anass/ABD-8465-2021
OI EL HADDADI, Anass/0000-0002-3338-2477
ZA 0
ZB 0
Z8 0
ZR 0
TC 5
ZS 0
Z9 7
U1 1
U2 9
SN 2468-2276
DA 2024-02-25
UT WOS:001165433200001
ER

PT J
AU Correa, Christian
   Dujovne, Diego
   Bolano, Fernando
TI Design and Implementation of an Embedded Edge-Processing Water Quality
   Monitoring System for Underground Waters
SO IEEE EMBEDDED SYSTEMS LETTERS
VL 15
IS 2
BP 81
EP 84
DI 10.1109/LES.2022.3184925
DT Article
PD JUN 2023
PY 2023
AB Global warming effects are seen around the world and Latin American
   countries are not an exception, especially for expanding drought areas.
   Therefore, underground water resources used in the region are
   incrementing exponentially. However, temporal and spatial underground
   water information concerning availability and quality is scarce,
   disabling proper decision making. In order to close that breach, we
   propose and embedded edge-processing Internet of Things (IoT)-based
   water quality monitoring system. This letter introduces the design and
   implementation of this solution, specifically targeted to monitor
   irrigation and drinking water extracted from water wells. The system is
   designed to be deployed in central Chile, considering the topographic
   conditions, which severely affect power availability and communication
   resources. The captured data are stored in a data lake, for further
   processing according to water quality models.
RI Correa, Christian/AAP-1823-2020
OI Correa, Christian/0000-0002-4748-2129
ZA 0
ZS 0
Z8 0
ZR 0
TC 7
ZB 1
Z9 7
U1 0
U2 30
SN 1943-0663
EI 1943-0671
DA 2023-06-25
UT WOS:000995882700007
ER

PT J
AU Zagan, Elisabeta
   Danubianu, Mirela
TI Data Lake Architecture for Storing and Transforming Web Server Access
   Log Files
SO IEEE ACCESS
VL 11
BP 40916
EP 40929
DI 10.1109/ACCESS.2023.3270368
DT Article
PD 2023
PY 2023
AB Web server access log files are text files containing important data
   about server activities, client requests addressed to a server, server
   responses, etc. Large-scale analysis of these data can contribute to
   various improvements in different areas of interest. The main problem
   lies in storing these files in their raw form, over long time, to allow
   analysis processes to be run at any time enabling information to be
   extracted as foundation for high quality decisions. Our research focuses
   on offering an economical, secure, and high-performance solution for the
   storage of large amount of raw log files. Proposed system implements a
   Data Lake (DL) architecture in cloud using Azure Data Lake Storage Gen2
   (ADLS Gen2) for extract-load-transform (ELT) pipelines. This
   architecture allows large volumes of data to be stored in their raw
   form. Afterwards they can be subjected to transformation and advanced
   analysis processes without the need of a structured writing scheme. The
   main contribution of this paper is to provide a solution that is
   affordable and more accessible to perform web server access log data
   ingestion, storage and transformation over the newest technology, Data
   Lake. As derivative contribution, we proposed the use of Azure Blob
   Trigger Function to implement the algorithm of transforming log files
   into parquet files leading to 90% reduction in storage space compared to
   their original size. That means much lower storage costs than if they
   had been stored as log files. A hierarchical data storage model has also
   been proposed for shared access to data over different layers in the DL
   architecture, on top of which Data Lifecycle Management (DLM) rules have
   been proposed for storage cost efficiency. We proposed ingesting log
   files into a Data Lake deployed in cloud due to ease of deployment and
   low storage costs. The aim is to maintain this data in the long term, to
   be used in future advanced analytics processes by cross-referencing with
   other organizational or external data. That could bring important
   benefits. While the proposed solution is explicitly based on ADLS Gen2,
   it represents an important benchmark in approaching a cloud DL solution
   offered by any other vendor.
RI Zagan, Elisabeta/NGQ-7564-2025; Danubianu, Mirela/O-3620-2014
OI Danubianu, Mirela/0000-0002-5470-1406
ZA 0
ZB 0
TC 7
Z8 0
ZR 0
ZS 0
Z9 7
U1 3
U2 8
SN 2169-3536
DA 2023-05-18
UT WOS:000981871700001
ER

PT C
AU Cherradi, Mohamed
   El Haddadi, Anass
   Routaib, Hayat
BE BenAhmed, M
   Teodorescu, HNL
   Mazri, T
   Subashini, P
   Boudhir, AA
TI Data Lake Management Based on DLDS Approach
SO NETWORKING, INTELLIGENT SYSTEMS AND SECURITY
SE Smart Innovation Systems and Technologies
VL 237
BP 679
EP 690
DI 10.1007/978-981-16-3637-0_48
DT Proceedings Paper
PD 2022
PY 2022
AB Over the past few years, big data is at the center of the concerns of
   actors in all fields of activity. The rapid growth of this massive data
   requires the question of its storage. Data lakes meet these storage
   needs, offering data storage without a predefined schema. In this
   context, a strategy for building a clear data catalog is fundamental for
   any organization that stores big data, helping to ensure the effective
   and efficient use of information. Setting up a data catalog in a data
   lake remains a complicated task and presents a major issue for data
   managers. However, the data catalog is still essential. This article
   presents the use of XML and JAXB technologies in the modeling of the
   data catalog by proposing an approach called DLDS (stands for Data Lake
   Description Service) and enables to build a central catalog file that
   allows the users to search, locate, understand and query different data
   sources stored in the lake.
CT 4th International Conference on Networks, Intelligent Systems and
   Security (NISS)
CY APR 01-02, 2021
CL Kenitra, MOROCCO
RI ROUTAIB, Hayat/; EL HADDADI, Anass/ABD-8465-2021
OI ROUTAIB, Hayat/0000-0003-4461-8868; 
ZB 0
TC 4
ZS 0
Z8 0
ZA 0
ZR 0
Z9 7
U1 0
U2 2
SN 2190-3018
EI 2190-3026
BN 978-981-16-3637-0; 978-981-16-3636-3
DA 2022-11-04
UT WOS:000866307400048
ER

PT C
AU Dhaouadi, Asma
   Bousselmi, Khadija
   Monnet, Sebastien
   Gammoudi, Mohamed Mohsen
   Hammoudi, Slimane
BE Barolli, L
   Hussain, F
   Enokido, T
TI A Multi-layer Modeling for the Generation of New Architectures for Big
   Data Warehousing
SO ADVANCED INFORMATION NETWORKING AND APPLICATIONS, AINA-2022, VOL 2
SE Lecture Notes in Networks and Systems
VL 450
BP 204
EP 218
DI 10.1007/978-3-030-99587-4_18
DT Proceedings Paper
PD 2022
PY 2022
AB With the explosion of new data processing and storage technologies
   nowadays, businesses are looking to harness the hidden value of data,
   each in their own way. Many contributions were proposed defining
   pipelines dedicated to Big Data processing and storage, but they target
   usually particular types of data and specific technologies to meet
   precise needs without considering the evolution of requirements or the
   data characteristics' change. Thus, no approach has defined a generic
   architecture for Big Data warehousing process. In this paper, we propose
   a multi-layer model that integrates all the necessary elements and
   concepts in the different phases of a data warehousing process. It also
   contributes to generate an architecture that considers the specificity
   of data and applications and the suitable technologies. To illustrate
   our contribution, we have implemented the proposed model through a
   Business model and a Big Data architecture for the analysis of
   multi-source and social networks data.
CT 36th International Conference on Advanced Information Networking and
   Applications (AINA)
CY APR 13-15, 2022
CL Sydney, AUSTRALIA
RI khadija, bousselmi/HRC-0326-2023; Gammoudi, Mohamed Mohsen/
OI khadija, bousselmi/0000-0002-9477-3455; Gammoudi, Mohamed
   Mohsen/0000-0002-4669-228X
Z8 0
ZS 0
TC 6
ZA 0
ZR 0
ZB 0
Z9 7
U1 0
U2 1
SN 2367-3370
EI 2367-3389
BN 978-3-030-99587-4; 978-3-030-99586-7
DA 2023-02-25
UT WOS:000926668000018
ER

PT C
AU Ataei, Pouya
   Litchfield, Alan
GP IEEE
TI NeoMycelia: A software reference architecturefor big data systems
SO 2021 28TH ASIA-PACIFIC SOFTWARE ENGINEERING CONFERENCE (APSEC 2021)
SE Asia-Pacific Software Engineering Conference
BP 452
EP 462
DI 10.1109/APSEC53868.2021.00052
DT Proceedings Paper
PD 2021
PY 2021
AB The big data revolution began when the volume, velocity, and variety of
   data completely overwhelmed the systems used to store, manipulate and
   analyze that data. As a result, a new class of software systems emerged
   called big data systems. While many attempted to harness the power of
   these new systems, it is estimated that approximately 75% of the big
   data projects have failed within the last decade. One of the root causes
   of this is software engineering and architecture aspect of these
   systems. This paper aims to facilitate big data system development by
   introducing a software reference architecture. The work provides an
   event driven microservices architecture that addresses specific
   limitations in current big data reference architectures (RA). The
   artefact development has followed the principles of empirically grounded
   RAs. The RA has been evaluated by developing a prototype that solves a
   real-world problem in practice. At the end, succesful implementation of
   the reference architecture have been presented. The results displayed a
   good degree of applicability with respect to quality factors.
CT 28th Asia-Pacific Software Engineering Conference (APSEC)
CY DEC 06-09, 2021
CL ELECTR NETWORK
SP Minist Sci & Technol; Taipei Med Univ; Acad Sinica; Foxconn Technol Grp;
   PremiumSoft
RI Litchfield, Alan/L-5949-2019; Ataei, Pouya/
OI Litchfield, Alan/0000-0002-3876-0940; Ataei, Pouya/0000-0002-0993-3574
ZA 0
ZB 0
ZR 0
TC 7
ZS 0
Z8 0
Z9 7
U1 1
U2 4
SN 1530-1362
BN 978-1-6654-3784-4
DA 2022-06-15
UT WOS:000802192700045
ER

PT C
AU Ren, Peng
   Hou, Wei
   Sheng, Ming
   Li, Xin
   Li, Chao
   Zhang, Yong
BE Siuly, S
   Wang, H
   Chen, L
   Guo, Y
   Xing, C
TI MKGB: A Medical Knowledge Graph Construction Framework Based on Data
   Lake and Active Learning
SO HEALTH INFORMATION SCIENCE, HIS 2021
SE Lecture Notes in Computer Science
VL 13079
BP 245
EP 253
DI 10.1007/978-3-030-90885-0_22
DT Proceedings Paper
PD 2021
PY 2021
AB Medical knowledge graph (MKG) provides ideal technical support for
   integrating multi-source heterogeneous data and enhancing graph-based
   services. These multi-source data are usually huge, heterogeneous, and
   difficult to manage. To ensure that the generated MKG has higher
   quality, the construction of MKG using these data requires a large
   number of medical experts to participate in the annotation based on
   their expertise. However, faced with such a large amount of data, manual
   annotation turns out to be a high labor cost task. In addition, the
   medical data are generated rapidly, which requires us to manage and
   annotate efficiently to keep up with the pace of data accumulation.
   Prior researches lacked efficient data management for massive medical
   data, and few studies focused on the construction of large-scale and
   high-quality MKG.
   We propose a Medical Knowledge Graph Builder (MKGB) based on Data Lake
   and active learning, which is used to solve the problems mentioned
   above. There are four modules in MKGB, data acquiring module, data
   management framework module based on Data Lake, active learning module
   for reducing labor cost and MKG construction module. With the efficient
   management for extensive medical data in data management framework based
   on Data Lake, MKGB uses active learning based on doctor-in-the-loop idea
   to reduce the labor cost of annotation process, while ensuring the
   quality of annotation and enabling the construction of large-scale and
   high-quality MKG. Based on the efficient data management, we demonstrate
   that our approach significantly reduces the cost of manual annotation
   and generates more reliable MKG.
CT 10th International Conference on Health Information Science (HIS)
CY OCT 25-28, 2021
CL Melbourne, AUSTRALIA
ZR 0
ZA 0
ZS 0
ZB 2
Z8 0
TC 4
Z9 7
U1 1
U2 8
SN 0302-9743
EI 1611-3349
BN 978-3-030-90885-0; 978-3-030-90884-3
DA 2022-03-16
UT WOS:000758752400022
ER

PT C
AU Ren, Peng
   Li, Shuaibo
   Hou, Wei
   Zheng, Wenkui
   Li, Zhen
   Cui, Qin
   Chang, Wang
   Li, Xin
   Zeng, Chun
   Sheng, Ming
   Zhang, Yong
BE Xing, C
   Fu, X
   Zhang, Y
   Zhang, G
   Borjigin, C
TI MHDP: An Efficient Data Lake Platform for Medical Multi-source
   Heterogeneous Data
SO WEB INFORMATION SYSTEMS AND APPLICATIONS (WISA 2021)
SE Lecture Notes in Computer Science
VL 12999
BP 727
EP 738
DI 10.1007/978-3-030-87571-8_63
DT Proceedings Paper
PD 2021
PY 2021
AB In medical domain, huge amounts of data are generated at all times.
   These data are usually difficult to access, with poor data quality and
   many data islands. Besides, with a wide range of sources and complex
   structure, these data contain essential information and are difficult to
   manage. However, few existing data management frameworks based on Data
   Lake excel in solving the persistence and the analysis efficiency for
   medical multi-source heterogeneous data. In this paper, we propose an
   efficient Multi-source Heterogeneous Data Lake Platform (MHDP) to
   realize the efficient medical data management. Firstly, we propose an
   efficient and unified method based on Data Lake to store data of
   different types and different sources persistently. Secondly, based on
   the unified data store, an efficient multi-source heterogeneous data
   fusion is implemented to effectively manage data. Finally, an efficient
   data query strategy is carried out to assist doctors in medical
   decision-making. In-depth analysis on applications shows that MHDP
   delivers better performance for data management in medical domain.
CT 18th Web Information Systems and Applications Conference (WISA)
CY SEP 24-26, 2021
CL Kaifeng, PEOPLES R CHINA
SP China Comp Federat Tech Comm Informat Syst; Henan Univ; China Comp
   Federat
RI Cui, Qin/GQQ-3888-2022
TC 5
ZB 0
ZA 0
ZS 0
ZR 0
Z8 0
Z9 7
U1 2
U2 29
SN 0302-9743
EI 1611-3349
BN 978-3-030-87571-8; 978-3-030-87570-1
DA 2022-03-25
UT WOS:000767941100063
ER

PT J
AU Talha, Mohamed
   Elmarzouqi, Nabil
   Kalam, Anas Abou El
TI Towards a Powerful Solution for Data Accuracy Assessment in the Big Data
   Context
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 11
IS 2
BP 419
EP 429
DT Article
PD FEB 2020
PY 2020
AB Data Accuracy is one of the main dimensions of Data Quality; it measures
   the degree to which data are correct. Knowing the accuracy of an
   organization's data reflects the level of reliability it can assign to
   them in decision-making processes. Measuring data accuracy in Big Data
   environment is a process that involves comparing data to assess with
   some "reference data" considered by the system to be correct. However,
   such a process can be complex or even impossible in the absence of
   appropriate reference data. In this paper, we focus on this problem and
   propose an approach to obtain the reference data thanks to the emergence
   of Big Data technologies. Our approach is based on the upstream
   selection of a set of criteria that we define as "Accuracy Criteria". We
   use furthermore a set of techniques such as Big Data Sampling, Schema
   Matching, Record Linkage, and Similarity Measurement. The proposed model
   and experiment results allow us to be more confident in the importance
   of data quality assessment solution and the configuration of the
   accuracy criteria to automate the selection of reference data in a Data
   Lake.
RI abou el kalam, anas/; ELMARZOUQI, Nabil/AAK-8004-2020
OI abou el kalam, anas/0000-0001-7714-4801; 
TC 4
Z8 0
ZR 0
ZB 0
ZA 0
ZS 0
Z9 7
U1 0
U2 10
SN 2158-107X
EI 2156-5570
DA 2020-03-24
UT WOS:000518468600054
ER

PT C
AU Apurva, Aviral
   Ranakoti, Pranshu
   Yadav, Saurav
   Tomer, Shashank
   Roy, Nihar Ranjan
GP IEEE
TI Redefining Cyber Security with Big Data Analytics
SO 2017 INTERNATIONAL CONFERENCE ON COMPUTING AND COMMUNICATION
   TECHNOLOGIES FOR SMART NATION (IC3TSN)
BP 199
EP 203
DT Proceedings Paper
PD 2017
PY 2017
AB The cyber world is expanding rapidly day by day and more and more people
   are getting connected to this world, resulting in generation of a large
   amount of data called Big Data Along with the cyber world, the number of
   cyber criminals is also expanding rapidly. To fight against the cyber
   criminals and safeguard the interest of innocent civilians, we take the
   help of Big Data Analytics. Big data is large in both quantity and
   quality and can be efficiently used to analyze certain patterns and
   behavior anomaly which can help us prevent or he prepared for the thread
   or any upcoming attack. This proactive and analytical approach will help
   us greatly reduce the rate of Cyber Crimes and also get the knowledge
   out of that data which was not previously observable. This paper
   discusses some of the key characteristics of Big data, architecture,
   categories of cybercrimes and big data analytic techniques that can be
   used.
CT International Conference on Computing and Communication Technologies for
   Smart Nation (IC3TSN)
CY OCT 12-14, 2017
CL Gurgaon, INDIA
RI Roy, Nihar Ranjan/L-3325-2019
OI Roy, Nihar Ranjan/0000-0001-6581-0803
Z8 0
TC 3
ZB 0
ZS 0
ZR 0
ZA 0
Z9 7
U1 0
U2 7
BN 978-1-5386-0627-8
DA 2018-03-19
UT WOS:000425888700037
ER

PT C
AU Gong, Yikai
   Rimba, Paul
   Sinnott, Richard O.
GP ACM
TI A Big Data Architecture for Near Real-time Traffic Analytics
SO COMPANION PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON UTILITY
   AND CLOUD COMPUTING (UCC'17 COMPANION)
BP 157
EP 162
DI 10.1145/3147234.3151010
DT Proceedings Paper
PD 2017
PY 2017
AB Big data is a popular research topic that has brought about a range of
   new IT challenges and opportunities. The transport domain is one area
   that has much to benefit from big data platforms. It requires
   capabilities for processing voluminous amounts of heterogeneous data
   that is often created in near real time and at high velocity from a
   multitude of distributed sensors. It can also require the application of
   performance-oriented spatial data processing of such data. In this
   paper, we present a platform (SMASH) that tackles many of the specific
   challenges raised by the transport domain. We present a range of case
   studies applying SMASH to transport and other data used to understand
   traffic phenomenon across the State of Victoria, Australia. The novelty
   of this work is that this Cloud-based platform is not designed for a
   specific type of data or for a specific form of data processing. Rather
   it supports a range of data flavours with a range of data processing
   possibilities. In particular we show how the platform can be used for
   analyzing social media data used for traffic jam identification through
   spatial and temporal clustering tweets on the road network and compare
   the results with official real-time traffic data based on the Sydney
   Coordinated Adaptive Traffic System (SCATS - www.scats.com.au) that has
   been rolled out across Victoria.
CT 10th International Conference on Utility and Cloud Computing (UCC) / 4th
   International Conference on Big Data Computing, Applications and
   Technologies (BDCAT)
CY DEC 05-08, 2017
CL Austin, TX
SP Assoc Comp Machinery; IEEE Comp Soc; IEEE TCSC; ACM SIGARCH; IEEE
RI Sinnott, Richard/E-7197-2014
ZS 0
ZA 0
TC 7
ZR 0
Z8 0
ZB 0
Z9 7
U1 0
U2 3
BN 978-1-4503-5195-9
DA 2017-01-01
UT WOS:000570497300026
ER

PT J
AU Li, Beibei
   Liu, Bo
   Lin, Weiwei
   Zhang, Ying
TI Performance analysis of clustering algorithm under two kinds of big data
   architecture
SO JOURNAL OF HIGH SPEED NETWORKS
VL 23
IS 1
BP 49
EP 57
DI 10.3233/JHS-170556
DT Article
PD 2017
PY 2017
AB To compare the performance of the clustering algorithm on two data
   processing architectures, the implementations of k-means clustering
   algorithm on two big data architectures are given at first in this
   paper. Then we focus on the differences of theoretical performance of
   k-means algorithm on two architectures from the mathematical point of
   view. The theoretical analysis shows that Spark architecture is superior
   to the Hadoop in aspects of the average execution time and I/O time.
   Finally, a text data set of social networking site of users' behaviors
   is employed to conduct algorithm experiments. The results show that
   Spark is significantly less than MapReduce in aspects of the execution
   time and I/O time based on k-means algorithm. The theoretical analysis
   and the implementation technology of the big data algorithm proposed in
   this paper are a good reference for the application of big data
   technology.
RI Li, Beibei/P-2243-2014
ZA 0
Z8 0
TC 3
ZS 0
ZB 0
ZR 0
Z9 7
U1 0
U2 7
SN 0926-6801
EI 1875-8940
DA 2017-03-29
UT WOS:000395300400004
ER

PT C
AU Miloslavskaya, Natalia
BE Awan, I
   Portela, F
   Younas, M
TI Security Intelligence Centers for Big Data Processing
SO 2017 5TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD
   WORKSHOPS (FICLOUDW) 2017
BP 7
EP 13
DI 10.1109/FiCloudW.2017.68
DT Proceedings Paper
PD 2017
PY 2017
AB Today numerous information security (IS) incidents in organizations'
   networks have become not only more sophisticated but also damaging.
   Hence the systems with proper security services in place to mitigate and
   promptly respond to IS threats by helping organizations better
   understand their current network situation, as well as to perform
   routine work in big IS-related data processing in automatic mode are
   needed as never before. They are known as Security Operations Centers
   (SOCs) and Security Intelligence Centers (SICs) as their next evolution
   step. The key features of SICs are summarized. The SIC business logic
   and data architecture are proposed. These results lead to the main area
   of further research.
CT IEEE 5th International Conference on Future Internet of Things and Cloud
   (FiCloud)
CY AUG 21-23, 2017
CL Prague, CZECH REPUBLIC
SP IEEE; IEEE Comp Soc Tech Comm Internet; Charles Univ
RI Miloslavskaya, Natalia/F-7562-2011
OI Miloslavskaya, Natalia/0000-0002-1231-1805
Z8 0
ZB 0
ZS 0
TC 7
ZR 0
ZA 0
Z9 7
U1 0
U2 1
BN 978-1-5386-3281-9
DA 2018-06-29
UT WOS:000435144700002
ER

PT J
AU Mitrovic, Stanislav
TI Specifics of the integration of Business Intelligence and Big Data
   technologies in the processes of economic analysis
SO BIZNES INFORMATIKA-BUSINESS INFORMATICS
VL 42
IS 4
BP 40
EP 46
DI 10.17323/1998-0663.2017.4.40.46
DT Article
PD 2017
PY 2017
AB The volume of data used for economic analysis of the activities of
   organizations is growing every year. Despite the fact that all
   information required for economic analysis is available from various
   sources, such data are very often useless for analysis from the point of
   view of their economic potential.
   The purpose of this study is to outline a foundation for integrating
   Business Intelligence and Big Data into economic analysis processes. The
   theoretical and methodological basis of this study is provided by
   scientific research, methodological and practical developments of
   domestic and foreign authors on the application of IT solutions in
   economic analysis.
   According to the results of the research, modern information
   technologies, in particular, the Business Intelligence and Big Data
   systems have considerably changed the possibilities for improving
   economic analysis and reducing decision-making time. From the
   methodological point of view, many aspects of integration of BI and Big
   Data solutions and their implementation in the economic analysis
   processes in Russia's companies remain insufficiently developed. The
   foreign market of modern information technologies for business analytics
   has a longer history and is being developed more rapidly.
   The main conclusions of the study indicate that modern organizations
   operating on a highly competitive market should understand that the
   accumulation of Big Data does not always lead to the expected business
   benefits. In this context, the conclusion is that a modern company
   should not set as its goal to process all the available data in order to
   improve the quality of its economic analysis. It is more significant to
   use the entire volume of data for segmentation, which allows effective
   construction of a large number of models for small clusters, solving
   specific problems of economic analysis based on the application of
   modern IT systems.
RI Stanislav, Mitrovic/A-3060-2009
OI Stanislav, Mitrovic/0000-0003-0664-7270
ZS 0
Z8 0
ZR 0
ZA 0
TC 2
ZB 0
Z9 7
U1 0
U2 19
SN 1998-0663
DA 2018-12-28
UT WOS:000426970600004
ER

PT J
AU Aiken, Peter
TI EXPERIENCE: Succeeding at Data Management-BigCo Attempts to Leverage
   Data
SO ACM JOURNAL OF DATA AND INFORMATION QUALITY
VL 7
IS 1-2
AR 8
DI 10.1145/2893482
DT Article
PD JUN 2016
PY 2016
AB In amanner similar tomost organizations, BigCompany (BigCo) was
   determined to benefit strategically from its widely recognized and vast
   quantities of data. (U.S. government agencies make regular visits to
   BigCo to learn from its experiences in this area.) When faced with an
   explosion in data volume, increases in complexity, and a need to respond
   to changing conditions, BigCo struggled to respond using a traditional,
   information technology (IT) project-based approach to address these
   challenges. As BigCo was not data knowledgeable, it did not realize that
   traditional approaches could not work. Two full years into the
   initiative, BigCo was far from achieving its initial goals. How much
   more time, money, and effort would be required before results were
   achieved? Moreover, could the results be achieved in time to support a
   larger, critical, technology-driven challenge that also depended on
   solving the data challenges? While these questions remain unaddressed,
   these considerations increase our collective understanding of data
   assets as separate from IT projects. Only by reconceiving data as a
   strategic asset can organizations begin to address these new challenges.
   Transformation to a data-driven culture requires far more than
   technology, which remains just one of three required "stool legs"
   (people and process being the other two). Seven prerequisites to
   effectively leveraging data are necessary, but insufficient awareness
   exists in most organizations-hence, the widespread misfires in these
   areas, especially when attempting to implement the so-called big data
   initiatives. Refocusing on foundational data management practices is
   required for all organizations, regardless of their organizational or
   data strategies.
ZA 0
Z8 0
ZS 0
TC 6
ZB 0
ZR 0
Z9 7
U1 0
U2 16
SN 1936-1955
DA 2016-06-01
UT WOS:000445684600008
ER

PT J
AU Moguel, Enrique
   Preciado, Juan C.
   Sanchez-Figueroa, Fernando
   Preciado, Miguel A.
   Hernandez, Juan
TI Multilayer Big Data Architecture for Remote Sensing in Eolic Parks
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING
VL 8
IS 10
SI SI
BP 4714
EP 4719
DI 10.1109/JSTARS.2015.2415583
DT Article
PD OCT 2015
PY 2015
AB Due to their nature, Eolic parks are situated in zones with difficult
   access. As a result, management of Eolic parks using remote sensing
   techniques is of great importance. In addition, the huge amount of data
   managed by Eolic parks, together with their nature (distributed,
   heterogeneous, produced, consumed at different times, etc.) makes them
   ideal to apply big data techniques. In this paper, we present a
   multilayer hardware/software architecture that applies cloud computing
   techniques for managing big data from Eolic parks. This architecture
   allows tackling the processing of large, distributed, and heterogeneous
   data sets in a remote sensing context. An innovative contribution of
   this work is the combination of different techniques at three different
   layers of the proposed hardware/software architecture for Eolic park big
   data management and processing.
RI Preciado, Miguel/A-6431-2009; Preciado, Juan Carlos/; Moguel, Enrique/D-9813-2016; Hernandez, Juan/L-5065-2014; Sanchez-Figueroa, Fernando/L-4978-2014
OI Preciado, Juan Carlos/0000-0002-2582-9742; Moguel,
   Enrique/0000-0002-4096-1282; Hernandez, Juan/0000-0002-6343-7395;
   Sanchez-Figueroa, Fernando/0000-0002-6861-1540
ZB 0
Z8 0
ZA 0
TC 7
ZS 0
ZR 0
Z9 7
U1 0
U2 28
SN 1939-1404
EI 2151-1535
DA 2016-02-17
UT WOS:000368904000011
ER

PT C
AU Puri, Colin
   Dukatz, Carl
BE Spies, M
   Wagner, RR
   Tjoa, AM
TI Analyzing and Predicting Security Event Anomalies: Lessons Learned from
   a Large Enterprise Big Data Streaming Analytics Deployment
SO 2015 26TH INTERNATIONAL WORKSHOP ON DATABASE AND EXPERT SYSTEMS
   APPLICATIONS (DEXA)
SE International Workshop on Database and Expert Systems Applications-DEXA
BP 152
EP 158
DI 10.1109/DEXA.2015.46
DT Proceedings Paper
PD 2015
PY 2015
AB This paper presents a novel and unique live operational and situational
   awareness implementation bringing big data architectures, graph
   analytics, streaming analytics, and interactive visualizations to a
   security use case with data from a large Global 500 company. We present
   the data acceleration patterns utilized, the employed analytics
   framework and its complexities, and finally demonstrate the creation of
   rich interactive visualizations that bring the story of the data
   acceleration pipeline and analytics to life. We deploy a novel solution
   to learn typical network agent behaviors and extract the degree to which
   a network event is anomalous for automatic anomaly rule learning to
   provide additional context to security alerts. We implement and evaluate
   the analytics over a data acceleration framework that performs the
   analysis and model creation at scale in a distributed parallel manner.
   Additionally, we talk about the acceleration architecture considerations
   and demonstrate how we complete the analytics story with rich
   interactive visualizations designed for the security and business
   analyst alike. This paper concludes with evaluations and lessons
   learned.
CT 26th International Workshop on Database and Expert Systems Applications
   (DEXA)
CY SEP 01-04, 2015
CL Valencia, SPAIN
ZA 0
ZR 0
Z8 1
ZS 0
ZB 0
TC 4
Z9 7
U1 0
U2 4
SN 1529-4188
BN 978-1-4673-7581-8
DA 2016-09-08
UT WOS:000380461300025
ER

PT J
AU Salierno, Giulio
   Leonardi, Letizia
   Cabri, Giacomo
TI A Big Data Architecture for Digital Twin Creation of Railway Signals
   Based on Synthetic Data
SO IEEE OPEN JOURNAL OF INTELLIGENT TRANSPORTATION SYSTEMS
VL 5
BP 1
EP 18
DI 10.1109/OJITS.2024.3412820
DT Article
PD 2024
PY 2024
AB Industry 5.0 has introduced new possibilities for defining key features
   of the factories of the future. This trend has transformed traditional
   industrial production by exploiting Digital Twin (DT) models as virtual
   representations of physical manufacturing assets. In the railway
   industry, Digital Twin models offer significant benefits by enabling
   anticipation of developments in rail systems and subsystems, providing
   insight into the future performance of physical assets, and allowing
   testing and prototyping solutions prior to implementation. This paper
   presents our approach for creating a Digital Twin model in the railway
   domain. We particularly emphasize the critical role of Big Data in
   supporting decision-making for railway companies and the importance of
   data in creating virtual representations of physical objects in railway
   systems. Our results show that the Digital Twin model of railway switch
   points, based on synthetic data, accurately represents the behavior of
   physical railway switches in terms of data points.
RI Leonardi, Letizia/L-9722-2015; Salierno, Giulio/; Cabri, Giacomo/M-6723-2015
OI Leonardi, Letizia/0000-0003-4035-8560; Salierno,
   Giulio/0000-0002-9617-4448; Cabri, Giacomo/0000-0002-4942-2453
ZA 0
Z8 0
TC 4
ZB 0
ZR 0
ZS 0
Z9 6
U1 1
U2 12
EI 2687-7813
DA 2024-07-28
UT WOS:001273038700001
ER

PT J
AU Munshi, Amr
   Alhindi, Ahmad
   Qadah, Thamir M.
   Alqurashi, Amjad
TI An Electronic Commerce Big Data Analytics Architecture and Platform
SO APPLIED SCIENCES-BASEL
VL 13
IS 19
AR 10962
DI 10.3390/app131910962
DT Article
PD OCT 2023
PY 2023
AB The COVID-19 pandemic significantly increased e-commerce growth, adding
   more than 218 billion US dollars to the United States e-commerce sales.
   With this significant growth, various operational challenges have
   appeared, including logistic difficulties and customer satisfaction.
   Businesses that strive to take advantage of increased e-commerce growth
   must understand data and rely on e-commerce analytics. The large scale
   of e-commerce data requires sophisticated information technology
   techniques and cyber-infrastructure to leverage and analyze. This study
   presents a big e-commerce data platform to address several challenges in
   e-commerce. The presented platform's design is based on a distributed
   system architecture that supports e-commerce analytics applications
   using historical and real-time data and features a continuous feedback
   loop to observe the decision-making and evaluation processes to achieve
   the desired objectives. The platform was validated using two analytical
   applications. The first application was to identify the periods in which
   customers prefer to place orders, while the second was used to verify
   the big e-commerce data platform. The resulting insights and findings
   promote informed e-commerce decisions. Furthermore, viewing and acting
   on insight results and findings promote informed decisions that
   potentially benefit the e-commerce industry. The proposed platform can
   perform numerous e-commerce applications that potentially benefit the
   e-commerce industry.
RI Qadah, Thamir/AAG-7508-2019; Munshi, Amr/AHB-7543-2022; Alhindi, Ahmad/U-5347-2019
OI Qadah, Thamir/0000-0003-0754-0504; Munshi, Amr/0000-0002-4002-3755;
   Alhindi, Ahmad/0000-0002-0516-7868
ZS 0
ZA 0
ZR 0
Z8 0
ZB 0
TC 4
Z9 6
U1 17
U2 106
EI 2076-3417
DA 2023-11-01
UT WOS:001085550300001
ER

PT J
AU Capello, Alessio
   Fresta, Matteo
   Bellotti, Francesco
   Haghighi, Hamed
   Hiller, Johannes
   Mozaffari, Sajjad
   Berta, Riccardo
TI Exploiting Big Data for Experiment Reporting: The Hi-Drive Collaborative
   Research Project Case
SO SENSORS
VL 23
IS 18
AR 7866
DI 10.3390/s23187866
DT Article
PD SEP 2023
PY 2023
AB As timely information about a project's state is key for management, we
   developed a data toolchain to support the monitoring of a project's
   progress. By extending the Measurify framework, which is dedicated to
   efficiently building measurement-rich applications on MongoDB, we were
   able to make the process of setting up the reporting tool just a matter
   of editing a couple of .json configuration files that specify the names
   and data format of the project's progress/performance indicators. Since
   the quantity of data to be provided at each reporting period is
   potentially overwhelming, some level of automation in the extraction of
   the indicator values is essential. To this end, it is important to make
   sure that most, if not all, of the quantities to be reported can be
   automatically extracted from the experiment data files actually used in
   the project. The originating use case for the toolchain is a
   collaborative research project on driving automation. As data
   representing the project's state, 330+ numerical indicators were
   identified. According to the project's pre-test experience, the tool is
   effective in supporting the preparation of periodic progress reports
   that extensively exploit the actual project data (i.e., obtained from
   the sensors-real or virtual-deployed for the project). While the
   presented use case concerns the automotive industry, we have taken care
   that the design choices (particularly, the definition of the resources
   exposed by the Application Programming Interfaces, APIs) abstract the
   requirements, with an aim to guarantee effectiveness in virtually any
   application context.
RI Fresta, Matteo/NFS-1404-2025; Bellotti, Francesco/; Hiller, Johannes/; Berta, Riccardo/O-8165-2019; Capello, Alessio/
OI Fresta, Matteo/0009-0000-7265-7501; Bellotti,
   Francesco/0000-0003-4109-4675; Hiller, Johannes/0000-0003-1835-6067;
   Berta, Riccardo/0000-0003-1937-3969; Capello,
   Alessio/0000-0003-4277-7283
ZA 0
ZS 0
TC 6
Z8 0
ZB 0
ZR 0
Z9 6
U1 0
U2 5
EI 1424-8220
DA 2023-11-23
UT WOS:001095468300001
PM 37765923
ER

PT J
AU Benjelloun, Sarah
   El Aissi, Mohamed El Mehdi
   Lakhrissi, Younes
   El Haj Ben Ali, Safae
TI Data Lake Architecture for Smart Fish Farming Data-Driven Strategy
SO APPLIED SYSTEM INNOVATION
VL 6
IS 1
AR 8
DI 10.3390/asi6010008
DT Article
PD FEB 2023
PY 2023
AB Thanks to continuously evolving data management solutions, data-driven
   strategies are considered the main success factor in many domains. These
   strategies consider data as the backbone, allowing advanced data
   analytics. However, in the agricultural field, and especially in fish
   farming, data-driven strategies have yet to be widely adopted. This
   research paper aims to demystify the situation of the fish farming
   domain in general by shedding light on big data generated in fish farms.
   The purpose is to propose a dedicated data lake functional architecture
   and extend it to a technical architecture to initiate a fish farming
   data-driven strategy. The research opted for an exploratory study to
   explore the existing big data technologies and to propose an
   architecture applicable to the fish farming data-driven strategy. The
   paper provides a review of how big data technologies offer multiple
   advantages for decision making and enabling prediction use cases. It
   also highlights different big data technologies and their use. Finally,
   the paper presents the proposed architecture to initiate a data-driven
   strategy in the fish farming domain.
RI Lakhrissi, Younes/AAA-8819-2021
OI Lakhrissi, Younes/0000-0003-2718-7090
ZS 0
TC 5
ZA 0
Z8 0
ZB 2
ZR 0
Z9 6
U1 1
U2 8
EI 2571-5577
DA 2023-03-20
UT WOS:000938241700001
ER

PT J
AU Shih, Wen-Chung
   Yang, Chao-Tung
   Jiang, Cheng-Tian
   Kristiani, Endah
TI Implementation and visualization of a netflow log data lake system for
   cyberattack detection using distributed deep learning
SO JOURNAL OF SUPERCOMPUTING
VL 79
IS 5
BP 4983
EP 5012
DI 10.1007/s11227-022-04802-y
EA OCT 2022
DT Article
PD MAR 2023
PY 2023
AB Big data and artificial intelligence (AI) technology are complicated
   systems that will continue developing in recent years. This paper
   implemented a data lake architecture to handle massive data and perform
   data analysis in a real-time system. Using a data lake and AI model, a
   NetFlow storage monitoring system was deployed to perform a platform
   that can cover the storage, query, analysis, and visualization of
   massive volumes of data. The big data platform was built on Cloudera,
   which utilized big data tools like Kafka, Spark, HBase, Hive, and
   Impala. In addition, we used Spark to develop network threat recognition
   models using distributed deep learning. Also, we used the deep neural
   network (DNN) to train the model. Then, we evaluated the model
   performance, which reached 94% accuracy while decreasing by 48% of
   training time. The results of the studies demonstrate that deep learning
   model training time is significantly shortened. Additionally, this
   system employs several configurations to assess the elements influencing
   accuracy and performance. The model is evaluated using the confusion
   matrix to demonstrate that it can accurately detect attack behavior in
   log data. Furthermore, we have developed a real-time log data monitoring
   and analysis system to demonstrate the proposed architecture.
RI Yang, Chao-Tung/B-4562-2009; Kristiani, Endah/AAA-9579-2020
OI Yang, Chao-Tung/0000-0002-9579-4426; 
ZR 0
Z8 0
ZB 0
ZS 0
ZA 0
TC 6
Z9 6
U1 3
U2 21
SN 0920-8542
EI 1573-0484
DA 2022-10-16
UT WOS:000864553600001
ER

PT J
AU Di Martino, Beniamino
   Cante, Luigi Colucci
   D'Angelo, Salvatore
   Esposito, Antonio
   Graziano, Mariangela
   Marulli, Fiammetta
   Lupi, Pietro
   Cataldi, Alessandra
TI A Big Data Pipeline and Machine Learning for Uniform Semantic
   Representation of Data and Documents From IT Systems of the Italian
   Ministry of Justice
SO INTERNATIONAL JOURNAL OF GRID AND HIGH PERFORMANCE COMPUTING
VL 14
IS 1
DI 10.4018/IJGHPC.301579
DT Article
PD 2022
PY 2022
AB In this paper, a big data pipeline is presented, taking in consideration
   both structured and unstructured data made available by the Italian
   Ministry of Justice, regarding their telematic civil process. Indeed,
   the complexity and volume of the data provided by the ministry requires
   the application of big data analysis techniques, in concert with machine
   and deep learning frameworks, to be correctly analysed and to obtain
   meaningful information that could support the ministry itself in better
   managing civil processes. The pipeline has two main objectives: to
   provide a consistent workflow of activities to be applied to the
   incoming data, aiming at extracting useful information for the
   ministry's decision making tasks, and to homogenize the incoming data,
   so that they can be stored in a centralized and coherent data lake to be
   used as a reference for further analysis and considerations.
RI Di Martino, Beniamino/O-6876-2015; Esposito, Antonio/AHC-3301-2022; Colucci Cante, Luigi/; Graziano, Mariangela/HPF-2471-2023; Marulli, Fiammetta/AAD-4051-2022; D'Angelo, Salvatore/GQB-4948-2022
OI Di Martino, Beniamino/0000-0001-7613-1312; Esposito,
   Antonio/0000-0002-2004-4815; Colucci Cante, Luigi/0009-0005-5226-6737;
   Graziano, Mariangela/0000-0002-1258-8249; 
ZA 0
ZS 0
ZB 0
TC 5
ZR 0
Z8 0
Z9 6
U1 0
U2 1
SN 1938-0259
EI 1938-0267
DA 2023-02-17
UT WOS:000916579600019
ER

PT C
AU Pingos, Michalis
   Andreou, Andreas S.
BE Kaindl, H
   Mannion, M
   Maciaszek, L
TI A Data Lake Metadata Enrichment Mechanism via Semantic Blueprints
SO ENASE: PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON EVALUATION OF
   NOVEL APPROACHES TO SOFTWARE ENGINEERING
BP 186
EP 196
DI 10.5220/0011080400003176
DT Proceedings Paper
PD 2022
PY 2022
AB One of the greatest challenges in Smart Big Data Processing nowadays
   revolves around handling multiple heterogeneous data sources that
   produce massive amounts of structured, semi-structured and unstructured
   data through Data Lakes. The latter requires a disciplined approach to
   collect, store and retrieve/analyse data to enable efficient predictive
   and prescriptive modelling, as well as the development of other advanced
   analytics applications on top of it. The present paper addresses this
   highly complex problem and proposes a novel standardization framework
   that combines mainly the 5Vs Big Data characteristics, blueprint
   ontologies and Data Lakes with ponds architecture, to offer a metadata
   semantic enrichment mechanism that enables fast storing to and efficient
   retrieval from a Data Lake. The proposed mechanism is compared
   qualitatively against existing metadata systems using a set of
   functional characteristics or properties, with the results indicating
   that it is indeed a promising approach.
CT 17th International Conference on Evaluation of Novel Approaches to
   Software Engineering (ENASE)
CY APR 25-26, 2022
CL ELECTR NETWORK
SP INSTICC
RI Pingos, Michalis/OGO-6362-2025; Pingos, Michalis/
OI Pingos, Michalis/0000-0001-6293-6478
ZR 0
Z8 0
ZA 0
ZB 0
TC 5
ZS 0
Z9 6
U1 1
U2 10
BN 978-989-758-568-5
DA 2022-07-01
UT WOS:000814765400016
ER

PT C
AU Ramchand, Sonam
   Mahmood, Tariq
BE Leong, HV
   Sarvestani, SS
   Teranishi, Y
   Cuzzocrea, A
   Kashiwazaki, H
   Towey, D
   Yang, JJ
   Shahriar, H
TI BIG DATA ARCHITECTURES FOR DATA LAKES: A SYSTEMATIC LITERATURE REVIEW
SO 2022 IEEE 46TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE
   (COMPSAC 2022)
BP 1141
EP 1146
DI 10.1109/COMPSAC54236.2022.00179
DT Proceedings Paper
PD 2022
PY 2022
AB The rise in big technologies has been demanding different concepts and
   practices for data exploitation; among them data lake is a recently
   emerged concept that is meant to deal with the heterogeneous data. Data
   lakes have been residing in the big data era since 2010, but there has
   not been any systematic review yet over data lake implementation. In
   this research survey, we conduct a review and provide a road map to
   researcher that elaborates what has happened to data lakes till now. We
   aim to give understanding for basic concept of data lakes and propose a
   novel data lake definition that could best describe the concept based on
   the literature review. One of the main problem while implementing data
   lake is deciding the technologies to use, this study covers technologies
   that can potentially be used for data lake implementation. Furthermore,
   data lake architectures and their variants are discussed in detail.
   Moreover, we analyze current state, challenges, pros and cons of the
   data lake. This study is all in one place for researchers who try to
   understand data lake concept, architectures, technologies, approaches,
   current state and challenges.
CT 46th Annual IEEE-Computer-Society International Computers, Software, and
   Applications Conference (COMPSAC) - Computers, Software, and
   Applications in an Uncertain World
CY JUN 27-JUL 01, 2022
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc
ZA 0
TC 1
ZB 0
Z8 1
ZR 0
ZS 0
Z9 6
U1 2
U2 35
BN 978-1-6654-8810-5
DA 2022-09-30
UT WOS:000855983300171
ER

PT J
AU Souza, Raissa P. P. M.
   dos Santos, Leonardo J. A.
   Coimbra, Gabriel T. P.
   Silva, Fabricio A.
   Silva, Thais R. M. B.
TI A Big Data-Driven Hybrid Solution to the Indoor-Outdoor Detection
   Problem
SO BIG DATA RESEARCH
VL 24
AR 100194
DI 10.1016/j.bdr.2021.100194
EA JAN 2021
DT Article
PD MAY 15 2021
PY 2021
AB Context-aware mobile applications are emerging as a relevant technology
   to improve the users' satisfaction. By being aware of the current
   situation of a user, a context-aware application can adapt itself to
   provide the most suitable services at that moment. Companies from
   different areas are investing in turning their applications aware of the
   users' context with the objective of increasing their
   quality-of-experience and reducing churn. Among many possible contexts,
   the situation of a mobile user in terms of the type of environment
   (i.e., indoor or outdoor) is a relevant yet difficult to obtain
   information. In this work, we propose the HybridIO, a solution to the
   Indoor-Outdoor Detection Problem that advances the state-of-the-art,
   since it requires less sensor data to operate and generalizes well in
   other domains. We validate our solution considering a large amount of
   real data, and the results reveal an improvement of up to 14% in
   precision when compared to other baselines. We also deploy the proposed
   solution into a real-time big data architecture that is able to enrich
   up to 400 records/second with the indoor-outdoor information. (C) 2021
   Elsevier Inc. All rights reserved.
RI , Gabriel/; Mataruna-Dos-Santos, Leonardo/N-4919-2018
OI , Gabriel/0000-0001-5767-1081; 
Z8 0
ZB 0
ZA 0
TC 5
ZR 0
ZS 0
Z9 6
U1 0
U2 7
SN 2214-5796
DA 2021-05-20
UT WOS:000642459200002
ER

PT C
AU Liu, Pengfei
   Loudcher, Sabine
   Darmont, Jerome
   Nous, Camille
BE Desai, BC
TI ArchaeoDAL: A Data Lake for Archaeological Data Management and Analytics
SO IDEAS 2021: 25TH INTERNATIONAL DATABASE ENGINEERING & APPLICATIONS
   SYMPOSIUM
SE International Database Engineering and Applications Symposium -
   Proceedings
BP 252
EP 262
DI 10.1145/3472163.3472266
DT Proceedings Paper
PD 2021
PY 2021
AB With new emerging technologies, such as satellites and drones,
   archaeologists collect data over large areas. However, it becomes
   difficult to process such data in time. Archaeological data also have
   many different formats (images, texts, sensor data) and can be
   structured, semi-structured and unstructured. Such variety makes data
   difficult to collect, store, manage, search and analyze effectively. A
   few approaches have been proposed, but none of them covers the full data
   lifecycle nor provides an efficient data management system. Hence, we
   propose the use of a data lake to provide centralized data stores to
   host heterogeneous data, as well as tools for data quality checking,
   cleaning, transformation and analysis. In this paper, we propose a
   generic, flexible and complete data lake architecture. Our metadata
   management system exploits goldMEDAL, which is the most generic metadata
   model currently available. Finally, we detail the concrete
   implementation of this architecture dedicated to an archaeological
   project.
CT 25th International Database Applications and Engineering Symposium
   (IDAES)
CY JUL 14-16, 2021
CL Concordia Univ, ELECTR NETWORK
HO Concordia Univ
OI Loudcher, Sabine/0000-0002-0494-0169
ZB 0
ZS 0
ZA 0
Z8 1
TC 3
ZR 0
Z9 6
U1 0
U2 9
SN 1098-8068
BN 978-1-4503-8991-4
DA 2022-02-11
UT WOS:000749525000028
ER

PT C
AU Shakir, Aamir
   Staegemann, Daniel
   Volk, Matthias
   Jamous, Naoum
   Turowski, Klaus
BE Abramowicz, W
   Auer, S
   Lewanska, E
TI Towards a Concept for Building a Big Data Architecture with
   Microservices
SO 24TH INTERNATIONAL CONFERENCE ON BUSINESS INFORMATION SYSTEMS (BIS):
   ENTERPRISE KNOWLEDGE AND DATA SPACES
BP 83
EP 94
DI 10.52825/bis.v1i.67
DT Proceedings Paper
PD 2021
PY 2021
AB Microservices and Big Data are renowned hot topics in computer science
   that have gained a lot of hype. While the use of microservices is an
   approach that is used in modern software development to increase
   flexibility, Big Data allows organizations to turn today's information
   deluge into valuable insights. Many of those Big Data architectures have
   rather monolithic elements. However, a new trend arises in which
   monolithic architectures are replaced with more modularized ones, such
   as microservices. This transformation provides the benefits from
   microservices such as modularity, evolutionary design and extensibility
   while maintaining the old monolithic product's functionality. This is
   also valid for Big Data architectures. To facilitate the success of this
   transformation, there are certain beneficial factors. In this paper,
   those aspects will be presented and the transformation of an exemplary
   Big Data architecture with somewhat monolithic elements into a
   microservice favoured one is outlined.
CT 24th International Conference on Business Information Systems (BIS)
CY JUN 14-17, 2021
CL ELECTR NETWORK
SP BIS Steering Comm; TIB Leibniz Informat Ctr Sci & Technol; Univ Hannover
RI Staegemann, Daniel/MWO-8533-2025
ZS 0
ZB 0
TC 6
ZR 0
ZA 0
Z8 0
Z9 6
U1 0
U2 2
BN *****************
DA 2022-06-24
UT WOS:000811810900008
ER

PT C
AU Wrembel, Robert
BE Guetl, C
   Ceravolo, P
   Jararweh, Y
   Benkhelifa, E
   Adedugbe, O
TI Still Open Problems in Data Warehouse and Data Lake Research extended
   abstract
SO 2021 EIGHTH INTERNATIONAL CONFERENCE ON SOCIAL NETWORK ANALYSIS,
   MANAGEMENT AND SECURITY (SNAMS)
BP 181
EP 183
DI 10.1109/SNAMS53716.2021.9732098
DT Proceedings Paper
PD 2021
PY 2021
CT 8th International Conference on Social Network Analysis, Management and
   Security (SNAMS)
CY DEC 06-09, 2021
CL ELECTR NETWORK
SP IEEE Spain Sect; Univ Politecnica Valencia; Al Ain Univ; TU Graz;
   Staffordshire Univ
RI Wrembel, Robert/F-7482-2014
OI Wrembel, Robert/0000-0001-6037-5718
TC 2
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
Z9 6
U1 1
U2 9
BN 978-1-6654-9495-3
DA 2022-07-01
UT WOS:000813133100025
ER

PT J
AU Lehmann, Claude
   Huber, Lilach Goren
   Horisberger, Thomas
   Scheiba, Georg
   Sima, Ana Claudia
   Stockinger, Kurt
TI Big Data architecture for intelligent maintenance: a focus on query
   processing and machine learning algorithms
SO JOURNAL OF BIG DATA
VL 7
IS 1
AR 61
DI 10.1186/s40537-020-00340-7
DT Article
PD AUG 12 2020
PY 2020
AB Exploiting available condition monitoring data of industrial machines
   for intelligent maintenance purposes has been attracting attention in
   various application fields. Machine learning algorithms for fault
   detection, diagnosis and prognosis are popular and easily accessible.
   However, our experience in working at the intersection of academia and
   industry showed that the major challenges of building an end-to-end
   system in a real-world industrial setting go beyond the design of
   machine learning algorithms. One of the major challenges is the design
   of an end-to-end data management solution that is able to efficiently
   store and process large amounts of heterogeneous data streams resulting
   from a variety of physical machines. In this paper we present the design
   of an end-to-end Big Data architecture that enables intelligent
   maintenance in a real-world industrial setting. In particular, we will
   discuss various physical design choices for optimizing high-dimensional
   queries, such as partitioning and Z-ordering, that serve as the basis
   for health analytics. Finally, we describe a concrete fault detection
   use case with two different health monitoring algorithms based on
   machine learning and classical statistics and discuss their advantages
   and disadvantages. The paper covers some of the most important aspects
   of the practical implementation of such an end-to-end solution and
   demonstrates the challenges and their mitigation for the specific
   application of laser cutting machines.
RI Stockinger, Kurt/; Lehmann, Claude/; Sima, Ana Claudia/LMO-1072-2024
OI Stockinger, Kurt/0000-0003-4034-4812; Lehmann,
   Claude/0000-0002-4693-0444; Sima, Ana Claudia/0000-0003-3213-4495
ZA 0
ZB 0
ZS 0
Z8 0
TC 6
ZR 0
Z9 6
U1 1
U2 11
EI 2196-1115
DA 2020-12-18
UT WOS:000596164700001
ER

PT J
AU Asemi, Asefeh
   Ebrahimi, Fezzeh
TI A Thematic Analysis of the Articles on the Internet of Things in the Web
   of Science With HAC Approach
SO INTERNATIONAL JOURNAL OF DISTRIBUTED SYSTEMS AND TECHNOLOGIES
VL 11
IS 2
SI SI
BP 1
EP 17
DI 10.4018/IJDST.2020040101
DT Article
PD APR-JUN 2020
PY 2020
AB This research was carried out using the bibliometric method to
   thematically analyze the articles on IoT in the Web of Science with
   Hierarchical Agglomerative Clustering approach. First, the descriptors
   of the related articles published from 2002 to 2016 were extracted from
   WoS, by conducting a keyword search using the "Internet of Things"
   keyword. Data analysis and clustering were carried out in SPSS, UCINET,
   and PreMap. The analysis results revealed that the scientific literature
   published on IoT during the period had grown exponentially, with an
   approximately 48% growth rate in the last two years of the study period
   (i.e. 2015 and 2016). After analyzing the themes of the documents, the
   resulting concepts were classified into twelve clusters. The twelve main
   clusters included: Privacy and Security, Authentication and
   Identification, Computing, Standards and Protocols, IoT as a component,
   Big Data, Architecture, Applied New Techniques in IoT, Application,
   Connection and Communication Tools, Wireless Network Protocols, and
   Wireless Sensor Networks.
RI Asemi, Asefeh/N-2174-2019; Ebrahimi, Fezzeh/AAX-2556-2021
OI Asemi, Asefeh/0000-0003-1667-4408; Ebrahimi, Fezzeh/0000-0002-6709-8327
TC 5
ZB 0
ZS 0
ZR 0
Z8 0
ZA 0
Z9 6
U1 0
U2 29
SN 1947-3532
EI 1947-3540
DA 2020-02-20
UT WOS:000511366000002
ER

PT J
AU Kachaoui, Jabrane
   Belangour, Abdessamad
TI Enhanced Data Lake Clustering Design based on K-means Algorithm
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 11
IS 4
BP 547
EP 554
DT Article
PD APR 2020
PY 2020
AB In recent years, Big Data requirements have evolved. Organizations are
   trying more than ever to accent their efforts on industrial development
   of all data at their disposal and move further away from underpinning
   technologies. After investing around Data Lake concept, organizations
   must now overhaul their data architecture to face IoT (Internet of
   Things) and AI (Artificial Intelligence) expansion. Efficient and
   effective data mapping treatments could serve in understanding the
   importance of data being transformed and used for decision-making
   process endorsement. As current relational databases are not able to
   manage large amounts of data, organizations headed towards NoSQL (Not
   only Structured Query Language) databases. One such known NoSQL database
   is MongoDB, which has a high scalability. This article mainly put
   forward a new data model able to extract, classify, and then map data
   for the purpose of generating new more structured data that meet
   organizational needs. This can be carried out by calculating various
   metadata attributes weights, which are considered as important
   information. It also processed on data clustering stored into MongoDB.
   This categorization based on data mining clustering algorithm named
   K-Means.
RI Belangour, Abdessamad/KAL-6712-2024
ZB 1
TC 6
ZA 0
ZR 0
Z8 0
ZS 0
Z9 6
U1 0
U2 4
SN 2158-107X
EI 2156-5570
DA 2020-06-16
UT WOS:000537489900072
ER

PT C
AU Ghane, Kamran
GP IEEE
TI Big Data Pipeline with ML-based and Crowd Sourced Dynamically Created
   and Maintained Columnar Data Warehouse for Structured and Unstructured
   Big Data
SO 2020 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER
   TECHNOLOGIES (ICICT 2020)
BP 60
EP 67
DI 10.1109/ICICT50521.2020.00018
DT Proceedings Paper
PD 2020
PY 2020
AB The existing big data platforms take data through distributed processing
   platforms and store them in a data lake. The architectures such as
   Lambda and Kappa address the real-time and batch processing of data.
   Such systems provide real time analytics on the raw data and delayed
   analytics on the curated data. The data denormalization, creation and
   maintenance of a columnar dimensional data warehouse is usually time
   consuming with no or limited support for unstructured data. The system
   introduced in this paper automatically creates and dynamically maintains
   its data warehouse as a part of its big data pipeline in addition to its
   data lake. It creates its data warehouse on structured, semi-structured
   and unstructured data. It uses Machine Learning to identify and create
   dimensions. It also establishes relations among data from different data
   sources and creates the corresponding dimensions. It dynamically
   optimizes the dimensions based on the crowd sourced data provided by end
   users and also based on query analysis.
CT 3rd International Conference on Information and Computer Technologies
   (ICICT)
CY MAR 09-12, 2020
CL San Jose, CA
ZS 0
Z8 0
ZA 0
ZB 0
ZR 0
TC 5
Z9 6
U1 2
U2 7
BN 978-1-7281-7283-5
DA 2020-11-24
UT WOS:000582696300011
ER

PT J
AU Martinez-Mosquera, Diana
   Navarrete, Rosa
   Lujan-Mora, Sergio
TI Development and Evaluation of a Big Data Framework for Performance
   Management in Mobile Networks
SO IEEE ACCESS
VL 8
BP 226380
EP 226396
DI 10.1109/ACCESS.2020.3045175
DT Article
PD 2020
PY 2020
AB In telecommunications, Performance Management (PM) data are collected
   from network elements to a centralized system, the Network Management
   System (NMS), which acts as a business intelligence tool specialized in
   monitoring and reporting network performance. Performance Management
   files contain the metrics and named counters used to quantify the
   performance of the network. Current NMS implementations have limitations
   in scalability and support for volume, variety, and velocity of the
   collected PM data, especially for 5G and 6G mobile network technologies.
   To overcome these limitations, we proposed a Big Data framework based on
   an analysis of the following components: software architecture,
   ingestion, data lake, processing, reporting, and deployment. Our work
   analyzed the PM files' format on a real data set from four different
   vendors and 2G, 3G, 4G, and 5G technologies. Then, we experimentally
   assessed our proposed framework's feasibility through a case study
   involving 5G PM files. Test results of the ingestion and reporting
   components are presented, identifying the hardware and software required
   to support up to one billion counters per hour. This proposal can help
   telecommunications operators to have a reference Big Data framework to
   face the current and future challenges in the NMS, for instance, the
   support of data analytics in addition to the well-known services.
RI Navarrete, Rosa/; Luján-Mora, Sergio/D-9207-2013; Martinez-Mosquera, Diana/
OI Navarrete, Rosa/0000-0002-1435-5000; Luján-Mora,
   Sergio/0000-0001-5000-864X; Martinez-Mosquera, Diana/0000-0002-0573-8640
Z8 0
ZS 0
ZB 0
ZR 0
ZA 0
TC 4
Z9 6
U1 0
U2 7
SN 2169-3536
DA 2021-01-19
UT WOS:000604553200001
ER

PT J
AU Simionato, Rafael
   Torres Neto, Jose Rodrigues
   dos Santos, Carla Julciane
   Ribeiro, Bruno Silva
   Britto de Araujo, Fernando Cesar
   de Paula, Antonio Robson
   de Lima Oliveira, Pedro Augusto
   Fernandes, Paulo Silas
   Yi, Jin Hong
TI Survey on connectivity and cloud computing technologies:
   State-of-the-art applied to Agriculture 4.0
SO REVISTA CIENCIA AGRONOMICA
VL 51
SI SI
AR e20207755
DI 10.5935/1806-6690.20200085
DT Article
PD 2020
PY 2020
AB In recent years, agriculture has faced many challenges, from a growing
   global population to be fed, the work power evasion in the sector, to
   sustainability requirements and environmental constraints. To satisfy
   the increasingly demanding stakeholders, the agricultural sector has
   looked for new ways to tackle these issues. In this context, Information
   and Communications Technologies (ICTs) have been applied to help the
   agricultural sector overcome these challenges. This article investigates
   how two ICTs - connectivity and cloud computing - can leverage and
   traverse other ICTs, such as Internet of Things and artificial
   intelligence, enabling the entire productive sector to be supported by
   decision-making systems, which in turn are based on data-driven models.
   Moreover, a successful case study on how cloud computing has helped one
   of SiDi's biggest customers - a global company - improve its operational
   performance by obtaining insights from its data is presented.
ZS 0
ZR 0
ZA 0
ZB 2
TC 3
Z8 0
Z9 6
U1 1
U2 15
SN 0045-6888
EI 1806-6690
DA 2021-09-17
UT WOS:000692719200019
ER

PT C
AU Lamrhari, Soumaya
   Elghazi, Hamid
   Sadiki, Tayeb
   El Faker, Abdellatif
BE Essaaidi, M
   ElHani, S
TI A Profile-Based Big Data Architecture for Agricultural Context
SO 2016 INTERNATIONAL CONFERENCE ON ELECTRICAL AND INFORMATION TECHNOLOGIES
   (ICEIT)
BP 22
EP 27
DT Proceedings Paper
PD 2016
PY 2016
AB Bringing Big data technologies into agriculture presents a significant
   challenge; at the same time, this technology contributes effectively in
   many countries' economic and social development. In this work, we will
   study environmental data provided by precision agriculture information
   technologies, which represents a crucial source of data in need of being
   wisely managed and analyzed with appropriate methods and tools in order
   to extract the meaningful information.
   Our main purpose through this paper is to propose an effective Big data
   architecture based on profiling system which can assist (among others)
   producers, consulting companies, public bodies and research laboratories
   to make better decisions by providing them real time data processing,
   and a dynamic big data service composition method, to enhance and
   monitor the agricultural productivity. Thus, improve their traditional
   decision-making process, and allow better management of the natural
   resources.
CT 2nd International Conference on Electrical and Information Technologies
   (ICEIT)
CY MAY 04-07, 2016
CL Tangier, MOROCCO
RI EL GHAZI, Hamid/KFB-5688-2024; EL FAKER, Abdellatif/HGD-3815-2022
OI EL GHAZI, Hamid/0000-0002-2790-4419; 
ZS 0
ZB 1
ZR 0
Z8 0
TC 5
ZA 0
Z9 6
U1 0
U2 3
BN 978-1-4673-8469-8
DA 2017-02-08
UT WOS:000391354500004
ER

PT C
AU Singh, Karamjit
   Paneri, Kaushal
   Pandey, Aditeya
   Gupta, Garima
   Sharma, Geetika
   Agarwal, Puneet
   Shroff, Gautam
GP IEEE
TI Visual Bayesian Fusion to Navigate a Data Lake
SO 2016 19TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION)
BP 987
EP 994
DT Proceedings Paper
PD 2016
PY 2016
AB The evolution from traditional business intelligence to big data
   analytics has witnessed the emergence of 'Data Lakes' in which data is
   ingested in raw form rather than into traditional data warehouses. With
   the increasing availability of many more pieces of information about
   each entity of interest, e.g., a customer, often from diverse sources
   (socialmedia, mobility, internet-of-things), fusing, visualizing and
   deriving insights from such data pose a number of challenges: First,
   disparate datasets often lack a natural join key. Next, datasets may
   describe measures at different levels of granularity, e.g., individual
   vs. aggregate data, and finally, different datasets may be derived from
   physically distinct populations. Moreover, once data has been fused,
   queries are often an inefficient and inaccurate mechanism to derive
   insight from high-dimensional data. In this paper we describe iFuse, a
   data-fusion based visual analytics platform for navigating a data lake
   to derive insights. We rely on Bayesian graphical models to provide
   useful rudder with which to fuse and analyze disparate islands of data
   in a systematic manner. Our platform allows for rich interactive
   visualizations, querying and keyword-based search within and across
   datasets or models, as well as intuitive visual interfaces for
   value-imputation or model-based predictions. We illustrate the use of
   our platform in multiple scenarios, including two public data challenges
   as well as a real-life industry use-case involving the probabilistic
   fusion of datasets that lack a natural join-key.
CT 19th International Conference on Information Fusion (FUSION)
CY JUL 05-08, 2016
CL Heidelberg, GERMANY
SP Robert Bosch GmbH; ATLAS ELEKTRONIK; RANDOM SETS LLC; Syst & Technol
   Res; Metron; Continental; AIRBUS; BECKHOFF; Springer; Int Soc Informat
   Fus; Intelligent Sensor Actuator Syst; Karlsruhe Inst Technol;
   Fraunhofer; VDE; COMO; Conf Catalysts LLC; IEEE, AESS Soc
ZA 0
ZS 0
TC 5
ZR 0
ZB 0
Z8 0
Z9 6
U1 0
U2 3
BN 978-0-9964-5274-8
DA 2017-02-01
UT WOS:000391273400132
ER

PT C
AU Oliveira e Sa, Jorge
   Martins, Cesar
   Simoes, Paulo
BE Rocha, A
   Correia, AM
   Costanzo, S
   Reis, LP
TI Big Data in Cloud: A Data Architecture
SO NEW CONTRIBUTIONS IN INFORMATION SYSTEMS AND TECHNOLOGIES, VOL 1, PT 1
SE Advances in Intelligent Systems and Computing
VL 353
BP 723
EP 732
DI 10.1007/978-3-319-16486-1_71
PN 1
DT Proceedings Paper
PD 2015
PY 2015
AB Nowadays, organizations have at their disposal a large volume of data
   with a wide variety of types. Technology-driven organizations want to
   capture process and analyze this data at a fast velocity, in order to
   better understand and manage their customers, their operations and their
   business processes. As much as data volume and variety increases and as
   faster analytic results are needed, more demanding is for a data
   architecture. This data architecture should enable collecting, storing,
   and analyzing Big Data in Cloud Environment. Cloud Computing, ensures
   timeliness, ubiquity and easy access by users. This paper proposes to
   develop a data architecture to support Big Data in Cloud and, finally,
   validate the architecture with a proof of concept.
CT World Conference on Information Systems and Technologies (WorldCIST)
CY APR 01-03, 2015
CL Univ Azores, Ponta Delgada, PORTUGAL
HO Univ Azores
RI Simoes, Paulo/; Oliveira e Sá, Jorge/B-7176-2012
OI Simoes, Paulo/0000-0002-5079-8327; Oliveira e Sá,
   Jorge/0000-0003-4095-3431
ZR 0
Z8 1
TC 3
ZB 0
ZS 0
ZA 0
Z9 6
U1 0
U2 10
SN 2194-5357
EI 2194-5365
BN 978-3-319-16486-1; 978-3-319-16485-4
DA 2016-09-22
UT WOS:000381744400071
ER

PT J
AU Gebler, Richard
   Reinecke, Ines
   Sedlmayr, Martin
   Goldammer, Miriam
TI Enhancing Clinical Data Infrastructure for AI Research: Comparative
   Evaluation of Data Management Architectures
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 27
AR e74976
DI 10.2196/74976
DT Article
PD AUG 1 2025
PY 2025
AB Background:The rapid growth of clinical data, driven by digital
   technologies and high-resolution sensors, presents significant
   challenges for health care organizations aiming to support advanced
   artificial intelligence research and improve patient care. Traditional
   data management approaches may struggle to handle the large, diverse,
   and rapidly updating datasets prevalent in modern clinical environments.
   Objective:This study aimed to compare 3 clinical data management
   architectures-clinical data warehouses, clinical data lakes, and
   clinical data lakehouses-by analyzing their performance using the FAIR
   (findable, accessible, interoperable, and reusable) principles and the
   big data 5 V's (volume, variety, velocity, veracity, and value). The aim
   was to provide guidance on selecting an architecture that balances
   robust data governance with the flexibility required for advanced
   analytics. Methods:We developed a comprehensive analysis framework that
   integrates aspects of data governance with technical performance
   criteria. A rapid literature review was conducted to synthesize evidence
   from multiple studies, focusing on how each architecture manages large,
   heterogeneous, and dynamically updating clinical data. The review
   assessed key dimensions such as scalability, real-time processing
   capabilities, metadata consistency, and the technical expertise required
   for implementation and maintenance. Results:The results show that
   clinical data warehouses offer strong data governance, stability, and
   structured reporting, making them well suited for environments that
   require strict compliance and reliable analysis. However, they are
   limited in terms of real-time processing and scalability. In contrast,
   clinical data lakes offer greater flexibility and cost-effective
   scalability for managing heterogeneous data types, although they may
   suffer from inconsistent metadata management and challenges in
   maintaining data quality. Clinical data lakehouses combine the strengths
   of both approaches by supporting real-time data ingestion and structured
   querying; however, their hybrid nature requires high technical expertise
   and involves complex integration efforts. Conclusions:The optimal data
   management architecture for clinical applications depends on an
   organization's specific needs, available resources, and strategic goals.
   Health care institutions need to weigh the trade-offs between robust
   data governance, operational flexibility, and scalability to build
   future-proof infrastructures that support both clinical operations and
   artificial intelligence research. Further research should focus on
   simplifying the complexity of hybrid models and improving the
   integration of clinical standards to improve overall system reliability
   and ease of implementation.
RI Reinecke, Ines/HPF-0540-2023; Gebler, Richard/KDM-7676-2024; Goldammer, Miriam/; Sedlmayr, Martin/D-8828-2011
OI Goldammer, Miriam/0000-0003-2126-290X; Sedlmayr,
   Martin/0000-0002-9888-8460
ZB 0
TC 4
ZS 0
ZR 0
ZA 0
Z8 0
Z9 5
U1 3
U2 3
SN 1439-4456
EI 1438-8871
DA 2025-11-24
UT WOS:001615183600002
PM 40749197
ER

PT J
AU He, Zilong
   Fang, Wei
TI Research data management in institutional repositories: an architectural
   approach using data lakehouses
SO DIGITAL LIBRARY PERSPECTIVES
VL 41
IS 1
BP 145
EP 178
DI 10.1108/DLP-02-2024-0022
EA DEC 2024
DT Article
PD JAN 28 2025
PY 2025
AB PurposeThis paper aims to address the pressing challenges in research
   data management within institutional repositories, focusing on the
   escalating volume, heterogeneity and multi-source nature of research
   data. The aim is to enhance the data services provided by institutional
   repositories and modernise their role in the research
   ecosystem.Design/methodology/approachThe authors analyse the evolution
   of data management architectures through literature review, emphasising
   the advantages of data lakehouses. Using the design science research
   methodology, the authors develop an end-to-end data lakehouse
   architecture tailored to the needs of institutional repositories. This
   design is refined through interviews with data management professionals,
   institutional repository administrators and researchers.FindingsThe
   authors present a comprehensive framework for data lakehouse
   architecture, comprising five fundamental layers: data collection, data
   storage, data processing, data management and data services. Each layer
   articulates the implementation steps, delineates the dependencies
   between them and identifies potential obstacles with corresponding
   mitigation strategies.Practical implicationsThe proposed data lakehouse
   architecture provides a practical and scalable solution for
   institutional repositories to manage research data. It offers a range of
   benefits, including enhanced data management capabilities, expanded data
   services, improved researcher experience and a modernised institutional
   repository ecosystem. The paper also identifies and addresses potential
   implementation obstacles and provides valuable guidance for institutions
   embarking on the adoption of this architecture. The implementation in a
   university library showcases how the architecture enhances data sharing
   among researchers and empowers institutional repository administrators
   with comprehensive oversight and control of the university's research
   data landscape.Originality/valueThis paper enriches the theoretical
   knowledge and provides a comprehensive research framework and paradigm
   for scholars in research data management. It details a pioneering
   application of the data lakehouse architecture in an academic setting,
   highlighting its practical benefits and adaptability to meet the
   specific needs of institutional repositories.
ZR 0
ZA 0
ZB 0
TC 3
Z8 1
ZS 0
Z9 5
U1 12
U2 25
SN 2059-5816
EI 2054-1694
DA 2024-12-25
UT WOS:001380162800001
ER

PT J
AU Al Jawarneh, Isam Mashhour
   Bellavista, Paolo
   Corradi, Antonio
   Foschini, Luca
   Montanari, Rebecca
TI SpatialSSJP: QoS-Aware Adaptive Approximate Stream-Static Spatial Join
   Processor
SO IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS
VL 35
IS 1
BP 73
EP 88
DI 10.1109/TPDS.2023.3330669
DT Article
PD JAN 2024
PY 2024
AB The widespread adoption of Internet of Things (IoT) motivated the
   emergence of mixed workloads in smart cities, where fast arriving
   geo-referenced big data streams are joined with archive tables, aiming
   at enriching streams with descriptive attributes that enable insightful
   analytics. Applications are now relying on finding, in real-time, to
   which geographical regions data streaming tuples belong. This problem
   requires a computationally intensive stream-static join for joining a
   dynamic stream with a disk-resident static table. In addition, the
   time-varying nature of fluctuation in geospatial data arriving online
   calls for an approximate solution that can trade-off QoS constraints
   while ensuring that the system survives sudden spikes in data loads. In
   this paper, we present SpatialSSJP, an adaptive spatial-aware
   approximate query processing system that specifically focuses on
   stream-static joins in a way that guarantees achieving an agreed set of
   Quality-of-Service goals and maintains geo-statistics of stateful online
   aggregations over stream-static join results. SpatialSSJP employs a
   state-of-art stratified-like sampling design to select well-balanced
   representative geospatial data stream samples and serve them to a
   stream-static geospatial join operator downstream. We implemented a
   prototype atop Spark Structured Streaming. Our extensive evaluations on
   big real datasets show that our system can survive and mitigate harsh
   join workloads and outperform state-of-art baselines by significant
   magnitudes, without risking rigorous error bounds in terms of the
   accuracy of the output results. SpatialSSJP achieves a relative accuracy
   gain against plain Spark joins of approximately 10% in worst cases but
   reaching up to 50% in best case scenarios.
RI Foschini, Luca/H-6876-2015; montanari, rebecca/; Corradi, Antonio/L-7480-2015; Bellavista, Paolo/H-7256-2014; AL JAWARNEH, ISAM MASHHOUR/O-8313-2019; Foschini, Luca/L-7480-2015
OI Foschini, Luca/0000-0001-9062-3647; montanari,
   rebecca/0000-0002-3687-0361; Corradi, Antonio/0000-0002-5107-1023;
   Bellavista, Paolo/0000-0003-0992-7948; AL JAWARNEH, ISAM
   MASHHOUR/0000-0002-4796-2181; Foschini, Luca/0000-0001-9062-3647
ZA 0
ZB 0
TC 5
ZR 0
ZS 0
Z8 0
Z9 5
U1 0
U2 3
SN 1045-9219
EI 1558-2183
DA 2023-12-25
UT WOS:001122809600002
ER

PT J
AU Ali, A. Althaf
   Umamaheswari, S.
   Khan, A. B. Feroz
   Ramakrishnan, Jayabrabu
TI Fuzzy rules-based Data Analytics and Machine Learning for Prognosis and
   Early Diagnosis of Coronary Heart Disease
SO JOURNAL OF INFORMATION AND ORGANIZATIONAL SCIENCES
VL 48
IS 1
BP 167
EP 181
DI 10.31341/jios.48.1.9
DT Article
PD 2024
PY 2024
AB Globally, cardiovascular diseases stand as the primary cause of
   mortality. In response to the imperative to enhance operational
   efficiency and reduce expenses, healthcare organizations are currently
   undergoing a transformation. The incorporation of analytics into their
   IT strategy is vital for the successful execution of this transition.
   The approach involves consolidating data from various sources into a
   data lake, which is then leveraged with analytical models to
   revolutionize predictive analytics. The deployment of IoT-based
   predictive systems is aimed at diminishing mortality rates, particularly
   in the domain of coronary heart disease prognosis. However, the abundant
   and diverse nature of data across various disciplines poses significant
   challenges in terms of data analysis, extraction, management, and
   configuration within these large-scale data technologies and tools. In
   this context, a multi -level fuzzy rule generation approach is put
   forward to identify the characteristics necessary for heart disease
   prediction. These features are subsequently trained using an optimized
   recurrent neural network. Medical professionals assess and categorize
   the features into labeled classes based on the perceived risk. This
   categorization allows for early diagnosis and prompt treatment. In
   comparison to conventional systems, the proposed method demonstrates
   superior performance.
RI Ramakrishnan, Jayabrabu/IQT-7992-2023; Feroz khan, AB/A-3032-2016
OI Ramakrishnan, Jayabrabu/0000-0002-9081-116X; Feroz khan,
   AB/0000-0002-9395-9493
TC 3
ZA 0
ZS 0
Z8 0
ZR 0
ZB 0
Z9 5
U1 1
U2 6
SN 1846-3312
EI 1846-9418
DA 2024-08-01
UT WOS:001278531600001
ER

PT C
AU Abughazala, Moamin
   Muccini, Henry
GP IEEE
TI Modeling Data Analytics Architecture for IoT Applications using DAT
SO 2023 IEEE 20TH INTERNATIONAL CONFERENCE ON SOFTWARE ARCHITECTURE
   COMPANION, ICSA-C
SE IEEE International Conference on Software Architecture Workshops
BP 284
EP 291
DI 10.1109/ICSA-C57050.2023.00066
DT Proceedings Paper
PD 2023
PY 2023
AB Data analysis plays a significant role in extracting meaningful
   information from big data. Data analysis consists of acquisition,
   storage, management, analytics, and visualization. Providing an abstract
   view of data analytics applications is crucial to ensure that the data
   will transfer into meaningful information. Data Architecture is one of
   the ways to provide that. This article shows industrial experiences in
   building data analytics architecture (DAA). We use model-driven
   engineering to model a data-analytics architecture for applications
   using DAT. We evaluated this work by modeling Analytics Data Warehouses
   as a case study from one company, receiving feedback.
CT IEEE 20th International Conference on Software Architecture (ICSA)
CY MAR 13-17, 2023
CL Aquila, ITALY
SP IEEE; IEEE Comp Soc
OI Abughazala, Moamin/0000-0003-4946-6269
ZR 0
TC 5
ZB 0
Z8 0
ZS 0
ZA 0
Z9 5
U1 0
U2 0
SN 2768-427X
BN 978-1-6654-6459-8
DA 2023-06-14
UT WOS:000990534100050
ER

PT C
AU Yuan, Qin
   Yuan, Ye
   Wen, Zhenyu
   Wang, He
   Tang, Shiyuan
GP ACM
TI An Effective Framework for Enhancing Query Answering in a Heterogeneous
   Data Lake
SO PROCEEDINGS OF THE 46TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH
   AND DEVELOPMENT IN INFORMATION RETRIEVAL, SIGIR 2023
BP 770
EP 780
DI 10.1145/3539618.3591637
DT Proceedings Paper
PD 2023
PY 2023
AB There has been a growing interest in cross-source searching to gain rich
   knowledge in recent years. A data lake collects massive raw and
   heterogeneous data with different data schemas and query interfaces.
   Many real-life applications require query answering over the
   heterogeneous data lake, such as e-commerce, bioinformatics and
   healthcare. In this paper, we propose LakeAns that semantically
   integrates heterogeneous data schemas of the lake to enhance the
   semantics of query answers. To this end, we propose a novel framework to
   efficiently and effectively perform the cross-source searching. The
   framework exploits a reinforcement learning method to semantically
   integrate the data schemas and further create a global relational schema
   for the heterogeneous data. It then performs a query answering algorithm
   based on the global schema to find answers across multiple data sources.
   We conduct extensive experimental evaluations using real-life data to
   verify that our approach outperforms existing solutions in terms of
   effectiveness and efficiency.
CT 46th International ACM SIGIR Conference on Research and Development in
   Information Retrieval (SIGIR)
CY JUL 23-27, 2023
CL Taipei, TAIWAN
SP Assoc Comp Machinery; ACM Special Interest Grp Informat Retrieval
RI Wang, He/; Wen, Zhenyu/LUY-2813-2024; Yuan, Qin/
OI Wang, He/0000-0003-1080-6406; Yuan, Qin/0009-0001-7123-6155
Z8 0
TC 5
ZR 0
ZS 0
ZB 0
ZA 0
Z9 5
U1 1
U2 6
BN 978-1-4503-9408-6
DA 2024-02-14
UT WOS:001118084000078
ER

PT J
AU Valero-Ramon, Zoe
   Louro, Pedro
   Irio, Luis
   Dimitriadis, Ilias
   Poiitis, Marinos
   Toliopoulos, Theodoros
   Lagakis, Paraskevas
   Petridis, Georgios
   Papachristou, Nikolaos
   Nunez-Benjumea, Francisco J
   Hors-Fraile, Santiago
   Vakali, Athena
   Gounaris, Anastasios
   Shapiro, Dany
   Naranjo, Juan-Carlos
   Levva, Sofia
   Billis, Antonis
   Traver, Vicente
   Bamidis, Panagiotis
TI A Collective Intelligence Platform to Support Older Cancer Survivors:
   Towards the Definition of LifeChamps System and Big Data Reference
   Architecture.
SO Studies in health technology and informatics
VL 290
BP 1008
EP 1009
DI 10.3233/SHTI220241
DT Journal Article
PD 2022-Jun-06
PY 2022
AB Within the most recent years, most of the cancer patients are older age,
   which implies the necessity to a better understanding of aging and
   cancer connection. This work presents the LifeChamps solution built on
   top of cutting-edge Big Data architecture and HPC infrastructure
   concepts. An innovative architecture was envisioned supported by the Big
   Data Value Reference Model and answering the system requirements from
   high to low level and from logical to physical perspective, following
   the "4+1 architectural model".
ZS 0
ZA 0
Z8 0
TC 5
ZB 0
ZR 0
Z9 5
U1 0
U2 1
EI 1879-8365
DA 2022-06-09
UT MEDLINE:35673179
PM 35673179
ER

PT J
AU Huang, Xiaohui
   Fan, Junqing
   Deng, Ze
   Yan, Jining
   Li, Jiabao
   Wang, Lizhe
TI Efficient IoT Data Management for Geological Disasters Based on Big
   Data-Turbocharged Data Lake Architecture
SO ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION
VL 10
IS 11
AR 743
DI 10.3390/ijgi10110743
DT Article
PD NOV 2021
PY 2021
AB Multi-source Internet of Things (IoT) data, archived in institutions'
   repositories, are becoming more and more widely open-sourced to make
   them publicly accessed by scientists, developers, and decision makers
   via web services to promote researches on geohazards prevention. In this
   paper, we design and implement a big data-turbocharged system for
   effective IoT data management following the data lake architecture. We
   first propose a multi-threading parallel data ingestion method to ingest
   IoT data from institutions' data repositories in parallel. Next, we
   design storage strategies for both ingested IoT data and processed IoT
   data to store them in a scalable, reliable storage environment. We also
   build a distributed cache layer to enable fast access to IoT data. Then,
   we provide users with a unified, SQL-based interactive environment to
   enable IoT data exploration by leveraging the processing ability of
   Apache Spark. In addition, we design a standard-based metadata model to
   describe ingested IoT data and thus support IoT dataset discovery.
   Finally, we implement a prototype system and conduct experiments on real
   IoT data repositories to evaluate the efficiency of the proposed system.
RI Wang, Lizhe/L-7453-2014; Li, Jiabao/; Yan, Jining/AAB-7885-2022; Huang, Xiaohui/KRP-2903-2024
OI Wang, Lizhe/0000-0003-2766-0845; Li, Jiabao/0009-0004-9425-8981; Yan,
   Jining/0000-0003-0680-5427; Huang, Xiaohui/0000-0002-0394-2357
ZS 0
Z8 0
ZB 0
ZA 0
ZR 0
TC 5
Z9 5
U1 3
U2 46
EI 2220-9964
DA 2021-12-05
UT WOS:000723488100001
ER

PT J
AU Zhao, Hongxin
TI Construction of Multimedia-Assisted English Teaching Mode in Big Data
   Network Environment
SO WIRELESS COMMUNICATIONS & MOBILE COMPUTING
VL 2021
AR 1609187
DI 10.1155/2021/1609187
DT Article
PD SEP 15 2021
PY 2021
AB Oral English teaching is the weakest link in multimedia English teaching
   at this stage. English teachers are constantly exploring effective
   approaches to improve oral English Teaching in their own educational
   practice. The big data multimedia English teaching mode conforms to
   embark on the historical stage. Firstly, this paper constructs the big
   data architecture of English teaching model mining and divides the
   construction of the teaching model into three parts: data mining,
   teaching model evaluation, and improvement optimization. Data mining
   uses the advanced DBN network to send data into the DBN-DELM network,
   which significantly improves the accuracy of the multimedia assisted
   English construction model. The simulation results show that teaching
   mode construction based on big data can effectively improve students'
   interest in English learning; attitude; and oral English level including
   pronunciation, pronunciation and intonation, dialogue and communication,
   and oral expression and improve students' group cooperation and
   communication ability, autonomous learning ability, evaluation
   consciousness, and ability.
Z8 0
ZA 0
TC 3
ZR 0
ZS 0
ZB 0
Z9 5
U1 2
U2 50
SN 1530-8669
EI 1530-8677
DA 2021-10-03
UT WOS:000700353100002
ER

PT C
AU Cardoso, Beatriz Batista
   Righetto, Sophia Boing
   Martins, Eduardo Luiz
   Izumida Martins, Marcos Aurelio
   Pereira, Andre Luiz
   de Francisci, Silvia
GP IEEE
TI Data Lake Architecture for Distribution System Operator
SO 2021 IEEE POWER & ENERGY SOCIETY INNOVATIVE SMART GRID TECHNOLOGIES
   CONFERENCE (ISGT)
SE Innovative Smart Grid Technologies
DI 10.1109/ISGT49243.2021.9372181
DT Proceedings Paper
PD 2021
PY 2021
AB Ignited by the advent of digital technologies, power distribution
   utilities are generating more and more data about their own assets and
   their environment. To handle this amount of data, some solutions emerge
   to help distribution system operators in understanding their own data
   and turning this Big Data into actionable insights. One of the solutions
   is a Data Lake. This article illustrates the architecture of a
   cloud-based Data Lake developed by Enel Distribuicao Sao Paulo to manage
   big data from systems such as GIS, SCADA O&M systems and other data
   generated in a Network Digital Twin (R) model in the city of Sao Paulo
   This Data Lake has a combination of data sources. It stores data in raw,
   processed, and refined format using structured, unstructured and
   semi-structured data. It uses tools to execute queries, searches,
   processing streams and to visualize data. This paper presents the design
   and implementation details, as well as usage scenarios of the data lake
   in a smart grid project.
CT IEEE-Power-and-Energy-Society Innovative Smart Grid Technologies
   Conference (ISGT)
CY FEB 16-18, 2021
CL Washington, DC
SP IEEE Power & Energy Soc
RI Martins, Marcos Aurelio Izumida/AAD-8419-2022
OI Martins, Marcos Aurelio Izumida/0000-0002-8622-2792
Z8 0
ZR 0
ZS 0
TC 5
ZA 0
ZB 0
Z9 5
U1 1
U2 6
SN 2167-9665
BN 978-1-7281-8897-3
DA 2021-08-04
UT WOS:000662927400030
ER

PT C
AU Sawadogo, Pegdwende N.
   Darmont, Jerome
   Nous, Camille
BE Bellatreche, L
   Dumas, M
   Karras, P
   Matulevicius, R
TI Joint Management and Analysis of Textual Documents and Tabular Data
   Within the AUDAL Data Lake
SO ADVANCES IN DATABASES AND INFORMATION SYSTEMS, ADBIS 2021
SE Lecture Notes in Computer Science
VL 12843
BP 88
EP 101
DI 10.1007/978-3-030-82472-3_8
DT Proceedings Paper
PD 2021
PY 2021
AB In 2010, the concept of data lake emerged as an alternative to data
   warehouses for big data management. Data lakes follow a schema-on-read
   approach to provide rich and flexible analyses. However, although trendy
   in both the industry and academia, the concept of data lake is still
   maturing, and there are still few methodological approaches to data lake
   design. Thus, we introduce a new approach to design a data lake and
   propose an extensive metadata system to activate richer features than
   those usually supported in data lake approaches. We implement our
   approach in the AUDAL data lake, where we jointly exploit both textual
   documents and tabular data, in contrast with structured and/or
   semi-structured data typically processed in data lakes from the
   literature. Furthermore, we also innovate by leveraging metadata to
   activate both data retrieval and content analysis, including Text-OLAP
   and SQL querying. Finally, we show the feasibility of our approach using
   a real-word use case on the one hand, and a benchmark on the other hand.
CT 25th European Conference on Advances in Databases and Information
   Systems (ADBIS)
CY AUG 24-26, 2021
CL Univ Tartu, Inst Comp Sci, Tartu, ESTONIA
HO Univ Tartu, Inst Comp Sci
OI Sawadogo, Pegdwendé N/0000-0001-6180-5476
ZS 0
TC 5
ZR 0
Z8 0
ZB 0
ZA 0
Z9 5
U1 0
U2 2
SN 0302-9743
EI 1611-3349
BN 978-3-030-82472-3; 978-3-030-82471-6
DA 2022-04-13
UT WOS:000775738600008
ER

PT C
AU Silva, Alecio
   Souza, Gilberto F. M.
GP IEEE
TI Prognosis Smart System AI-based Applied to Equipment Health Monitoring
   in 4.0 Industry Scenario
SO 67TH ANNUAL RELIABILITY & MAINTAINABILITY SYMPOSIUM (RAMS 2021)
SE Reliability and Maintainability Symposium
DI 10.1109/RAMS48097.2021.9605722
DT Proceedings Paper
PD 2021
PY 2021
AB In the age of IIoT - Industrial Internet of Things, data lake, data
   mining, big data, and cloud computing, the smart manufacturing enables
   to make more informed decisions in real-time by using the database
   extracted from sensors in its equipment. During an operational campaign,
   the Health Monitoring System (HMS) also allows an understanding of how
   component degradation is affecting the performance of the equipment.
   Through a structure supported by AI, as data lake and cloud computing,
   the HMS provides to monitored equipment a fault detection system, early
   warning alarms to prevent failures and a calculation of the remaining
   useful life (RUL).
   The purpose of this paper is to present a prognosis smart system based
   on AI applied to HMS to support decision-making regarding operational
   performance of equipment. A Recurrent Neural Network (RNN) procedure is
   developed to continuously analyze the mass of monitoring data generated
   during the machine operation. The ability to learn the behavior patterns
   of the collected signals and in this way to be able to make parameter
   predictions with high accuracy makes artificial neural networks a
   powerful tool to carry out an effective prognosis. Machine operational
   parameters are monitored simultaneously by the prognosis smart system.
   Then, this information is processed by the neural network and used to
   characterize the machine operational condition. Upon detecting a failure
   trend for one or more parameters monitored by recognizing deterioration
   patterns, the prognosis system calculates the remaining useful life
   (RUL) and allows maintainers to take early actions before the failure
   occurrence.
   The proposed methodology is applied as part of a HMS of a hydro
   generator based on parameters registered in operator inspections routes
   designed to identify critical equipment degradation. The registered data
   representing one operational year are used to train the neural network
   regarding normal and abnormal machine condition. After training, the
   neural network is able to predict failure trends for monitored
   temperature parameters of the hydro-generator lubricating system that is
   critical to support equipment performance. Comparing prediction data and
   data collected by the sensors, the developed neural network reached
   about 0,98 RMSE score. The remaining useful life prognosis proved to be
   an important tool to avoid hydro generator components unexpected
   failures which may affect power output and cause penalties to the power
   generation company.
CT 67th Annual Reliability and Maintainability Symposium (RAMS)
CY MAY 24-27, 2021
CL Orlando, FL
RI de Souza, Gilberto/P-1299-2018
ZR 0
ZS 0
TC 5
ZA 0
Z8 0
ZB 0
Z9 5
U1 0
U2 3
SN 0149-144X
BN 978-1-7281-8017-5
DA 2022-05-11
UT WOS:000784131300024
ER

PT J
AU Bowles, Juliana K F
   Mendoza-Santana, Juan
   Vermeulen, Andreas F
   Webber, Thais
   Blackledge, Euan
TI Integrating Healthcare Data for Enhanced Citizen-Centred Care and
   Analytics.
SO Studies in health technology and informatics
VL 275
BP 17
EP 21
DI 10.3233/SHTI200686
DT Journal Article
PD 2020-Nov-23
PY 2020
AB The potential of healthcare systems worldwide is expanding as new
   medical devices and data sources are regularly presented to healthcare
   providers which could be used to personalise, improve and revise
   treatments further. However, there is presently a large gap between the
   data collected, the systems that store the data, and any ability to
   perform big data analytics to combinations of such data. This paper
   suggests a novel approach to integrate data from multiple sources and
   formats, by providing a uniform structure to the data in a healthcare
   data lake with multiple zones reflecting how refined the data is: from
   raw to curated when ready to be consumed or used for analysis. The
   integration further requires solutions that can be proven to be secure,
   such as patient-centric data sharing agreements (smart contracts) on a
   blockchain, and novel privacy-preserving methods for extracting metadata
   from data sources, originally derived from partially-structured or from
   completely unstructured data. Work presented here is being developed as
   part of an EU project with the ultimate aim to develop solutions for
   integrating healthcare data for enhanced citizen-centred care and
   analytics across Europe.
ZR 0
ZS 0
Z8 0
ZB 0
TC 4
ZA 0
Z9 5
U1 0
U2 1
EI 1879-8365
DA 2020-11-25
UT MEDLINE:33227732
PM 33227732
ER

PT C
AU Ben Hamadou, Hamdi
   Pedersen, Torben Bach
   Thomsen, Christian
BE Wu, XT
   Jermaine, C
   Xiong, L
   Hu, XH
   Kotevska, O
   Lu, SY
   Xu, WJ
   Aluru, S
   Zhai, CX
   Al-Masri, E
   Chen, ZY
   Saltz, J
TI The Danish National Energy Data Lake: Requirements, Technical
   Architecture, and Tool Selection
SO 2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
BP 1523
EP 1532
DI 10.1109/BigData50022.2020.9378368
DT Proceedings Paper
PD 2020
PY 2020
AB Renewable Energy Sources such as wind and solar do not emit CO2 but
   their production vary considerably depending on time and weather. Thus,
   it is important to use the flexibility in device loads to shift energy
   consumption to follow the production. For example, an Electrical Vehicle
   (EV) can be charged very flexibly between arriving home at 5PM and
   leaving again at 7AM. Utilizing all available energy flexibility
   requires applying machine learning and AI on massive amounts of Big Data
   from many different actors and devices, ranging from private consumers,
   over companies, to energy network operators, and using this to create
   digital solutions to enable and exploit flexibility. The project
   Flexible Energy Denmark (FED) is building the foundation for this for
   the entire Danish society. Specifically, FED collects data from a number
   of Living Labs (LLs) in representative real-life physical environments.
   The data is stored in the Danish National Energy Data Lake, called FED
   Data Lake (FEDDL) to enable efficient and advanced analysis. FEDDL is
   built using only open source tools which can run both on-premise and in
   cloud settings. In this paper, we describe the requirements for FEDDL
   based on a representative LL case study, present its technical
   architecture, and provide a comparison of relevant tools along with the
   arguments for which ones we selected.
CT 8th IEEE International Conference on Big Data (Big Data)
CY DEC 10-13, 2020
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc; IBM; Ankura
OI Thoen, Christian/0000-0002-0862-0509; Pedersen, Torben
   Bach/0000-0002-1615-777X; Ben Hamadou, Hai/0000-0002-4140-4584
TC 4
ZR 0
ZS 0
Z8 0
ZB 0
ZA 0
Z9 5
U1 1
U2 10
SN 2639-1589
BN 978-1-7281-6251-5
DA 2021-07-25
UT WOS:000662554701080
ER

PT C
AU Holom, Roxana-Maria
   Rafetseder, Katharina
   Kritzinger, Stefanie
   Sehrschoen, Herald
BE Longo, F
   Qiao, F
   Padovano, A
TI Metadata management in a big data infrastructure
SO INTERNATIONAL CONFERENCE ON INDUSTRY 4.0 AND SMART MANUFACTURING (ISM
   2019)
SE Procedia Manufacturing
VL 42
BP 375
EP 382
DI 10.1016/j.promfg.2020.02.060
DT Proceedings Paper
PD 2020
PY 2020
AB The adoption of the Internet of Things (IoT) in industry provides the
   opportunity to gather valuable data. Nevertheless, this amount of data
   must be analyzed to identify patterns in the data, model behaviors of
   equipment and to enable prediction. Although big data found its
   initiation already some years ago, there are still many challenges to be
   solved, e.g. metadata representation and management are still a research
   topic. The big data architecture of the RISC data analytics framework
   relies on the combination of big data technologies with semantic
   approaches, to process and store large volumes of data from
   heterogeneous sources, provided by FILL, which is a key machine tool
   provider. The proposed architecture is capable of handling sensor data
   using big data technologies such as Spark on Hadoop, InfluxDB and
   Elasticsearch. The metadata representation and management approach is
   adopted in order to define the structure and the relations (i.e., the
   connections) between the various data sources provided by the sensors
   and logging information system. On the other hand, using a metadata
   approach in our big data environment enhances RISC data analytics
   framework by making it generic, reusable and responsive in case of
   changes, thus keeping the data lakes up-to-date and ensuring the
   validity of the analytics results. The work presented here is part of an
   ongoing project (BOOST 4.0) currently addressed under the EU H2020
   program. (C) 2020 The Authors. Published by Elsevier B.V.
CT International Conference on Industry 4.0 and Smart Manufacturing (ISM)
CY NOV 20-22, 2019
CL Rende, ITALY
OI Holom, Roxana-Maria/0000-0002-1321-0475
ZB 0
Z8 0
ZS 0
ZR 0
TC 3
ZA 0
Z9 5
U1 0
U2 8
SN 2351-9789
BN *****************
DA 2020-01-01
UT WOS:000865885000056
ER

PT C
AU Inibhunu, Catherine
   McGregor, Carolyn A. M.
BE Wu, XT
   Jermaine, C
   Xiong, L
   Hu, XH
   Kotevska, O
   Lu, SY
   Xu, WJ
   Aluru, S
   Zhai, CX
   Al-Masri, E
   Chen, ZY
   Saltz, J
TI Edge Computing with Big Data Cloud Architecture: A Case Study in Smart
   Building
SO 2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
BP 3387
EP 3393
DI 10.1109/BigData50022.2020.9377918
DT Proceedings Paper
PD 2020
PY 2020
AB The growth of buildings embedded with technologies that can monitor the
   internal building environment with respect to energy consumption such as
   heating, ventilation, air conditioning, wind, motion as well occupancy
   have an immense potential. From energy management, occupancy
   administration, security maintenance as well as improving the health and
   quality of life for humans in indoor or outdoor spaces. These potentials
   can be realized by a clear understanding of the interplay between vast
   environmental conditions, humans and their health as well as the many
   smart products they interact with in their lives. This is a complex
   process that requires thorough testing and evaluation within smart
   buildings simulation environments where multiple buildings data can be
   generated and then effectively analyzed. This can be facilitated by a
   robust data management process that utilizes big data computing
   technologies to harness large volumes, variety and velocity of data that
   can be captured within smart buildings while maintain the security and
   privacy of data sources.
   In this paper we describe a smart building architecture that has been
   designed and developed for management of data from a smart building. In
   particular the architecture enables acquisition, processing and
   distribution of simulated environmental building data to multiple
   consumers and workflows for further processing and analysis locally and
   in a high performance cloud computing platform. The research premise is
   that such an architecture enables effective management of multiple data
   sources within climatic based simulated testing in smart buildings to
   further research.
CT 8th IEEE International Conference on Big Data (Big Data)
CY DEC 10-13, 2020
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc; IBM; Ankura
OI McGregor, Carolyn/0000-0002-0491-4403
TC 5
Z8 0
ZA 0
ZS 0
ZR 0
ZB 0
Z9 5
U1 0
U2 6
SN 2639-1589
BN 978-1-7281-6251-5
DA 2021-07-25
UT WOS:000662554703066
ER

PT C
AU Neves, Ricardo A.
   Cruvinel, Paulo E.
GP IEEE
TI Model for Semantic Base Structuring of Digital Data to Support
   Agricultural Management
SO 2020 IEEE 14TH INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC
   2020)
SE IEEE International Conference on Semantic Computing
BP 337
EP 340
DI 10.1109/ICSC.2020.00067
DT Proceedings Paper
PD 2020
PY 2020
AB This article presents a semantic model for structuring digital databases
   to function in a cloud environment and connect to data sources
   originating from Big Data. The work examines the process of receiving
   structured, semi-structured and unstructured data for use in
   agricultural risk management. It is conceived as an architecture that
   combines Data Mart, Data Warehouse (NoSQL), and Data Lake resources to
   support decision making, through knowledge discovery and applies
   algorithms for data mining by machine learning resources. The
   configuration presented addresses scenarios involving agricultural data,
   obtained from sensors operating in multiple modes.
CT 14th IEEE International Conference on Semantic Computing (ICSC)
CY FEB 03-05, 2020
CL San Diego, CA
SP IEEE; IEEE Comp Soc
RI Cruvinel, Paulo/C-7687-2015
ZR 0
TC 4
ZB 0
Z8 0
ZS 0
ZA 0
Z9 5
U1 1
U2 4
SN 2325-6516
BN 978-1-7281-6332-1
DA 2020-09-17
UT WOS:000565450400058
ER

PT C
AU Daki, Houda
   El Hannani, Asmaa
   Aqqal, Abdelhak
   Haidine, Abdelfattah
   Dahbi, Aziz
   Ouahmane, Hassan
BE Essaaidi, M
   Zbakh, M
TI Towards Adopting Big Data Technologies by Mobile Networks Operators: a
   Moroccan Case Study
SO 2016 2ND INTERNATIONAL CONFERENCE ON CLOUD COMPUTING TECHNOLOGIES AND
   APPLICATIONS (CLOUDTECH)
BP 154
EP 161
DT Proceedings Paper
PD 2016
PY 2016
AB Big Data is the term for large and complex data sets that are difficult
   to treat using traditional management tools. Nowadays, Big Data has an
   important role in different industries. This huge quantity of datasets
   can be a great opportunity to help many organizations to understand
   quantified results better, improve their decisions and be able to make
   predictive actions to generate more revenues. Many networking
   infrastructures relay on Big Data applications to make daily activities
   smarter and faster. In this paper we present the case of a Moroccan
   operator which migrated from a manual reporting process to a fully
   automated one in order to have high interactivity, optimal management
   and great vision on their networks performance indicators. Furthermore,
   we discuss Big Data opportunities for the mobile network operators (MNO)
   and propose a Big Data architecture that may be adopted by the Moroccan
   MNO to overcome the limits of the traditional technologies used in the
   automatic reporting system.
CT 2nd International Conference on Cloud Computing Technologies and
   Applications (CloudTech)
CY MAY 24-26, 2016
CL Marrakech, MOROCCO
SP IEEE; Higher Natl Sch Comp Sci & Syst Anal ENSIAS; Mohamed V Univ Rabat;
   Moroccan Assoc Cloud Comp; IEEE Morocco Sect
RI El Hannani, Asmaa/IYS-5432-2023; AQQAL, Abdelhak/HZK-3224-2023
OI El Hannani, Asmaa/0000-0003-4765-0222; 
ZR 0
TC 5
ZS 0
ZB 0
Z8 1
ZA 0
Z9 5
U1 0
U2 2
BN 978-1-4673-8894-8
DA 2017-05-31
UT WOS:000400821200021
ER

PT C
AU Wiener, Patrick
   Stein, Manuel
   Seebacher, Daniel
   Bruns, Julian
   Frank, Matthias
   Simko, Viliam
   Zander, Stefan
   Nimis, Jens
BE Ali, M
   Newsam, S
   Ravada, S
   Renz, M
   Trajcevski, G
TI BigGIS: A Continuous Refinement Approach to Master Heterogeneity and
   Uncertainty in Spatio-Temporal Big Data (Vision Paper)
SO 24TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC
   INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2016)
DI 10.1145/2996913.2996931
DT Proceedings Paper
PD 2016
PY 2016
AB Geographic information systems (GIS) are important for decision support
   based on spatial data. Due to technical and economical progress an ever
   increasing number of data sources are available leading to a rapidly
   growing fast and unreliable amount of data that can be bene ficial (1)
   in the approximation of multivariate and causal predictions of future
   values as well as (2) in robust and proactive decision-making processes.
   However, today's GIS are not designed for such big data demands and
   require new methodologies to effectively model uncertainty and generate
   meaningful knowledge. As a consequence, we introduce BigGIS, a
   predictive and prescriptive spatio-temporal analytics platform, that
   symbiotically combines big data analytics, semantic web technologies and
   visual analytics methodologies. We present a novel continuous refinement
   model and show future challenges as an intermediate result of a
   collaborative research project into big data methodologies for
   spatio-temporal analysis and design for a big data enabled GIS.
CT 24th ACM SIGSPATIAL International Conference on Advances in Geographic
   Information Systems (ACM SIGSPATIAL GIS)
CY OCT 31-NOV 03, 2016
CL San Francisco, CA
SP ACM Special Interest Grp Spatial Informat; ACM; Amazon; ESRI; Facebook;
   Google; Oracle; Microsoft
OI Nimis, Jens/0000-0001-8300-0134; Stein, Manuel/0000-0002-7198-1438;
   Bruns, Julian/0000-0002-6592-7371
ZR 0
ZB 0
TC 5
ZA 0
ZS 0
Z8 0
Z9 5
U1 0
U2 9
BN 978-1-4503-4589-7
DA 2017-07-04
UT WOS:000403647900008
ER

PT J
AU Ch'ng, Eugene
TI Social information landscapes Automated mapping of large multimodal,
   longitudinal social networks
SO INDUSTRIAL MANAGEMENT & DATA SYSTEMS
VL 115
IS 9
SI SI
BP 1724
EP 1751
DI 10.1108/IMDS-02-2015-0055
DT Article
PD 2015
PY 2015
AB Purpose - The purpose of this paper is to present a Big Data solution as
   a methodological approach to the automated collection, cleaning,
   collation, and mapping of multimodal, longitudinal data sets from social
   media. The paper constructs social information landscapes (SIL).
   Design/methodology/approach - The research presented here adopts a Big
   Data methodological approach for mapping user-generated contents in
   social media. The methodology and algorithms presented are generic, and
   can be applied to diverse types of social media or user-generated
   contents involving user interactions, such as within blogs, comments in
   product pages, and other forms of media, so long as a formal data
   structure proposed here can be constructed.
   Findings - The limited presentation of the sequential nature of content
   listings within social media and Web 2.0 pages, as viewed on web
   browsers or on mobile devices, do not necessarily reveal nor make
   obvious an unknown nature of the medium; that every participant, from
   content producers, to consumers, to followers and subscribers, including
   the contents they produce or subscribed to, are intrinsically connected
   in a hidden but massive network. Such networks when mapped, could be
   quantitatively analysed using social network analysis (e.g.
   centralities), and the semantics and sentiments could equally reveal
   valuable information with appropriate analytics. Yet that which is
   difficult is the traditional approach of collecting, cleaning,
   collating, and mapping such data sets into a sufficiently large sample
   of data that could yield important insights into the community structure
   and the directional, and polarity of interaction on diverse topics. This
   research solves this particular strand of problem.
   Research limitations/implications - The automated mapping of extremely
   large networks involving hundreds of thousands to millions of nodes,
   encapsulating high resolution and contextual information, over a long
   period of time could possibly assist in the proving or even disproving
   of theories. The goal of this paper is to demonstrate the feasibility of
   using automated approaches for acquiring massive, connected data sets
   for academic inquiry in the social sciences.
   Practical implications - The methods presented in this paper, together
   with the Big Data architecture can assist individuals and institutions
   with a limited budget, with practical approaches in constructing SIL.
   The software-hardware integrated architecture uses open source software,
   furthermore, the SIL mapping algorithms are easy to implement.
   Originality/value - The majority of research in the literature uses
   traditional approaches for collecting social networks data. Traditional
   approaches can be slow and tedious; they do not yield adequate sample
   size to be of significant value for research. Whilst traditional
   approaches collect only a small percentage of data, the original methods
   presented here are able to collect and collate entire data sets in
   social media due to the automated and scalable mapping techniques.
RI Ch'ng, Eugene/Q-8277-2019
OI Ch'ng, Eugene/0000-0003-3992-8335
ZB 0
ZA 0
ZR 0
TC 5
Z8 0
ZS 0
Z9 5
U1 0
U2 8
SN 0263-5577
EI 1758-5783
DA 2016-02-24
UT WOS:000369166900010
ER

PT J
AU Riedel, Morris
   Wittenburg, Peter
   Reetz, Johannes
   van de Sanden, Mark
   Rybicki, Jedrzej
   von St Vieth, Benedikt
   Fiameni, Giuseppe
   Mariani, Giacomo
   Michelini, Alberto
   Cacciari, Claudio
   Elbers, Willem
   Broeder, Daan
   Verkerk, Robert
   Erastova, Elena
   Lautenschlaeger, Michael
   Budig, Reinhard
   Thielmann, Hannes
   Coveney, Peter
   Zasada, Stefan
   Haidar, Ali
   Buechner, Otto
   Manzano, Cristina
   Memon, Shiraz
   Memon, Shahbaz
   Helin, Heikki
   Suhonen, Jari
   Lecarpentier, Damien
   Koski, Kimmo
   Lippert, Thomas
TI A data infrastructure reference model with applications: towards
   realization of a ScienceTube vision with a data replication service
SO JOURNAL OF INTERNET SERVICES AND APPLICATIONS
VL 4
AR 1
DI 10.1186/1869-0238-4-1
DT Article
PD 2013
PY 2013
AB The wide variety of scientific user communities work with data since
   many years and thus have already a wide variety of data infrastructures
   in production today. The aim of this paper is thus not to create one new
   general data architecture that would fail to be adopted by each and any
   individual user community. Instead this contribution aims to design a
   reference model with abstract entities that is able to federate existing
   concrete infrastructures under one umbrella. A reference model is an
   abstract framework for understanding significant entities and
   relationships between them and thus helps to understand existing data
   infrastructures when comparing them in terms of functionality, services,
   and boundary conditions. A derived architecture from such a reference
   model then can be used to create a federated architecture that builds on
   the existing infrastructures that could align to a major common vision.
   This common vision is named as 'ScienceTube' as part of this
   contribution that determines the high-level goal that the reference
   model aims to support. This paper will describe how a well-focused use
   case around data replication and its related activities in the EUDAT
   project aim to provide a first step towards this vision. Concrete
   stakeholder requirements arising from scientific end users such as those
   of the European Strategy Forum on Research Infrastructure (ESFRI)
   projects underpin this contribution with clear evidence that the EUDAT
   activities are bottom-up thus providing real solutions towards the so
   often only described 'high-level big data challenges'. The followed
   federated approach taking advantage of community and data centers (with
   large computational resources) further describes how data replication
   services enable data-intensive computing of terabytes or even petabytes
   of data emerging from ESFRI projects.
RI Riedel, Morris/AAD-7768-2021; Michelini, Alberto/D-1765-2018; Manzano, Cristina/; van de Sanden, Mark/; Mariani, Giuliano/HDL-8734-2022; Riedel, Morris/; Reetz, Johannes/
OI Michelini, Alberto/0000-0001-6716-8551; Manzano,
   Cristina/0000-0003-1775-6165; van de Sanden, Mark/0000-0002-2718-8918;
   Riedel, Morris/0000-0003-1810-9330; Reetz, Johannes/0000-0001-8183-846X
ZA 0
TC 4
ZB 0
ZS 0
ZR 0
Z8 0
Z9 5
U1 0
U2 1
SN 1867-4828
EI 1869-0238
DA 2013-01-01
UT WOS:000214535700001
ER

PT J
AU Talwar, Ankoor
   Talwar, Abhinav A.
   Broach, Robyn B.
   Ungar, Lyle H.
   Hashimoto, Daniel A.
   Fischer, John P.
TI Artificial intelligence unlocks the healthcare data lake
SO ARTIFICIAL INTELLIGENCE SURGERY
VL 5
IS 2
BP 239
EP 246
DI 10.20517/ais.2024.109
DT Editorial Material
PD JUN 2025
PY 2025
AB Artificial intelligence (AI) is poised to revolutionize surgical care by
   leveraging the vast and complex "data lake" of healthcare information.
   This perspective piece outlines how AI may harness structured and
   unstructured data to improve patient outcomes. Advances in deep learning
   and foundational models have enabled the development of predictive
   analytics, automated clinical documentation, personalized patient
   chatbots, remote monitoring, and enhanced medical imaging. Examples
   include the ACS NSQIP risk calculator, Sepsis ImmunoScore, startups in
   ambient transcription, and cutting-edge AI applications in
   intraoperative imaging and real-time diagnostics. However, the adoption
   of AI in healthcare requires overcoming challenges, including data
   privacy, bias, integration into clinical workflows, interoperability,
   cost, ethical concerns, and regulatory hurdles. As AI technologies
   evolve, collaboration between surgeons and scientists will be critical
   to ensure ethical, patient-centered designs. This manuscript calls for
   surgeons to lead AI applications role in surgery, bridging technology
   with meaningful use cases to positively align with clinical practice.
RI Hashimoto, Daniel/H-3473-2019
ZB 0
ZR 0
ZS 0
Z8 0
TC 4
ZA 0
Z9 4
U1 2
U2 2
EI 2771-0408
DA 2025-07-13
UT WOS:001524770300008
ER

PT J
AU Guo, Siyan
   Zhao, Cong
TI iEVEM: Big Data-Empowered Framework for Intelligent Electric Vehicle
   Energy Management
SO SYSTEMS
VL 13
IS 2
AR 118
DI 10.3390/systems13020118
DT Article
PD FEB 2025
PY 2025
AB Recent years have witnessed an unprecedented boom of Electric Vehicles
   (EVs). However, EVs' further development confronts critical bottlenecks
   due to EV Energy (EVE) issues like battery hazards, range anxiety, and
   charging inefficiency. Emerging data-driven EVE Management (EVEM) is a
   promising solution but still faces fundamental challenges, especially in
   terms of reliability and efficiency. This article presents iEVEM, the
   first big data-empowered intelligent EVEM framework, providing
   systematic support to the essential driver-, enterprise-, and
   social-level intelligent EVEM applications. Particularly, a layered data
   architecture from heterogeneous EVE data management to
   knowledge-enhanced intelligent solution design is provided, and an
   edge-cloud collaborative architecture for the networked system is
   proposed for reliable and efficient EVEM, respectively. We conducted a
   proof-of-concept case study on a typical EVEM task (i.e., EV energy
   consumption outlier detection) using real driving data from 4000+ EVs
   within three months. The experimental results show that iEVEM achieves a
   significant boost in reliability and efficiency (i.e., up to 47.48%
   higher in detection accuracy and at least 3.07x faster in response speed
   compared with the state-of-art approaches). As the first intelligent
   EVEM framework, iEVEM is expected to inspire more intelligent energy
   management applications exploiting skyrocketing EV big data.
OI Zhao, Cong/0000-0002-4317-5535
ZA 0
ZS 0
ZR 0
TC 4
Z8 0
ZB 0
Z9 4
U1 4
U2 8
EI 2079-8954
DA 2025-03-03
UT WOS:001431741900001
ER

PT J
AU Alghamdi, Ahmed Mohammed
   Al Shehri, Waleed A.
   Almalki, Jameel
   Jannah, Najlaa
   Alsubaei, Faisal S.
TI An architecture for COVID-19 analysis and detection using big data, AI,
   and data architectures
SO PLOS ONE
VL 19
IS 8
AR e0305483
DI 10.1371/journal.pone.0305483
DT Article
PD AUG 1 2024
PY 2024
AB The COVID-19 epidemic is affecting individuals in many ways and
   continues to spread all over the world. Vaccines and traditional medical
   techniques are still being researched. In diagnosis and therapy,
   biological and digital technology is used to overcome the fear of this
   disease. Despite recovery in many patients, COVID-19 does not have a
   definite cure or a vaccine that provides permanent protection for a
   large number of people. Current methods focus on prevention, monitoring,
   and management of the spread of the disease. As a result, new
   technologies for combating COVID-19 are being developed. Though
   unreliable due to a lack of sufficient COVID-19 datasets,
   inconsistencies in the datasets availability, non-aggregation of the
   database because of conflicting data formats, incomplete information,
   and distortion, they are a step in the right direction. Furthermore, the
   privacy and confidentiality of people's medical data are only partially
   ensured. As a result, this research study proposes a novel, cooperative
   approach that combines big data analytics with relevant Artificial
   Intelligence (AI) techniques and blockchain to create a system for
   analyzing and detecting COVID-19 instances. Based on these technologies,
   the reliability, affordability, and prominence of dealing with the above
   problems required time. The architecture of the proposed model will
   analyze different data sources for preliminary diagnosis, detect the
   affected area, and localize the abnormalities. Furthermore, the
   blockchain approach supports the decentralization of the central
   repository so that it is accessible to every stakeholder. The model
   proposed in this study describes the four-layered architecture. The
   purpose of the proposed architecture is to utilize the latest
   technologies to provide a reliable solution during the pandemic; the
   proposed architecture was sufficient to cover all the current issues,
   including data security. The layers are unique and individually
   responsible for handling steps required for data acquisition, storage,
   analysis, and reporting using blockchain principles in a decentralized
   P2P network. A systematic review of the technologies to use in the
   pandemic covers all possible solutions that can cover the issue best and
   provide a secure solution to the pandemic.
RI Al Shehri, Waleed/ADQ-9037-2022; Almalki, Jameel/AAT-2125-2020; Alsubaei, Faisal/X-5580-2019; Alghai, Ahmed/G-3901-2019
OI Al Shehri, Waleed/0000-0003-2862-4897; Almalki,
   Jameel/0000-0001-6288-6474; Alghai, Ahmed/0000-0001-7644-5039
ZS 0
ZR 0
TC 4
ZB 0
ZA 0
Z8 0
Z9 4
U1 2
U2 10
SN 1932-6203
DA 2024-09-10
UT WOS:001284650900042
PM 39088543
ER

PT J
AU Zhou, Qingguo
   Zhao, Rui
   Hu, Yilin
   Wang, Jinqiang
   Zhou, Rui
TI Hierarchical Hybrid Networks for Automatic Pulmonary Blood Vessel
   Segmentation in Computed Tomography Images
SO IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS
VL 21
IS 4
BP 778
EP 788
DI 10.1109/TCBB.2023.3281828
DT Article
PD JUL-AUG 2024
PY 2024
AB Pulmonary arterial hypertension (PAH) is considered the third most
   common cardiovascular disease after coronary heart disease and
   hypertension. The diagnosis of PAH is mainly based on the comprehensive
   judgment of computed tomography and other medical image examinations.
   Medical image processing based on deep learning has achieved significant
   success. However, the data belongs to the patient's privacy; therefore,
   the medical institutions as data custodians have the responsibility to
   protect the security of their data privacy. This situation makes medical
   institutions face a dilemma when building data-driven deep
   learning-assisted medical diagnosis methods. On the one hand, they need
   to pursue more high-quality data based on Big Data architecture for deep
   learning; on the other hand, they need to protect patient privacy to
   avoid data leakage. In response to the above challenges, we propose a
   hierarchical hybrid automatic segmentation model for pulmonary blood
   vessels based on local learning and federated learning approaches for
   segmenting the pulmonary blood vessels. The experiments prove the
   proposal could automatically segment the vessels from the original CT.
   It also indicates that the model based on a federated learning approach
   can achieve impressive performance under the premise of protecting data
   privacy for Big Data.
RI Zhou, Qingguo/LXU-8469-2024; Wang, Jin-Qiang/HJG-9139-2022; Zhao, Rui/; Zhao, Rui/
OI Zhao, Rui/0000-0003-4884-474X; Zhao, Rui/0009-0005-0684-8613
TC 3
ZS 0
ZR 0
ZA 0
Z8 0
ZB 1
Z9 4
U1 4
U2 16
SN 1545-5963
EI 1557-9964
DA 2024-08-30
UT WOS:001290429100003
PM 37262116
ER

PT J
AU Orescanin, Drazen
   Hlupic, Tomislav
   Vrdoljak, Boris
TI Managing Personal Identifiable Information in Data Lakes
SO IEEE ACCESS
VL 12
BP 32164
EP 32180
DI 10.1109/ACCESS.2024.3365042
DT Article
PD 2024
PY 2024
AB Privacy is a fundamental human right according to the Universal
   Declaration of Human Rights of the United Nations. Adoption of the
   General Data Protection Regulation (GDPR) in European Union in 2018 was
   turning point in management of personal data, specifically personal
   identifiable information (PII). Although there were many previous
   privacy laws in existence before, GDPR has brought privacy topic in the
   regulatory spotlight. Two most important novelties are seven basic
   principles related to processing of personal data and huge fines defined
   for violation of the regulation. Many other countries have followed the
   EU with the adoption of similar legislation. Personal data management
   processes in companies, especially in analytical systems and Data Lakes,
   must comply with the regulatory requirements. In Data Lakes, there are
   no standard architectures or solutions for the need to discover personal
   identifiable information, match data about the same person from
   different sources, or remove expired personal data. It is necessary to
   upgrade the existing Data Lake architectures and metadata models to
   support these functionalities. The goal is to study the current Data
   Lake architecture and metadata models and to propose enhancements to
   improve the collection, discovery, storage, processing, and removal of
   personal identifiable information. In this paper, a new metadata model
   that supports the handling of personal identifiable information in a
   Data Lake is proposed.
OI Hlupić, Tomislav/0000-0003-0814-8774; Vrdoljak,
   Boris/0000-0003-0081-172X; Oreščanin, ažen/0000-0002-0233-3971
ZR 0
ZB 0
ZA 0
ZS 0
Z8 0
TC 4
Z9 4
U1 6
U2 22
SN 2169-3536
DA 2024-03-26
UT WOS:001176926600001
ER

PT J
AU Zhao, Xiaoyan
   Zhang, Conghui
   Guan, Shaopeng
TI A data lake-based security transmission and storage scheme for streaming
   big data
SO CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS
VL 27
IS 4
BP 4741
EP 4755
DI 10.1007/s10586-023-04201-9
EA DEC 2023
DT Article
PD JUL 2024
PY 2024
AB Streaming big data presents unique security challenges due to its
   real-time generation and distributed transmission methods, making it
   vulnerable to security threats such as data leakage and tampering. To
   address these challenges, we propose a secure transmission and storage
   scheme for streaming big data based on a data lake architecture. Our
   approach leverages an ECC lightweight encryption algorithm in a
   streaming data encryption interceptor to filter and encrypt key
   information at the data source. We also introduce the SSL secure
   transmission protocol to ensure secure data transmission over a
   multi-source streaming data transmission channel constructed using Flume
   and Kafka. Furthermore, we design a data partition scheme based on LZO
   compression and implement a data lake storage system using Hadoop. Our
   proposed scheme can efficiently and securely transfer streaming data
   from multiple data sources to the data lake, while providing high data
   query efficiency. Experimental results show that our stream data
   encryption interceptor reduces memory load by 18% and filters and
   encrypts key information at a faster speed compared to similar schemes.
   In addition, our data lake storage scheme demonstrates lower data write
   latency and space occupancy, making it well-suited for large-scale
   streaming data applications.
RI Guan, Shaopeng/HMD-7737-2023
OI Guan, Shaopeng/0000-0001-9647-7396
Z8 0
ZS 0
ZB 0
TC 3
ZR 0
ZA 0
Z9 4
U1 5
U2 10
SN 1386-7857
EI 1573-7543
DA 2023-12-19
UT WOS:001117512100001
ER

PT J
AU Colmenares-Quintero, Ramon Fernando
   Maestre-Gongora, Gina
   Valderrama-Riveros, Oscar Camilo
   Baquero-Almazo, Marieth
   Stansfield, Kim E.
TI A Data-Driven Architecture for Smart Renewable Energy Microgrids in
   Non-Interconnected Zones: A Colombian Case Study
SO ENERGIES
VL 16
IS 23
AR 7900
DI 10.3390/en16237900
DT Article
PD DEC 2023
PY 2023
AB Implementing smart microgrids for Non-Interconnected Zones (NIZs) has
   become an alternative solution to provide electrical energy by taking
   advantage of the resources available through the generation of renewable
   energy within these isolated areas. Within this context, in this study,
   the challenges related to microgrids and data analysis are presented,
   and different relevant data architectures described in the literature
   are compared. This paper focuses on the design of a data architecture
   for a smart microgrid for NIZs whose microgrid contains two 260 W solar
   panels, a 480 W inverter, and two 260 Ah batteries. Regarding the
   Colombian context, this paper describes the limitations (connectivity,
   isolation, appropriation of technologies) and opportunities (low demand,
   access to natural resources, state interest) from which the functional
   and non-functional requirements for the architecture are established.
   Finally, a data architecture is proposed and implemented in a NIZ in
   Colombia, and this paper also includes a description of the
   architecture, its characteristics, its associated opportunities and
   challenges, and discussions regarding its implementation.
RI Baquero-Almazo, Marieth Celeste/; Maestre-Gongora, Gina/C-4202-2019; Colmenares-Quintero, Ramón Fernando/ADN-2778-2022; Valderrama Riveros, Oscar Camilo/; Baquero, Marieth/KEH-1450-2024
OI Baquero-Almazo, Marieth Celeste/0000-0001-9823-9607; Maestre-Gongora,
   Gina/0000-0002-2880-9245; Colmenares-Quintero, Ramón
   Fernando/0000-0003-1166-1982; Valderrama Riveros, Oscar
   Camilo/0000-0003-3168-2086; 
ZR 0
TC 4
Z8 0
ZA 0
ZS 0
ZB 0
Z9 4
U1 0
U2 5
EI 1996-1073
DA 2023-12-19
UT WOS:001116214800001
ER

PT J
AU Kodipalli, Ashwini
   Devi, Susheela
   Dasar, Santosh
TI Semantic segmentation and classification of polycystic ovarian disease
   using attention UNet, Pyspark, and ensemble learning model
SO EXPERT SYSTEMS
VL 41
IS 3
DI 10.1111/exsy.13498
EA NOV 2023
DT Article
PD MAR 2024
PY 2024
AB Ovarian abnormality like polycystic ovarian disease (PCOD) is one of the
   most common diseases among women worldwide. PCOD not only has an impact
   on infertility but also hurts the psychological well-being of women
   affecting their quality of life. In this study, a two-class pattern
   learning problem is designed for the classification of PCOD. In total,
   37 clinical parameters and abdominal ultrasound images of women are
   collected under the proper ethical protocol. Using only clinical data,
   an accuracy of 93.7% is obtained using Random Forest as the classifier
   which is further improved to 95.54% by using a Randomized Search CV
   during Random Forest classification. The ultrasound images are
   classified using the proposed Attention-UNet architecture and a mean
   Dice score of 0.945 is obtained indicating more accurate segmentation.
   The segmented images are passed through the state-of-the-art
   EfficientNet B6 for the classification of PCOS and non-PCOS and recorded
   an accuracy of 95.47%. Using big data architecture Pyspark, the
   performance is further enhanced to 96.8% and 96.3% for clinical and
   ultrasound images respectively along with the reduced computational
   speed. The results of these classifiers are then used to create metadata
   and a customized Artificial Neural Network is applied for the final
   prediction of PCOD and non-PCOD. From the results, it can be seen that
   the stacking model outperformed with an accuracy of 98.12% when compared
   to the single classifier. Our proposed method has very good performance
   with less computation, contributing a new architecture to evaluate PCOD
   and hence helping to improve the wellness of women.
RI Dasar, Dr Santosh/AAX-5797-2021
ZR 0
ZS 0
ZB 2
ZA 0
Z8 0
TC 4
Z9 4
U1 1
U2 10
SN 0266-4720
EI 1468-0394
DA 2023-11-29
UT WOS:001097864500001
ER

PT J
AU Vidal, Juan
   Carrasco, Ramon A.
   Cobo, Manuel J.
   Blasco, Maria F.
TI Data Sources as a Driver for Market-Oriented Tourism Organizations: a
   Bibliometric Perspective
SO JOURNAL OF THE KNOWLEDGE ECONOMY
VL 15
IS 2
BP 7588
EP 7621
DI 10.1007/s13132-023-01334-5
EA MAY 2023
DT Article
PD JUN 2024
PY 2024
AB This paper presents a conceptual framework that accurately represents
   the current and future perspectives of data-driven companies in tourism
   by means of an analysis of the data sources used in the data-driven
   tourism research literature, as well as the research topics to which
   they are applied. For this purpose, a bibliometric analysis of
   data-driven tourism research is carried out. The framework of the study
   is all tourism-related publications whose research was based on data
   sources during the period 1982-2020. The results show some of the basic
   bibliometric performance indicators and the maps of science. The main
   themes of research interest are identified, and the conceptual evolution
   is obtained based on these maps. Three major thematic areas are
   identified: tourism research topics, information sources, and data
   analysis techniques. Based on these three thematic areas, the conceptual
   model of data architecture and processes of a data-driven organization
   in the tourism sector are obtained. An additional qualitative analysis
   of the three thematic areas is performed.
RI Vidal Gil, Juan/; Blasco, Francisca/AAG-2553-2021; Cobo Martí­n, Manuel Jesús/C-5581-2011
OI Vidal Gil, Juan/0000-0003-4604-1599; 
ZA 0
ZS 0
ZR 0
ZB 0
Z8 0
TC 2
Z9 4
U1 0
U2 21
SN 1868-7865
EI 1868-7873
DA 2023-06-05
UT WOS:000994740700004
PM 40479121
ER

PT C
AU Falconi, Matteo
   Plebani, Pierluigi
BE Chang, CK
   Chang, RN
   Fan, J
   Fox, GC
   Jin, Z
   Pravadelli, G
   Shahriar, H
TI Adopting Data Mesh principles to Boost Data Sharing for Clinical Trials
SO 2023 IEEE INTERNATIONAL CONFERENCE ON DIGITAL HEALTH, ICDH
BP 298
EP 306
DI 10.1109/ICDH60066.2023.00051
DT Proceedings Paper
PD 2023
PY 2023
AB An effective clinical research requires the availability of relevant
   data and tools that make possible their efficient analysis. Among the
   several possibilities, data mesh, a distributed data architecture that
   is organized around specific domains and provides a self-service
   platform for accessing and using the data, is gaining the attention of
   the data and software engineering communities, mainly because of its
   ability to reduce the tension between the platform that manages the data
   and the teams that are in charge of managing them. Nevertheless, data
   mesh mainly focuses on how to manage data in a single organization by
   defining the sphere of responsibilities in the data management.
   Conversely, the continuous increase of data produced by hospitals calls
   for new approaches that enable the data sharing between clinical
   research centers.
   Goal of this paper is to extend the data mesh approach by considering
   the sharing of data among organizations which are members of a
   federation. Under the umbrella of a clinical trial which defines a
   temporary agreement among hospitals, a federated data mesh solution is
   designed to support the data management when data products from
   different organizations are considered. This implies the study on how
   the data ownership defined in the data mesh somehow becomes data
   sovereignty when data is shared with other organizations.
CT IEEE International Conference on Digital Health (IEEE ICDH) at the IEEE
   World Congress on Services (SERVICES)
CY JUL 02-08, 2023
CL Chicago, IL
SP IEEE; IEEE Comp Soc; IEEE Tech Comm Serv Comp; IEEE TSC; IBM; China Comp
   Federat, Tech Comm Serv Comp
OI Falconi, Matteo/0009-0001-8771-5767
ZA 0
ZS 0
ZB 0
Z8 0
TC 4
ZR 0
Z9 4
U1 0
U2 3
BN 979-8-3503-4103-4
DA 2023-10-05
UT WOS:001062475200041
ER

PT C
AU Hirsch, Eduard
   Hoher, Simon
   Huber, Stefan
BE Dorksen, H
   Scanzio, S
   Jasperneite, J
   Wisniewski, L
   Man, KF
   Sauter, T
   Seno, L
   Trsek, H
   Vyatkin, V
TI An OPC UA-based industrial Big Data architecture
SO 2023 IEEE 21ST INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS, INDIN
SE IEEE International Conference on Industrial Informatics INDIN
DI 10.1109/INDIN51400.2023.10217899
DT Proceedings Paper
PD 2023
PY 2023
CT IEEE 21st International Conference on Industrial Informatics (INDIN)
CY JUL 17-20, 2023
CL Lemgo, GERMANY
SP IEEE; INIT THOWL; Fraunhofer IOSB INA; IEEE Ind Elect Soc
OI Huber, Stefan/0000-0002-8871-5814
TC 4
Z8 0
ZB 0
ZA 0
ZS 0
ZR 0
Z9 4
U1 0
U2 2
SN 1935-4576
BN 978-1-6654-9313-0
DA 2023-10-18
UT WOS:001066089800023
ER

PT C
AU Wrembel, Robert
BE Strauss, C
   Amagasa, T
   Kotsis, G
   Tjoa, AM
   Khalil, I
TI Data Integration Revitalized: From Data Warehouse Through Data Lake to
   Data Mesh
SO DATABASE AND EXPERT SYSTEMS APPLICATIONS, DEXA 2023, PT I
SE Lecture Notes in Computer Science
VL 14146
BP 3
EP 18
DI 10.1007/978-3-031-39847-6_1
DT Proceedings Paper
PD 2023
PY 2023
AB For years, data integration (DI) architectures evolved from those
   supporting virtual integration, through physical integration, to those
   supporting both virtual and physical integration. Regardless of its
   type, all of the developed DI architectures include an integration
   layer. This layer is implemented by a sophisticated software, which runs
   the so-called DI processes. The integration layer is responsible for
   ingesting data from various sources (typically heterogeneous and
   distributed) and for homogenizing data into formats suitable for future
   processing and analysis. Nowadays, in all business domains, large
   volumes of highly heterogeneous data are produced, e.g., medical
   systems, smart cities, smart agriculture, which require further
   advancements in the data integration technologies. In this keynote talk
   paper, I present my personal opinion on still-to-be developed data
   integration techniques - potential research directions, namely: (1) more
   flexible DI, (2) quality assurance in complex multi-modal systems, (3)
   execution optimization of DI processes.
CT 34th International Conference on Database and Expert Systems
   Applications (DEXA)
CY AUG 28-30, 2023
CL Penang, MALAYSIA
SP Scch; Inst telecooperat; Web applicat Soc
RI Wrembel, Robert/F-7482-2014
OI Wrembel, Robert/0000-0001-6037-5718
ZA 0
ZS 0
ZR 0
ZB 0
Z8 0
TC 2
Z9 4
U1 0
U2 3
SN 0302-9743
EI 1611-3349
BN 978-3-031-39846-9; 978-3-031-39847-6
DA 2024-11-23
UT WOS:001328571200001
ER

PT J
AU Li, Lin
   Tsai, Sang-Bing
TI An Empirical Study on the Precise Employment Situation-Oriented Analysis
   of Digital-Driven Talents with Big Data Analysis
SO MATHEMATICAL PROBLEMS IN ENGINEERING
VL 2022
AR 8758898
DI 10.1155/2022/8758898
DT Article
PD JAN 4 2022
PY 2022
AB This paper conducts an in-depth research analysis on the precise
   employment of college graduates in the context of big data using a
   number-driven approach. The textual information of the study is obtained
   by using in-depth interviews, and the evaluation index system of college
   students' employment quality is constructed by combining the
   step-by-step coding method with rooting theory. The research on the
   current situation of employment recommendation platform research and the
   application status of big data in the employment recommendation platform
   is explored by using a bibliometric approach. And the innovative use of
   web crawler technology is used to comprehensively understand the
   recommendation function and status quo of the same type of
   recommendation platform, which provides a reference for the research of
   this platform. Based on the preliminary analysis of platform
   requirements and overall design, the overall design and functional
   implementation of the big data employment recommendation platform are
   carried out by using big data crawler technology, big data architecture
   technology, text mining technology, database technology, etc. The
   construction of a recommendation module based on user history
   information, a recommendation based on real-time user online behavior
   data, and hybrid recommendation carried out on the recommendation module
   to grasp all-round the platform is built based on a stakeholder
   perspective. Based on the platform construction, the initial platform
   operation and maintenance management mechanism was established from the
   stakeholder's perspective. The Pearson correlation coefficient is used
   to objectively evaluate the current situation of talent supply in
   universities and talent demand in enterprises from the perspective of
   image and data. In the research on the development status of the big
   data education industry, the Lorenz curve and Gini coefficient are used
   to match the status of new big data majors with their college
   construction volume in each province and provide data support for the
   reasonable adjustment of majors setting in each province according to
   the education level.
RI Tsai, Sang-Bing/C-2978-2014
OI Tsai, Sang-Bing/0000-0001-6988-5829
ZR 0
TC 2
ZB 0
Z8 0
ZS 0
ZA 0
Z9 4
U1 4
U2 74
SN 1024-123X
EI 1563-5147
DA 2022-01-30
UT WOS:000746443200005
ER

PT C
AU Bayaga, Anass
   Kyobe, Michael
GP IEEE
TI PLS-SEM modelling in Information Systems
SO 2022 CONFERENCE ON INFORMATION COMMUNICATIONS TECHNOLOGY AND SOCIETY
   (ICTAS)
BP 1
EP 6
DI 10.1109/ICTAS53252.2022.9744685
DT Proceedings Paper
PD 2022
PY 2022
AB While partial least squares-structural equation modeling (PLS-SEM) as
   applied in information systems (IS) is on the rise, there still remain
   crucial challenges such as model specification and interpretations. As
   such, the present study examines IS research design via the PLS-SEM
   technique, modeling, specifications and interpretations. Though the
   limited use of PLS-SEM has been noticed in IS too, in contrast, PLS-SEM
   is increasingly becoming the approach of choice to authenticate and
   model concepts. Such authentication of conceptual models has primarily
   been directed and coordinated through the assessment of modelling,
   specifications and interpretation of PLS-SEM. In IS scholarship
   (e.g.:FinTech, EdTech, AgriTech) or community (e. g.:postgraduates,
   novice, and experienced researchers), comprehending the processes that
   precede the assessment of modelling, specifications and interpretation
   are paramount in second generation techniques (PLS-SEM). In response to
   such importance, a systematic analysis of extant work is analysed. The
   analysis is based on influencers of PLS-SEM technique, with reference on
   ease of modelling, specifications and interpretation. Based on the
   importance of the procedures that precede the modelling, specifications
   and interpretation as opposed to first generation techniques (multiple
   regressions), the contribution is to present an all-inclusive IS
   related-approach using PLS-SEM.
CT 6th Annual International Conference on Information Communications
   Technology and Society (ICTAS)
CY MAR 09-10, 2022
CL ELECTR NETWORK
SP IEEE; Durban Univ Technol; IEEE, S Africa Sect, Comp Soc Chapter; ICT &
   Soc
OI Bayar, Alp/0000-0002-5542-9766
ZA 0
ZS 0
ZR 0
ZB 0
TC 3
Z8 0
Z9 4
U1 4
U2 44
BN 978-1-6654-4019-6; 978-1-6654-4017-2
DA 2022-09-08
UT WOS:000848128800001
ER

PT C
AU Cappiello, Cinzia
   Gribaudo, Marco
   Plebani, Pierluigi
   Salnitri, Mattia
   Tanca, Letizia
BE Rezig, EK
   Gadepally, V
   Mattson, T
   Stonebraker, M
   Kraska, T
   Kong, J
   Luo, G
   Teng, D
   Wang, F
TI Enabling Real-World Medicine with Data Lake Federation: A Research
   Perspective
SO HETEROGENEOUS DATA MANAGEMENT, POLYSTORES, AND ANALYTICS FOR HEALTHCARE,
   DMAH 2022
SE Lecture Notes in Computer Science
VL 13814
BP 39
EP 56
DI 10.1007/978-3-031-23905-2_4
DT Proceedings Paper
PD 2022
PY 2022
AB The collection of data during the routine delivery of care is changing
   the healthcare sector. Indeed, only from the clinical trial data it is
   difficult to obtain such a complete picture of the status of a patient
   as that provided by real-world data. However, the creation of valuable
   real-word evidence requires the adoption of an appropriate solution to
   ingest, store, and process the enormous amount of information coming
   from all the involved, typically heterogeneous data sources.
   Data lake technologies are depicted as promising solutions for enhancing
   data management and analysis capabilities in the healthcare domain: we
   can rely on them to manage the complexity of big data volume and
   variety, providing data analysts with a self-service environment in
   which advanced analytics can be applied. In this paper we envision the
   adoption of a data lake federation through which organizations could
   achieve further benefits by sharing data. Exchanging data adds new
   research challenges related to guaranteeing data reliability and
   sovereignty. For instance, the collected data should be accurately
   described in order to document their quality, facilitate their
   discovery, define security and privacy policies. On the basis of the
   experience in Health Big Data, we are going to present an architecture
   for gathering real-world evidence, also identifying the research
   challenges from an IT perspective.
CT VLDB Workshop on Polystore Systems for Heterogeneous Data in Multiple
   Databases with Privacy and Security Assurances (Poly) / 8th
   International Workshop on Data Management and Analytics for Medicine and
   Healthcare (DMAH)
CY SEP09, 2022
CL ELECTR NETWORK
RI Salnitri, Mattia/AAG-6353-2021; GRIBAUDO, MARCO/AAI-5402-2021
OI Salnitri, Mattia/0000-0002-9736-2774; GRIBAUDO,
   MARCO/0000-0002-1415-5287
TC 3
ZA 0
ZR 0
Z8 0
ZS 0
ZB 0
Z9 4
U1 1
U2 5
SN 0302-9743
EI 1611-3349
BN 978-3-031-23904-5; 978-3-031-23905-2
DA 2023-04-30
UT WOS:000967784600004
ER

PT C
AU Yuan, Qin
   Yuan, Ye
   Wen, Zhenyu
   Wang, He
   Chen, Chen
   Wang, Guoren
GP ACM
TI Exploring Heterogeneous Data Lake based on Unified Canonical Graphs
SO PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH
   AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22)
BP 1834
EP 1838
DI 10.1145/3477495.3531759
DT Proceedings Paper
PD 2022
PY 2022
AB A data lake is a repository for massive raw and heterogeneous data,
   which includes multiple data models with different data schemas and
   query interfaces. Keyword search can extract valuable information for
   users without the knowledge of underlying schemas and query languages.
   However, conventional keyword searches are restricted to a certain data
   model and cannot easily adapt to a data lake. In this paper, we study a
   novel keyword search. To achieve high accuracy and efficiency, we
   introduce canonical graphs and then integrate semantically related
   vertices based on vertex representations. A matching entity based
   keyword search algorithm is presented to find answers across multiple
   data sources. Finally, extensive experimental study shows the
   effectiveness and efficiency of our solution.
CT 45th International ACM SIGIR Conference on Research and Development in
   Information Retrieval (SIGIR)
CY JUL 11-15, 2022
CL Madrid, SPAIN
SP Assoc Comp Machinery; ACM Special Interest Grp Informat Retrieval
RI Wen, Zhenyu/LUY-2813-2024; Yuan, Qin/
OI Yuan, Qin/0009-0001-7123-6155
ZS 0
Z8 1
ZB 0
ZA 0
TC 3
ZR 0
Z9 4
U1 1
U2 8
BN 978-1-4503-8732-3
DA 2022-10-27
UT WOS:000852715901088
ER

PT J
AU Belov, Vladimir
   Kosenkov, Alexander N.
   Nikulchev, Evgeny
TI Experimental Characteristics Study of Data Storage Formats for Data
   Marts Development within Data Lakes
SO APPLIED SCIENCES-BASEL
VL 11
IS 18
AR 8651
DI 10.3390/app11188651
DT Article
PD SEP 2021
PY 2021
AB One of the most popular methods for building analytical platforms
   involves the use of the concept of data lakes. A data lake is a storage
   system in which the data are presented in their original format, making
   it difficult to conduct analytics or present aggregated data. To solve
   this issue, data marts are used, representing environments of stored
   data of highly specialized information, focused on the requests of
   employees of a certain department, the vector of an organization's work.
   This article presents a study of big data storage formats in the Apache
   Hadoop platform when used to build data marts.
RI Belov, Vladimir/AAT-3723-2021; Nikulchev, Evgeny/G-6557-2015
OI Nikulchev, Evgeny/0000-0003-1254-9132
ZA 0
Z8 0
ZB 0
ZS 0
TC 2
ZR 0
Z9 4
U1 1
U2 26
EI 2076-3417
DA 2021-10-02
UT WOS:000699253800001
ER

PT J
AU Sfaxi, Lilia
   Ben Aissa, Mohamed Mehdi
TI Babel: A Generic Benchmarking Platform for Big Data Architectures
SO BIG DATA RESEARCH
VL 24
AR 100186
DI 10.1016/j.bdr.2021.100186
EA JAN 2021
DT Article
PD MAY 15 2021
PY 2021
AB In this era of big and fast data, software architects tend to find it
   really hard to make consistent decisions about which architecture and
   technologies are ideal for a certain business need. It is even harder to
   make them while dealing with the scarcity of clear methodologies, best
   practices and reference architectures. In this prospect, architecture
   evaluation through benchmarking can be of great interest, as it enables
   the detection of performance anomalies or bottlenecks as you go. The
   problem when talking about Big Data benchmarking, is that existing
   solutions remain technology-related, and do not deal with the
   heterogeneous aspect of complex architectures. In addition to that,
   businesses are in general dealing with multi-layered complex systems,
   involving various technologies, paradigms and micro-architectures. This
   means that the benchmarking solution must be able to give fine-grained
   insights about each of the layers. A successful benchmarking system must
   also be seamless, easy to use, scalable, and preferably cloud native. To
   satisfy these requirements, we designed and implemented Babel, a generic
   Big Data benchmarking platform, that insures an end-to-end performance
   evaluation and monitoring. We present in this paper the principles,
   architecture, integration and deployment steps of Babel. (C) 2021
   Elsevier Inc. All rights reserved.
RI Sfaxi, Lilia/AAO-6154-2021
TC 3
ZB 0
ZS 0
ZA 0
Z8 0
ZR 0
Z9 4
U1 0
U2 2
SN 2214-5796
DA 2021-05-20
UT WOS:000642459200005
ER

PT C
AU Cassavia, Nunziato
   Masciari, Elio
GP IEEE COMP SOC
TI Sigma: a Scalable High Performance Big Data Architecture
SO 2021 29TH EUROMICRO INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED
   AND NETWORK-BASED PROCESSING (PDP 2021)
SE Euromicro Conference on Parallel, Distributed and Network-Based
   Processing
BP 236
EP 239
DI 10.1109/PDP52278.2021.00044
DT Proceedings Paper
PD 2021
PY 2021
AB In this paper, a new architecture, named Sigma (as Sigma uppercase
   usually denote a sum and this architecture is a summarization of Lambda
   and Kappa), is proposed to provide a solution for build a complete Big
   Data System, interactive and scalable, using a variety of tools and
   techniques. The architecture have been tested in a real life scenario
   and the early results obtained are quite encouraging both on scalability
   and high performance.
CT 29th Euromicro International Conference on Parallel, Distributed and
   Network-Based Processing (PDP)
CY MAR 10-12, 2021
CL ELECTR NETWORK
SP Univ Valladolid
ZA 0
Z8 0
TC 3
ZB 1
ZS 0
ZR 0
Z9 4
U1 0
U2 1
SN 1066-6192
BN 978-1-6654-1455-5
DA 2021-07-23
UT WOS:000670865900034
ER

PT C
AU Ren, Peng
   Mao, Ziyun
   Li, Shuaibo
   Xiao, Yang
   Ke, Yating
   Yao, Lanyu
   Lan, Hao
   Li, Xin
   Sheng, Ming
   Zhang, Yong
BE Xing, C
   Fu, X
   Zhang, Y
   Zhang, G
   Borjigin, C
TI Intelligent Visualization System for Big Multi-source Medical Data Based
   on Data Lake
SO WEB INFORMATION SYSTEMS AND APPLICATIONS (WISA 2021)
SE Lecture Notes in Computer Science
VL 12999
BP 706
EP 717
DI 10.1007/978-3-030-87571-8_61
DT Proceedings Paper
PD 2021
PY 2021
AB With the rapid development of information technology, large amounts of
   multi-source data are constantly being generated in medical field. The
   automatic visualization system based on them has gained a lot of
   attention, since the intuitive data presentation can help even
   non-professional users effectively get the information hidden behind the
   separate data obtained from different scenarios and make better
   decisions. In this paper, based on the Data Lake architecture, we
   improve the performance of an existing novel data visualization
   recommendation system and resolve three challenges about the processing
   of multi-source and heterogeneous data. First, we build the framework
   based on Data Lake to store multi-source and heterogeneous data. Second,
   we optimize the data manipulation module in the visualization system
   based on the distributed processing power of Data Lake to get
   potentially interesting visualization candidates in a short time. Third,
   we efficiently run exploratory queries on large datasets based on the
   calculation capability of Data Lake to meet the actual needs of users.
   According to the experiment results, our system demonstrates a
   remarkable acceleration effect on the task of automatic visualization of
   big multi-source medical data.
CT 18th Web Information Systems and Applications Conference (WISA)
CY SEP 24-26, 2021
CL Kaifeng, PEOPLES R CHINA
SP China Comp Federat Tech Comm Informat Syst; Henan Univ; China Comp
   Federat
ZB 0
ZS 0
ZA 0
TC 3
ZR 0
Z8 0
Z9 4
U1 1
U2 9
SN 0302-9743
EI 1611-3349
BN 978-3-030-87571-8; 978-3-030-87570-1
DA 2022-03-25
UT WOS:000767941100061
ER

PT C
AU Ciampi, Mario
   De Pietro, Giuseppe
   Masciari, Elio
   Silvestri, Stefano
GP ACM
TI Some Lessons Learned Using Health Data Literature For Smart Information
   Retrieval
SO PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING
   (SAC'20)
BP 931
EP 934
DI 10.1145/3341105.3374128
DT Proceedings Paper
PD 2020
PY 2020
AB Big Data paradigm is leading both research and industry effort calling
   for new approaches in many computer science areas. In this paper, we
   show how semantic similarity search for natural language texts can be
   leveraged in biomedical domain by Word Embedding models obtained by
   word2vec algorithm, exploiting a specifically developed Big Data
   architecture. We tested our approach using a dataset extracted from the
   whole PubMed library. Moreover, we describe a user friendly web
   front-end able to show the usability of this methodology on a real
   context that allowed us to learn some useful lessons about this peculiar
   kind of data.
CT 35th Annual ACM Symposium on Applied Computing (SAC)
CY MAR 30-APR 03, 2020
CL Czech Tech Univ, ELECTR NETWORK
HO Czech Tech Univ
SP ACM; Masaryk Univ Czechia; Microsoft Res; ACM Special Interest Grp Appl
   Comp; Natl Inst Technol Calicut
RI Ciampi, Mario/B-3874-2015; Silvestri, Stefano/IUP-0829-2023; De Pietro, Giuseppe/AAZ-1151-2020
OI Ciampi, Mario/0000-0002-7286-6212; Silvestri,
   Stefano/0000-0002-9890-8409; 
TC 4
ZS 0
ZB 0
ZA 0
ZR 0
Z8 0
Z9 4
U1 0
U2 2
BN 978-1-4503-6866-7
DA 2020-10-08
UT WOS:000569720900135
ER

PT C
AU Herden, Olaf
BE Bellatreche, L
   Goyal, V
   Fujita, H
   Mondal, A
   Reddy, PK
TI Architectural Patterns for Integrating Data Lakes into Data Warehouse
   Architectures
SO 8TH INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS, BDA 2020
SE Lecture Notes in Computer Science
VL 12581
BP 12
EP 27
DI 10.1007/978-3-030-66665-1_2
DT Proceedings Paper
PD 2020
PY 2020
AB Data Warehouses are an established approach for analyzing data. But with
   the advent of big data the approach hits its limits due to lack of
   agility, flexibility and system complexity. To overcome these limits,
   the idea of data lakes has been proposed. The data lake is not a
   replacement for data warehouses. Moreover, both solutions have their
   application areas. So it is necessary to integrate both approaches into
   a common architecture. This paper describes and compares both
   approaches, shows different ways of integrating data lakes into data
   warehouse architectures.
CT 8th International Conference on Big Data Analytics (BDA)
CY DEC 15-18, 2020
CL Ashoka Univ, Sonepat, INDIA
HO Ashoka Univ
TC 2
Z8 0
ZS 0
ZA 0
ZR 0
ZB 0
Z9 4
U1 0
U2 1
SN 0302-9743
EI 1611-3349
BN 978-3-030-66664-4; 978-3-030-66665-1
DA 2024-11-01
UT WOS:001308350300002
ER

PT C
AU Alili, Hiba
   Belhajjame, Khalid
   Grigori, Daniela
   Drira, Rim
   Ben Ghezala, Henda Hajjami
BE Abramowicz, W
TI On Enriching User-Centered Data Integration Schemas in Service Lakes
SO BUSINESS INFORMATION SYSTEMS (BIS 2017)
SE Lecture Notes in Business Information Processing
VL 288
BP 3
EP 15
DI 10.1007/978-3-319-59336-4_1
DT Proceedings Paper
PD 2017
PY 2017
AB In the Big Data era, companies are moving away from traditional
   data-warehouse solutions whereby expensive and time-consuming ETL
   (Extract-Transform-Load) processes are used, towards data lakes, which
   can be viewed as storage repositories holding a vast amount of raw data.
   In this paper, we position ourselves in the recurrent context where a
   user has a local dataset that is not sufficient for processing the
   queries that are of interest to him. In this context, we show how the
   data lake, or more specifically the service lake since we are focusing
   on data providing services, can be leveraged to enrich the local dataset
   with concepts that cater for the processing of user queries.
   Furthermore, we present the algorithms we have developed for this
   purpose and showcase the working of our solution using a study case.
CT 20th International Conference on Business Information Systems (BIS)
CY JUN 28-30, 2017
CL Poznan, POLAND
SP Polish Sci Soc Business Informat; Poznan Univ Econ, Dept Informat Syst
RI Ben Ghezala, Henda/AAK-7052-2021
Z8 0
ZA 0
ZB 0
TC 4
ZS 0
ZR 0
Z9 4
U1 0
U2 2
SN 1865-1348
BN 978-3-319-59336-4; 978-3-319-59335-7
DA 2018-12-28
UT WOS:000452527700001
ER

PT C
AU Fetjah, Laila
   Benzidane, Karim
   El Alloussi, Hassan
   El Warrak, Othman
   Jai-Andaloussi, Said
   Sekkaki, Abderrahim
GP IEEE
TI Toward a Big Data Architecture for Security Events Analytic
SO 2016 IEEE 3RD INTERNATIONAL CONFERENCE ON CYBER SECURITY AND CLOUD
   COMPUTING (CSCLOUD)
BP 190
EP 197
DI 10.1109/CSCloud.2016.53
DT Proceedings Paper
PD 2016
PY 2016
AB Cloud Computing did come up with so many attractive advantages such as
   scalability, flexibility, accessibility, rapid application deployment,
   and user self service. However in hindsight, Cloud Computing makes
   ensuring security within these environments so much challenging.
   Therefore traditional security mechanisms such as firewalls and
   antivirus softwares have proven insufficient and incapable of dealing
   with the sheer amount of data and events generated within a Cloud
   infrastructure. Herein, we present a highly scalable module based system
   that relies upon Big Data techniques and tools providing a comprehensive
   solution to process and analyze relevant events (packets flow, logs
   files) in order to generate an informative decisions that will be
   handled accordingly and swiftly.
CT 3rd IEEE International Conference on Cyber Security and Cloud Computing
   (IEEE CSCloud) / 2nd IEEE International Conference of Scalable and Smart
   Cloud (IEEE SSC)
CY JUN 25-27, 2016
CL Beijing, PEOPLES R CHINA
SP IEEE; Beihang Univ; IEEE CSCloud Comm; IEEE SSC Comm; IEEE TCSC; IEEE
   Comp Soc; Pace Univ; Longxiang High Tech Grp Inc; N Amer Chinese Talents
   Assoc
RI Benzidane, Karim/; Fetjah, Laila/KTI-8815-2024; jai andaloussi, said/AFT-2416-2022
OI Benzidane, Karim/0000-0003-1066-2174; Fetjah, Laila/0000-0003-4588-0825;
   jai andaloussi, said/0000-0002-6864-1141
ZA 0
Z8 0
ZR 0
ZB 0
TC 4
ZS 0
Z9 4
U1 0
U2 7
BN 978-1-5090-0946-6
DA 2016-01-01
UT WOS:000386978300032
ER

PT S
AU Fogelman-Soulie, Francoise
   Lu, Wenhuan
BE Japkowicz, N
   Stefanowski, J
TI Implementing Big Data Analytics Projects in Business
SO BIG DATA ANALYSIS: NEW ALGORITHMS FOR A NEW SOCIETY
SE Studies in Big Data
VL 16
BP 141
EP 158
DI 10.1007/978-3-319-26989-4_6
DT Article; Book Chapter
PD 2016
PY 2016
AB Big Data analytics present both opportunities and challenges for
   companies. It is important that, before embarking on a Big Data project,
   companies understand the value offered by Big Data and the processes
   needed to extract it. This chapter discusses why companies should
   progressively increase their data volumes and the process to follow for
   implementing a Big Data project. We present a variety of architectures,
   from in-memory servers to Hadoop, to handle Big Data. We introduce the
   concept of Data Lake and discuss its benefits for companies and the
   research still required to fully deploy it. We illustrate some of the
   points discussed in the chapter through the presentation of various
   architectures available for running Big Data initiatives, and discuss
   the expected evolution of hardware and software tools in the near
   future.
Z8 0
ZS 0
ZB 0
ZR 0
TC 3
ZA 0
Z9 4
U1 0
U2 5
SN 2197-6503
BN 978-3-319-26989-4; 978-3-319-26987-0
DA 2016-03-16
UT WOS:000369157100007
D2 10.1007/978-3-319-26989-4
ER

PT C
AU Baldominos, Alejandro
   Saez, Yago
   Albacete, Esperanza
   Marrero, Ignacio
BE Ismail, L
TI An Efficient and Scalable Recommender System for the Smart Web
SO 2015 11TH INTERNATIONAL CONFERENCE ON INNOVATIONS IN INFORMATION
   TECHNOLOGY (IIT)
SE IEEE International Conference on Innovations in Information Technology
BP 296
EP 301
DT Proceedings Paper
PD 2015
PY 2015
AB This work describes the development of a web recommender system
   implementing both collaborative filtering and content-based filtering.
   Moreover, it supports two different working modes, either sponsored or
   related, depending on whether websites are to be recommended based on a
   list of ongoing ad campaigns or in the user preferences.
   Novel recommendation algorithms are proposed and implemented, which
   fully rely on set operations such as union and intersection in order to
   compute the set of recommendations to be provided to end users.
   The recommender system is deployed over a real-time big data
   architecture designed to work with Apache Hadoop ecosystem, thus
   supporting horizontal scalability, and is able to provide
   recommendations as a service by means of a RESTful API. The performance
   of the recommender is measured, resulting in the system being able to
   provide dozens of recommendations in few milliseconds in a single-node
   cluster setup.
CT 11th International Conference on Innovations in Information Technology
   (IIT)
CY NOV 01-03, 2015
CL Dubai, U ARAB EMIRATES
SP IEEE; IEEE Comp Soc; UAE Univ, Coll Informat Technol; Univ New S Wales;
   IBM; Gen Elect Software; UAE Univ; Australian Defense Force Acad;
   Springer
RI Saez, Yago/B-9763-2013; Baldominos, Alejandro/Z-3709-2019
OI Saez, Yago/0000-0002-0998-2907; 
Z8 0
ZB 0
ZR 0
TC 4
ZA 0
ZS 0
Z9 4
U1 0
U2 0
SN 1819-9127
BN 978-1-4673-8511-4
DA 2015-01-01
UT WOS:000412207900053
ER

PT C
AU Daniel, Alfred
   Paul, Anand
   Ahmad, Awais
GP IEEE
TI Near Real-Time Big Data Analysis on Vehicular Networks
SO PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON SOFT-COMPUTING AND
   NETWORKS SECURITY (ICSNS 2015)
DT Proceedings Paper
PD 2015
PY 2015
AB In this cutthroat era of 21st Century Traffic information is considered
   as one of the prominent valuable resources in vehicular networks for big
   data analysis. In order to effectively utilize the acquired resources,
   big data analysis in near real time will be an appropriate way to
   produce valuable information from raw data. In order to exhibit the
   importance of big data investigation, an efficient architecture has been
   proposed for near real time big data analysis in vehicular networks,
   which indeed will keep pace with the latest trends and development with
   respect to emerging big-data paradigm. The proposed architecture,
   comprises centralized data storage mechanism for batch processing and
   distributed data storage mechanism for streaming processing in real time
   analysis. Furthermore a work flow model has also been designed for big
   data architecture to examine streaming data in near real time process.
   Furthermore, an algorithm is designed for organizing the vehicle flow in
   a particular location or place. The proposed system model is for optimal
   utilization of the massive data set, meant for streaming data in near
   real time process intended for ITS (Intelligent Transportation System)
   in a vehicular environment.
CT International Conference on Soft-Computing and Network Security (ICSNS)
CY FEB 25-27, 2015
CL Coimbatore, INDIA
SP IEEE Madras Sect; IEEE; Comp Soc India
RI Daniel, Alfred/O-1875-2017; Paul, Anand/V-6724-2017; Ahmad, Awais/AAA-4504-2019
OI Daniel, Alfred/0000-0003-0602-3425; 
ZS 0
ZB 0
Z8 0
ZR 0
ZA 0
TC 4
Z9 4
U1 0
U2 0
BN 978-1-4799-1753-2
DA 2015-01-01
UT WOS:000380457300035
ER

PT C
AU Wang, Yinfeng
   Zhong, Guiquan
   Kun, Lin
   Wang, Longxiang
   Kai, Huang
   Guo, Fuliang
   Liu, Chengzhe
   Dong, Xiaoshe
GP IEEE
TI The Performance Survey of In Memory Database
SO 2015 IEEE 21ST INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED
   SYSTEMS (ICPADS)
SE International Conference on Parallel and Distributed Systems -
   Proceedings
BP 815
EP 820
DI 10.1109/ICPADS.2015.109
DT Proceedings Paper
PD 2015
PY 2015
AB To satisfy the ever-increasing performance demand of Big Data and
   critical applications the data management needs to offer the flexible
   schema, high availability, light weight replica, high volume and high
   scalability features so as to facilitate the transaction. The in memory
   database (IMDB) eliminates the I/O bottleneck by storing data in main
   memory. We give a deeper analysis of current main-stream IMDB systems
   performance which focuses on the data structure, architecture, volume,
   concurrency, availability and scalability. The V3 performance model is
   proposed to evaluate the Velocity, Volume and Varity of the 19 IMDB
   systems, in order to highlight the candidates with real-time transaction
   and high volume processing capacity coordinately. Test results clearly
   demonstrate that NewSQL is better at dealing with high-frequency trading
   models. To fully utilize the advantages of the multi-core and many-core
   processors capability improvements, a three-level optimization design
   strategy, which includes the memory-access level, the kernel-speedup
   level and the data-partition level also be proposed using the hardware
   parallelism for achieving task-level and data-level parallelism of IMDB
   programs, guarantees the IMDB could accelerate the real-time transaction
   in an efficient way. We believe that IMDB should become a compulsive
   option for enterprise users.
CT 21st IEEE International Conference on Parallel and Distributed Systems
   ICPADS
CY DEC 14-17, 2015
CL Melbourne, AUSTRALIA
SP IEEE; IEEE Comp Soc; RMIT Univ; IEEE Comp Soc Tech Comm Parallel Proc
OI Dong, Xiaoshe/0000-0002-9003-2625
ZB 1
TC 3
Z8 0
ZA 0
ZR 0
ZS 0
Z9 4
U1 0
U2 9
SN 1521-9097
BN 978-0-7695-5785-4
DA 2016-09-28
UT WOS:000381621400101
ER

PT C
AU Zhong, Tao
   Doshi, Kshitij A.
   Tang, Xi
   Lou, Ting
   Lu, Zhongyan
   Li, Hong
BE Hu, X
   Lin, TY
   Raghavan, V
   Wah, B
   BaezaYates, R
   Fox, G
   Shahabi, C
   Smith, M
   Yang, Q
   Ghani, R
   Fan, W
   Lempel, R
   Nambiar, R
TI On Mixing High-Speed Updates and In-Memory Queries A Big-Data
   Architecture for Real-time Analytics
SO 2013 IEEE INTERNATIONAL CONFERENCE ON BIG DATA
SE IEEE International Conference on Big Data
DT Proceedings Paper
PD 2013
PY 2013
AB Up-to-date business intelligence has become a critical differentiator
   for the modern data-driven highly engaged enterprise. It requires rapid
   integration of new information on a continuous basis for subsequent
   analyses. ETL-based and traditionally batch-processing oriented methods
   of absorbing changes into a relational database schema take time, and
   are therefore incompatible with very low-latency demands of real-time
   analytics. Instead, in-memory clustered stores that employ tunable
   consistency mechanisms are becoming attractive since they dispense with
   the need to transform and transit data between storage layouts and
   tiers.
   When data is updated infrequently, in-memory approaches such as RDD
   transformations in Spark can suffice, but as updates become frequent,
   such in-memory approaches need to be extended to support dynamic
   datasets. This paper describes a few key additional requirements that
   result from having to support in-memory processing of data while updates
   proceed concurrently. The paper describes Real-time Analytics Foundation
   (RAF), an architecture to meet the new requirements. Performance of an
   early implementation of RAF is also described: for an unaudited TPC-H
   derived workload, RAF shows a node-to-node scaling ratio of 88% at 8
   nodes, and for a query equivalent to Q6 in the TPC-H set, RAF is able to
   show 9x improvement over that of Hive-Hadoop. The paper also describes
   two RAF based solutions that are being put together by two independent
   software vendors in China.
CT IEEE International Conference on Big Data (Big Data)
CY OCT 06-09, 2013
CL Santa Clara, CA
SP IEEE; IEEE Comp Soc; CCF; Yahoo Labs; CISCO
RI Zhong, Tao/GOV-5060-2022
Z8 0
ZS 0
TC 4
ZA 0
ZB 0
ZR 0
Z9 4
U1 0
U2 4
SN 2639-1589
BN 978-1-4799-1292-6; 978-1-4799-1293-3
DA 2014-03-19
UT WOS:000330831300161
ER

PT J
AU Gurcan, Fatih
   Gudek, Beyza
   Dalveren, Gonca Gokce Menekse
   Derawi, Mohammad
TI Future-Ready Skills Across Big Data Ecosystems: Insights from Machine
   Learning-Driven Human Resource Analytics
SO APPLIED SCIENCES-BASEL
VL 15
IS 11
AR 5841
DI 10.3390/app15115841
DT Article
PD MAY 22 2025
PY 2025
AB This study aims to analyze online job postings using machine
   learning-based, semantic approaches and to identify the expertise roles
   and competencies required for big data professions. The methodology of
   this study employs latent Dirichlet allocation (LDA), a probabilistic
   topic modeling technique, to reveal hidden semantic structures within a
   corpus of big data job postings. As a result of our analysis, we have
   identified seven expertise roles, six proficiency areas, and 32
   competencies (knowledge, skills, and abilities) necessary for big data
   professions. These positions include "developer", "engineer",
   "architect", "analyst", "manager", "administrator", and "consultant".
   The six essential proficiency areas for big data are "big data
   knowledge", "developer skills", "big data analytics", "cloud services",
   "soft skills", and "technical background". Furthermore, the top five
   skills emerged as "big data processing", "big data tools",
   "communication skills", "remote development", and "big data
   architecture". The findings of our study indicated that the competencies
   required for big data careers cover a broad spectrum, including
   technical, analytical, developer, and soft skills. Our findings provide
   a competency map for big data professions, detailing the roles and
   skills required. It is anticipated that the findings will assist big
   data professionals in assessing and enhancing their competencies,
   businesses in meeting their big data labor force needs, and academies in
   customizing their big data training programs to meet industry
   requirements.
RI GÜDEK, Beyza/; Gurcan, Fatih/AAJ-7503-2021; Menekse Dalveren, Gonca Gokce/HHS-4591-2022
OI GÜDEK, Beyza/0000-0002-7432-9234; Gurcan, Fatih/0000-0001-9915-6686;
   Menekse Dalveren, Gonca Gokce/0000-0002-8649-1909
ZB 0
ZA 0
Z8 0
TC 3
ZR 0
ZS 0
Z9 3
U1 15
U2 18
EI 2076-3417
DA 2025-06-15
UT WOS:001505757200001
ER

PT J
AU Almutairi, Laila
   Abugabah, Ahed
   Alhumyani, Hesham
   Mohamed, Ahmed A.
TI Intelligent biomedical image classification in a big data architecture
   using metaheuristic optimization and gradient approximation
SO WIRELESS NETWORKS
VL 30
IS 8
BP 7087
EP 7108
DI 10.1007/s11276-023-03573-5
DT Article
PD NOV 2024
PY 2024
AB Medical imaging has experienced significant development in contemporary
   medicine and can now record a variety of biomedical pictures from
   patients to test and analyze the illness and its severity. Computer
   vision and artificial intelligence may outperform human diagnostic
   ability and uncover hidden information in biomedical images. In
   healthcare applications, fast prediction and reliability are of the
   utmost importance parameters to assure the timely detection of disease.
   The existing systems have poor classification accuracy, and higher
   computation time and the system complexity is higher. Low-quality images
   might impact the processing method, leading to subpar results.
   Furthermore, extensive preprocessing techniques are necessary for
   achieving accurate outcomes. Image contrast is one of the most essential
   visual parameters. Insufficient contrast may present many challenges for
   computer vision techniques. Traditional contrast adjustment techniques
   may not be adequate for many applications. Occasionally, these
   technologies create photos that lack crucial information. The primary
   contribution of this work is designing a Big Data Architecture (BDA) to
   improve the dependability of medical systems by producing real-time
   warnings and making precise forecasts about patient health conditions. A
   BDA-based Bio-Medical Image Classification (BDA-BMIC) system is designed
   to detect the illness of patients using Metaheuristic Optimization
   (Genetic Algorithm) and Gradient Approximation to improve the biomedical
   image classification process. Extensive tests are conducted on publicly
   accessible datasets to demonstrate that the suggested retrieval and
   categorization methods are superior to the current methods. The
   suggested BDA-BMIC system has average detection accuracy of 94.6% and a
   sensitivity of 97.3% in the simulation analysis.
RI Abugabah, Ahed/AAR-2323-2020
ZR 0
ZA 0
Z8 0
ZB 0
ZS 0
TC 2
Z9 3
U1 0
U2 0
SN 1022-0038
EI 1572-8196
DA 2024-11-01
UT WOS:001679284100001
ER

PT C
AU Grandi, Claudio
   Bettoni, Diego
   Boccali, Tommaso
   Carlino, Gianpaolo
   Cesini, Daniele
   dell'Agnello, Luca
   Donvito, Giacinto
   Salomoni, Davide
   Zoccoli, Antonio
BE Espinal, X
   DeVita, R
   Laycock, P
   Shadura, O
TI ICSC: The Italian National Research Centre on HPC, Big Data and Quantum
   computing
SO 26TH INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY AND NUCLEAR
   PHYSICS, CHEP 2023
SE EPJ Web of Conferences
VL 295
AR 10003
DI 10.1051/epjconf/202429510003
DT Proceedings Paper
PD 2024
PY 2024
AB ICSC ("Italian Center for SuperComputing") is one of the five Italian
   National Centres created within the framework of the NextGenerationEU
   funding by the European Commission. The aim of ICSC, designed and
   approved through 2022 and eventually started in September 2022, is to
   create the national digital infrastructure for research and innovation,
   leveraging existing HPC, HTC and Big Data infrastructures and evolving
   towards a cloud data-lake model. It will be available to the scientific
   and industrial communities through flexible and uniform cloud web
   interfaces and will be relying on a high-level support team; as such, it
   will form a globally attractive ecosystem based on strategic
   public-private partnerships to fully exploit top level digital
   infrastructure for scientific and technical computing and promote the
   development of new computing technologies. The ICSC IT infrastructure is
   built upon existing scientific digital infrastructures as provided by
   the major national players: GARR, the Italian NREN, provides the network
   infrastructure, whose capacity will be upgraded to multiples of Tbps;
   CINECA hosts Leonardo, one of the world largest HPC systems, with a
   power of over 250 Pflops, to be further increased and complemented with
   a quantum computer; INFN contributes with its distributed Big Data cloud
   infrastructure, built in the last decades to respond to the needs of the
   HEP community. On top of the IT infrastructure, several thematic
   activities will be funded and will focus on the development of tools and
   applications in several research domains. Of particular relevance to
   this audience are the activities on "Fundamental Research and Space
   Economy" and "Astrophysics and Cosmos Observations", strictly aligned
   with the INFN and HEP core activities. Finally, two technological
   research activities will foster research on "Future HPC and Big Data"
   and "Quantum Computing".
CT 26th International Conference on Computing in High Energy and Nuclear
   Physics (CHEP)
CY MAY 08-12, 2023
CL Norfolk, VA
SP Int Union Pure & Appl Phys; Ideas4hpc; Dell Technologies; SEAL Storage
   Technologies; CTG Federal; Sycomp; IBM; SuperMicro; WEKA; CAEN
   Technologies; Thomas Jefferson National Lab; Jefferson Science
   Associates; DOE Off Nucl Phys; DOE Off High Energy Phys
RI Grandi, Claudio/B-5654-2015; Donvito, Giacinto/AAC-9872-2020
OI Donvito, Giacinto/0000-0002-0628-1080
Z8 0
TC 0
ZA 0
ZS 0
ZR 0
ZB 0
Z9 3
U1 0
U2 0
SN 2100-014X
BN *****************
DA 2024-09-04
UT WOS:001244151902097
ER

PT J
AU Le, Ngoc-Bao-van
   Seo, Yeong-Seok
   Huh, Jun-Ho
TI Artificial Intelligence in Finance: Coffee Commodity Trading Big Data
   for Informed Decision Making
SO IEEE ACCESS
VL 12
BP 91780
EP 91792
DI 10.1109/ACCESS.2024.3409762
DT Article
PD 2024
PY 2024
AB Coffee, the second-largest global soft commodity, can take advantage of
   a comprehensive mining of daily and historical market data for more
   effective informed trading decisions. Advanced ICT and data mining
   technologies can change the trading market operation. The existing
   systems are confronted with certain constraints, including incomplete
   data, insufficient documentation for storage, and a requirement for a
   scalable infrastructure for big data analytics, such as a data warehouse
   or data lakehouse. To address this issue, the paper presents a design
   and implementation of a coffee commodity trading big data warehouse
   capable of analyzing various essential parameters for supporting
   informed decision-making. First, the designed system can automatically
   collect coffee trading data for New York Arabica coffee futures prices
   from selected worldwide reports and financial data portals. Next, the
   Extract, transform, and load (ETL) process is adopted to ingest coffee
   futures trading crawled data into the 3 layers data warehouse. Finally,
   the analytical system will extract and visualize selected key dimensions
   that influence coffee futures prices within different observation
   windows and perspectives. As a result, we implement a prototype of a
   coffee trading data warehouse on the crawled data from January 2000 to
   October 2022 and visualize trends in coffee futures prices based on the
   collected data for informed decision-making. The construction system is
   capable of stably operating and processing large volumes of transaction
   data. This paper will be valuable documentation for reference and
   decision support for coffee commodity trading enterprises and contribute
   to the development of future forecasting algorithms.
RI Le, Ngoc Bao Van/IXW-9767-2023; Huh, Jun-Ho/AAC-1518-2022; Seo, Yeong-Seok/AAF-2849-2019
OI Le, Ngoc Bao Van/0000-0002-3464-1274; Huh, Jun-Ho/0000-0001-6735-6456;
   Seo, Yeong-Seok/0000-0002-5319-7674
ZR 0
TC 3
ZB 0
Z8 0
ZS 0
ZA 0
Z9 3
U1 6
U2 23
SN 2169-3536
DA 2024-07-23
UT WOS:001269900500001
ER

PT J
AU Sreepathy, H. V.
   Rao, B. Dinesh
   Jaysubramanian, Mohan Kumar
   Rao, B. Deepak
TI Data Ingestions as a Service (DIaaS): A Unified Interface for
   Heterogeneous Data Ingestion, Transformation, and Metadata Management
   for Data Lake
SO IEEE ACCESS
VL 12
BP 156131
EP 156145
DI 10.1109/ACCESS.2024.3479736
DT Article
PD 2024
PY 2024
AB Data ingestion tools are critical component of Data Lake. Existing data
   ingestion tools face challenges of handling large variety, formats,
   sources of data. There exists void for unified data ingestion interface
   to handle the above research problems. This study proposes an innovative
   and integrated framework for data ingestion in a data lake, addressing
   the challenges posed by heterogeneous data sources, formats, and
   metadata management. The framework comprises three novel modules: First
   Unified Data Integration Connectors (UDIC), which provide seamless
   connectivity and data retrieval capabilities from diverse sources
   including databases, data warehouses, file systems, cloud storage, and
   APIs; Second, Adaptive Data Variety Transformation (ADVT), a module that
   intelligently handles the transformation and processing of structured,
   semi-structured, and unstructured data types, ensuring efficient
   ingestion into the data lake; and third, Intelligent Metadata Management
   (IMM), a module that captures, stores, and manages metadata associated
   with the ingested data, offering advanced search, discovery, and
   enrichment functionalities. Comparative study corroborates features
   offered by the service with existing data ingestion tools to evaluate
   the novelty and significance of the study. Performance validation shows
   varying ingestion latencies across different data types: approximately
   148.1 microseconds per record for structured data, 234.2 microseconds
   per record for semi-structured data, 65.6 microseconds per kilobyte (KB)
   for video data, and 42.7 microseconds per KB for image data. These
   results underscore the importance of considering data structure and size
   in optimizing ingestion processes. Overall, this research aims to
   revolutionize data ingestion in data lake environments by providing a
   unified solution for handling diverse data sources, formats, and
   metadata management.
OI Jayasubramanian, Mohan Kumar/0000-0002-7559-0071; Rao,
   Dinesh/0000-0002-5160-482X
TC 2
ZR 0
ZA 0
Z8 0
ZB 0
ZS 0
Z9 3
U1 5
U2 9
SN 2169-3536
DA 2024-11-11
UT WOS:001346093900001
ER

PT J
AU Bimonte, Sandro
   Coulibaly, Fagnine Alassane
   Rizzi, Stefano
TI An approach to on-demand extension of multidimensional cubes in
   multi-model settings: Application to IoT-based agro-ecology
SO DATA & KNOWLEDGE ENGINEERING
VL 150
AR 102267
DI 10.1016/j.datak.2023.102267
EA DEC 2023
DT Article
PD MAR 2024
PY 2024
AB Managing unstructured and heterogeneous data, integrating them, and
   enabling their analysis are among the key challenges in data ecosystems,
   together with the need to accommodate a progressive growth in these
   systems by seamlessly supporting extensibility. This is particularly
   relevant for OLAP analyses on multidimensional cubes stored in data
   warehouses (DWs), which naturally span large portions of heterogeneous
   data, possibly relying on different data models (relational,
   document-based, graph-based). While the management of model
   heterogeneity in DWs, using for instance multi-model databases, has
   already been investigated, not much has been done to support
   extensibility. In a previous paper we have investigated a schemaonread
   scenario aimed at granting the extensibility of multidimensional cubes
   by proposing an architecture to support it and discussing the main open
   issues associated. This paper takes a step further by presenting xCube,
   an approach to provide on-demand extensibility of multidimensional cubes
   in a supply-driven fashion. xCube lets users choose a multidimensional
   element to be extended, using additional data, possibly uploaded from a
   data lake. Then, the multidimensional schema is extended by considering
   the functional dependencies implied by these additional data, and the
   extended multidimensional schema is made available to users for OLAP
   analyses. After explaining our approach with reference to a motivating
   case study in agro-ecology, we propose a proof-of-concept implementation
   using AgensGraph and Mondrian.
RI Rizzi, Stefano/IUO-7212-2023
TC 3
Z8 0
ZS 0
ZA 0
ZB 0
ZR 0
Z9 3
U1 1
U2 5
SN 0169-023X
EI 1872-6933
DA 2024-03-02
UT WOS:001165836200001
ER

PT C
AU Cha, Ji-hyun
   Jeong, Heung-gyun
   Han, Seung-woo
   Kim, Dong-chul
   Oh, Jung-hun
   Hwang, Seok-hee
   Park, Byeong-ju
BE Kurosu, M
   Hashizume, A
TI Development of MLOps Platform Based on Power Source Analysis for
   Considering Manufacturing Environment Changes in Real-Time Processes
SO HUMAN-COMPUTER INTERACTION, HCI 2023, PT IV
SE Lecture Notes in Computer Science
VL 14014
BP 224
EP 236
DI 10.1007/978-3-031-35572-1_15
DT Proceedings Paper
PD 2023
PY 2023
AB Smart factories have led to the introduction of automated facilities in
   manufacturing lines and the increase in productivity using
   semi-automatic equipment or work auxiliary tools that use power sources
   in parallel with the existing pure manual manufacturing method. The
   productivity and quality of manual manufacturing work heavily depend on
   the skill level of the operators. Therefore, changes in manufacturing
   input factors can be determined by analyzing the pattern change of power
   sources such as electricity and pneumatic energy consumed in work-aid
   tools or semi-automatic facilities used by skilled operators. The manual
   workflow can be optimized by modeling this pattern and the image
   information of the operator and analyzing it in real time. Machine
   learning operations (MLOps) technology is required to respond to rapid
   changes in production systems and facilities and work patterns that
   frequently occur in small-batch production methods. MLOps can
   selectively configure Kubeflow, the MLOps solution, and the data lake
   based on Kubernetes for the entire process, from collecting and
   analyzing data to learning and deploying ML models, enabling the
   provision of fast and differentiated services from model development to
   distribution by the scale and construction stage of the manufacturing
   site. In this study, the manual work patterns of operators, which are
   unstructured data, were formulated into power source consumption
   patterns and analyzed along with image information to develop a
   manufacturing management platform applicable to manual-based,
   multi-variety, small-volume production methods and eventually for
   operator training in connection with three-dimensional visualization
   technology.
CT Human-Computer Interaction Thematic Area Conference (HCI) Held as Part
   of the 25th International Conference on Human-Computer Interaction
   (HCII)
CY JUL 23-28, 2023
CL Copenhagen, DENMARK
OI cha, jihyun/0000-0001-7041-5453
Z8 0
ZA 0
ZR 0
TC 2
ZB 0
ZS 0
Z9 3
U1 1
U2 1
SN 0302-9743
EI 1611-3349
BN 978-3-031-35571-4; 978-3-031-35572-1
DA 2024-09-19
UT WOS:001289343000015
ER

PT C
AU Oukhouya, Lamya
   El Haddadi, Anass
   Er-Raha, Brahim
   Asri, Hiba
   Laaz, Naziha
BE BenAhmed, M
   Abdelhakim, BA
   Ane, BK
   Rosiyadi, D
TI A Proposed Big Data Architecture Using Data Lakes for Education Systems
SO EMERGING TRENDS IN INTELLIGENT SYSTEMS & NETWORK SECURITY
SE Lecture Notes on Data Engineering and Communications Technologies
VL 147
BP 53
EP 62
DI 10.1007/978-3-031-15191-0_6
DT Proceedings Paper
PD 2023
PY 2023
AB Nowadays, educational data can be defined through the 3Vs of Big Data:
   volume, variety and velocity. Data sources produce massive and complex
   data, which makes knowledge extraction with traditional tools difficult
   for educational organizations. Indeed, the actual architecture of data
   warehouses do not possess the capability of storing and managing this
   huge amount of varied data. The same goes for analytical processes;
   which no longer satisfy business analysts; in terms of data availability
   and speed of execution of queries. These constraints have implied an
   evolution towards more modern architectures, integrating Big Data
   solutions capable of promoting smart learning to students. In this
   context, the present paper proposes a new big data architecture for
   education systems covering multiple data sources. Using this
   architecture, data is organized through a set of layers, starting with
   the management of the different data sources to their final consumption.
   The proposal approach includes data lake as a means of modernizing
   decision-making processes, in particular data warehouses and OLAP
   methods. It will be used as a means for data consolidation for the
   integration of heterogeneous data sources.
CT 5th International Conference on Networks, Intelligent Systems and
   Security (NISS)
CY MAR 30-31, 2022
CL Bandung, INDONESIA
RI EL HADDADI, Anass/ABD-8465-2021; Laaz, Naziha/GSM-8175-2022
ZR 0
ZB 0
Z8 0
ZA 0
TC 1
ZS 0
Z9 3
U1 2
U2 10
SN 2367-4512
BN 978-3-031-15191-0; 978-3-031-15190-3
DA 2023-01-15
UT WOS:000894285000006
ER

PT C
AU Pingos, Michalis
   Andreou, Andreas S.
BE Kaindl, H
   Mannion, M
   Maciaszek, LA
TI Exploiting Metadata Semantics in Data Lakes Using Blueprints
SO EVALUATION OF NOVEL APPROACHES TO SOFTWARE ENGINEERING, ENASE 2022
SE Communications in Computer and Information Science
VL 1829
BP 220
EP 242
DI 10.1007/978-3-031-36597-3_11
DT Proceedings Paper
PD 2023
PY 2023
AB Smart processing of Big Data has been recently emerged as a field that
   provides quite a few challenges related to how multiple heterogeneous
   data sources that produce massive amounts of structured, semi-structured
   and unstructured data may be handled. One solution to this problem is
   manage this fusion of disparate data sources through Data Lakes. The
   latter, though, suffers from the lack of a disciplined approach to
   collect, store and retrieve data to support predictive and prescriptive
   analytics. This chapter tackles this challenge by introducing a novel
   standardization framework for managing data in Data Lakes that combines
   mainly the 5Vs Big Data characteristics and blueprint ontologies. It
   organizes a Data Lake using a ponds architecture and describes a
   metadata semantic enrichment mechanism that enables fast storing to and
   efficient retrieval. The mechanism supports Visual Querying and offers
   increased security via Blockchain and Non-Fungible Tokens. The proposed
   approach is compared against other known metadata systems utilizing a
   set of functional properties with very encouraging results.
CT 17th International Conference on Evaluation of Novel Approaches to
   Software Engineering (ENASE)
CY APR 25-26, 2022
CL ELECTR NETWORK
SP INSTICC
RI ANEOU, ANEAS/; Pingos, Michalis/; Pingos, Michalis/OGO-6362-2025
OI ANEOU, ANEAS/0000-0001-7104-2097; Pingos, Michalis/0000-0001-6293-6478; 
ZA 0
ZS 0
TC 3
Z8 0
ZB 0
ZR 0
Z9 3
U1 2
U2 2
SN 1865-0929
EI 1865-0937
BN 978-3-031-36596-6; 978-3-031-36597-3
DA 2024-12-03
UT WOS:001351540600011
ER

PT C
AU Ulbig, Michael
   Merschak, Simon
   Hehenberger, Peter
   Bachler, Johann
BE Noel, F
   Nyffenegger, F
   Rivest, L
   Bouras, A
TI Requirements on and Selection of Data Storage Technologies for Life
   Cycle Assessment
SO PRODUCT LIFECYCLE MANAGEMENT PLM IN TRANSITION TIMES: THE PLACE OF
   HUMANS AND TRANSFORMATIVE TECHNOLOGIES, PLM 2022
SE IFIP Advances in Information and Communication Technology
VL 667
BP 86
EP 95
DI 10.1007/978-3-031-25182-5_9
DT Proceedings Paper
PD 2023
PY 2023
AB The importance of a centralized data storage system for life cycle
   assessment (LCA) will be addressed in this paper. Further, the
   decision-making process for a suitable data storage system is discussed.
   LCA requires a lot of relevant data such as resource/material data,
   production process data and logistics data, originating from many
   different sources, which must be integrated. Therefore, data collection
   for LCA is quite difficult. In practice, relevant data for LCA is often
   not available or is uncertain and has therefore to be estimated or
   generalized. This implies less accuracy of the calculated carbon
   footprint. State of the Art research shows that the LCA data collection
   process can benefit from data engineering approaches. Key of these
   approaches is a suitable and efficient data storage system like a data
   warehouse or a data lake. Depending on the LCA use case, a data storage
   system can also benefit from the combination with other technologies
   such as big data and cloud computing. As a result, in this paper a
   criteria catalog is developed and presented. It can be used to evaluate
   and decide which data storage systems and additional technologies are
   recommended to store and process data for more efficient and more
   precise carbon footprint calculation in life cycle assessment.
CT 19th IFIP WG 5.1 International Conference on Product Lifecycle
   Management (PLM)
CY JUL 10-13, 2022
CL Grenoble, FRANCE
SP IFIP WG 5 1; Univ Grenoble Alpes, Grenoble INP; CNRS; G Sci Concept
   Optimisat Prod; Springer
OI Merschak, Simon/0000-0001-8903-6146; Ulbig, Michael/0009-0009-4574-5974
TC 2
ZA 0
ZB 0
ZS 0
Z8 0
ZR 0
Z9 3
U1 2
U2 10
SN 1868-4238
EI 1868-422X
BN 978-3-031-25181-8; 978-3-031-25182-5
DA 2023-05-03
UT WOS:000968187600009
ER

PT J
AU Dessokey, Maha
   Saif, Sherif M.
   Eldeeb, Hesham
   Salem, Sameh
   Saad, Elsayed
TI Importance of Memory Management Layer in Big Data Architecture
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 13
IS 5
BP 460
EP 466
DT Article
PD MAY 2022
PY 2022
AB The generation of daily massive amounts of heterogeneous data from a
   variety of sources presents a challenge in terms of storage and analysis
   capabilities and brings new problems into high-performance computing
   clusters. To better utilize this huge and heterogeneous data, the
   continuous development of advanced Big Data platforms and Big Data
   analytic techniques are required. One of the significant issues with
   in-memory Big Data processing platforms, such as Apache Spark, is the
   user's responsibility to decide whether the intermediate data should be
   cached or not. In addition, the data may be kept in several storage
   systems and physically scattered over different racks, regions, and
   clouds. Data need to be close to the computation nodes and hence data
   locality issue is a challenge. In this paper, using a distinct memory
   management layer between the data processing layer and the data storage
   layer, which automatically caches data without the need for any
   interaction from the applications' developers, is evaluated. K-means,
   PageRank and WordCount workloads from the HiBench benchmark beside a
   real case to predict the price of Real Estate that is implemented using
   Gradient Boosting Regression Tree model, are used to evaluate this
   framework. Experiments show that the memory management layer outperforms
   the Apache Spark in reducing the execution time.
RI Dessokey, Maha/; Saif, Sherif/KYR-3181-2024; Salem, Sameh/U-1662-2019
OI Dessokey, Maha/0000-0003-3394-1950; 
TC 3
ZB 0
ZR 0
ZA 0
ZS 0
Z8 0
Z9 3
U1 0
U2 2
SN 2158-107X
EI 2156-5570
DA 2022-08-12
UT WOS:000835095800001
ER

PT J
AU Martin, Angel
   Maria Capilla, Raquel
   Belen Anquela, Ana
TI Big data architecture and data mining analysis for market segment
   applications of differential global navigation satellite system (GNSS)
   services: case study of the analysis of the demand for navigation and
   agriculture
SO JOURNAL OF NAVIGATION
VL 75
IS 2
BP 421
EP 436
AR PII S0373463322000030
DI 10.1017/S0373463322000030
EA FEB 2022
DT Article
PD MAR 2022
PY 2022
AB Location and navigation services based on global navigation satellite
   systems (GNSS) are needed for real-time high-precision positioning
   applications in relevant economic sectors, such as precision
   agriculture, transport, civil engineering or mapping. Real-time
   navigation users of GNSS networks have significantly increased all
   around the world, since the 1990s, and usage has exceeded initial
   expectations. Therefore, if the evolution of GNSS network users is
   monitored, the dynamics of market segments can be studied. The
   implementation of this hypothesis requires the treatment of big volumes
   of navigation data over several years and the continuous monitoring of
   customers. This paper is focused on the management of massive connection
   of GNSS users in an efficient way, in order to obtain analysis and
   statistics. Big data architecture and data analyses based on data mining
   algorithms have been implemented as the best way to approach the
   hypothesis. Results demonstrate the dynamic of users of different market
   segments, the increasing demand over the years and, specifically,
   conclusions are obtained about the trends, year-on-year correlation and
   business volume recovering after economic crisis periods.
RI Martin, Angel/I-8049-2017; ANQUELA JULIÁN, ANA BELÉN/JAC-5662-2023
OI ANQUELA JULIÁN, ANA BELÉN/0000-0001-6024-3790
ZR 0
Z8 1
ZA 0
ZB 0
ZS 0
TC 2
Z9 3
U1 2
U2 17
SN 0373-4633
EI 1469-7785
DA 2022-02-14
UT WOS:000752126300001
ER

PT C
AU Beheshti, Amin
   Benatallah, Boualem
   Sheng, Quan Z.
   Casati, Fabio
   Nezhad, Hamid-Reza Motahari
   Yang, Jian
   Ghose, Aditya
BE Hacid, H
   Aldwairi, M
   Bouadjenek, MR
   Petrocchi, M
   Faci, N
   Outay, F
   Beheshti, A
   Thamsen, L
   Dong, H
TI AI-Enabled Processes: The Age of Artificial Intelligence and Big Data
SO SERVICE-ORIENTED COMPUTING, ICSOC 2021 WORKSHOPS
SE Lecture Notes in Computer Science
VL 13236
BP 321
EP 335
DI 10.1007/978-3-031-14135-5_29
DT Proceedings Paper
PD 2022
PY 2022
AB Business processes, i.e., a set of coordinated tasks and activities
   carried out manually/automatically to achieve a business objective or
   goal, are central to the operation of public and private enterprises.
   Modern processes are often highly complex, data-driven, and
   knowledge-intensive. In such processes, it is not sufficient to focus on
   data storage/analysis; and the knowledge workers will need to collect,
   understand, and relate the big data (from open, private, social, and IoT
   data islands) to process analysis. Today, the advancement in Artificial
   Intelligence (AI) and Data Science can transform business processes in
   fundamental ways; by assisting knowledge workers in communicating
   analysis findings, supporting evidence, and making decisions. This
   tutorial gives an overview of services in organizations, businesses, and
   society. We introduce notions of Data Lake as a Service and Knowledge
   Lake as a Service and discuss their role in analyzing data-centric and
   knowledge-intensive processes in the age of Artificial Intelligence and
   Big Data. We introduce the novel notion of AI-enabled Processes and
   discuss methods for building intelligent Data Lakes and Knowledge Lakes
   as the foundation for Process Automation and Cognitive Augmentation in
   Business Process Management. The tutorial also points out challenges and
   research opportunities.
CT 19th International Conference on Service-Oriented Computing (ICSOC)
CY NOV 22-25, 2021
CL ELECTR NETWORK
RI Beheshti, Amin/ADI-5438-2022; Casati, Fabio/I-7952-2013; Sheng, Michael/ITV-5105-2023; Yang, Jian/
OI Beheshti, Amin/0000-0002-5988-5494; Sheng, Michael/0000-0002-3326-4147;
   Yang, Jian/0000-0002-4408-1952
Z8 0
ZR 0
ZB 0
TC 3
ZS 0
ZA 0
Z9 3
U1 3
U2 38
SN 0302-9743
EI 1611-3349
BN 978-3-031-14135-5; 978-3-031-14134-8
DA 2022-11-04
UT WOS:000870606000028
ER

PT C
AU Chen, Fei
   Yan, Zhengzheng
   Gu, Liang
BE Chen, J
   He, D
   Lu, R
TI Towards Low-Latency Big Data Infrastructure at Sangfor
SO EMERGING INFORMATION SECURITY AND APPLICATIONS, EISA 2022
SE Communications in Computer and Information Science
VL 1641
BP 37
EP 54
DI 10.1007/978-3-031-23098-1_3
DT Proceedings Paper
PD 2022
PY 2022
AB As a top cybersecurity vendor, Sangfor needs collects log streams from
   thousands of endpoint detection devices such as NTA, STA, EDR and
   identifies security threats in real-time way everyday. The discovery and
   disposal of network security incidents are highly real-time in nature
   with seconds or even milliseconds response time to prevent possible
   cyber attacks and data leaks. In order to extract more valuable
   information, the log streams are analyzed using stream processing with
   pattern matching like CEP (Complex Event Processing) in memory, and then
   stored in a persistent storage systems such as a data ware-house system
   or a search engine system for data scientists and network security
   engineers to do OLAP (Online Analytical Processing). Sangfor needs to
   build a low-latency big data platform to meet the challenges of massive
   logs.
   More and more open source systems are proposed to solve the problem of
   data processing in a certain aspect. Many decisions must be made to
   balance the benefits when designing a real-time big data infrastructure.
   What's more, how to architecture these systems and construct a one-stack
   unified big data platform have been the key obstacles for big data
   analytics. In this paper, we present the overall architecture of our
   low-latency big data infrastructure and identify four important design
   decisions i.e. message queue, stream processing, OLAP, and data lake. We
   analyze the advantages and disadvantages of existing open source system
   and clarify the reason behind our choices. We also describe the
   improvements and optimizations to make the open-source stacks fit in
   Sangfor's environments, including designing a real-time development
   platform based on Flink and re-architecting Apache Kylin, Clickhouse and
   Presto as a HOLAP system. Then we highlight two important use cases to
   verify the rationality of our infrastructure.
CT 3rd International Conference on Emerging Information Security and
   Applications (EISA)
CY OCT 29-30, 2022
CL Wuhan, PEOPLES R CHINA
SP Cent China Normal Univ, Sch Comp Sci
Z8 0
ZR 0
TC 2
ZS 0
ZA 0
ZB 0
Z9 3
U1 1
U2 4
SN 1865-0929
EI 1865-0937
BN 978-3-031-23097-4; 978-3-031-23098-1
DA 2023-05-09
UT WOS:000972252300003
ER

PT C
AU Chen, Zhe
   Shao, Hangyu
   Li, Yuping
   Lu, Hongru
   Jin, Jiahui
GP IEEE
TI Policy-Based Access Control System for Delta Lake
SO 2022 TENTH INTERNATIONAL CONFERENCE ON ADVANCED CLOUD AND BIG DATA, CBD
SE International Conference on Advanced Cloud and Big Data
BP 60
EP 65
DI 10.1109/CBD58033.2022.00020
DT Proceedings Paper
PD 2022
PY 2022
AB Delta lake is a new generation of data storage solutions. It stores both
   transaction log and data files in one directory, and provides ACID
   transactions, scalable metadata handling, and unifies streaming and
   batch data processing on top of existing data lakes, such as S3, ADLS,
   GCS, and HDFS. Different from data warehouses, delta lakes allow data to
   be stored in the original format, retain complete data information, and
   provide efficient and low-cost storage solutions for data computing and
   analysis businesses. However, Since Delta Lake metadata is scattered in
   different resource files, the lack of a unified metadata view increases
   the difficulty of data governance. Also, Delta Lake adopts an open
   source storage system as the underlying storage, and its basic access
   control does not isolate different users, which may lead the risk of
   data leakage. At present, most common storage systems use data tables'
   row and column fields for access control, while delta lake treats the
   file group as an object. In this paper, aiming at the difficulty of data
   governance, we design a data lake metadata management method to achieve
   unified and efficient management of metadata information in
   heterogeneous data. Then, we design a policy-based data lake access
   control mechanism, combined with the open source permission framework,
   and complete the access request for different users and roles in Delta
   Lake.
CT 10th International Conference on Advanced Cloud and Big Data (CBD)
CY NOV 04-05, 2022
CL Guilin, PEOPLES R CHINA
SP Guangxi Normal Univ; Guilin Univ Technol; SE Univ
RI Jin, Jiahui/JPX-2144-2023
ZB 0
ZS 0
ZA 0
Z8 0
ZR 0
TC 1
Z9 3
U1 1
U2 6
SN 2573-301X
BN 979-8-3503-0971-3
DA 2023-06-01
UT WOS:000976903900011
ER

PT C
AU Ouafiq, El Mehdi
   Saadane, Rachid
   Chehri, Abdellah
   Wahbi, Mohamed
GP IEEE
TI 6G Enabled Smart Environments and Sustainable Cities: an Intelligent Big
   Data Architecture
SO 2022 IEEE 95TH VEHICULAR TECHNOLOGY CONFERENCE (VTC2022-SPRING)
SE IEEE Vehicular Technology Conference VTC
DI 10.1109/VTC2022-Spring54318.2022.9860772
DT Proceedings Paper
PD 2022
PY 2022
AB Nowadays, there is an important need for fault-tolerant and
   energy-efficient self-organization systems, especially within smart
   cities. Internet of Things (IoT) proved capable of observing and
   examining the environment, generating & processing data. IoT is now
   applicable to almost every industry, including transportation and
   logistics, utilities, agriculture, smart cities, and more. In these
   industries, various types of meters, sensors, and trackers are used to
   constantly monitor activities, automate processes and optimize tasks.
   With the help of big data analytics, they can drive decision-making
   systems based on observations. As a result, the cities-management
   challenges are growing. The smart cities requirements are increasing to
   remedy the challenges, which requires a self-organized network composed
   of a sizeable number of nodes distributed across an area of interest.
   The traditional communication systems show limitations, especially when
   dealing with massive data rates, latency, the explosive growth of
   vehicular communication, and dynamic mobility. In this study, we explore
   a way to leverage the capabilities of wireless communication and big
   data analytics in favor of Smart Cities.
CT IEEE 95th Vehicular Technology Conference: (VTC-Spring)
CY JUN 19-22, 2022
CL Helsinki, FINLAND
SP IEEE; Nokia; Huawei; Samsung; Technol Innovat Inst; Pix Moving
RI Wahbi, Mohamed/I-3076-2013; Chehri, Abdellah/X-9516-2019; rachid, saadane/J-4558-2019
OI Chehri, Abdellah/0000-0002-4193-6062; rachid,
   saadane/0000-0002-0197-8313
ZA 0
ZS 0
ZR 0
TC 2
ZB 0
Z8 0
Z9 3
U1 1
U2 4
BN 978-1-6654-8243-1
DA 2022-10-29
UT WOS:000861825802017
ER

PT C
AU Wreinbel, Robert
BE Pardede, E
   Haghighi, PD
   Khalil, I
   Kotsis, G
TI Data Integration, Cleaning, and Deduplication: Research Versus
   Industrial Projects
SO INFORMATION INTEGRATION AND WEB INTELLIGENCE, IIWAS 2022
SE Lecture Notes in Computer Science
VL 13635
BP 3
EP 17
DI 10.1007/978-3-031-21047-1_1
DT Proceedings Paper
PD 2022
PY 2022
AB In business applications, data integration is typically implemented as a
   data warehouse architecture. In this architecture, heterogeneous and
   distributed data sources are accessed and integrated by means of
   Extract-Transform-Load (ETL) processes. Designing these processes is
   challenging due to the heterogeneity of data models and formats, data
   errors and missing values, multiple data pieces representing the same
   realworld objects. As a consequence, ETL processes are very complex,
   which results in high development and maintenance costs aswell as long
   runtimes. To ease the development of ETL processes, various research and
   technological solutions were development. They include among others: (1)
   ETL design methods, (2) data cleaning pipelines, (3) data deduplication
   pipelines, and (4) performance optimization techniques. In spite of the
   fact that these solutions were included in commercial (and some open
   license) ETL design environments and ETL engines, there still exist
   multiple open issues and the existing solutions still need to advance.
   In this paper (and its accompanying talk), I will provoke a discussion
   on what problems one can encounter while implementing ETL pipelines in
   real business (industrial) projects. The presented findings are based on
   my experience from research and commercial data integration projects in
   financial, healthcare, and software development sectors. In particular,
   I will focus on a few particular issues, namely: (1) performance
   optimization of ETL processes, (2) cleaning and deduplicating large
   row-like data sets, and (3) integrating medical data.
CT 24th International Conference on Information Integration and Web
   Intelligence (iiWAS)
CY NOV 28-30, 2022
CL ELECTR NETWORK
RI Wrembel, Robert/F-7482-2014
OI Wrembel, Robert/0000-0001-6037-5718
ZS 0
TC 3
ZA 0
Z8 0
ZR 0
ZB 0
Z9 3
U1 2
U2 9
SN 0302-9743
EI 1611-3349
BN 978-3-031-21046-4; 978-3-031-21047-1
DA 2023-03-04
UT WOS:000927884900001
ER

PT J
AU Mrozek, Dariusz
   Stepien, Krzysztof
   Grzesik, Piotr
   Malysiak-Mrozek, Bozena
TI A Large-Scale and Serverless Computational Approach for Improving
   Quality of NGS Data Supporting Big Multi-Omics Data Analyses
SO FRONTIERS IN GENETICS
VL 12
AR 699280
DI 10.3389/fgene.2021.699280
DT Article
PD JUL 13 2021
PY 2021
AB Various types of analyses performed over multi-omics data are driven
   today by next-generation sequencing (NGS) techniques that produce large
   volumes of DNA/RNA sequences. Although many tools allow for parallel
   processing of NGS data in a Big Data distributed environment, they do
   not facilitate the improvement of the quality of NGS data for a large
   scale in a simple declarative manner. Meanwhile, large sequencing
   projects and routine DNA/RNA sequencing associated with molecular
   profiling of diseases for personalized treatment require both good
   quality data and appropriate infrastructure for efficient storing and
   processing of the data. To solve the problems, we adapt the concept of
   Data Lake for storing and processing big NGS data. We also propose a
   dedicated library that allows cleaning the DNA/RNA sequences obtained
   with single-read and paired-end sequencing techniques. To accommodate
   the growth of NGS data, our solution is largely scalable on the Cloud
   and may rapidly and flexibly adjust to the amount of data that should be
   processed. Moreover, to simplify the utilization of the data cleaning
   methods and implementation of other phases of data analysis workflows,
   our library extends the declarative U-SQL query language providing a set
   of capabilities for data extraction, processing, and storing. The
   results of our experiments prove that the whole solution supports
   requirements for ample storage and highly parallel, scalable processing
   that accompanies NGS-based multi-omics data analyses.
RI Małysiak-Mrozek, Bożena/AAJ-9652-2020; Mrozek, Dariusz/C-4149-2013
ZS 0
Z8 0
ZR 0
ZB 1
TC 3
ZA 0
Z9 3
U1 0
U2 6
EI 1664-8021
DA 2021-08-08
UT WOS:000678611000001
PM 34326863
ER

PT J
AU Zagan, Elisabeta
   Danubianu, Mirela
TI HADOOP: A Comparative Study between Single-Node and Multi-Node Cluster
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 12
IS 2
BP 53
EP 58
DT Article
PD FEB 2021
PY 2021
AB Data analysis has become a challenge in recent years as the volume of
   data generated has become difficult to manage, therefore more hardware
   and software resources are needed to store and process this huge amount
   of data. Apache Hadoop is a free framework, widely used thanks to the
   Hadoop Distributed Files System (HDFS) and its ability to relate to
   other data processing and analysis components such as MapReduce for
   processing data, Spark - in-memory Data Processing, Apache Drill - SQL
   on Hadoop, and many other. In this paper, we analyze the Hadoop
   framework implementation making a comparative study between Single-node
   and Multi-node cluster on Hadoop. We will explain in detail the two
   layers at the base of the Hadoop architecture: HDFS Layer with its
   deamons NameNode, Secondary NameNode, DataNodes and MapReuce Layer with
   JobTrackers, TaskTrackers daemons. This work is part of a complex one
   aiming to perform data processing in Data Lake structures.
RI Zagan, Elisabeta/NGQ-7564-2025; Danubianu, Mirela/O-3620-2014
OI Danubianu, Mirela/0000-0002-5470-1406
Z8 0
ZA 0
ZB 0
TC 3
ZS 0
ZR 0
Z9 3
U1 0
U2 7
SN 2158-107X
EI 2156-5570
DA 2021-04-20
UT WOS:000630189900008
ER

PT C
AU Ceaparu, Catalin
BE Dzitac, I
   Filip, FG
   Manolescu, MJ
   Dzitac, S
   Kacprzyk, J
   Oros, H
TI IT Solutions for Big Data Processing and Analysis in the Finance and
   Banking Sectors
SO INTELLIGENT METHODS IN COMPUTING, COMMUNICATIONS AND CONTROL
SE Advances in Intelligent Systems and Computing
VL 1243
BP 133
EP 144
DI 10.1007/978-3-030-53651-0_11
DT Proceedings Paper
PD 2021
PY 2021
AB This paper aims to give a general overview of the technologies used by
   two important trends in Business Intelligence nowadays, that continue to
   reshape the Data Architecture landscape worldwide. Bringing equally
   relevant value to businesses today, Fast Data and Big Data complete each
   other in order to enable both quick/short term as well as thorough/long
   term commercial strategies of companies, regardless of the industry they
   are part of. The body and conclusion of this paper will focus on the
   benefits of using the newest FinTech solutions for both aforementioned
   data processing models, while clearly stating the differences between
   the two. Both open source and proprietary type of solutions will be
   presented with the purpose to offer a thorough picture as to what the
   best architectural landscape of Big Data analytics should look like.
CT 8th International Conference on Computers Communications and Control
   (ICCCC)
CY MAY 11-15, 2020
CL ELECTR NETWORK
OI Ceaparu, Catalin/0000-0003-3113-5568
Z8 0
ZR 0
ZA 0
ZS 0
ZB 0
TC 1
Z9 3
U1 1
U2 10
SN 2194-5357
EI 2194-5365
BN 978-3-030-53651-0
DA 2021-04-19
UT WOS:000621675100011
ER

PT C
AU Ren, Peng
   Lin, Weihang
   Liang, Ye
   Wang, Ruoyu
   Liu, Xingyue
   Zuo, Baifu
   Chen, Tan
   Li, Xin
   Sheng, Ming
   Zhang, Yong
BE Siuly, S
   Wang, H
   Chen, L
   Guo, Y
   Xing, C
TI HMDFF: A Heterogeneous Medical Data Fusion Framework Supporting
   Multimodal Query
SO HEALTH INFORMATION SCIENCE, HIS 2021
SE Lecture Notes in Computer Science
VL 13079
BP 254
EP 266
DI 10.1007/978-3-030-90885-0_23
DT Proceedings Paper
PD 2021
PY 2021
AB As we move forwards to Healthcare Industry 4.0 era, more and more high
   accuracy and technological devices are applied into medical field, for
   the better services in health domain. However, the traditional storage
   methods of scaled data limit the application of data analysis. Besides,
   electronic medical data (EMR) and electronic health data (EHR),
   unstructured data including MRIs, CT scans, X-ray and PET scans are the
   fastest growing part of medical data. Due to different regulations
   between structured and unstructured data, and among various unstructured
   imaging data, though the traditional data warehouses can solve the
   problems of scalability and data fragmentation, the cost of solution is
   high and it is not real-time. The multimodal data are separated and this
   ineffective data storage is insufficient to enable further efficient
   hybrid query.
   Therefore, to solve the fragmentation of multimodal data storage and
   enable hybrid query for further data analytics services, this paper
   proposes a highefficiency heterogenous medical data fusion framework
   (HMDFF) for multimodal and heterogeneous data in medical field based on
   data lake. This framework aims to fuse the fragmented medical data and
   provide users with effective management methods and user-friendly
   interfaces to perform hybrid query.
CT 10th International Conference on Health Information Science (HIS)
CY OCT 25-28, 2021
CL Melbourne, AUSTRALIA
RI Lin, Weihang/JPY-1060-2023
ZS 0
ZB 0
ZR 0
ZA 0
TC 2
Z8 0
Z9 3
U1 3
U2 12
SN 0302-9743
EI 1611-3349
BN 978-3-030-90885-0; 978-3-030-90884-3
DA 2022-03-16
UT WOS:000758752400023
ER

PT J
AU Chen, Xi
   Ruan, Fangming
   Zhang, Lvyang
   Zhao, Yang
TI Design of Cyberspace Security Talents Training System Based on Knowledge
   Graph
SO INTERNATIONAL JOURNAL OF DIGITAL CRIME AND FORENSICS
VL 12
IS 4
SI SI
BP 44
EP 53
DI 10.4018/IJDCF.2020100104
DT Article
PD OCT-DEC 2020
PY 2020
AB Internet, big data, global society, economy, life, politics, military,
   and culture are deeply integrated and have developed into an era of
   overlapping cyberspace and real society. Cyberspace security has become
   the most complex, comprehensive, and severe non-traditional security
   challenge facing all countries in the world. However, the talents in the
   field of cyberspace security cannot meet the practical needs of the
   development of cyberspace security. This paper puts forward the training
   scheme of network security talents, discusses the relationship between
   knowledge atlas and network space security, gives the construction and
   distribution of network space full knowledge atlas, and then constructs
   an education big data architecture for cyberspace security based on
   knowledge graph around the use of knowledge.
RI yang, Zhao/JDC-3554-2023
ZB 0
ZA 0
TC 1
ZR 0
ZS 0
Z8 1
Z9 3
U1 1
U2 29
SN 1941-6210
EI 1941-6229
DA 2020-09-24
UT WOS:000568384300005
ER

PT C
AU Ciampi, Mario
   De Pietro, Giuseppe
   Masciari, Elio
   Silvestri, Stefano
GP IEEE Com Soc
TI Health Data Information Retrieval For Improved Simulation
SO 2020 28TH EUROMICRO INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED
   AND NETWORK-BASED PROCESSING (PDP 2020)
SE Euromicro Conference on Parallel, Distributed and Network-Based
   Processing
BP 364
EP 368
DI 10.1109/PDP50117.2020.00062
DT Proceedings Paper
PD 2020
PY 2020
AB In this paper we propose an architecture specifically devoted to the
   analysis of huge natural language biomedical textual collections, with
   the purpose of searching for semantic similarity in order to obtain
   useful hints for effective simulation that could help physicians in
   diagnosis tasks. We leverage Word Embedding models trained with word2vec
   algorithm and a Big Data architecture for their processing and
   management. We performed some preliminary analyses using a dataset
   extracted from the whole PubMed library and we developed a web front-end
   to show the usability of this methodology in a real context.
CT 28th Euromicro International Conference on Parallel, Distributed and
   Network-Based Processing (PDP)
CY MAR 11-13, 2020
CL ELECTR NETWORK
SP Malardalen Univ; Euromicro
RI Ciampi, Mario/B-3874-2015; Silvestri, Stefano/IUP-0829-2023; De Pietro, Giuseppe/AAZ-1151-2020
OI Ciampi, Mario/0000-0002-7286-6212; Silvestri,
   Stefano/0000-0002-9890-8409; 
ZB 0
ZS 0
TC 3
ZR 0
Z8 0
ZA 0
Z9 3
U1 1
U2 3
SN 1066-6192
BN 978-1-7281-6582-0
DA 2020-11-10
UT WOS:000582555800055
ER

PT C
AU Freymann, Andreas
   Maier, Florian
   Schaefer, Kristian
   Boehnel, Tom
BE Wills, G
   Kacsuk, P
   Chang, V
TI Tackling the Six Fundamental Challenges of Big Data in Research Projects
   by Utilizing a Scalable and Modular Architecture
SO PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON INTERNET OF THINGS,
   BIG DATA AND SECURITY (IOTBDS)
BP 249
EP 256
DI 10.5220/0009388602490256
DT Proceedings Paper
PD 2020
PY 2020
AB Over the last decades the necessity for processing and storing huge
   amounts of data has increased enormously, especially in the fundamental
   research area. Beside the management of large volumes of data, research
   projects are facing additional fundamental challenges in terms of data
   velocity, data variety and data veracity to create meaningful data
   value. In order to cope with these challenges solutions exist. However,
   they often show shortcomings in adaptability, usability or have high
   licence fees. Thus, this paper proposes a scalable and modular
   architecture based on open source technologies using micro-services
   which are deployed using Docker. The proposed architecture has been
   adopted, deployed and tested within a current research project. In
   addition, the deployment and handling is compared with another
   technology. The results show an overcoming of the fundamental challenges
   of processing huge amounts of data and the handling of Big Data in
   research projects.
CT 5th International Conference on Internet of Things, Big Data and
   Security (IoTBDS)
CY MAY 07-09, 2020
CL Prague, CZECH REPUBLIC
OI Freymann, Aneas/0000-0002-3735-4545; Schaefer,
   Kristian/0000-0002-7855-6741
Z8 0
ZS 0
ZB 0
ZA 0
TC 3
ZR 0
Z9 3
U1 1
U2 8
BN 978-989-758-426-8
DA 2021-02-23
UT WOS:000615960700026
ER

PT J
AU Rosa, Reinaldo R.
TI Data Science Strategies for Multimessenger Astronomy
SO ANAIS DA ACADEMIA BRASILEIRA DE CIENCIAS
VL 93
AR e20200861
DI 10.1590/0001-3765202020200861
SU 1
DT Article
PD 2020
PY 2020
AB This article aims to identify and suggest data science strategies to
   strengthen scientific research in astronomy. The improvements in data
   workflow performance that can be provided by these strategies can be
   crucial to the multimessenger astronomy (MMA). A special focus is given
   to the treatment of raw data in the context of big data networks for
   BRICS astronomy initiatives. A preliminary design of a prototype that
   incorporates an MMA data cube into a data lake system is presented.
RI Rosa, Reinaldo/ABG-7411-2021
OI Rosa, Reinaldo/0000-0002-2962-4322
ZS 0
ZB 0
ZA 0
Z8 0
ZR 0
TC 1
Z9 3
U1 0
U2 10
SN 0001-3765
EI 1678-2690
DA 2020-10-23
UT WOS:000576769700001
PM 32997040
ER

PT C
AU Timoney, Padraig R.
   Luthra, Roma
   Elia, Alex
   Liu, Haibo
   Isbester, Paul K.
   Levy, Avi
   Shifrin, Michael
   Bringoltz, Barak
   Rabinovich, Eylon
   Broitman, Ariel
   Rothstein, Eitan
   Jacoby, Ran
   Rubinovich, Ilya
   Kim, YongHa
   Shlagman, Ofer
   Ben-Nahum, Barak
   Zolkin, Marina
   Turovets, Igor
BE Adan, O
   Robinson, JC
TI Advanced Machine Learning Eco-System to Address HVM Optical Metrology
   Requirements
SO METROLOGY, INSPECTION, AND PROCESS CONTROL FOR MICROLITHOGRAPHY XXXIV
SE Proceedings of SPIE
VL 11325
AR 113251H
DI 10.1117/12.2552058
DT Proceedings Paper
PD 2020
PY 2020
AB Machine learning (ML) techniques have been successfully deployed to
   resolve optical metrology challenges in semiconductor industry during
   recent years. With more advanced computing technology and algorithms,
   the ML system can be improved further to address High Volume
   Manufacturing (HVM) requirements. In this work, an advanced ML ecosystem
   was implemented based on big data architecture to generate fast and
   user-friendly ML predictive models for metrology purposes. Application
   work and results completed by using this ML eco-system have revealed its
   capability to quickly refine solutions to predict both external
   reference data and to improve the throughput of conventional Optical
   Critical Dimension (OCD) metrology. The time-to-solution has been
   significantly improved and human operational time has also been greatly
   reduced. Results were shown for both front end and back end of line
   measurement applications, demonstrating good correlations and small
   errors in comparison with either external reference or conventional OCD
   results. The incremental retraining from this ML eco-system improved the
   correlation to external references, and multiple retrained models were
   analyzed to understand retraining effects and corresponding
   requirements. Quality Metric (QM) was also shown to have relevance in
   monitoring recipe performance. It has successfully demonstrated that
   with this advanced ML eco-system, streamlined ML models can be readily
   updated for high sensitivity and process development applications in HVM
   scenarios.
CT Conference on Metrology, Inspection, and Process Control for
   Microlithography XXXIV
CY FEB 24-27, 2020
CL San Jose, CA
SP SPIE; Nova Measuring Ltd
RI Liu, Haibo/ABG-2454-2020
Z8 0
ZA 0
ZR 0
ZB 0
ZS 0
TC 3
Z9 3
U1 0
U2 4
SN 0277-786X
EI 1996-756X
BN 978-1-5106-3418-3
DA 2020-10-23
UT WOS:000576666600033
ER

PT C
AU Chong, Henry
BE Budiharto, W
   Wulandhari, LA
   Gunawan, AAS
   Chowanda, A
   Ham, H
   Meiliana
   Suryani, D
   Hanafiah, N
TI SeCBD: The Application Idea from Study Evaluation of Ransomware Attack
   Method in Big Data Architecture
SO DISCOVERY AND INNOVATION OF COMPUTER SCIENCE TECHNOLOGY IN ARTIFICIAL
   INTELLIGENCE ERA
SE Procedia Computer Science
VL 116
BP 358
EP 364
DI 10.1016/j.procs.2017.10.065
DT Proceedings Paper
PD 2017
PY 2017
AB Numerous ransomware attack was launched at May 2017 since it become
   emerge as trending for new cybercrime business source income model. The
   attack to several Big Data Architecture causing problem to over 150
   countries. Meanwhile, the research on prevention idea of this attack
   method are still under struggle. This make ransomware becoming a serious
   and growing cyber issue. In spite of that, the objective of this paper
   is to share the application ideas from crypto-ransomware attack method
   toward future prevention mechanism application to make the Big Data
   Architecture more immune from future ransomware attack. The method used
   in this paper is systematic-review. The result is application prevention
   mechanism named as SeCBD which abbreviation for "Secure Big Data". The
   application idea that result from the systematic review (SeCBD)
   concludes that most of crypto-ransomware still have chance to disabled,
   detected, or prevented. (C) 2017 The Authors. Published by Elsevier B.V.
CT 2nd International Conference on Computer Science and Computational
   Intelligence (ICCSCI)
CY OCT 13-14, 2017
CL Bali, INDONESIA
ZA 0
TC 2
Z8 0
ZR 0
ZS 0
ZB 0
Z9 3
U1 0
U2 1
SN 1877-0509
BN *****************
DA 2018-03-02
UT WOS:000424023700046
ER

PT C
AU Fotache, Marin
   Hrubaru, Ionut
BE Boja, C
   Doinea, M
   Ciurea, C
   Pocatilu, P
   Batagan, L
   Velicanu, A
   Popescu, ME
   Manafi, I
   Zamfiroiu, A
   Zurini, M
TI BIG DATA TECHNOLOGY ON MEDIUM-SIZED DATA. PRELIMINARY RESULTS FOR
   NON-AGGREGATE QUERIES
SO INTERNATIONAL CONFERENCE ON INFORMATICS IN ECONOMY, IE 2016: EDUCATION,
   RESEARCH & BUSINESS TECHNOLOGIES
SE International Conference on Informatics in Economy
BP 273
EP 278
DT Proceedings Paper
PD 2016
PY 2016
AB Big Data Technologies were developed to solve problems related to
   information massiveness, heterogeneity and dynamics mainly by means of
   distributed storage and processing. Current paper examines an apparently
   nonsensical question, whether Big Data persistence and query
   technologies are a proper competitor for SQL data stores when dealing
   with medium-sized data, i.e. million records data not exceeding a
   single-disc (node) storage capacity. There are companies with
   medium-sized databases expecting a dramatic increase of their data.
   These companies might contemplate the idea of adopting Big Data
   technologies with current single-node infrastructure, so that further
   transition to proper, distributed, Big Data architecture will run
   smoothly. How penalizing is this early adoption of a big data technology
   relative a classical solution based on a SQL database server?
   Considering general acknowledged benchmark TPC-H database schema, random
   records were inserted into PostgreSQL and Hive/Hadoop sub-schemas
   created for scale loadings between 0.1 and I. For each scale factor, 500
   random, non-aggregate queries were generated. Main query parameters were
   the number of attributes in SELECT and WHERE clauses, number of joins,
   of BETWEEN, IN operator, number of values appearing in IN clauses,
   number of AND logical connectors, etc. Query execution durations were
   collected using jMeter. To compare the performances of the two data
   stores, the authors conducted an exploratory data analysis.
CT 15th International Conference on Informatics in Economy (IE 2016),
   Education, Research & Business Technologies
CY JUN 02-05, 2016
CL Cluj Napoca, ROMANIA
SP Bucharest Univ Econ Studies; Dept Econ Informat & Cybernet; INFOREC Ass
RI Fotache, Marin/E-1439-2018; Hrubaru, Ionut/IAM-0955-2023
ZS 0
ZA 0
TC 2
Z8 0
ZR 0
ZB 0
Z9 3
U1 0
U2 5
SN 2284-7472
BN *****************
DA 2016-11-16
UT WOS:000386192300043
ER

PT C
AU Gupta, Shalu
   Tripathi, Pooja
BE Singh, G
TI Big Data Lakes Can Support Better Population Health for Rural India -
   Swastha Bharat
SO 2016 1ST INTERNATIONAL CONFERENCE ON INNOVATION AND CHALLENGES IN CYBER
   SECURITY (ICICCS 2016)
BP 145
EP 150
DT Proceedings Paper
PD 2016
PY 2016
AB As India seeks to become a world power, there's maybe nothing additional
   vital than the health and well-being of its citizens. Better population
   health is one among the vital considerations in India. While, those
   living in cities and massive cities have access to high finish health
   services, the ample folks living in rural India, notably within the
   remote elements of the country face issues of inadequate facilities and
   poor access to attention. The inefficiencies and inequities within the
   public health care access in India have pushed forward the necessity for
   power and innovative solutions to strengthen a similar. Paper identifies
   the large shortage of correct health care facilities and addresses a way
   to give bigger access to primary health care services in rural India.
   any this paper, it conjointly addresses the important computing and
   analytical ability of Big Data in process huge volumes of transactional
   information in real time things to show the dream of Swastha Bharat
   (Healthy India) into reality. Furthur the target of this paper is to
   suggest the reforms within the health care sector and boosts the
   discussions on how government will harness innovations within the big
   data analytics to boost the health care system.
CT 1st International Conference on Innovation and Challenges in Cyber
   Security (ICICCS)
CY FEB 03-05, 2016
CL Noida, INDIA
SP IEEE; IEEE UP Sect; Univ Northampton; Amity Univ; Minist Def, Def Res &
   Dev Org
RI Gupta, Shalu/ABG-7269-2021; Tripathi, Pooja/AAK-9698-2021
ZR 0
ZA 0
ZS 0
Z8 0
TC 2
ZB 0
Z9 3
U1 0
U2 1
BN 978-1-5090-2084-3
DA 2016-10-12
UT WOS:000383740800029
ER

PT C
AU Koley, Santanu
   Nandy, Sudarshan
   Dutta, Palash
   Dhar, Sudipto
   Sur, Tapashri
GP IEEE
TI Big Data Architecture with Mobile Cloud in CDroid Operating System for
   Storing Huge Data
SO 2016 INTERNATIONAL CONFERENCE ON COMPUTING, ANALYTICS AND SECURITY
   TRENDS (CAST)
BP 12
EP 17
DT Proceedings Paper
PD 2016
PY 2016
AB We are stepping frontward for an epoch of zeta bytes from
   Giga/Tera/Peta/Exa bytes in this era of computer science. The data
   storage in clouds is not the only preference as big data technology is
   obtainable for processing of both structured and unstructured data.
   Today a colossal sum of data is engendered by mobile phones (Smartphone)
   of both the composition types. For the sake of faster processing and
   elegant data utilization for gigantic quantity of data CDroid operating
   systems can be implemented with big data. Big data architecture is
   realized with mobile cloud for paramount deployment of wherewithal. The
   faster execution can be ended feasible for the make use of a new data
   centric architecture of MapReduce technology where as HDFS (Hadoop
   Distributed File System) also plays a big accountability in using data
   with dissimilar structures. As time advances the degree of data and
   information generated from smartphone augments and faster execution is
   call for the same. As per our research and development the only
   resolution for this mammoth amount of data is to put into practice Big
   data with CDroid scheme for best use of it. We believe this effort will
   budge a step ahead on the road to healthier civilization in close
   proximity to expectations.
CT International Conference on Computing, Analytics and Security Trends
   (CAST)
CY DEC 19-21, 2016
CL Pune, INDIA
SP IEEE Pune Chapter; Coll Engn, Dept Comp Engn & Informat Technol; Coll
   Engn
RI Koley, Santanu/AFT-7239-2022; Nandy, Sudarshan/AAA-2554-2019
OI Koley, Santanu/0000-0001-8082-1332; Nandy, Sudarshan/0000-0002-2369-4792
Z8 0
ZA 0
TC 3
ZR 0
ZS 0
ZB 0
Z9 3
U1 0
U2 0
BN 978-1-5090-1338-8
DA 2017-08-15
UT WOS:000406486400003
ER

PT J
AU Kovacs, Laszlo
   Szabo, Gabor
TI Conceptualization with Incremental Bron-Kerbosch Algorithm in Big Data
   Architecture
SO ACTA POLYTECHNICA HUNGARICA
VL 13
IS 2
BP 139
EP 158
DT Article
PD 2016
PY 2016
AB The paper introduces a novel conceptualization algorithm optimized for a
   distributed, Big Data environment. The proposed method uses a concept
   generation module based on clique detection in the context graph. The
   presented work proposes a novel incremental version of the Bron-Kerbosch
   maximal clique detection method. The efficiency of the method is
   evaluated with random context tests. The presented incremental model is
   even comparable with the usual batch methods. The analysis of the clique
   detection algorithm in MapReduce architecture provides efficiency
   comparison for large scale contexts.
RI Kovacs, Laszlo/PCR-6501-2025
ZS 0
Z8 0
ZA 0
ZB 0
TC 2
ZR 0
Z9 3
U1 0
U2 2
SN 1785-8860
DA 2016-04-27
UT WOS:000373491700008
ER

PT C
AU Tangsatjatham, Pittayut
   Nupairoj, Natawut
GP IEEE
TI Hybrid Big Data Architecture for High-Speed Log Anomaly Detection
SO 2016 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER SCIENCE AND
   SOFTWARE ENGINEERING (JCSSE)
SE International Joint Conference on Computer Science and Software
   Engineering
BP 538
EP 543
DT Proceedings Paper
PD 2016
PY 2016
AB Log processing can be very challenging, especially for environments with
   lots of servers. In these environments, log data is large, coming at
   high-speed, and have various formats, the classic case of big data
   problem. This makes anomaly detection very difficult due to the fact
   that to get good accuracy, large amount of data must be processed in
   real-time. To solve this problem, this paper proposes a hybrid
   architecture for log anomaly detection using Apache Spark for data
   processing and Apache Flume for data collecting. To demonstrate the
   capabilities of our proposed solution, we implement a SARIMA-based
   anomaly detection as a case study. The experimental results clearly
   indicated that our proposed architecture can support log processing in
   large-scale environment effectively.
CT 13th International Joint Conference on Computer Science and Software
   Engineering (JCSSE)
CY JUL 13-15, 2016
CL Khon Kaen, THAILAND
ZA 0
TC 3
ZR 0
ZB 0
ZS 0
Z8 0
Z9 3
U1 0
U2 1
SN 2372-1642
BN 978-1-5090-2033-1
DA 2016-12-21
UT WOS:000389036200097
ER

PT C
AU Wheelus, Charles
   Bou-Harb, Elias
   Zhu, Xingquan
BE Badra, M
   Pau, G
   Vassiliou, V
TI Towards a Big Data Architecture for Facilitating Cyber Threat
   Intelligence
SO 2016 8TH IFIP INTERNATIONAL CONFERENCE ON NEW TECHNOLOGIES, MOBILITY AND
   SECURITY (NTMS)
DT Proceedings Paper
PD 2016
PY 2016
AB Internet and organizational network security is still threatened by
   devastating malicious activities. Given the continuous escalation of
   such attacks in terms of their frequency, sophistication and
   stealthiness, it is of paramount importance to generate effective cyber
   threat intelligence that aim at inferring, attributing, characterizing
   and mitigating such misdemeanors. Nevertheless, such imperative tasks
   are partially impeded by the lack of approaches that can produce prompt
   and accurate actionable intelligence by investigating various network
   traffic sources.
   In this paper, we propose and evaluate a big data architecture that is
   rooted in real-time network traffic processing, distributed messaging
   and scalable data storage. The key benefit of the proposed architecture
   is that it automates the aggregation and transformations of
   heterogeneous network data, allowing for greater focus on cyber threat
   intelligence analytics, rather than data management, aggregation,
   reconciliation and formatting. Empirical evaluations investigating the
   application of machine learning analytics by exploiting the artifacts
   resulting from a prototype of the proposed architecture and by using 100
   GB of real network traffic, indeed demonstrate the practicality,
   effectiveness, and added-value of the proposed architecture.
CT 8th IFIP International Conference on New Technologies, Mobility and
   Security (NTMS)
CY NOV 21-23, 2016
CL Larnaca, CYPRUS
SP IFIP TC6.5 working grp; IEEE; IEEE Commun Soc; Univ Cyprus; TELECOM
   ParisTech; CNRS; IEEE COMSOC
RI Zhu, Xingquan/; ZOU, Fengcai/ABE-4598-2021; Bou-Harb, Elias/AAJ-3396-2021
OI Zhu, Xingquan/0000-0003-4129-9611; Bou-Harb, Elias/0000-0001-8040-4635
ZS 0
TC 3
ZR 0
ZA 0
ZB 0
Z8 0
Z9 3
U1 0
U2 1
BN 978-1-5090-2914-3
DA 2017-02-08
UT WOS:000391578700062
ER

PT C
AU Singh, Tripty
   Darshan, V. S.
GP IEEE
TI A Modern Data Architecture with Apache Hadoop
SO 2015 INTERNATIONAL CONFERENCE ON GREEN COMPUTING AND INTERNET OF THINGS
   (ICGCIOT)
BP 574
EP 579
DT Proceedings Paper
PD 2015
PY 2015
AB This paper represents the analysis of the existing architecture
   framework used across domains.It also emphasizes on the modern
   architecture in integration with apache Hadoop. The existing data
   architecture is under pressure from new data and machine generated data
   for the upcoming years that is due to emergence of new data types there
   has been tremendous pressure on all the data systems within an
   enterprise over the years. An Exponential Growth has been estimated from
   2.8 Zeta Byte of data in 2012 to grow to 40 Zeta Byte by 2020 and new
   data type growth is estimated to be eighty five percent with computer
   created information being expanded to 15 times all the more by 2020
CT International Conference on Green Computing and Internet of Things
   (ICGCIoT)
CY OCT 08-10, 2015
CL GALGOTIAS Educ Inst, Greater Noida, INDIA
HO GALGOTIAS Educ Inst
SP IEEE Advancing Technol Human; IEEE UP Sect India; IEEE Comp Soc; IEEE
   Syst Man & Cybernet Soc; Malaysia Chapter; Inst Neural Network Soc
OI Singh, Tripty/0000-0002-3688-4392
Z8 0
ZB 0
ZA 0
ZS 0
ZR 0
TC 3
Z9 3
U1 0
U2 1
BN 978-1-4673-7910-6
DA 2016-09-13
UT WOS:000380517100114
ER

PT C
AU Smorodin, Gennady
   Kolesnichenko, Olga
GP IEEE
TI Big Data as the Big Game Changer Big Data-driven world needs Big
   Data-driven ideology
SO 2015 9TH INTERNATIONAL CONFERENCE ON APPLICATION OF INFORMATION AND
   COMMUNICATION TECHNOLOGIES (AICT)
SE International Conference on Application of Information and Communication
   Technologies
BP 40
EP 43
DT Proceedings Paper
PD 2015
PY 2015
AB Big Data is the phenomenon of the Information era. Big Data is a new
   dimension to explore, collecting Big Data we fix the time. Big Data has
   some functions, including impact on society, form spatio-temporal
   structures, change the world and future, and integration society with IT
   technologies. Most important aspect is risk in Cloud computing. To
   leverage risks, secure Cloud services and get additional benefits an
   Integrated Approach should be applied. It is important to separate the
   various kinds of "Security" needs when considering Cloud computing
   issues. Also Security Analyst should be included into Data Science Team.
   Data-driven economy is based on three points: open data, legislation for
   Big Data, and education. For students is very important practical
   training that engages students into the culture of Big Data Analytics.
   This opportunity provides the EMC Academic Alliance Russia & CIS through
   the establishment of ad-hoc Big Data Analytics Teams among universities.
   The results of the first stage of launched in 2015 the Big Data
   Analytics Multicenter Study are presented.
CT 9th International Conference of Information and Communiation
   Technologies (AICT)
CY OCT 14-16, 2015
CL Rostov-on-Don, RUSSIA
SP SO FEDERAL UNIV; QAFQAZ UNIV; IEEE; IEEE ADV TECHNOLOGY HUMANITY; SO
   FEDERAL UNIV
RI Smorodin, Gennadii/OZD-4464-2025; Kolesnichenko, Olga/R-5482-2018
OI Kolesnichenko, Olga/0000-0002-4523-6485
ZA 0
TC 3
Z8 0
ZR 0
ZS 0
ZB 0
Z9 3
U1 0
U2 12
SN 2378-8232
EI 2472-8586
BN 978-1-4673-6855-1
DA 2016-09-08
UT WOS:000380404000009
ER

PT J
AU Zhang, Wei
   Dai, Zhixiang
   Xia, Taiwu
   Chen, Gangping
   Zhang, Yihua
   Zhou, Jun
   Liu, Cui
TI Multi-Source Heterogeneous Data-Driven Digital Delivery System for Oil
   and Gas Surface Engineering
SO SYSTEMS
VL 13
IS 6
AR 447
DI 10.3390/systems13060447
DT Article
PD JUN 6 2025
PY 2025
AB To address the challenges of data fragmentation, inconsistent standards,
   and weak interactivity in oil and gas field surface engineering, this
   study proposes an intelligent delivery system integrated with
   three-dimensional dynamic modeling. Utilizing a layered collaborative
   framework, the system combines optimization algorithms and anomaly
   detection methods during data processing to enhance the relevance and
   reliability of high-dimensional data. The model construction adopts a
   structured data architecture and dynamic governance strategies,
   supporting multi-project secure collaboration and full lifecycle data
   management. At the application level, it integrates three-dimensional
   visualization and semantic parsing capabilities to achieve interactive
   display and intelligent analysis of cross-modal data. Validated through
   practical engineering cases, the platform enables real-time linkage of
   equipment parameters, documentation, and three-dimensional models,
   significantly improving data integration efficiency and decision-making
   capabilities. This advancement drives the transformation of oil and gas
   field engineering toward intelligent and knowledge-driven practices.
ZS 0
ZR 0
ZB 0
TC 2
ZA 0
Z8 0
Z9 2
U1 4
U2 7
EI 2079-8954
DA 2025-07-01
UT WOS:001516216200001
ER

PT J
AU Chatelain, Clement
   Lessard, Samuel
   Klinger, Katherine
   Khader, Shameer
   de Rinaldis, Emanuele
TI Building a human genetic data lake to scale up insights for drug
   discovery
SO DRUG DISCOVERY TODAY
VL 30
IS 6
AR 104385
DI 10.1016/j.drudis.2025.104385
EA MAY 2025
DT Review
PD JUN 2025
PY 2025
AB Genome-wide association studies (GWAS) have identified numerous
   disease-associated variants, yet efficient storage and analysis of
   genetic data remain a challenge. Here, we propose a scalable genetic
   data lake (GDL) integrating GWAS, molecular quantitative trait loci
   (mQTL), and epigenetic data within a big data infrastructure to enable
   rapid analysis. This framework allows large-scale computations,
   prioritizing 54 586 gene-trait associations, including 34 779 found
   exclusively in consortium data sets. By leveraging public, consortium,
   and private data, this approach enhances target discovery and indication
   selection, accelerating drug development.
ZS 0
ZB 0
TC 2
ZA 0
Z8 0
ZR 0
Z9 2
U1 1
U2 1
SN 1359-6446
EI 1878-5832
DA 2025-06-09
UT WOS:001502571300001
PM 40409403
ER

PT J
AU Abouzaid, Ahmed
   Barclay, Peter J.
   Chrysoulas, Christos
   Pitropakis, Nikolaos
TI Building a modern data platform based on the data lakehouse architecture
   and cloud-native ecosystem
SO DISCOVER APPLIED SCIENCES
VL 7
IS 3
AR 166
DI 10.1007/s42452-025-06545-w
DT Article
PD FEB 22 2025
PY 2025
AB In today's Big Data world, organisations can gain a competitive edge by
   adopting data-driven decision-making. However, a modern data platform
   that is portable, resilient, and efficient is required to manage
   organisations' data and support their growth. Furthermore, the change in
   the data management architectures has been accompanied by changes in
   storage formats, particularly open standard formats like Apache Hudi,
   Apache Iceberg, and Delta Lake. With many alternatives, organisations
   are unclear on how to combine these into an effective platform. Our work
   investigates capabilities provided by Kubernetes and other Cloud-Native
   software, using DataOps methodologies to build a generic data platform
   that follows the Data Lakehouse architecture. We define the data
   platform specification, architecture, and core components to build a
   proof of concept system. Moreover, we provide a clear implementation
   methodology by developing the core of the proposed platform, which are
   infrastructure (Kubernetes), ingestion and transport (Argo Workflows),
   storage (MinIO), and finally, query and processing (Dremio). We then
   conducted performance benchmarks using an industry-standard benchmark
   suite to compare cold/warm start scenarios and assess Dremio's caching
   capabilities, demonstrating a 12% median enhancement of query duration
   with caching.
RI AbouZaid, Ahmed/; Pitropakis, Nikolaos/ACW-7211-2022; Chrysoulas, Christos/AAD-8176-2020
OI AbouZaid, Ahmed/0009-0007-5524-5055; Pitropakis,
   Nikolaos/0000-0002-3392-9970; Chrysoulas, Christos/0000-0001-9817-003X
ZS 0
TC 1
ZB 0
ZA 0
Z8 0
ZR 0
Z9 2
U1 3
U2 6
EI 3004-9261
DA 2025-02-27
UT WOS:001427902800002
ER

PT J
AU Karcher, Claude
   Font, Roger
   Marcos-Jorquera, Diego
   Gilart-Iglesias, Virgilio
   Manchado, Carmen
TI Evaluating external load responses to cumulative playing time and
   position in the European Handball Federation Women's Euro 2022 through
   an IoT and Big Data architecture approach
SO BIOLOGY OF SPORT
VL 42
IS 2
BP 225
EP 235
DI 10.5114/biolsport.2025.144409
DT Article
PD 2025
PY 2025
AB The quantification of physical demands placed upon handball players,
   segmented by their specific roles and duration of play, is crucial for
   sustaining high performance and minimizing the risk of injury.
   Leveraging advanced inertial measurement units, this investigation
   captured and analyzed the external load data of athletes participating
   in the EHF Women's EURO 2022. The aim of this study was to provide
   coaching staff with information on fatigue development during periods of
   high match density. The study evaluated the effects of playing position
   and cumulative playing time on external load metrics, using linear mixed
   models that treated individual players as random effects. The study
   employed a cutting-edge computational framework integrating sensor
   network technologies, Local Positioning Systems (LPS), and Big Data
   Analytics within a descriptive analytics methodology. From over half a
   billion raw records, we distilled 1,013 data entries from 47 matches for
   analysis. The findings reveal that the wings demonstrated the highest
   levels of total and high-speed running distances, though they sustained
   lower PlayerLoad relative to backs. Interestingly, cumulative playing
   time did not markedly alter load profiles, which may be attributed to
   strategic substitution decisions by coaches and the players' own pacing
   strategies. Notable discrepancies within positional demands were
   observed over time, such as centers displaying increased distance
   coverage within the 2-3 hour play interval. This study underscores the
   efficacy of strategic load management and tailored pacing in sustaining
   player performance throughout high-stakes tournaments. It elucidates the
   relationship between managerial tactics and player-specific
   characteristics in the context of external load distribution.
RI Font, Roger/IYJ-0838-2023; Manchado, Carmen/IZD-9788-2023; Marcos-Jorquera, Diego/M-1628-2014
OI Manchado, Carmen/0000-0001-6448-1104; 
ZR 0
TC 2
Z8 0
ZA 0
ZS 0
ZB 0
Z9 2
U1 2
U2 5
SN 0860-021X
EI 2083-1862
DA 2025-04-29
UT WOS:001471397400002
PM 40182724
ER

PT J
AU da Costa, Douglas R.
   Medeiros, Felipe A.
TI Big data for imaging assessment in glaucoma
SO TAIWAN JOURNAL OF OPHTHALMOLOGY
VL 14
IS 3
BP 299
EP 318
DI 10.4103/tjo.TJO-D-24-00079
DT Review
PD JUL-SEP 2024
PY 2024
AB Glaucoma is the leading cause of irreversible blindness worldwide, with
   many individuals unaware of their condition until advanced stages,
   resulting in significant visual field impairment. Despite effective
   treatments, over 110 million people are projected to have glaucoma by
   2040. Early detection and reliable monitoring are crucial to prevent
   vision loss. With the rapid development of computational technologies,
   artificial intelligence (AI) and deep learning (DL) algorithms are
   emerging as potential tools for screening, diagnosing, and monitoring
   glaucoma progression. Leveraging vast data sources, these technologies
   promise to enhance clinical practice and public health outcomes by
   enabling earlier disease detection, progression forecasting, and deeper
   understanding of underlying mechanisms. This review evaluates the use of
   Big Data and AI in glaucoma research, providing an overview of most
   relevant topics and discussing various models for screening, diagnosis,
   monitoring disease progression, correlating structural and functional
   changes, assessing image quality, and exploring innovative technologies
   such as generative AI.
RI Medeiros, Felipe/MCK-1612-2025; Costa, Douglas/
OI Medeiros, Felipe/0000-0003-3924-2720; Costa, Douglas/0000-0002-9156-3196
ZB 0
Z8 0
ZA 0
TC 2
ZR 0
ZS 0
Z9 2
U1 2
U2 6
SN 2211-5056
EI 2211-5072
DA 2024-10-10
UT WOS:001316677100003
PM 39430345
ER

PT J
AU La Gatta, Valerio
   Moscato, Vincenzo
   Postiglione, Marco
   Sperli, Giancarlo
TI An eXplainable Artificial Intelligence Methodology on Big Data
   Architecture
SO COGNITIVE COMPUTATION
VL 16
IS 5
BP 2642
EP 2659
DI 10.1007/s12559-024-10272-6
EA APR 2024
DT Article
PD SEP 2024
PY 2024
AB Although artificial intelligence has become part of everyone's real
   life, a trust crisis against such systems is occurring, thus increasing
   the need to explain black-box predictions, especially in the military,
   medical, and financial domains. Modern eXplainable Artificial
   Intelligence (XAI) techniques focus on benchmark datasets, but the
   cognitive applicability of such solutions under big data settings is
   still unclear due to memory or computation constraints. In this paper,
   we extend a model-agnostic XAI methodology, named Cluster-Aided Space
   Transformation for Local Explanation (CASTLE), to be able to deal with
   high-volume datasets. CASTLE aims to explain the black-box behavior of
   predictive models by combining both local (i.e., based on the input
   sample) and global (i.e., based on the whole scope for action of the
   model) information. In particular, the local explanation provides a
   rule-based explanation for the prediction of a target instance as well
   as the directions to update the likelihood of the predicted class. Our
   extension leverages modern big data technologies (e.g., Apache Spark) to
   handle the high volume, variety, and velocity of huge datasets. We have
   evaluated the framework on five datasets, in terms of temporal
   efficiency, explanation quality, and model significance. Our results
   indicate that the proposed approach retains the high-quality
   explanations associated with CASTLE while efficiently handling large
   datasets. Importantly, it exhibits a sub-linear, rather than
   exponential, dependence on dataset size, making it a scalable solution
   for massive datasets or in any big data scenario.
RI Postiglione, Marco/ABG-7516-2021; Moscato, Vincenzo/H-2526-2012; Sperlì, Giancarlo/; La Gatta, Valerio/HLW-2658-2023
OI Sperlì, Giancarlo/0000-0003-4033-3777; La Gatta,
   Valerio/0000-0002-5941-4684
ZA 0
ZR 0
ZS 0
ZB 0
Z8 0
TC 2
Z9 2
U1 5
U2 15
SN 1866-9956
EI 1866-9964
DA 2024-04-24
UT WOS:001200367000002
ER

PT J
AU Maass, Laura
   Badino, Manuel
   Iyamu, Ihoghosa
   Holl, Felix
TI Assessing the Digital Advancement of Public Health Systems Using
   Indicators Published in Gray Literature: Narrative Review
SO JMIR PUBLIC HEALTH AND SURVEILLANCE
VL 10
AR e63031
DI 10.2196/63031
DT Review
PD 2024
PY 2024
AB Background: Revealing the full potential of digital public health (DiPH)
   systems requires a wide-ranging tool to assess their maturity and
   readiness for emerging technologies. Although a variety of indices exist
   to assess digital health systems, questions arise about the inclusion of
   indicators of information and communications technology maturity and
   readiness, digital (health) literacy, and interest in DiPH tools by the
   society and workforce, as well as the maturity of the legal framework
   and the readiness of digitalized health systems. Existing tools
   frequently target one of these domains while overlooking the others. In
   addition, no review has yet holistically investigated the available
   national DiPH system maturity and readiness indicators using a
   multidisciplinary lens. Objective: We used a narrative review to map the
   landscape of DiPH system maturity and readiness indicators published in
   the gray literature. Methods: As original indicators were not published
   in scientific databases, we applied predefined search strings to the
   DuckDuckGo and Google search engines for 11 countries from all
   continents that had reached level 4 of 5 in the latest Global Digital
   Health Monitor evaluation. In addition, we searched the literature
   published by 19 international organizations for maturity and readiness
   indicators concerning DiPH. Results: Of the 1484 identified references,
   137 were included, and they yielded 15,806 indicators. We deemed 286
   indicators from 90 references relevant for DiPH system maturity and
   readiness assessments. The majority of these indicators (133/286, 46.5%)
   had legal relevance (targeting big data and artificial intelligence
   regulation, cybersecurity, national DiPH strategies, or health data
   governance), and the smallest number of indicators (37/286, 12.9%) were
   related to social domains (focusing on internet use and access, digital
   literacy and digital health literacy, or the use of DiPH tools,
   smartphones, and computers). Another 14.3% (41/286) of indicators
   analyzed the information and communications technology infrastructure
   (such as workforce, electricity, internet, and smartphone availability
   or interoperability standards). The remaining 26.2% (75/286) of
   indicators described the degree to which DiPH was applied (including
   health data architecture, storage, and access; the implementation of
   DiPH interventions; or the existence of interventions promoting health
   literacy and digital inclusion). Conclusions:Our work is the first to
   conduct a multidisciplinary analysis of the gray literature on DiPH
   maturity and readiness assessments. Although new methods for
   systematically researching gray literature are needed, our study holds
   the potential to develop more comprehensive tools for DiPH system
   assessments. We contributed toward a more holistic understanding of
   DiPH. Further examination is required to analyze the suitability and
   applicability of all identified indicators in diverse health care
   settings. By developing a standardized method to assess DiPH system
   maturity and readiness, we aim to foster informed decision-making among
   health care planners and practitioners to improve resource distribution
   and continue to drive innovation in health care delivery.
RI Iyamu, Ihoghosa/IWE-5004-2023; Badino, Manuel/; Holl, Felix/Y-9648-2019; Maaß, Laura/AEX-5567-2022
OI Iyamu, Ihoghosa/0000-0003-0271-9468; Badino, Manuel/0000-0003-2193-2168;
   Holl, Felix/0000-0002-4020-9509; Maaß, Laura/0000-0001-7354-8120
ZR 0
ZS 0
ZB 0
Z8 0
TC 2
ZA 0
Z9 2
U1 5
U2 11
SN 2369-2960
DA 2025-01-07
UT WOS:001388074000001
PM 39566910
ER

PT C
AU Sukhobokov, Artem A.
   Gapanyuk, Yury E.
   Vetoshkin, Artem A.
   Mironova, Alexandra R.
   Nikolskiy, Daniil R.
   Morozevich, Maria A.
   Klyukin, Nikita A.
   Afanasev, Rodion A.
   Lakhvich, Dmitriy S.
GP IEEE
TI Universal Data Model as a Way to Build Multi-paradigm Data Lakes
SO 2024 9TH INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS, ICBDA 2024
BP 203
EP 211
DI 10.1109/ICBDA61153.2024.10607189
DT Proceedings Paper
PD 2024
PY 2024
AB The paper focuses on data lakes building that combine all data from
   different models stored and processed across the enterprise in both OLAP
   and OLTP modes. As a test of this idea, an experiment was conducted to
   evaluate the performance of a multi-paradigm data lake built on a single
   SparkSQL-HDFS platform against 4 specialized DBMSs that contained the
   same data. The experiment showed that such a solution is possible, but
   further experimentation on a larger amount of data and using
   unstructured data is needed to confirm. As a further development of the
   idea of creating multi-paradigm data lakes on a single technological
   platform, the article proposes the concept of a universal data model. It
   is based on the archigraph structure supporting graph, tabular and
   multidimensional data representation, text documents, and Search Engine
   search index. Unlike the first multi-paradigm data lake experiment, the
   second one develops a metagraph DBMS as a unified technology platform.
   The architecture of the data lake management system built on the
   universal model basis is developed with its use, and the variant of
   representation of the archigraph data describing the lake structure in
   the metagraph DBMS is given. The article is furthered by, the paper
   briefly describes an ongoing project to build a data lake management
   system using a universal data model, and a planned experiment to
   evaluate the performance and scalability of the implemented data lake
   construction method.
CT 9th International Conference on Big Data Analytics-ICBDA-Annual
CY MAR 16-18, 2024
CL Waseda University, Tokyo, JAPAN
HO Waseda University
SP Oslo Metropolitan University (OsloMet); National Institute of Technology
   and Evaluation; California State University East Bay
RI Mironova, Aleksandra/ABD-7125-2021; Sukhobokov, Artem/Y-8244-2019; Afanasev, Rodion/; Gapanyuk, Yuriy/GZK-6257-2022; Никольский, Даниил/; Morozevich, Maria/; Ветошкин, Артём/
OI Afanasev, Rodion/0009-0007-4591-6281; Gapanyuk,
   Yuriy/0000-0001-9005-8174; Никольский, Даниил/0009-0002-4940-9838;
   Morozevich, Maria/0009-0009-7494-6273; Ветошкин,
   Артём/0009-0005-3510-6942
ZS 0
ZB 0
TC 2
ZR 0
Z8 0
ZA 0
Z9 2
U1 0
U2 0
BN 979-8-3503-5297-9; 979-8-3503-5296-2
DA 2025-07-23
UT WOS:001506442400029
ER

PT J
AU Weintraub, Grisha
   Gudes, Ehud
   Dolev, Shlomi
   Ullman, Jeffrey D.
TI Optimizing Cloud Data Lake Queries With a Balanced Coverage Plan
SO IEEE TRANSACTIONS ON CLOUD COMPUTING
VL 12
IS 1
BP 84
EP 99
DI 10.1109/TCC.2023.3339208
DT Article
PD JAN 2024
PY 2024
AB Cloud data lakes emerge as an inexpensive solution for storing very
   large amounts of data. The main idea is the separation of compute and
   storage layers. Thus, cheap cloud storage is used for storing the data,
   while compute engines are used for running analytics on this data in
   "on-demand" mode. However, to perform any computation on the data in
   this architecture, the data should be moved from the storage layer to
   the compute layer over the network for each calculation. Obviously, that
   hurts calculation performance and requires huge network bandwidth. In
   this paper, we study different approaches to improve query performance
   in a data lake architecture. We define an optimization problem that can
   provably speed up data lake queries. We prove that the problem is
   NP-hard and suggest heuristic approaches. Then, we demonstrate through
   the experiments that our approach is feasible and efficient (up to x30
   query execution time improvement based on the TPC-H benchmark).
RI Ullman, Jeffrey/; Dolev, Shlomi/; גודס, אהוד/; GUDES, EHUD/F-1168-2012; Weintraub, Grisha/MHQ-8602-2025
OI Ullman, Jeffrey/0000-0002-1847-3426; Dolev, Shlomi/0000-0001-5418-6670;
   גודס, אהוד/0000-0002-4805-0651; Weintraub, Grisha/0000-0003-4823-4757
ZB 0
TC 2
ZA 0
Z8 0
ZS 0
ZR 0
Z9 2
U1 3
U2 5
SN 2168-7161
DA 2024-03-24
UT WOS:001181455600007
ER

PT J
AU Zouari, Firas
   Ghedira-Guegan, Chirine
   Boukadi, Khouloud
   Kabachi, Nadia
TI A semantic and service-based approach for adaptive mutli-structured data
   curation in data lakehouses
SO WORLD WIDE WEB-INTERNET AND WEB INFORMATION SYSTEMS
VL 26
IS 6
BP 4001
EP 4023
DI 10.1007/s11280-023-01218-3
EA NOV 2023
DT Article
PD NOV 2023
PY 2023
AB Recently, we noticed the emergence of several data management
   architectures to cope with the challenges imposed by big data. Among
   them, data lakehouses are receiving much interest from industrial and
   academic fields due to their ability to hold disparate multi-structured
   batch and streaming data sources in a single data repository. Thus, the
   heterogeneous and complex aspect of the data requires a dedicated
   process to improve their quality and retrieve value from them.
   Therefore, data curation encompasses several tasks that clean and enrich
   data to ensure it continues to fit the user requirements. Nevertheless,
   most existing data curation approaches need more dynamics, flexibility,
   and customization in constituting the data curation pipeline to align
   with end user requirements that may vary according to her/his decision
   context. Moreover, they are dedicated to curating only a single type of
   structure of batch data sources (e.g., semi-structured). Considering the
   changing requirements of the user and the need to build a customized
   data curation pipeline according to the users and the data source
   characteristics, we propose a service-based framework for adaptive data
   curation in data lakehouses that encompasses five modules: data
   collection, data quality evaluation, data characterization, curation
   service composition, and data curation. The proposed framework is built
   upon new data characterization and evaluation modular ontology and a
   curation service composition approach that we detail in the following
   paper. The experimental findings validate the contributions' performance
   in terms of effectiveness and execution time.
RI Zouari, Firas/; KABACHI, Nadia/; GHEDIRA GUEGAN, Chirine/JDV-7343-2023; Boukadi, Khouloud/KRQ-1263-2024
OI Zouari, Firas/0000-0001-6403-2157; KABACHI, Nadia/0000-0001-8071-8490;
   GHEDIRA GUEGAN, Chirine/0000-0003-0908-2711; Boukadi,
   Khouloud/0000-0002-6744-711X
Z8 0
ZS 0
TC 1
ZB 0
ZA 0
ZR 0
Z9 2
U1 1
U2 14
SN 1386-145X
EI 1573-1413
DA 2023-11-27
UT WOS:001096969400001
ER

PT J
AU Alsalemi, Abdullah
   Amira, Abbes
   Malekmohamadi, Hossein
   Diao, Kegong
TI Energy data classification at the edge: a comparative studyfor energy
   efficiency applications
SO CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS
VL 27
IS 3
BP 3259
EP 3275
DI 10.1007/s10586-023-04142-3
EA SEP 2023
DT Article
PD JUN 2024
PY 2024
AB As the global economy is increasingly influenced by energy policy and
   efficiency, the opportunities of energy data classification are
   broadening. Performance metrics, especially for Deep Learning (DL), have
   motivated the accelerated development of computing hardware. In
   particular, High-Performance Edge Computing (HPEC) has an important role
   in complementing cloud computing in the collection, pre-processing, and
   post-processing of big data in a more privacy-preserving manner. Yet,
   such opportunities bring challenges concerning the selection of hardware
   platforms and classification algorithms. Therefore, in this article, we
   aim to carry out a comparative study that examines the merits,
   performance and efficiency metrics, and limitations of notable HPEC
   platforms centered around a DL classification framework. In this
   implementation, a 2D Gramian Angular Field (GAF) energy data classifier
   is presented and run on a publicly available dataset. Following, a DL
   classifier is trained and exported as a lightweight counterpart for
   validation on several commonly used HPEC platforms, namely the Jetson
   Nano, ODROID-XU4, Coral Dev Board, and the BeagleBone AI. Current
   results show competitive computational performance among the reviewed
   platforms, as fast as similar to 8 ms per classified image.
RI Malekmohamadi, Hossein/I-4793-2017
OI Malekmohamadi, Hossein/0000-0003-1457-0162
ZA 0
ZS 0
ZB 0
Z8 0
TC 2
ZR 0
Z9 2
U1 0
U2 3
SN 1386-7857
EI 1573-7543
DA 2023-10-18
UT WOS:001073784000001
ER

PT J
AU Sreepathy, H., V
   Rao, B. Dinesh
   Kumar, J. Mohan
   Rao, B. Deepak
TI Design an efficient data driven decision support system to predict
   flooding by analysing heterogeneous and multiple data sources using Data
   Lake
SO METHODSX
VL 11
AR 102262
DI 10.1016/j.mex.2023.102262
EA JUN 2023
DT Article
PD DEC 2023
PY 2023
AB Floods are the most common natural disaster in several countries
   throughout the world. Flooding has a major impact on people's lives and
   livelihoods. The impact of flood disasters on human lives can be
   mitigated by developing effective flood forecasting and prediction
   models. The majority of flood prediction models do not take all
   flood-causing factors into account when they are designed. It is
   difficult to collect and handle some of these flood-causing variables
   since they are heterogeneous in nature. This paper presents a new big
   data architecture called Data Lake, which can ingest and store all
   important flood-causing heterogeneous data sources in their raw format
   for machine learning model creation. The statistical relevance of
   important flood producing factors on flood prediction outcome is
   determined utilizing inferential statistical approaches. The outcome of
   this research is to create flood warning systems that can alert the
   public and government officials so that they can make decisions in the
   event of a severe flood, reducing socioeconomic loss. & BULL; Flood
   causing factors are from heterogeneous sources, so there is no big data
   architecture for handling variety of data sources. & BULL; To provide
   data architectural solution using data lake for collecting and analysing
   heterogeneous flood causing factors. & BULL; Uses inferential
   statistical approach to determine importance of different flood causing
   factors in design of efficient flood prediction models.
OI Jayasubramanian, Mohan Kumar/0000-0002-7559-0071
ZB 0
Z8 0
ZA 0
ZS 0
TC 2
ZR 0
Z9 2
U1 0
U2 15
EI 2215-0161
DA 2023-09-16
UT WOS:001060254000001
PM 37448950
ER

PT J
AU Essaidi, Abdessamad
   Bellafkih, Mostafa
TI A New Big Data Architecture for Analysis: The Challenges on Social Media
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 14
IS 3
BP 634
EP 639
DT Article
PD MAR 2023
PY 2023
AB The streams of social media big data are now becoming an important
   issue. But the analytics method and tools for this data may not be able
   to find the useful information from this massive amount of data. The
   question then becomes: how do we create a high-performance platform and
   a method to efficiently analyse social networks' big data; how to
   develop a suitable mining algorithm for finding useful information from
   social media big data. In this work, we propose a new hierarchical big
   data analysis for understanding human interaction, and we present a new
   method to measure the useful tweets of Twitter users based on the three
   factors of tweet texts. Finally, we use this test implementation score,
   in order to detect useful and classification tweets by interested
   degree.
OI ESSAIDI, abdessamad/0009-0005-0743-0585; BELLAFKIH,
   Mostafa/0000-0002-6530-6719
ZR 0
TC 2
Z8 0
ZS 0
ZA 0
ZB 0
Z9 2
U1 0
U2 2
SN 2158-107X
EI 2156-5570
DA 2023-05-28
UT WOS:000988715500001
ER

PT C
AU Cherradi, Mohamed
   El Haddadi, Anass
BE BenAhmed, M
   Abdelhakim, BA
   Ane, BK
   Rosiyadi, D
TI DLDB-Service: An Extensible Data Lake System
SO EMERGING TRENDS IN INTELLIGENT SYSTEMS & NETWORK SECURITY
SE Lecture Notes on Data Engineering and Communications Technologies
VL 147
BP 211
EP 220
DI 10.1007/978-3-031-15191-0_20
DT Proceedings Paper
PD 2023
PY 2023
AB Big Data, as a topic of research innovation, still poses numerous
   research challenges, particularly in terms of data diversity. The wide
   variety of data sources results in data silos, which correspond to a set
   of raw data that is only accessible to a part of the company, isolated
   from the rest of the organization. In this context, the Data Lake
   concept has been proposed as a powerful solution to handle big data
   issues by providing schema-less storage for raw data. However, putting
   heterogeneous data "as-is" into a data lake with no metadata management
   system will result in a "data swamp", a collection of undocumented data,
   poorly designed, or inadequately maintained data lake. To avoid this
   gap, we propose DLDB-Service (stands for Data Lake Database management
   Service). The contribution is conceived as a data lake management system
   with advanced metadata handling over raw data derived from diverse data
   sources. Indeed, DLDB-Service allows different users to create a data
   lake, merge heterogeneous data sources into a data lake regardless of
   format, annotate raw data with information semantics, data querying, and
   visualize data statistics. During the demo, we'll go over each component
   of theDLDB-Service. Furthermore, the proposed solutionwill be used in
   real-world scenarios to demonstrate its usefulness in terms of
   scalability, extensibility, and flexibility.
CT 5th International Conference on Networks, Intelligent Systems and
   Security (NISS)
CY MAR 30-31, 2022
CL Bandung, INDONESIA
RI EL HADDADI, Anass/ABD-8465-2021
ZA 0
ZB 0
Z8 0
ZS 0
ZR 0
TC 1
Z9 2
U1 1
U2 15
SN 2367-4512
BN 978-3-031-15191-0; 978-3-031-15190-3
DA 2023-01-15
UT WOS:000894285000020
ER

PT C
AU Errami, Soukaina Ait
   Hajji, Hicham
   El Kadi, Kenza Ait
   Badir, Hassan
BE BenAhmed, M
   Abdelhakim, BA
   Ane, BK
   Rosiyadi, D
TI Managing Spatial Big Data on the Data LakeHouse
SO EMERGING TRENDS IN INTELLIGENT SYSTEMS & NETWORK SECURITY
SE Lecture Notes on Data Engineering and Communications Technologies
VL 147
BP 323
EP 331
DI 10.1007/978-3-031-15191-0_31
DT Proceedings Paper
PD 2023
PY 2023
AB The objective of this paper is to propose some of the best storage
   practices for using Spatial Big data on the Data Lakehouse. In fact,
   handling Big Spatial Data showed the limits of current approaches to
   store massive spatial data, either traditional such as geographic
   information systems or new ones such as extensions of augmented Big Data
   approaches. Our article is divided into four parts. In the first part,
   we will give a brief background of the data management system scene. In
   the second part, we will present the Data LakeHouse and how it responds
   to the problems of storage, processing and exploitation of big data
   while ensuring consistency and efficiency as in data warehouses. Then,
   we will recall the constraints posed by the management of Big Spatial
   Data. We end our paper with an experimental study showing the best
   storage practice for Spatial Big data on the Data LakeHouse. Our
   experiment shows that the partitioning of Spatial Big data over Geohash
   index is an optimal solution for the storage.
CT 5th International Conference on Networks, Intelligent Systems and
   Security (NISS)
CY MAR 30-31, 2022
CL Bandung, INDONESIA
RI aitelkadi, aitelkadi/; Hassan, BADIR/R-6226-2019
OI aitelkadi, aitelkadi/0000-0002-4233-1292; 
ZR 0
Z8 0
ZB 0
ZA 0
TC 1
ZS 0
Z9 2
U1 0
U2 17
SN 2367-4512
BN 978-3-031-15191-0; 978-3-031-15190-3
DA 2023-01-15
UT WOS:000894285000030
ER

PT J
AU Patil, Avinash M.
   Lagad, Priyanka M.
   Soge, Bhavana
   Meshram, Rohan
   Lokhande, Mahendra N.
   Lokhande, Pradeep D.
TI Ammonium chloride mediated synthesis of 2-aryl-phthalazinone from
   O-formyl benzoic acid and in silico applications
SO ARKIVOC
SI SI
BP 1
EP 11
DI 10.24820/ark.5550190.p011.972
PN 7
DT Article
PD 2023
PY 2023
AB Ammonium chloride mediated cyclization reaction leading to one-pot
   synthesis of 2-arylphthalazinone from 2 -carboxyl benzoic acid and aryl
   hydrazine in methanol was developed. This method was found to be
   tolerant of a broad range of functional groups. This novel protocol
   features mild reaction conditions, operational simplicity, and easy
   availability of starting material and very high yields. The molecular
   docking data indicates that compound have comparable free energy with
   the standard compound. They interact only with some conserved residues
   such as Leu387, Trp387, Phe381, Tyr385. Therefore, this compound can be
   considered for further analysis and they have enormous potential to be
   tested experimentally
OI N Patil, Ashish/0000-0001-7807-2686
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
TC 2
Z9 2
U1 0
U2 1
SN 1551-7004
EI 1551-7012
DA 2023-06-03
UT WOS:000994652600001
ER

PT C
AU Sousa, Vania
   Barros, Daniela
   Guimaraes, Pedro
   Santos, Antonina
   Santos, Maribel Yasmina
BE Cabanillas, C
   Perez, F
TI Conceptual Formalization of Massive Storage for Advancing
   Decision-Making with Data Analytics
SO INTELLIGENT INFORMATION SYSTEMS, CAISE FORUM 2023
SE Lecture Notes in Business Information Processing
VL 477
BP 121
EP 128
DI 10.1007/978-3-031-34674-3_15
DT Proceedings Paper
PD 2023
PY 2023
AB Data Lakes have been widely used to handle massive amounts of data
   arriving at high velocity and variety. However, if proper data
   management concerns are not addressed, this massive data storage can
   easily turn Data Lakes into Data Swamps. Furthermore, data must be
   associated with the data artefacts created to extract value from it,
   such as pipelines used to collect, treat, or process data and analytical
   artefacts such as analytical dashboards and machine learning models.
   This paper proposes a more comprehensive view of a Data Lake, in which
   all of these resources can be stored and managed. To that end, the
   conceptual meta-model incorporates a data catalog, data at various
   stages of maturity, pipelines, dashboards, and machine learning models.
   The proposed meta-model was instantiated in the ADM.IN (Advanced
   Decision Making in Productive Systems through Intelligent Networks)
   project, showing how vast amounts of data and their related artefacts
   can be managed to support decision-making processes with data analytics.
CT 35th CAiSE Conference on Cyber-Human Systems
CY JUN 12-16, 2023
CL Zaragoza, SPAIN
SP San Jorge Univ, SVIT Res Grp
RI Guimarães, Peo/; Santos, Maribel Yasmina/M-5214-2013; Sousa, Vânia/
OI Guimarães, Peo/0000-0003-3390-8528; Santos, Maribel
   Yasmina/0000-0002-3249-6229; Sousa, Vânia/0009-0002-1279-6651
TC 2
Z8 0
ZR 0
ZA 0
ZB 0
ZS 0
Z9 2
U1 0
U2 0
SN 1865-1348
EI 1865-1356
BN 978-3-031-34673-6; 978-3-031-34674-3
DA 2024-09-15
UT WOS:001284384200015
ER

PT J
AU Dhillon, Parveen
   Singh, Manpreet
TI An ontology oriented service framework for social IoT
SO COMPUTERS & SECURITY
VL 122
AR 102895
DI 10.1016/j.cose.2022.102895
EA SEP 2022
DT Article
PD NOV 2022
PY 2022
AB Social IoT (S-IoT) is a robust paradigm linked with big data and IoT for
   the sharing and interaction of various devices and services to obtain
   the desired goal. In recent applications, it is a challenging task to
   maintain the trust between various linked devices for the growth and
   creation of IoT systems. Therefore, this paper presents a separation
   architecture based on the Quality of Services by managing the trust
   be-tween the IoT devices. The proposed model is novel based on the
   selection of the attribute set against the separated class. The training
   and classification are done by using the ontology architecture. The data
   architecture is processed by a k-means algorithm for the generation of
   the class labels into three different groups. The groups have been
   labeled using a Fuzzy logic inference engine and a Genetic Algorithm has
   been used for optimization. The performance measure has been evaluated
   using multiple multiclass clas-sifiers for accuracy, True Positive Rate,
   (TPR), and False Positive Rate (FPR).The simulation results elucidate
   that accuracy of the proposed technique is improved by 10.7%, FPR by 6%,
   and TPR by 11% in comparison to state of art techniques.(c) 2022
   Elsevier Ltd. All rights reserved.
RI Singh, Manpreet/JGC-6971-2023
ZS 0
ZR 0
ZB 0
Z8 0
TC 1
ZA 0
Z9 2
U1 0
U2 6
SN 0167-4048
EI 1872-6208
DA 2022-10-13
UT WOS:000863091900011
ER

PT J
AU Cherradi, Mohamed
   EL Haddadi, Anass
TI Grover's Algorithm for Data Lake Optimization Queries
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 13
IS 8
BP 568
EP 576
DT Article
PD AUG 2022
PY 2022
AB the use of No-SQL databases is one of the potential options for storing
   and processing big data lakes. However, searching for large data in
   No-SQL databases is a complex and time-consuming task. Further,
   information retrieval from big data management suffers in terms of
   execution time. To reduce the execution time during the search process,
   we propose a fast and suitable approach based on the quantum Grover
   algorithm, which represents one of the best-known approaches for
   searching in an unstructured database and resolves the unsorted search
   query in O (root n) time complexity. To assess our proposal, a
   comparative study with linear and binary search algorithms was conducted
   to prove the effectiveness of Grover's algorithms. Then, we perform
   extensive experiment evaluations based on ibm_qasm_simulator for
   searching one item out of eight using Grover's search algorithm based on
   three qubits. The experiments outcomes revealed encouraging results,
   with an accuracy of 0.948, well in accordance with the theoretical
   result. Moreover, a discussion of the sensitivity of Grover's algorithm
   through different iterations was carried out. Then, exceeding the
   optimal number of iterations round (pi/4 root N), induces low accuracy
   of the marked state. Furthermore, the incorrect selection of this
   parameter can outline the solution.
RI EL HADDADI, Anass/ABD-8465-2021
ZS 0
ZA 0
Z8 0
TC 0
ZR 0
ZB 0
Z9 2
U1 0
U2 6
SN 2158-107X
EI 2156-5570
DA 2022-09-24
UT WOS:000855626300001
ER

PT J
AU Zhu, Bin
   Zhou, Jie
TI Rainfall erosion of mountain environment and people flow planning of
   Leshan giant Buddha site based on big data GIS
SO ACTA GEOPHYSICA
VL 71
IS 3
BP 1461
EP 1474
DI 10.1007/s11600-022-00803-w
EA JUN 2022
DT Article
PD JUN 2023
PY 2023
AB Based on the big data GIS system, this paper first analyzes the methods
   and research results of the visualization of soil and water conservation
   level data at home and abroad. And for the proposed visual protection
   plan for land and water resources, a relatively excellent data
   architecture and the latest persistence plan have been designed. Based
   on this point, this paper investigates the cause of rainfall erosion and
   fluctuation in the mountain environment of S city and studies and
   determines the important technical points of water and soil conservation
   data images. Based on the results of this kind of research, the article
   further analyzes the temporal and spatial characteristics of the data
   and, based on the RUSLE model, conducts a directional consideration of
   the distribution characteristics of environmental rainfall erosion in
   mountainous environments: that is, comprehensively examines the impact
   of rainfall erosion in mountainous environments from both natural and
   social factors. The main factor of the pattern. Using multi-linear
   regression equations, combined with environmental variables and
   bioclimatic variables, the spatial distribution of land in Q mountainous
   areas is predicted under the background of global change. Finally, the
   article conducted an in-depth study on the people flow planning of the
   Leshan Giant Buddha Scenic Spot, introduced the construction of the
   Leshan Giant Buddha Scenic Spot Project Network, and adopted
   comprehensive technical solutions such as "portrait photography
   reconnaissance surveillance camera + online photography AI portrait
   recognition technology" to build the number of passengers identify and
   plan the design of the system and elaborate on the system's functions,
   such as visitor statistics, hotspot analysis, travel route analysis,
   video tracking, and data display. The article applies the research
   results of rainfall erosion in mountain environment based on big data
   GIS to the research on the people flow planning of Leshan Giant Buddha,
   which promotes the rapid development of scenic spots.
TC 2
ZA 0
Z8 0
ZR 0
ZS 0
ZB 0
Z9 2
U1 5
U2 34
SN 1895-6572
EI 1895-7455
DA 2022-06-29
UT WOS:000813668700001
ER

PT J
AU Ozguven, Yavuz Melih
   Gonener, Utku
   Eken, Suleyman
TI A Dockerized Big Data Architecture for Sports Analytics
SO COMPUTER SCIENCE AND INFORMATION SYSTEMS
VL 19
IS 2
BP 957
EP 978
DI 10.2298/CSIS220118010O
DT Article
PD JUN 2022
PY 2022
AB The big data revolution has had an impact on sports analytics as well.
   Many large corporations have begun to see the financial benefits of
   integrating sports analytics with big data. When we rely on central
   processing systems to aggregate and analyze large amounts of sport data
   from many sources, we compromise the accuracy and timeliness of the
   data. As a response to these issues, distributed systems come to the
   rescue, and the MapReduce paradigm holds promise for largescale data
   analytics. We describe a big data architecture based on Docker
   containers with Apache Spark in this paper. We evaluate the architecture
   on four data-intensive case studies in sport analytics including
   structured analysis, streaming, machine learning approaches, and
   graph-based analysis.
RI Gonener, Utku/; Eken, Suleyman/A-1083-2018; Gönener, Utku/F-6606-2018
OI Gonener, Utku/0000-0002-6152-3353; 
Z8 0
ZR 0
ZA 0
ZS 0
ZB 0
TC 2
Z9 2
U1 1
U2 27
SN 1820-0214
DA 2022-11-13
UT WOS:000878619800010
ER

PT J
AU Shae, Zon-Yin
   Tsai, Jeffrey J. P.
TI A Clinical Kidney Intelligence Platform Based on Big Data, Artificial
   Intelligence, and Blockchain Technology
SO INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS
VL 31
IS 03
AR 2241007
DI 10.1142/S021821302241007X
DT Article
PD MAY 2022
PY 2022
AB The high prevalence and incidence of end-stage rental disease (ESRD) and
   the difficulty in the early predicting the acute kidney injury (AKI)
   event highlights the limits of existing kidney care model, particularly
   the fragmented care and fractured data. The era of medical big data and
   artificial intelligence (AI) opens an opportunity to fill these
   knowledge and practice gaps. To obtain multi-dimensional view of the
   profiles of patients receiving dialysis, we propose to provide coherent
   care services and to actively collect patients' multi-faceted
   information from home and hospital (e.g., photos of diets, sleep
   duration, or dermatologic manifestations). Furthermore, by introducing
   the blockchain in the infrastructure to enable the trustable medical
   exchange and effectively creates a large set of distributed medical data
   lake from various participated hospitals. We will introduce the medical
   coin, a virtual token, to vitalize digital service within the blockchain
   and create common interests among data generators, data vendors, and
   data users.We aim to create business models on top of its therapeutic
   effectiveness and unlock the academic and commercial value of medical
   data ecosystem.
ZB 0
TC 2
ZA 0
Z8 0
ZR 0
ZS 0
Z9 2
U1 1
U2 13
SN 0218-2130
EI 1793-6349
DA 2022-05-14
UT WOS:000790423900012
ER

PT J
AU Saputra, Ferry Astika
   Salman, Muhammad
   Hasim, Jauari Akhmad Nur
   Nadhori, Isbat Uzzin
   Ramli, Kalamullah
TI The Next-Generation NIDS Platform: Cloud-Based Snort NIDS Using
   Containers and Big Data
SO BIG DATA AND COGNITIVE COMPUTING
VL 6
IS 1
AR 19
DI 10.3390/bdcc6010019
DT Article
PD MAR 2022
PY 2022
AB Snort is a well-known, signature-based network intrusion detection
   system (NIDS). The Snort sensor must be placed within the same physical
   network, and the defense centers in the typical NIDS architecture offer
   limited network coverage, especially for remote networks with a
   restricted bandwidth and network policy. Additionally, the growing
   number of sensor instances, followed by a quick increase in log data
   volume, has caused the present system to face big data challenges. This
   research paper proposes a novel design for a cloud-based Snort NIDS
   using containers and implementing big data in the defense center to
   overcome these problems. Our design consists of Docker as the sensor's
   platform, Apache Kafka, as the distributed messaging system, and big
   data technology orchestrated on lambda architecture. We conducted
   experiments to measure sensor deployment, optimum message delivery from
   the sensors to the defense center, aggregation speed, and efficiency in
   the data-processing performance of the defense center. We successfully
   developed a cloud-based Snort NIDS and found the optimum method for
   message-delivery from the sensor to the defense center. We also
   succeeded in developing the dashboard and attack maps to display the
   attack statistics and visualize the attacks. Our first design is
   reported to implement the big data architecture, namely, lambda
   architecture, as the defense center and utilize rapid deployment of
   Snort NIDS using Docker technology as the network security monitoring
   platform.
RI Ramli, Kalamullah/AAI-1405-2019; Hasim, Jauari Akhmad Nur/GOK-1830-2022; Saputra, Ferry Astika/GQZ-6666-2022; Sulaman, Muhammad/I-7661-2019
OI Ramli, Kalamullah/0000-0002-0374-4465; Hasim, Jauari Akhmad
   Nur/0000-0002-2713-0919; Saputra, Ferry Astika/0000-0003-0550-1608; 
ZA 0
ZB 0
TC 1
ZR 0
Z8 0
ZS 0
Z9 2
U1 0
U2 6
EI 2504-2289
DA 2022-04-13
UT WOS:000775989400001
ER

PT C
AU Couto, Julia Colleoni
   Ruiz, Duncan Dubugras
BE Rocha, A
   Bordel, B
   Penalvo, FG
   Goncalves, R
TI An overview about data integration in data lakes
SO 2022 17TH IBERIAN CONFERENCE ON INFORMATION SYSTEMS AND TECHNOLOGIES
   (CISTI)
SE Iberian Conference on Information Systems and Technologies
DT Proceedings Paper
PD 2022
PY 2022
AB Integrating data in data lakes is essential so we can perform more
   complex analyses. However, data lakes are mainly composed of raw data,
   from structured, semi-structured, and even unstructured data. It turns
   out that traditional data integration algorithms usually expect to
   receive structured data as input, so those different types of data
   jeopardize big data integration. This paper presents a systematic
   literature review that generates a broad landscape about data
   integration in data lakes. We searched for papers in eight well-known
   search engines, following a structured process. From the 298 papers we
   retrieved, we selected 22 papers that answer our research questions. We
   identify examples of data lake integration models, how they calculate
   the similarity among the datasets, how the models are evaluated, the
   most common type of data they integrate, and the challenges inherent to
   the area, which points to future research directions in data integration
   in data lakes.
CT 17th Iberian Conference on Information Systems and Technologies (CISTI)
CY JUN 22-25, 2022
CL Madrid, SPAIN
SP Univ Politecnica Madrid; IEEE SMC Portugal Chapter; Informat & Technol
   Management Assoc; IEEE Spain Sect; IEEE SMC Spain Chapter; IEEE SMC
   France Chapter; IEEE SMC Italy Chapter; AISTI
RI Ruiz, Duncan/J-7498-2012
ZS 0
ZR 0
ZA 0
TC 1
Z8 0
ZB 0
Z9 2
U1 0
U2 5
SN 2166-0727
BN 978-989-33-3436-2
DA 2022-10-27
UT WOS:000848616300340
ER

PT C
AU Fernandes, Ana Xavier
   Ferreira, Filipa
   Leon, Ana
   Santos, Maribel Yasmina
BE Guizzardi, R
   Neumayr, B
TI Towards a Model-Driven Approach for Big Data Analytics in the Genomics
   Field
SO ADVANCES IN CONCEPTUAL MODELING, CMLS, EMPER AND JUSMOD
SE Lecture Notes in Computer Science
VL 13650
BP 5
EP 14
DI 10.1007/978-3-031-22036-4_1
DT Proceedings Paper
PD 2022
PY 2022
AB The use of techniques such as Next Generation Sequencing has allowed a
   fast increase in data generation due to the reduction of processing
   costs. What at the beginning seemed to be an important step forward for
   the development of new approaches such as Precision Medicine, turned
   into an exponential growth of data that currently challenges healthcare
   professionals and researchers. Since the problems derived from the
   storage and management of vast amounts of heterogeneous data are
   well-known for the Big Data and Information Systems communities, the
   application of this knowledge to the genomic data domain can help to
   improve the management of the data, reduce the bottlenecks, and reveal
   new insights on the causes of human disease. In this way, this work is
   focused on the problem of data storage by proposing a Big Data
   architecture supported by a model-driven approach to ensure an efficient
   and dynamic storage of genomic data. The proposed architecture has been
   designed considering the main requirements for an efficient data
   integration and for supporting data analysis tasks.
CT 3rd International Workshop on Conceptual Modeling for Life Sciences
   (CMLS)/ 5th International Workshop on Empirical Methods in Conceptual
   Modeling (EmpER)
CY OCT 17-20, 2022
CL Hyderabad, INDIA
RI Fernandes, Ana/GRO-1629-2022; Santos, Maribel Yasmina/M-5214-2013; Palacio, Ana/AAS-8136-2021; Ferreira, Filipa/
OI Santos, Maribel Yasmina/0000-0002-3249-6229; Ferreira,
   Filipa/0000-0002-0527-046X
ZR 0
ZS 0
Z8 0
ZB 1
ZA 0
TC 2
Z9 2
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-22035-7; 978-3-031-22036-4
DA 2023-03-03
UT WOS:000930743100001
ER

PT C
AU Menshchikov, Alexander
   Perfilev, Vladislav
   Roenko, Denis
   Zykin, Maksim
   Fedosenko, Maksim
BE Balandin, S
TI Comparative Analysis of Machine Learning Methods Application for
   Financial Fraud Detection
SO 2022 32ND CONFERENCE OF OPEN INNOVATIONS ASSOCIATION (FRUCT)
BP 178
EP 186
DT Proceedings Paper
PD 2022
PY 2022
AB This paper addresses the fraud detection problem in the context of Big
   Data used in remote banking systems. The paper aims to propose a new
   algorithm for automatic detection of fraudulent transactions using
   machine learning with a performance that allows to apply it in big data
   systems. The article identifies promising directions for optimizing the
   operation of methods for fraudulent transactions detection in anti-fraud
   systems. Architectural approaches to the operation of anti-fraud systems
   have been studied. Based on this, an architecture for illegal actions
   prediction in a near real-time mode was proposed. The research task of
   the article is to find the most suitable machine learning algorithm,
   with the least training and prediction time, demonstrating high
   classification performance. To achieve this goal, an analysis of the
   supervised and ensemble machine learning algorithms was made. The
   dataset was preprocessed for the experiment with SMOTE resampling and
   robust scaling techniques. The chosen methods were compared using
   different metrics: F1 score, AUC and time consumption for training and
   classification. As a result of a metrics comparison, it was found that
   multilayer perceptron (MLP) and boosting methods (Adaptive, Gradient,
   XGBoost) has the highest classification, but MLP outperforms boosting
   methods in terms of time consumption for classification. Thus, MLP was
   selected as the most appropriate algorithm for further integration to
   proposed Big Data architecture. Based on the data obtained during the
   experiments, the degree of their implementation in fraud detection
   systems was assessed and architecture for the anti-fraud detection
   system for big data was proposed.
CT 32nd Conference of Open-Innovations-Association-FRUCT
CY NOV 09-11, 2022
CL Tampere, FINLAND
SP IEEE; IEEE Commun Soc; IEEE Finland; Tampere Univ; FRUCT Open Innovat
   Assoc; Wirepas; MDPI, Elect Journal; Big Data & Cognit Comp
ZR 0
ZA 0
ZB 0
ZS 0
TC 1
Z8 0
Z9 2
U1 1
U2 8
BN 978-952-69244-8-9
DA 2023-01-20
UT WOS:000895933800021
ER

PT J
AU Ritchi, Hamzah
   Andriani, Gina
   Zulkarnaen, Reza
   Zaidaan, Akmal
TI "The state of implementing big data in banking business processes: An
   Indonesian perspective"
SO BANKS AND BANK SYSTEMS
VL 17
IS 3
DI 10.21511/bbs.17(3).2022.10
DT Article
PD 2022
PY 2022
AB Notwithstanding the perceived global potentiality, how big data enhances
   decision- making quality prompts an intriguing inquiry, especially in an
   increasingly competitive banking environment in developing economies.
   Building on an industry data-driven framework, this study strives to
   understand the state of implementing big data in the Indonesian banking
   sector. A deductively organized descriptive method employing indepth
   interviews was conducted with subject matter experts representing
   Indonesian banking-related areas. The result and the following analysis
   show the modest status of big data implementation across three major
   banks and two complementary companies, as indicated by many elements of
   the framework phases that were found during the early adoption stage.
   This denotes a steady buy-in across banking business processes as
   particularly reflected in the framework's four phases - continuing push
   to meet the variety aspect (intelligence), structured data architecture
   domination (design), limited choice of performance indicator for big
   data value (choice), and customer-corporate vision decoupling
   (implementation). While Indonesian banks have evidently initiated the
   big data implementation, further improvement remains imperative for the
   decision-making process. Accordingly, big data should be tightly coupled
   with a strong data-driven vision that drives decision-making across
   intra-firm actors. Handling data omnipresence shall be viewed as the
   embodiment of a data-driven vision.
RI Zaidaan, Akmal/; Ritchi, Hamzah/O-3531-2017
OI Zaidaan, Akmal/0000-0003-3714-3155; Ritchi, Hamzah/0000-0002-9374-9357
ZA 0
Z8 0
ZR 0
ZS 0
ZB 0
TC 2
Z9 2
U1 0
U2 4
SN 1816-7403
EI 1991-7074
DA 2022-01-01
UT WOS:001297163400010
ER

PT J
AU Zerega-Prado, Jose
   Llerena-Izquierdo, Joe
TI Information consolidation architecture for health insurance using Big
   Data
SO MEMORIA INVESTIGACIONES EN INGENIERIA
IS 23
BP 18
EP 31
DI 10.36561/ING.23.3
DT Article
PD 2022
PY 2022
AB The identification of data that is in various sources of information and
   its consolidation to deliver it as useful is achieved with Big Data. The
   overall objective of this work is to develop an information
   consolidation architecture design for health insurance using Big Data.
   For this research proposal, the analytical empirical method is used, of
   a quasi-experimental type with a quantitative approach, through the
   analysis of relevant references and specification of the architecture
   components. The results of this research allow categorizing different
   computational architectures for health insurance through a review of
   relevant literature, developing an architectural model of a
   computational system for an Ecuadorian health insurance company oriented
   to the consolidation of information, and evaluating the study
   methodology used to establish feasible factors of the model. The
   contribution of this work allows us to determine the applicability of
   the model to national or foreign health insurance companies by
   contrasting feasible factors in a specific company of the environment.
   It is concluded that the different sources of information or types of
   data used in the field of health insurance allow to know several edges
   of data analysis through a Big Data architecture, in addition to
   obtaining indicators to improve decision making; 73% of the established
   factors are viable in an Ecuadorian health insurance company.
RI Llerena Izquierdo, Joe/B-5941-2014
OI Llerena Izquierdo, Joe/0000-0001-9907-7048
Z8 0
ZA 0
ZR 0
ZS 0
ZB 0
TC 0
Z9 2
U1 0
U2 7
SN 2301-1092
EI 2301-1106
DA 2023-01-22
UT WOS:000906769400002
ER

PT J
AU Geva, Gil A.
   Ketko, Itay
   Nitecki, Maya
   Simon, Shoham
   Inbar, Barr
   Toledo, Itay
   Shapiro, Michael
   Vaturi, Barak
   Votta, Yoni
   Filler, Daniel
   Yosef, Roey
   Shpitzer, Sagi A.
   Hir, Nabil
   Markovich, Michal Peri
   Shapira, Shachar
   Fink, Noam
   Glasberg, Elon
   Furer, Ariel
TI Data Empowerment of Decision-Makers in an Era of a Pandemic:
   Intersection of "Classic" and Artificial Intelligence in the Service of
   Medicine
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 23
IS 9
AR e24295
DI 10.2196/24295
DT Article
PD SEP 10 2021
PY 2021
AB Background: The COVID-19 outbreak required prompt action by health
   authorities around the world in response to a novel threat. With
   enormous amounts of information originating in sources with uncertain
   degree of validation and accuracy, it is essential to provide
   executive-level decision-makers with the most actionable, pertinent, and
   updated data analysis to enable them to adapt their strategy swiftly and
   competently.
   Objective: We report here the origination of a COVID-19 dedicated
   response in the Israel Defense Forces with the assembly of an
   operational Data Center for the Campaign against Coronavirus.
   Methods: Spearheaded by directors with clinical, operational, and data
   analytics orientation, a multidisciplinary team utilized existing and
   newly developed platforms to collect and analyze large amounts of
   information on an individual level in the context of SARS-CoV-2
   contraction and infection.
   Results: Nearly 300,000 responses to daily questionnaires were recorded
   and were merged with other data sets to form a unified data lake. By
   using basic as well as advanced analytic tools ranging from simple
   aggregation and display of trends to data science application, we
   provided commanders and clinicians with access to trusted, accurate, and
   personalized information and tools that were designed to foster
   operational changes and mitigate the propagation of the pandemic. The
   developed tools aided in the in the identification of high-risk
   individuals for severe disease and resulted in a 30% decline in their
   attendance to their units. Moreover, the queue for laboratory
   examination for COVID-19 was optimized using a predictive model and
   resulted in a high true-positive rate of 20%, which is more than twice
   as high as the baseline rate (2.28%, 95% CI 1.63%-3.19%).
   Conclusions: In times of ambiguity and uncertainty, along with an
   unprecedented flux of information, health organizations may find
   multidisciplinary teams working to provide intelligence from diverse and
   rich data a key factor in providing executives relevant and actionable
   support for decision-making.
OI Geva, Gil/0000-0002-0322-8348; Shapiro, Michael/0000-0001-5943-6974;
   Filler, Daniel/0000-0002-0070-816X; Simon, Shoham/0000-0003-2544-1550;
   Ketko, Itay/0000-0001-7435-4424; Nitecki, Maya/0000-0003-1127-4552;
   Inbar, Barr/0000-0002-1978-1826
ZB 0
TC 2
Z8 0
ZA 0
ZS 0
ZR 0
Z9 2
U1 1
U2 19
SN 1439-4456
EI 1438-8871
DA 2021-09-10
UT WOS:000695740200002
PM 34313589
ER

PT J
AU Sassi, Imad
   Anter, Samir
   Bekkhoucha, Abdelkrim
TI ParaDist-HMM: A Parallel Distributed Implementation of Hidden Markov
   Model for Big Data Analytics using Spark
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 12
IS 4
BP 289
EP 303
DT Article
PD APR 2021
PY 2021
AB Big Data is an extremely massive amount of heterogeneous and multisource
   data which often requires fast processing and real time analysis.
   Solving big data analytics problems needs powerful platforms to handle
   this enormous mass of data and efficient machine learning algorithms to
   allow the use of big data full potential. Hidden Markov models are
   statistical models, rich and widely used in various fields especially
   for time varying data sequences modeling and analysis. They owe their
   success to the existence of many efficient and reliable algorithms. In
   this paper, we present ParaDist-HMM, a parallel distributed
   implementation of hidden Markov model for modeling and solving big data
   analytics problems. We describe the development and the implementation
   of the improved algorithms and we propose a Spark-based approach
   consisting in a parallel distributed big data architecture in cloud
   computing environment, to put the proposed algorithms into practice. We
   evaluated the model on synthetic and real financial data in terms of
   running time, speedup and prediction quality which is measured by using
   the accuracy and the root mean square error. Experimental results
   demonstrate that ParaDist-HMM algorithms outperforms other
   implementations of hidden Markov models in terms of processing speed,
   accuracy and therefore in efficiency and effectiveness.
RI SASSI, Imad/; ANTER, Samir/LXB-1242-2024
OI SASSI, Imad/0000-0002-3967-9554; ANTER, Samir/0000-0001-8286-9612
TC 1
ZA 0
ZS 0
Z8 0
ZB 0
ZR 0
Z9 2
U1 0
U2 5
SN 2158-107X
EI 2156-5570
DA 2021-06-04
UT WOS:000648867700039
ER

PT C
AU Abdelhedi, Fatma
   Jemmali, Rym
   Zurfluh, Gilles
BE Bernardino, J
   Masciari, E
   Rolland, C
   Filipe, J
TI Ingestion of a Data Lake into a NoSQL Data Warehouse: The Case of
   Relational Databases
SO PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON KNOWLEDGE
   DISCOVERY, KNOWLEDGE ENGINEERING AND KNOWLEDGE MANAGEMENT (KMIS), VOL 3
BP 64
EP 72
DI 10.5220/0010690600003064
DT Proceedings Paper
PD 2021
PY 2021
AB The exponential growth of collected data, following the digital
   transformation of companies, has led to the evolution of databases
   towards Big Data. Our work is part of this context and concerns more
   particularly the mechanisms allowing to extract datasets from a Data
   Lake and to store them in a unique Data Warehouse. This one will allow
   to realize, in a second time, decisional analyses facilitated by the
   functionalities offered by the NoSQL systems (richness of the data
   structures, query language, access performances). This article proposes
   an extraction mechanism applied only to relational databases of the Data
   Lake. This mechanism relies on an automatic approach based on the Model
   Driven Architecture (MDA) which provides a set of schema transformation
   rules, formalized with the Query/View/Transform (QVT) language. From the
   physical schemas describing relational databases, we propose
   transformation rules that allow to generate a physical model of a Data
   Warehouse stored on a document-oriented NoSQL system (OrientDB). This
   paper presents the successive steps of the transformation process from
   the meta-modeling of the datasets to the application of the rules and
   algorithms. We provide an experimentation using a case study related to
   the health care field.
CT 13th International Joint Conference on Knowledge Discovery, Knowledge
   Engineering and Knowledge Management (IC3K) / 13th International
   Conference on Knowledge Management and Information Systems (KMIS)
CY OCT 25-27, 2021
CL ELECTR NETWORK
SP INSTICC
RI Abdelhedi, Fatma/AAY-3404-2020; , Fatma/; Jemmali, Rym/
OI , Fatma/0000-0003-1658-8067; Jemmali, Rym/0000-0003-1276-7658
ZR 0
ZA 0
ZS 0
TC 2
Z8 0
ZB 0
Z9 2
U1 1
U2 6
BN 978-989-758-533-3
DA 2022-06-07
UT WOS:000796476100005
ER

PT C
AU Ciangottini, Diego
   Boccali, Tommaso
   Ceccanti, Andrea
   Spiga, Daniele
   Salomoni, Davide
   Tedeschi, Tommaso
   Tracolli, Mirco
BE Biscarat, C
   Campana, S
   Hegner, B
   Roiser, S
   Rovelli, CI
   Stewart, GA
TI First experiences with a portable analysis infrastructure for LHC at
   INFN
SO 25TH INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY AND NUCLEAR
   PHYSICS, CHEP 2021
SE EPJ Web of Conferences
VL 251
AR 02045
DI 10.1051/epjconf/202125102045
DT Proceedings Paper
PD 2021
PY 2021
AB The challenges proposed by the HL-LHC era are not limited to the sheer
   amount of data to be processed: the capability of optimizing the
   analyser's experience will also bring important benefits for the LHC
   communities, in terms of total resource needs, user satisfaction and in
   the reduction of end time to publication. At the Italian National
   Institute for Nuclear Physics (INFN) a portable software stack for
   analysis has been proposed, based on cloud-native tools and capable of
   providing users with a fully integrated analysis environment for the CMS
   experiment. The main characterizing traits of the solution consist in
   the user-driven design and the portability to any cloud resource
   provider. All this is made possible via an evolution towards a
   "python-based" framework, that enables the usage of a set of open-source
   technologies largely adopted in both cloud-native and data-science
   environments. In addition, a "single sign on"-like experience is
   available thanks to the standards-based integration of INDIGO-IAM with
   all the tools. The integration of compute resources is done through the
   customization of a JupyterHUB solution, able to spawn identity-aware
   user instances ready to access data with no further setup actions. The
   integration with GPU resources is also available, designed to sustain
   more and more widespread ML based workflow. Seamless connections between
   the user UI and batch/big data processing framework (Spark, HTCondor)
   are possible. Eventually, the experiment data access latency is reduced
   thanks to the integrated deployment of a scalable set of caches, as
   developed in the context of ESCAPE project, and as such compatible with
   the future scenarios where a data-lake will be available for the
   research community.
   The outcome of the evaluation of such a solution in action is presented,
   showing how a real CMS analysis workflow can make use of the
   infrastructure to achieve its results.
CT 25th International Conference on Computing in High Energy and Nuclear
   Physics (CHEP)
CY MAY 17-21, 2021
CL CERN, ELECTR NETWORK
HO CERN
RI Tedeschi, Tommaso/OMM-4578-2025; Tedeschi, Tommaso/ABB-8842-2021; Ciangottini, Diego/HJH-5914-2023
OI Tedeschi, Tommaso/0000-0002-7125-2905; Tedeschi,
   Tommaso/0000-0002-7125-2905; 
ZB 0
ZA 0
Z8 0
TC 1
ZS 0
ZR 0
Z9 2
U1 0
U2 0
SN 2100-014X
BN *****************
DA 2021-01-01
UT WOS:001329391600053
ER

PT C
AU Neves, Ricardo A.
   Cruvinel, Paulo E.
GP IEEE
TI Ontology for Structuring a Digital Databases for Decision Making in
   Grain Production
SO 2021 IEEE 15TH INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC
   2021)
SE IEEE International Conference on Semantic Computing
BP 386
EP 392
DI 10.1109/ICSC50631.2021.00071
DT Proceedings Paper
PD 2021
PY 2021
AB This paper presents an ontology for the structuring of digital databases
   with the objective of acting in a cloud environment and meeting big data
   sources in the agricultural context of grain production. Its conception
   is structured in three stages: the first stage presents an ontological
   architecture aimed at public and private cloud environments, the second
   stage deals with a semantic model at process level, and a pseudocode for
   ontological application is elaborated in the third stage, considering
   the technologies applied to the cloud. This work combines advanced
   features to support decision making from Data Lake storage solutions,
   semantic treatment of big data, as well as the presentation of
   strategies based on machine learning and data quality analysis to obtain
   data and metadata organized for application in a decision model. The
   configuration of the ontology presented meets the diversity of big data
   projects in the grain production context, the characteristics of which
   are based on interoperability in the use of heterogeneous data and its
   integration, elasticity of computational resources, and high
   availability of cloud access.
CT 15th IEEE International Conference on Semantic Computing (ICSC)
CY JAN 27-29, 2021
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc
RI Cruvinel, Paulo/C-7687-2015
ZS 0
Z8 0
ZA 0
ZB 1
ZR 0
TC 2
Z9 2
U1 0
U2 8
SN 2325-6516
BN 978-1-7281-8899-7
DA 2021-08-04
UT WOS:000668692000067
ER

PT C
AU Simakovic, Milan N.
   Cica, Zoran G.
   Masnikosa, Ina B.
GP IEEE
TI Big Data Architecture for Mobile Network Operators
SO 2021 15TH INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES, SYSTEMS AND
   SERVICES IN TELECOMMUNICATIONS, TELSIKS
BP 283
EP 286
DI 10.1109/TELSIKS52058.2021.9606290
DT Proceedings Paper
PD 2021
PY 2021
AB Mobile network operators are faced with tremendous data growth in their
   networks. Complex and multilayered networks, high-quality signal
   demands, and strong competition are only some of the aspects that push
   operators to further optimize their networks. Also, introduction of 5G
   in their networks enabled mobile operators to offer broad spectrum of 5G
   services, especially IoT (Internet of Things) and M2M (Machine to
   Machine) services which further increase traffic volume and the amount
   of collected data. Traditional data management solutions currently used
   are not able to successfully respond to such huge amounts of data. Big
   data technologies represent a modern approach for coping with the
   enormous data quantities. In this paper, we propose a big data solution
   that can collect and process huge amounts of data to extract valuable
   information and help mobile operators to bring their networks to
   enhanced quality level and offer full IoT solutions to their customers.
CT 15th International Conference on Advanced Technologies, Systems and
   Services in Telecommunications-TELSIKS
CY OCT 20-22, 2021
CL SERBIA
SP Institute of Electrical and Electronics Engineers Inc; University of Nis
ZB 0
ZA 0
ZR 0
TC 0
Z8 0
ZS 0
Z9 2
U1 0
U2 0
BN 978-1-6654-4442-2
DA 2021-01-01
UT WOS:001548423500060
ER

PT C
AU Sosa, David
   Paciello, Julio
BE Teran, L
   Pincay, J
   Portmann, E
TI Data Lake: A Case of Study of a Big Data Analytics Architecture for
   Public Procurements
SO 2021 EIGHT INTERNATIONAL CONFERENCE ON EDEMOCRACY & EGOVERNMENT (ICEDEG)
SE International Conference on eDemocracy and eGovernment ICEDEG
BP 194
EP 198
DI 10.1109/ICEDEG52154.2021.9530976
DT Proceedings Paper
PD 2021
PY 2021
AB Big Data technologies are facing problems of volume, velocity, variety
   and veracity of data, attending to the wide expansion of emerging
   technologies like IoT and IoE. Cyberocracy proposes a decision-making
   process of a Government based on the effective use of information. An
   important effort in this line, focusing on government public
   procurement, has been carried out by the Open Contracting Partnership
   (OCP), promoting the publication of more volumes of public procurement
   data in non-relational and machine processable formats every day. This
   work analyzes the underlying Big Data infrastructure for the analysis of
   public procurement data through a comparative case of study between a
   technology proposed by the OCP called KingFisher and emergent
   technologies based on Data Lakes. With an emphasis on storage
   requirements to support a high volume of payloads, also considering
   criteria of velocity and RAM use. Preliminary results show encouraging
   findings especially in terms of volume required by a Data Lake, even for
   different payload scenarios, up to 10 times less storage than the
   relational database-based model.
CT 8th International Conference on eDemocracy and eGovernment (ICEDEG)
CY JUL 28-30, 2021
CL ELECTR NETWORK
SP Escuela Super Politecnica Litoral; Escuela Politecnica Nacl; Inst Altos
   Estudios Nacl; Univ Amer; Univ Fuerzas Armadas; Univ Tecnica Ambato;
   Univ Fribourg; Univ San Francisco Quito; IEEE Comp Soc; IEEE Reg 9; IEEE
   Comp Soc eGovernment Special Tech Community
ZS 0
ZA 0
ZB 0
TC 1
Z8 0
ZR 0
Z9 2
U1 0
U2 4
SN 2573-2005
EI 2573-1998
BN 978-1-6654-2513-1
DA 2022-11-30
UT WOS:000847020200026
ER

PT J
AU Macak, Martin
   Ge, Mouzhi
   Buhnova, Barbora
TI A Cross-Domain Comparative Study of Big Data Architectures
SO INTERNATIONAL JOURNAL OF COOPERATIVE INFORMATION SYSTEMS
VL 29
IS 4
AR 2030001
DI 10.1142/S0218843020300016
DT Review
PD DEC 2020
PY 2020
AB Nowadays, a variety of Big Data architectures are emerging to organize
   the Big Data life cycle. While some of these architectures are proposed
   for general usage, many of them are proposed in a specific application
   domain such as smart cities, transportation, healthcare, and
   agriculture. There is, however, a lack of understanding of how and why
   Big Data architectures vary in different domains and how the Big Data
   architecture strategy in one domain may possibly advance other domains.
   Therefore, this paper surveys and compares the Big Data architectures in
   different application domains. It also chooses a representative
   architecture of each researched application domain to indicate which Big
   Data architecture from a given domain the researchers and practitioners
   may possibly start from. Next, a pairwise cross-domain comparison among
   the Big Data architectures is presented to outline the similarities and
   differences between the domain-specific architectures. Finally, the
   paper provides a set of practical guidelines for Big Data researchers
   and practitioners to build and improve Big Data architectures based on
   the knowledge gathered in this study.
RI Macak, Martin/HOF-6383-2023; Buhnova, Barbora/J-3364-2013
OI Macak, Martin/0000-0001-9655-9228; Buhnova, Barbora/0000-0003-4205-101X
TC 2
ZR 0
Z8 0
ZB 1
ZA 0
ZS 0
Z9 2
U1 0
U2 59
SN 0218-8430
EI 1793-6365
DA 2021-01-18
UT WOS:000603594000001
ER

PT J
AU Schokker, D.
   Athanasiadis, I. N.
   Visser, B.
   Veerkamp, R. F.
   Kamphuis, C.
TI Storing, combining and analysing turkey experimental data in the Big
   Data era
SO ANIMAL
VL 14
IS 11
BP 2397
EP 2403
AR PII S175173112000155X
DI 10.1017/S175173112000155X
DT Article
PD NOV 2020
PY 2020
AB With the increasing availability of large amounts of data in the
   livestock domain, we face the challenge to store, combine and analyse
   these data efficiently. With this study, we explored the use of a data
   lake for storing and analysing data to improve scalability and
   interoperability. Data originated from a 2-day animal experiment in
   which the gait score of approximately 200 turkeys was determined through
   visual inspection by an expert. Additionally, inertial measurement units
   (IMUs), a 3D-video camera and a force plate (FP) were installed to
   explore the effectiveness of these sensors in automating the visual gait
   scoring. We deployed a data lake using the IMU and FP data of a single
   day of that animal experiment. This encompasses data from 84 turkeys for
   which we preprocessed by performing an 'extract, transform and load'
   (ETL-) procedure. To test scalability of the ETL-procedure, we simulated
   increasing volumes of the available data from this animal experiment and
   computed the 'wall time' (elapsed real time) for converting FP data into
   comma-separated files and storing these files. With a simulated data set
   of 30 000 turkeys, the wall time reduced from 1 h to less than 15 min,
   when 12 cores were used compared to 1 core. This demonstrated the
   ETL-procedure to be scalable. Subsequently, a machine learning (ML)
   pipeline was developed to test the potential of a data lake to
   automatically distinguish between two classses, that is, very bad gait
   scoresv. other scores. In conclusion, we have set up a dedicated
   customized data lake, loaded data and developed a prediction model via
   the creation of an ML pipeline. A data lake appears to be a useful tool
   to face the challenge of storing, combining and analysing increasing
   volumes of data of varying nature in an effective manner.
RI Athanasiadis, Ioannis/F-6301-2010; Schokker, Dirkjan/MSY-2715-2025; veerkamp, roel/D-9313-2012
OI Athanasiadis, Ioannis/0000-0003-2764-0078; Schokker,
   Dirkjan/0000-0001-6082-7227; 
ZR 0
Z8 0
TC 2
ZS 0
ZA 0
ZB 1
Z9 2
U1 0
U2 9
SN 1751-7311
EI 1751-732X
DA 2020-10-19
UT WOS:000574897100020
PM 32624081
ER

PT J
AU Boateng, Samuel
   Lee, Kwang Ryeol
   Deepika
   Cho, Haneol
   Lee, Kyu Hwan
   Kim, Chansoo
TI KIST-NOMAD - a Repository to Manage Large Amounts of Computational
   Materials Science Data
SO KOREAN JOURNAL OF METALS AND MATERIALS
VL 58
IS 10
BP 728
EP 739
DI 10.3365/KJMM.2020.58.10.728
DT Article
PD OCT 2020
PY 2020
AB We introduce the Korea Institute of Science and Technology-Novel
   Materials Discovery (KIST-NOMAD) platform, a materials data repository.
   We describe its functionality and novel features from an academic
   viewpoint. It is a data repository designed for computational material
   science, especially focusing on managing and sharing the results of
   molecular dynamics simulation results as well as quantum mechanical
   computations. It consists of three main components: a database, file
   storage, and web-based front end. The database hosts material
   properties, which are extracted from the computational results. The
   front end has a graphical user interface and an open application
   programming interface, which allow researchers to interact with the
   system more easily. KIST-NOMAD's panel displays the searched results on
   a well-organized and research-oriented web page. All the open access
   data and files are available for downloading in comma-separated value
   format as well as zipped archives. This automated extraction function
   was developed by utilizing database parsers and JSON scripts. KIST-NOMAD
   also has an efficient option to download simulation and computation
   results on a large-scale. All of the above functions are designed to
   satisfy academic and research demands, and make high-throughput
   screening available, while incorporating machine learning for
   computational material engineering. We finally stress that the
   repository platform is user-driven and user-friendly. It is clearly
   designed to follow the modern big-data architecture and re-use
   principles for scientific data, such as being findable, accessible, and
   interoperable.
RI Boateng, Samuel/AAX-5135-2021
OI Boateng, Samuel/0000-0002-0307-3286
Z8 0
ZR 0
ZB 0
ZA 0
ZS 0
TC 2
Z9 2
U1 0
U2 14
SN 1738-8228
DA 2020-11-04
UT WOS:000580592500009
ER

PT C
AU Haller, David
   Lenz, Richard
BE Cellier, P
   Driessens, K
TI Pharos: Query-Driven Schema Inference for the Semantic Web
SO MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, ECML PKDD 2019,
   PT II
SE Communications in Computer and Information Science
VL 1168
BP 112
EP 124
DI 10.1007/978-3-030-43887-6_10
DT Proceedings Paper
PD 2020
PY 2020
AB The practical advantage of a data lake depends on the semantic
   understanding of its data. This knowledge is usually not externalized,
   but present in the minds of the data analysts who have used a great deal
   of cognitive effort to understand the semantic relationships of the
   heterogeneous data sources. The SQL queries they have written contain
   this hidden knowledge and should therefore serve as the foundation for a
   self-learning system. This paper proposes a methodology for extracting
   knowledge fragments from SQL queries and representing them in an
   RDF-based knowledge graph. The feasibility of this approach is
   demonstrated by a prototype implementation and evaluated using example
   data. It is shown that a query-driven knowledge graph is an appropriate
   tool to approximate the semantics of the data contained in a data lake
   and to incrementally provide interactive feedback to data analysts to
   help them with the formulation of queries.
CT European Conference on Machine Learning and Principles and Practice of
   Knowledge Discovery in Databases (ECML PKDD)
CY SEP 16-20, 2019
CL Wurzburg, GERMANY
SP Bosch; Fraunhofer IAIS; Huawei; ASML; IBM Res; NEC; Kreditech; McKinsey
   & Co; KNIME; European Res Ctr Informat Syst; Odgers Berndtson; Springer;
   Vogel Stiftung; German Res Fdn
OI Haller, David/0000-0001-5287-7187; Lenz, Richard/0000-0003-1551-4824
ZR 0
TC 2
ZB 0
ZS 0
Z8 0
ZA 0
Z9 2
U1 0
U2 3
SN 1865-0929
EI 1865-0937
BN 978-3-030-43887-6; 978-3-030-43886-9
DA 2021-11-30
UT WOS:000718590300010
ER

PT C
AU Li, Qing
   Xu, Zhiyong
   Wei, Hailong
   Yu, Chao
   Wang, ShuangShuang
BE Debruyne, C
   Panetto, H
   Guedria, W
   Bollen, P
   Ciuciu, I
   Karabatis, G
   Meersman, R
TI General Big Data Architecture and Methodology: An Analysis Focused
   Framework
SO ON THE MOVE TO MEANINGFUL INTERNET SYSTEMS, OTM 2019
SE Lecture Notes in Computer Science
VL 11878
BP 33
EP 43
DI 10.1007/978-3-030-40907-4_4
DT Proceedings Paper
PD 2020
PY 2020
AB With the development of information technologies such as cloud
   computing, the Internet of Things, the mobile Internet, and wireless
   sensor networks, big data technologies are driving the transformation of
   information technology and business models. Based on big data
   technology, data-driven artificial intelligence technology represented
   by deep learning and reinforcement learning has also been rapidly
   developed and widely used. But big data technology is also facing a
   number of challenges. The solution of these problems requires the
   support of a general big data reference architecture and analytical
   methodology. Based on the General Architecture Framework (GAF) and the
   Federal Enterprise Architecture Framework 2.0 (FEAF 2.0), this paper
   proposes a general big data architecture focusing on big data analysis.
   Based on GAF and CRISP-DM (cross-industry standard process for data
   mining), the general methodology and structural approach of big data
   analysis are proposed.
CT OnTheMove (OTM) International Federated Conference
CY OCT 21-25, 2019
CL Rhodes, GREECE
SP IFAC
RI 魏, 海龙/GVS-8012-2022; zhiyong, xu/NXC-2963-2025
ZA 0
ZS 0
ZB 0
ZR 0
TC 2
Z8 0
Z9 2
U1 1
U2 10
SN 0302-9743
EI 1611-3349
BN 978-3-030-40907-4; 978-3-030-40906-7
DA 2021-07-31
UT WOS:000674847000004
ER

PT J
AU Lu, You
   Fu, Qiming
   Xi, Xuefeng
   Chen, Zhenping
TI Cloud data acquisition and processing model based on blockchain
SO JOURNAL OF INTELLIGENT & FUZZY SYSTEMS
VL 39
IS 4
BP 5027
EP 5036
DI 10.3233/JIFS-179988
DT Article
PD 2020
PY 2020
AB Data outsourcing has gradually become a mainstream solution, but once
   data is outsourced, data owners will without the control of the data
   hardware, there is a possibility that the integrity of the data will be
   destroyed objectively. Many current studies have achieved low network
   overhead cloud data set verification by designing algorithmic structures
   (e.g., hashing, Merkel verification trees); however, cloud service
   providers may not recognize the incompleteness of cloud data to avoid
   liability or business factors fact. There is a need to build a secure,
   reliable, non-tamperable, and non-forgeable verification system for
   accountability. Blockchain is a chain-like data structure constructed by
   using data signatures, timestamps, hash functions, and proof-of-work
   mechanisms. Using blockchain technology to build an integrity
   verification system can achieve fault accountability. Blockchain is a
   chain-like data structure constructed by using data signatures,
   timestamps, hash functions, and proof-of-work mechanisms. Using
   blockchain technology to build an integrity verification system can
   achieve fault accountability. This paper uses the Hadoop framework to
   implement data collection and storage of the HBase system based on big
   data architecture. In summary, based on the research of blockchain cloud
   data collection and storage technology, based on the existing big data
   storage middleware, a large flow, high concurrency and high availability
   data collection and processing system has been realized.
RI Fu, Qiming/ITV-9722-2023
ZB 0
ZA 0
ZR 0
ZS 0
TC 2
Z8 0
Z9 2
U1 1
U2 21
SN 1064-1246
EI 1875-8967
DA 2020-11-09
UT WOS:000582322000021
ER

PT J
AU Mishra, Monika
   Goo, Kang， Min
   Woo, Jongwook
TI Leveraging Big Data for Spark Deep Learning to Predict Rating
SO Journal of Internet Computing and Services
S1 인터넷정보학회논문지
VL 21
IS 6
BP 33
EP 39
DI 10.7472/jksii.2020.21.6.33
DT research-article
PD 2020
PY 2020
AB The paper is to build recommendation systems leveraging Deep Learning
   and Big Data platform, Spark to predict item ratings of the Amazon
   e-commerce site. Recommendation system in e-commerce has become
   extremely popular in recent years and it is very important for both
   customers and sellers in daily life. It means providing the users with
   products and services they are interested in. Therecommendation systems
   need users’ previous shopping activities and digital footprints to make
   best recommendation purpose for next item shopping. We developed the
   recommendation models in Amazon AWS Cloud services to predict the users’
   ratings for the items with the massive data set of Amazon customer
   reviews. We also present Big Data architecture to afford the large scale
   data set for storing and computation. And, we adopted deep learning for
   machine learning community as it is known that it has higher accuracy
   for the massive data set. In the end, a comparative conclusion in terms
   of the accuracy as well as the performance is illustrated with the Deep
   Learning architecture with Spark ML and the traditional Big Data
   architecture, Spark ML alone.
ZR 0
ZA 0
ZB 0
ZS 0
TC 2
Z8 0
Z9 2
U1 0
U2 3
SN 1598-0170
DA 2021-06-22
UT KJD:ART002671170
ER

PT C
AU Vouros, George A.
   Glenis, Apostolos
   Doulkeridis, Christos
GP IEEE
TI The δ big data architecture for mobility analytics
SO 2020 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING SERVICE
   AND APPLICATIONS (BIGDATASERVICE 2020)
BP 25
EP 32
DI 10.1109/BigDataService49289.2020.00012
DT Proceedings Paper
PD 2020
PY 2020
AB Motivated by requirements in mobility analytics that require joint
   exploitation of streamed and voluminous archival data from diverse and
   heterogeneous data sources, this paper presents the delta (delta)
   architecture. Denoting "difference", delta emphasises on the different
   processing requirements from loosely-coupled components, which serve
   intertwined processing purposes, forming processing pipelines. The delta
   architecture, contributes principles for realizing systems, focusing on
   the requirements from components and pipelines, which are specified as
   constraints on performance indicators. The article presents a specific
   instantiation of the delta architecture to satisfy requirements for big
   data mobility analytics, exploiting real-world mobility data for
   performing realtime and batch analysis tasks.
CT 6th IEEE International Conference on Big Data Computing Service and
   Applications (IEEE BigDataService)
CY AUG 03-06, 2020
CL Oxford, ENGLAND
SP IEEE; IEEE Comp Soc
RI Vouros, George/AAF-3229-2020
OI Vouros, George/0000-0001-5451-622X
Z8 0
TC 2
ZR 0
ZB 0
ZA 0
ZS 0
Z9 2
U1 0
U2 2
BN 978-1-7281-7022-0
DA 2021-03-10
UT WOS:000621584100004
ER

PT J
AU Gorawski, Marcin
   Lorek, Michal
TI EFFICIENT STORAGE, RETRIEVAL AND ANALYSIS OF POKER HANDS: AN ADAPTIVE
   DATA FRAMEWORK
SO INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE
VL 27
IS 4
BP 713
EP 726
DI 10.1515/amcs-2017-0049
DT Article
PD DEC 2017
PY 2017
AB In online gambling, poker hands are one of the most popular and
   fundamental units of the game state and can be considered objects
   comprising all the events that pertain to the single hand played. In a
   situation where tens of millions of poker hands are produced daily and
   need to be stored and analysed quickly, the use of relational databases
   no longer provides high scalability and performance stability. The
   purpose of this paper is to present an efficient way of storing and
   retrieving poker hands in a big data environment. We propose a new,
   read-optimised storage model that offers significant data access
   improvements over traditional database systems as well as the existing
   Hadoop file formats such as ORC, RCFile or SequenceFile. Through
   index-oriented partition elimination, our file format allows reducing
   the number of file splits that needs to be accessed, and improves query
   response time up to three orders of magnitude in comparison with other
   approaches. In addition, our file format supports a range of new
   indexing structures to facilitate fast row retrieval at a split level.
   Both index types operate independently of the Hive execution context and
   allow other big data computational frameworks such as MapReduce or Spark
   to benefit from the optimized data access path to the hand information.
   Moreover, we present a detailed analysis of our storage model and its
   supporting index structures, and how they are organised in the overall
   data framework. We also describe in detail how predicate based
   expression trees are used to build effective file-level execution plans.
   Our experimental tests conducted on a production cluster, holding nearly
   40 billion hands which span over 4000 partitions, show that multi-way
   partition pruning outperforms other existing file formats, resulting in
   faster query execution times and better cluster utilisation.
ZS 0
ZR 0
TC 2
ZA 0
Z8 0
ZB 0
Z9 2
U1 1
U2 11
SN 1641-876X
EI 2083-8492
DA 2018-01-23
UT WOS:000419814400004
ER

PT J
AU Horia, Zaharia Mihai
TI A Multiagent Approach to Database Migration for Big Data Systems
SO NEW MATHEMATICS AND NATURAL COMPUTATION
VL 13
IS 2
SI SI
BP 159
EP 180
DI 10.1142/S1793005717400051
DT Article
PD JUL 2017
PY 2017
AB Presented in this paper is a possible solution for speeding up the
   integration of various data in the big data mainstream. The data
   enrichment and convergence of all possible sources is still at the
   beginning. As a result, existing techniques must be retooled in order to
   increase the integration of already existing databases or of the ones
   specific to Internet of Things in order to use the advantages of the big
   data to fulfill the final goal of web of data creation. In this paper,
   semantic web-specific solutions are used to design a system based on
   intelligent agents. It tries to solve some problems specific to
   automation of the database migration system with the final goal of
   creating a common ontology over various data repositories or producers
   in order to integrate them into systems based on big data architecture.
RI Zaharia, MIhai/ABC-4088-2020
ZR 0
ZB 0
ZS 0
TC 2
Z8 0
ZA 0
Z9 2
U1 0
U2 2
SN 1793-0057
EI 1793-7027
DA 2017-07-18
UT WOS:000404896600006
ER

PT C
AU Ahmad, Faisal
   Sarkar, Anirban
   Debnath, Narayan C.
BE Nguyen, V
   Bao, Q
   Trung, DQ
TI QoS Lake: Challenges, Design and Technologies
SO 2017 INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN SIGNAL PROCESSING,
   TELECOMMUNICATIONS & COMPUTING (SIGTELCOM)
BP 65
EP 70
DT Proceedings Paper
PD 2017
PY 2017
AB QoS evaluation based on their historical data not only helps in getting
   more accurate QoS, but also helps in making future QoS prediction,
   recommendation and knowledge discovery. [1] designed a generic QaaS
   (Quality as a service) model in the same line as PaaS and SaaS, where
   users can provide QoS attributes as inputs and the model returns
   services satisfying the user's QoS. It uses historical data to evaluate
   accurate QoS. Storing and evaluating QoS based on historical data and
   managing QoS for all services on the internet is challenging. This paper
   proposed a QoS lake in the same line of Data Lake for implementing QaaS
   model using big data technologies like Hadoop, Spark, and Yarn etc. The
   QoS Lake is a very large repository that stores all logs generated from
   services and its evaluated QoS data in its original context for all
   services on internet. The log data are processed to evaluate QoS either
   in batch or real time. QoS Lake is integrated with cutting-edge
   analytics, automation, orchestration and machine intelligence tools and
   languages which are used for future prediction, recommendation and
   knowledge discovery. QoS Lake has four loosely coupled layers namely;
   Ingestion layer, data layer, Analysis layer and Visualization layer. The
   challenges and advantages of the data lake are also discussed. The paper
   also presented the technologies available today to realize each layer
   and functionalities of the QoS Lake.
CT International Conference on Recent Advances in Signal Processing,
   Telecommunications and Computing (SigTelCom)
CY JAN 09-11, 2017
CL Duy Tan Univ, Da Nang, VIETNAM
HO Duy Tan Univ
SP IEEE; IEEE Vietnam Sect; Newton Fund
RI Ahmad, Faisal/AAA-1758-2019; Sarkar, Anirban/GMX-1785-2022
Z8 0
ZS 0
ZA 0
TC 2
ZR 0
ZB 0
Z9 2
U1 0
U2 6
BN 978-1-5090-2291-5
DA 2017-06-20
UT WOS:000402551500012
ER

PT C
AU Chitu, Claudia F.
   Stamatescu, Grigore
   Sgarciu, Valentin
GP Assoc Comp Machinery
TI Scalable Architectures for Stream Analytics and Data Predictions
   Dedicated to Smart Spaces
SO BUILDSYS'17: PROCEEDINGS OF THE 4TH ACM INTERNATIONAL CONFERENCE ON
   SYSTEMS FOR ENERGY-EFFICIENT BUILT ENVIRONMENTS
DI 10.1145/3137133.3141447
DT Proceedings Paper
PD 2017
PY 2017
AB Sensing and management of building systems became very exploited over
   the last decades, as well as the applications that enhance occupant
   comfort and energy efficiency in Smart Spaces. These are defined as a
   sensing and computing infrastructure employed to perceive and respond to
   human activity. Focusing on a real use case, we present a Big Data
   architecture in the cloud to enable the value extraction from the IoT to
   near-real time control for HVAC comfort in Smart Buildings. This elastic
   architecture enables a huge improvement over the off-line platform for
   data analytics and presents a novel integration of cloud services
   dedicated to Smart Spaces.
CT 4th ACM International Conference on Systems for Energy-Efficient Built
   Environments (BuildSys)
CY NOV 08-09, 2017
CL Delft, NETHERLANDS
SP Assoc Comp Machinery; ACM SIGCOMM; ACM SIGMOBILE; ACM SIGARCH; ACM
   SIGBED; ACM SIGMETRICS; ACM SIGOPS
RI Stamatescu, Grigore/E-7538-2012; Sgarciu, Valentin/F-2117-2013
OI Stamatescu, Grigore/0000-0002-9647-6817; 
ZA 0
TC 2
ZB 0
ZS 0
Z8 0
ZR 0
Z9 2
U1 0
U2 6
BN 978-1-4503-5476-9
DA 2017-01-01
UT WOS:000463798000046
ER

PT C
AU Kumaresan, Aravind
   Liberona, Dario
   Gnanamurthy, R. K.
BE Uden, L
   Lu, W
   Ting, IH
TI A Case Study on API-Centric Big Data Architecture
SO KNOWLEDGE MANAGEMENT IN ORGANIZATIONS (KMO 2017)
SE Communications in Computer and Information Science
VL 731
BP 459
EP 469
DI 10.1007/978-3-319-62698-7_38
DT Proceedings Paper
PD 2017
PY 2017
AB The digital transformation trend is a significant key factor in driving
   innovations in today's world. As a result the leaders of organizations
   are constantly under pressure to introduce a cost effective, performance
   enhanced big data systems to stay competitive. Organizations must run
   mission critical systems in a stable and secure ways at the same time
   they have to respond at pace with an ever changing customer
   expectations. Thus, an ability to scale the business operations to meet
   the demands from web apps, mobile apps, operational systems and big data
   analytical tools etc., are crucial. Integration patterns are at the
   heart of connecting the various systems in organizations for decades but
   now the industry is focusing more on API-centric approach as an
   alternate to that. API-centric software enables the component to solve a
   well detailed problem. So, the other programs can be built upon the
   existing functionalities of an API. This case study demonstrates the
   various available approach options to develop an API-centric Big Data
   system. It also explains the opportunities and challenges in various
   approaches. Among the various approaches the case study mainly focuses
   on utilizing the power of API building software Apigee and Azure cloud
   technologies to build the API-centric big data trading system for an
   asset management firm based in London, UK.
CT 12th International Conference on Knowledge Management in Organizations
   (KMO) - Emerging Technology
CY AUG 14-21, 2017
CL Beijing Jiaotong Univ, Beijing, PEOPLES R CHINA
HO Beijing Jiaotong Univ
RI Kumaresan, Aravind/; GNANAMURTHY, RAMASAMY KANNAN/JDD-1162-2023
OI Kumaresan, Aravind/0009-0003-9705-8072; GNANAMURTHY, RAMASAMY
   KANNAN/0009-0001-3289-530X
ZS 0
ZR 0
Z8 0
ZB 0
TC 2
ZA 0
Z9 2
U1 0
U2 1
SN 1865-0929
EI 1865-0937
BN 978-3-319-62698-7; 978-3-319-62697-0
DA 2018-12-28
UT WOS:000451588800038
ER

PT C
AU Melgarejo Galvan, Alonso Raul
   Clavo Navarro, Katerine Rocio
BE LossioVentura, JA
   AlatristaSalas, H
TI Big Data Architecture for Predicting Churn Risk in Mobile Phone
   Companies
SO INFORMATION MANAGEMENT AND BIG DATA
SE Communications in Computer and Information Science
VL 656
BP 120
EP 132
DI 10.1007/978-3-319-55209-5_10
DT Proceedings Paper
PD 2017
PY 2017
AB Nowadays in Peru, mobile phone companies have been affected by the
   problem of mobile number portability because since July 2014 customers
   can change their mobile operator in just 24 h. Companies look for
   solutions through the analysis of historical data of their customers in
   order to generate predictive models and to identify which customers
   would leave the company. However, the current way how this prediction is
   performed is too slow. In this paper, we show a Big Data architecture
   which solves the problems of the "classic architecture" using data from
   social networks in order to predict which customers may go to the
   competition company, according to their opinions. Data processing is
   performed by Hadoop, which implements MapReduce and can process large
   amounts of data in parallel way. After doing the tests and seeing the
   results, we got a high percentage of accuracy (90.03% of success).
CT 3rd Annual International Symposium on Information Management and Big
   Data (SIMBig)
CY SEP 01-03, 2016
CL Cusco, PERU
SP Univ Andina Cusco; Univ Florida; Univ Pacifico; Pontificia Univ Catolica
   Peru; Lab Informatique Robotique & Micro Electronique Montpellier; Univ
   Montpellier
ZR 0
ZS 0
ZA 0
TC 2
ZB 0
Z8 0
Z9 2
U1 0
U2 8
SN 1865-0929
EI 1865-0937
BN 978-3-319-55209-5; 978-3-319-55208-8
DA 2018-03-08
UT WOS:000425804300010
ER

PT C
AU Revathy, P.
   Mukesh, Rajeswari
BE Niranjan, SK
   Kurian, MZ
   Siddappa, M
TI Analysis of Big Data Security Practices
SO PROCEEDINGS OF THE 2017 3RD INTERNATIONAL CONFERENCE ON APPLIED AND
   THEORETICAL COMPUTING AND COMMUNICATION TECHNOLOGY (ICATCCT)
BP 264
EP 267
DT Proceedings Paper
PD 2017
PY 2017
AB In modern world, huge amount of data is common across all businesses
   which aim to unlock new economy from these sources. Hadoop was developed
   to analyze large scale data repository in a parallel computing
   architecture. The main task in this process is to handle this "Big Data"
   by applying proper strategies. So, present industry is focusing on the
   methods in which this "Big Data" can be used for their business growth.
   There's no suspicion that the setup of Data Lake on hadoop can provide a
   new way of analytics and intuition analysis. Beyond experimentations and
   POCs, today Hadoop is considered more into production. As we are moving
   towards the stage where Hadoop is considered for real-time production
   scenarios and major chunk of the production data is normally sensitive,
   or subject to many control measures, it becomes high priority to
   consider the security aspects in hadoop before deciding on Hadoop
   installation for any enterprise. This paper evaluates various issues in
   Hadoop ecosystem and its popular distributions by top big data players
   in the market. It further intends to investigate and compare the current
   security features being provided in those big data distributions along
   with other open source big data security solutions to help in building
   secure big data environment.
CT 3rd International Conference on Applied and Theoretical Computing and
   Communication Technology (iCATccT)
CY DEC 21-23, 2017
CL Sri Siddhartha Inst Technol, Tumkur, INDIA
HO Sri Siddhartha Inst Technol
SP IEEE Bangalore Sect; IEEE; IEEE Consumer Elect Soc, Malaysia Chapter
RI HITS, Hindustan Institute of Technology and Science/; Mukesh, Rajeswari/HNQ-5487-2023
OI HITS, Hindustan Institute of Technology and Science/0009-0004-3570-2675;
   
ZR 0
ZA 0
ZS 0
TC 2
ZB 0
Z8 0
Z9 2
U1 0
U2 0
BN 978-1-5386-1144-9
DA 2018-07-18
UT WOS:000437178100049
ER

PT C
AU Li, Jinfeng
   Shao, Bing
   Xu, Jian
   Li, Hongliang
   Wang, Qinghua
GP IEEE
TI A Big Data Based Product Ranking Solution
SO PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON SERVICE OPERATIONS AND
   LOGISTICS, AND INFORMATICS (SOLI)
BP 190
EP 194
DT Proceedings Paper
PD 2016
PY 2016
AB Users' online behavior, generated from endpoints of e-commerce website
   and app, is regarded as big data which can create large business value
   by mining them to acquire insights of users' preference, inclination and
   purpose. A good ranking result of product search and product assortment
   in online category classification can lift user's click rate, increase
   purchasing conversion rate and improve customer online experience. In
   this paper, we will introduce an online product based learning to rank
   model to intelligently learn product ranking. A big data architecture
   will also be introduced to implement this learning to rank model which
   analyzes huge amount of users' online behavior.
CT IEEE International Conference on Service Operations and Logistics, and
   Informatics (SOLI)
CY JUL 10-12, 2016
CL Beijing, PEOPLES R CHINA
SP IEEE; IEEE Intelligent Transportat Syst Soc; Informs Serv Sci
RI li, jinfeng/GVS-5425-2022
Z8 0
ZS 0
ZR 0
ZA 0
ZB 0
TC 1
Z9 2
U1 0
U2 0
BN 978-1-5090-2927-3
DA 2017-01-04
UT WOS:000389544400034
ER

PT C
AU Lopez Pena, Miguel Angel
   Area Rua, Carlos
   Segovia Lozoya, Sergio
GP IEEE
TI A "Fast Data" Architecture: Dashboard for Anomalous Traffic Analysis in
   Data Networks
SO 2016 ELEVENTH INTERNATIONAL CONFERENCE ON DIGITAL INFORMATION MANAGEMENT
   (ICDIM 2016)
BP 37
EP 42
DT Proceedings Paper
PD 2016
PY 2016
AB Fast Data is a new Big Data computing paradigm that ensures requirements
   such as Real-Time processing of continuous data stream, storage at high
   rates and low latency with no data losses. In this work we propose a
   "Fast Data" architecture for a specific kind of software application in
   which input data arrive very fast and the results for each processed
   data have to match such input rates. We applied this architecture to
   build a Dashboard for Anomalous Traffic Analysis in Data Networks. In
   order to fulfil the requirements of Real-Time processing and no data
   losses, we carry out a design that consists of a pattern of dynamic tree
   of process pipelines, where the number of branches increases
   proportionally to the input data rate. Two different approaches have
   been followed to implement this design pattern: one based in a
   well-known set of products from the Big Data ecosystem; and the other
   built with Kafka, Zookeeper and a set of components designed and
   implemented by us. These two implementations have been compared in terms
   of velocity and scalability performance. As a result, the implementation
   built with our own components is significantly faster and scalable than
   the traditional one. The good results obtained by using both the design
   pattern of dynamic tree of process pipelines and our implementation make
   them very suitable for its use in other scenarios and applications such
   as smart cities, environment monitoring, industry 4.0, distributed
   control systems, etc.
CT 11th International Conference on Digital Information Management (ICDIM)
CY SEP 19-21, 2016
CL Porto, PORTUGAL
SP IEEE
RI Lopez-Peña, Miguel Angel/AGY-9105-2022
OI Lopez-Peña, Miguel Angel/0000-0002-7493-0994
ZA 0
ZR 0
ZS 0
ZB 0
Z8 0
TC 2
Z9 2
U1 0
U2 1
BN 978-1-5090-2641-8
DA 2017-05-03
UT WOS:000398535200007
ER

PT C
AU Lakshmi, J. V. N.
   Sheshasaayee, Ananthi
GP IEEE
TI Machine Learning approaches on Map Reduce for Big Data Analytics
SO 2015 INTERNATIONAL CONFERENCE ON GREEN COMPUTING AND INTERNET OF THINGS
   (ICGCIOT)
BP 480
EP 484
DT Proceedings Paper
PD 2015
PY 2015
AB To analyze enormous datasets, collection of algorithms, associated
   systems and perform necessary processing on massive data structures
   there is obligation for a novel trend, which is framed by Big Data.
   Architecture of Big Data varies across compound machines and clusters
   with unique purpose sub systems. The data produced from several sources
   requires analysis and organization with meager amounts of time. To
   potentially speed up the processing, a unified way of machine learning
   is applied on MapReduce frame work. A broadly applicable programming
   model MapReduce is applied on different learning algorithms belonging to
   machine learning family for all business decisions. By using ML
   algorithms with Hadoop for better storage distribution will improve the
   time and processing speed. This paper presents parallel implementation
   of various machine learning algorithms implemented on top of MapReduce
   model for time and processing efficiency.
CT International Conference on Green Computing and Internet of Things
   (ICGCIoT)
CY OCT 08-10, 2015
CL GALGOTIAS Educ Inst, Greater Noida, INDIA
HO GALGOTIAS Educ Inst
SP IEEE Advancing Technol Human; IEEE UP Sect India; IEEE Comp Soc; IEEE
   Syst Man & Cybernet Soc; Malaysia Chapter; Inst Neural Network Soc
RI SHESHASAAYEE, ANANTHI/AAD-9954-2020; LAKSHMI, JUPUDI/D-1758-2017
OI SHESHASAAYEE, ANANTHI/0000-0001-6234-2485; LAKSHMI,
   JUPUDI/0000-0001-9230-2521
TC 1
ZR 0
ZB 0
ZS 0
Z8 0
ZA 0
Z9 2
U1 0
U2 4
BN 978-1-4673-7910-6
DA 2016-09-13
UT WOS:000380517100096
ER

PT C
AU Liu, Rong
   Li, Qicheng
   Li, Feng
   Mei, Lijun
   Lee, Juhnyoung
GP IEEE
TI Big Data Architecture for IT Incident Management
SO 2014 IEEE INTERNATIONAL CONFERENCE ON SERVICE OPERATIONS AND LOGISTICS,
   AND INFORMATICS (SOLI)
BP 424
EP 429
DT Proceedings Paper
PD 2014
PY 2014
AB IT incident management aims to restore normal service quality and
   availability of IT systems from interruptions. IT incidents often have
   complicated causes aggregated from an IT environment composed of
   thousands of interdependent components. Incident diagnosis then requires
   collecting and analyzing a large scale of data regarding these
   components, often, in real time to find suspect causes. It is extremely
   difficult to fulfill this requirement using traditional techniques. In
   this paper, we propose a new analysis architecture using Big Data
   techniques. This architecture leverages stream computing and MapReduce
   techniques to analyze data from various data sources, uses NoSQL
   databases to store incident-related documents and their relationships,
   and further utilizes other analytical techniques to examine the
   documents for root causes and failure prediction. We demonstrate this
   approach using a real-world example and present evaluation results from
   a recent pilot study.
CT IEEE International Conference on Service Operations and Logistics, and
   Informatics (SOLI)
CY OCT 08-10, 2014
CL Qingdao, PEOPLES R CHINA
SP IEEE
RI Li, Qicheng/AAH-2019-2020
ZR 0
ZA 0
ZS 0
TC 2
ZB 0
Z8 0
Z9 2
U1 0
U2 2
BN 978-1-4799-6058-3
DA 2015-07-05
UT WOS:000356136700080
ER

PT J
AU Ahmad, Abdelrahim
   Li, Peizheng
   Piechocki, Robert
   Inacio, Rui
TI Anomaly detection in offshore open radio access network using long
   short-term memory models on a novel artificial intelligence-driven
   cloud-native data platform
SO ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
VL 161
AR 112274
DI 10.1016/j.engappai.2025.112274
EA SEP 2025
PN C
DT Article
PD DEC 12 2025
PY 2025
AB The Radio Access Network (RAN) is a critical component of modern
   telecommunications infrastructure, currently evolving towards
   disaggregated and open architectures. These advancements are pivotal for
   integrating intelligent, data-driven applications aimed at enhancing
   network reliability and operational autonomy through the introduction of
   cognitive capabilities, as exemplified by the emerging Open Radio Access
   Network (O-RAN) standards. Despite its potential, the nascent nature of
   O-RAN technology presents challenges, primarily due to the absence of
   mature operational standards. This complicates the management of data
   and intelligent applications, particularly when integrating with
   traditional network management and operational support systems.
   Divergent vendor-specific design approaches further hinder migration and
   limit solution reusability. These challenges are compounded by a skills
   gap in telecommunications business-oriented engineering, which remains a
   key barrier to effective O-RAN deployment and intelligent application
   development. To address these challenges, Boldyn Networks developed a
   novel cloud-native data analytics platform, specifically designed to
   support scalable Artificial Intelligence (AI) integration within O-RAN
   deployments. This platform underwent rigorous testing in real-world
   scenarios, and applied advanced AI techniques to improve operational
   efficiency and customer experience. Implementation involved adopting
   Development Operations (DevOps) practices, leveraging data lakehouse
   architectures tailored for AI applications, and employing sophisticated
   data engineering strategies. The platform successfully addresses
   connectivity challenges inherent in real-world offshore wind farm
   deployments using Long Short-Term Memory (LSTM) models for anomaly
   detection in network connectivity. After integrating the LSTM models
   into the network control, more than 90 percent of connectivity issues
   were reduced in runtime. This marks a step toward autonomous,
   self-organizing, and self-healing networks.
RI Piechocki, Robert/; Li, Peizheng/OVZ-6733-2025; Ahmad, Abdelrahim/AAD-3780-2021
OI Piechocki, Robert/0000-0002-4879-1206; Ahmad,
   Abdelrahim/0000-0002-6980-5267
ZS 0
ZR 0
TC 1
Z8 0
ZA 0
ZB 0
Z9 1
U1 5
U2 5
SN 0952-1976
EI 1873-6769
DA 2025-10-02
UT WOS:001576783000006
ER

PT J
AU Tonnarelli, Marco
   Kumara, Indika
   Driessen, Stefan
   Tamburri, Damian Andrew
   van den Heuvel, Willem-Jan
   Oor, Patrick
TI Data catalog tools: A systematic multivocal literature review☆
SO JOURNAL OF SYSTEMS AND SOFTWARE
VL 230
AR 112584
DI 10.1016/j.jss.2025.112584
EA AUG 2025
DT Review
PD DEC 2025
PY 2025
AB A data catalog enables an organization to maintain an inventory of its
   data assets by collecting and managing the relevant metadata. We
   conducted a systematic multi-vocal literature review on data catalogs to
   understand their features and usage. We systematically selected and
   analyzed 86 literature sources and 39 catalog tools. We first utilized
   the findings from the literature to develop a classification framework
   comprising 24 finegrained and five high-level features, along with three
   maturity levels. Next, we analyzed 39 tools based on the classification
   framework. Organizations typically include a data catalog as a component
   in their big data platforms and use it to support the various phases of
   the metadata management lifecycle. Hence, we also mapped the catalog
   features to the requirements of metadata-driven big data architectures,
   namely data mesh, data lake, and data lakehouse. Moreover, the mappings
   of the features to the phases in a metadata management lifecycle were
   developed. Our findings shall aid organizations in making informed
   decisions when choosing data catalog tools and help researchers identify
   the critical research issues in data cataloging and metadata management.
RI iessen, Stefan/; Weerasingha Dewage, Indika Priyantha Kumara/; Tamburri, Damian Anew/AAJ-2507-2021; Tonnarelli, Marco/
OI iessen, Stefan/0000-0002-0523-1436; Weerasingha Dewage, Indika Priyantha
   Kumara/0000-0003-4355-0494; Tamburri, Damian Anew/0000-0003-1230-8961;
   Tonnarelli, Marco/0009-0004-7329-6663
Z8 0
TC 1
ZA 0
ZB 0
ZS 0
ZR 0
Z9 1
U1 9
U2 9
SN 0164-1212
EI 1873-1228
DA 2025-08-26
UT WOS:001553590200001
ER

PT J
AU Tolonen, Hanna M.
   Kaukovuori, Jouni
   Airaksinen, Marja
   Holmstrom, Anna-Riia
TI Are electronic health record big data ready for secondary use in
   research? Exploring potential limitations with opioids as a case study
SO HEALTH INFORMATICS JOURNAL
VL 31
IS 3
AR 14604582251363501
DI 10.1177/14604582251363501
DT Article
PD JUL 2025
PY 2025
AB Healthcare big data has raised expectations for secondary use in
   research and information-based management. This case study explores
   limitations of using electronic health record (EHR) data from a hospital
   data lake by deriving indicators on opioid prescribing. A multi-staged
   method to calculate indicators of rational opioid use was developed
   covering both inpatient orders and outpatient prescriptions. The process
   included data selection, editing, organization, and validation. Visual
   Basic was employed to calculate indicators and to semi-quantify data
   limitations. Data (2015-2019) covered 3.3 million patients with 179,853
   opioid and 22,415 benzodiazepine orders. Data quality issues, including
   unstructured, irregular, and invalid entries, limited analysis to
   indicators on opioid use and contraindications. In conclusion, secondary
   use should be considered in EHR system development. Data should be
   recorded in structured and unambiguous format, and methods for data
   quality measurement should be developed to ensure high-quality data is
   readily available in data lakes.
OI Holtröm, Anna-Riia/0000-0002-3908-5430; Airaksinen,
   Marja/0000-0002-6077-5671; Tolonen, Hanna/0000-0002-6824-7566
ZS 0
ZA 0
ZR 0
ZB 0
Z8 0
TC 1
Z9 1
U1 1
U2 1
SN 1460-4582
EI 1741-2811
DA 2025-08-05
UT WOS:001538835600001
PM 40721986
ER

PT J
AU Boffa, Antonio
   Di Cosmo, Roberto
   Ferragina, Paolo
   Guerra, Andrea
   Manzini, Giovanni
   Vinciguerra, Giorgio
   Zacchiroli, Stefano
TI On the compressibility of large-scale source code datasets
SO JOURNAL OF SYSTEMS AND SOFTWARE
VL 227
AR 112429
DI 10.1016/j.jss.2025.112429
EA APR 2025
DT Article
PD SEP 2025
PY 2025
AB Storing ultra-large amounts of unstructured data (often called objects
   or blobs) is a fundamental task for several object-based storage
   engines, data warehouses, data-lake systems, and key-value stores. These
   systems cannot currently leverage similarities between objects, which
   could be vital in improving their space and time performance. An
   important use case in which we can expect the objects to be highly
   similar is the storage of large-scale versioned source code datasets,
   such as the Software Heritage Archive (Di Cosmo and Zacchiroli, 2017).
   This use case is particularly interesting given the extraordinary size
   (1.5 PiB), the variegated nature, and the high repetitiveness of the
   at-issue corpus. In this paper we discuss and experiment with
   content-and context-based compression techniques for source-code
   collections that tailor known and novel tools to this setting in
   combination with state-of-the-art general-purpose compressors and the
   information coming from the Software Heritage Graph. We experiment with
   our compressors over a random sample of the entire corpus, and four
   large samples of source code files written in different popular
   languages: C/C++, Java, JavaScript, and Python. We also consider two
   scenarios of usage for our compressors, called Backup and File-Access
   scenario, where the latter adds to the former the support for single
   file retrieval. As a net result, our experiments show (i) how much
   "compressible" each language is, (ii) which content-or context-based
   techniques compress better and are faster to (de)compress by possibly
   supporting individual file access, and (iii) the ultimate compressed
   size that, according to our estimate, our best solution could achieve in
   storing all the source code written in these languages and available in
   the Software Heritage Archive: namely, in 3 TiB (down from their
   original 78 TiB total size, with an average compression ratio of 4%).
RI Boffa, Antonio/; Vinciguerra, Giorgio/; Di Cosmo, Roberto/; Zacchiroli, Stefano/; Ferragina, Paolo/AAZ-2580-2020; MANZINI, Giovanni/ABF-1910-2020
OI Boffa, Antonio/0000-0002-8178-135X; Vinciguerra,
   Giorgio/0000-0003-0328-7791; Di Cosmo, Roberto/0000-0002-7493-5349;
   Zacchiroli, Stefano/0000-0002-4576-136X; Ferragina,
   Paolo/0000-0003-1353-360X; 
Z8 0
ZA 0
ZR 0
TC 1
ZB 0
ZS 0
Z9 1
U1 0
U2 0
SN 0164-1212
EI 1873-1228
DA 2025-05-04
UT WOS:001475176000001
ER

PT J
AU Sheng, Ming
   Wang, Shuliang
   Zhang, Yong
   Wang, Kaige
   Wang, Jingyi
   Luo, Yi
   Hao, Rui
TI MQRLD: A multimodal data retrieval platform with query-aware feature
   representation and learned index based on data lake
SO INFORMATION PROCESSING & MANAGEMENT
VL 62
IS 4
AR 104101
DI 10.1016/j.ipm.2025.104101
EA FEB 2025
DT Article
PD JUL 2025
PY 2025
AB Multimodal data has become a crucial element in the realm of big data
   analytics, driving advancements in data exploration, data mining, and
   empowering artificial intelligence applications. To support high-quality
   retrieval for these cutting-edge applications, a robust multimodal data
   retrieval platform should meet the challenges of transparent data
   storage, rich hybrid queries, effective feature representation, and high
   query efficiency. However, among the existing platforms, traditional
   schema-on-write systems, multi-model databases, vector databases, and
   data lakes, which are the primary options for multimodal data retrieval,
   make it difficult to fulfill these challenges simultaneously. Therefore,
   there is an urgent need to develop a more versatile multimodal data
   retrieval platform to address these issues. In this paper, we introduce
   a Multimodal Data Retrieval Platform with Query-aware Feature
   Representation and Learned Index based on Data Lake (MQRLD). It
   leverages the transparent storage capabilities of data lakes, integrates
   the multimodal open API to provide a unified interface that supports
   rich hybrid queries, introduces a query-aware multimodal data feature
   representation strategy to obtain effective features, and offers
   high-dimensional learned indexes to optimize data query. We conduct a
   comparative analysis of the query performance of MQRLD against other
   methods for rich hybrid queries. Our results underscore the superior
   efficiency of MQRLD in handling multimodal data retrieval tasks,
   demonstrating its potential to significantly improve retrieval
   performance in complex environments. We also clarify some potential
   concerns in the discussion.
RI Wang, Shuliang/A-2626-2012; Zhang, Yong/; Wang, Kaige/KJM-6446-2024
OI Wang, Shuliang/0000-0001-5326-7209; Zhang, Yong/0000-0001-8803-2055; 
TC 1
Z8 0
ZA 0
ZB 0
ZR 0
ZS 0
Z9 1
U1 7
U2 26
SN 0306-4573
EI 1873-5371
DA 2025-03-01
UT WOS:001428601000001
ER

PT C
AU Castro, Joao P. C.
   Vasconcelos, Gabriel F. X.
   Vargas-Solar, Genoveva
   Aguiar, Cristina D.
BE Krogstie, J
   Rinderle-Ma, S
   Kappel, G
   Proper, HA
TI Building FAIR-Compliant Lakehouses with FLAMI
SO ADVANCED INFORMATION SYSTEMS ENGINEERING, CAISE 2025, PT II
SE Lecture Notes in Computer Science
VL 15702
BP 113
EP 129
DI 10.1007/978-3-031-94571-7_7
DT Proceedings Paper
PD 2025
PY 2025
AB This paper introduces FLAMI, a software reference architecture for
   building big data-sharing repositories that adhere to the FAIR
   (Findability, Accessibility, Interoperability, and Reusability)
   principles. Inspired by the data lakehouse concept, FLAMI can extract,
   load, and transform large volumes of data and metadata from
   heterogeneous data providers into a unified storage solution. It
   integrates with existing infrastructures of external repositories,
   allowing data stored outside the lakehouse to be fetched and processed
   within its infrastructure. FLAMI also introduces a set of guidelines for
   its implementation. We validate our proposal through a case study that
   instantiates FLAMI to the context of a real-world seismology dataset,
   exploiting analytical queries that encompass the needs of geoscientists.
   We also employ a framework to assess FLAMI's FAIR compliance, achieving
   100% in its conceptual version and over 73% in the seismology
   instantiation.
CT 37th International Conference on Advanced Information Systems
   Engineering-CAISE-Annual
CY JUN 16-17, 2025
CL Vienna, AUSTRIA
RI Solar, Genoveva/T-8621-2019; Castro, João Peo de Carvalho/; Aguiar, Cristina/D-9906-2011
OI Castro, João Peo de Carvalho/0000-0003-3566-0415; 
Z8 0
ZB 0
ZR 0
ZA 0
TC 1
ZS 0
Z9 1
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-94573-1; 978-3-031-94571-7
DA 2025-09-25
UT WOS:001553133100007
ER

PT J
AU Ataei, Pouya
TI Cybermycelium: a reference architecture for domain-driven distributed
   big data systems
SO FRONTIERS IN BIG DATA
VL 7
AR 1448481
DI 10.3389/fdata.2024.1448481
DT Article
PD NOV 5 2024
PY 2024
AB Introduction The ubiquity of digital devices, the infrastructure of
   today, and the ever-increasing proliferation of digital products have
   dawned a new era, the era of big data (BD). This era began when the
   volume, variety, and velocity of data overwhelmed traditional systems
   that used to analyze and store that data. This precipitated a new class
   of software systems, namely, BD systems. Whereas BD systems provide a
   competitive advantage to businesses, many have failed to harness the
   power of them. It has been estimated that only 20% of companies have
   successfully implemented a BD project. Methods This study aims to
   facilitate BD system development by introducing Cybermycelium, a
   domain-driven decentralized BD reference architecture (RA). The artifact
   was developed following the guidelines of empirically grounded RAs and
   evaluated through implementation in a real-world scenario using the
   Architecture Tradeoff Analysis Method (ATAM). Results The evaluation
   revealed that Cybermycelium successfully addressed key architectural
   qualities: performance (achieving <1,000 ms response times),
   availability (through event brokers and circuit breaking), and
   modifiability (enabling rapid service deployment and configuration). The
   prototype demonstrated effective handling of data processing,
   scalability challenges, and domain-specific requirements in a
   large-scale international company setting. Discussion The results
   highlight important architectural trade-offs between event backbone
   implementation and service mesh design. While the domain-driven
   distributed approach improved scalability and maintainability compared
   to traditional monolithic architectures, it requires significant
   technical expertise for implementation. This contribution advances the
   field by providing a validated reference architecture that addresses the
   challenges of adopting BD in modern enterprises.
ZS 0
ZR 0
ZA 0
ZB 0
TC 1
Z8 0
Z9 1
U1 0
U2 2
EI 2624-909X
DA 2024-11-25
UT WOS:001358035700001
PM 39564075
ER

PT J
AU Pingos, Michalis
   Christodoulou, Panayiotis
   Andreou, Andreas S.
TI Security and Ownership in User-Defined Data Meshes
SO ALGORITHMS
VL 17
IS 4
AR 169
DI 10.3390/a17040169
DT Article
PD APR 2024
PY 2024
AB Data meshes are an approach to data architecture and organization that
   treats data as a product and focuses on decentralizing data ownership
   and access. It has recently emerged as a field that presents quite a few
   challenges related to data ownership, governance, security, monitoring,
   and observability. To address these challenges, this paper introduces an
   innovative algorithmic framework leveraging data blueprints to enable
   the dynamic creation of data meshes and data products in response to
   user requests, ensuring that stakeholders have access to specific
   portions of the data mesh as needed. Ownership and governance concerns
   are addressed through a unique mechanism involving Blockchain and
   Non-Fungible Tokens (NFTs). This facilitates the secure and transparent
   transfer of data ownership, with the ability to mint time-based NFTs. By
   combining these advancements with the fundamental tenets of data meshes,
   this research offers a comprehensive solution to the challenges
   surrounding data ownership and governance. It empowers stakeholders to
   navigate the complexities of data management within a decentralized
   architecture, ensuring a secure, efficient, and user-centric approach to
   data utilization. The proposed framework is demonstrated using
   real-world data from a poultry meat production factory.
RI ANEOU, ANEAS/; Pingos, Michalis/OGO-6362-2025; Pingos, Michalis/
OI ANEOU, ANEAS/0000-0001-7104-2097; Pingos, Michalis/0000-0001-6293-6478
ZB 0
Z8 0
TC 1
ZA 0
ZS 0
ZR 0
Z9 1
U1 0
U2 3
EI 1999-4893
DA 2024-05-05
UT WOS:001210099800001
ER

PT J
AU Kulkarni, Apurva
   Ramanathan, Chandrashekar
   Venugopal, Vinu E.
TI Toward Sustainable Data Practices: Integrating Open Data With SDG-Based
   Data Lake Frameworks
SO IEEE TECHNOLOGY AND SOCIETY MAGAZINE
VL 43
IS 1
BP 62
EP 69
DI 10.1109/MTS.2024.3365591
DT Article
PD MAR 2024
PY 2024
OI Kulkarni, Apurva/0000-0002-9215-2049
ZS 0
ZR 0
Z8 0
TC 1
ZA 0
ZB 0
Z9 1
U1 3
U2 7
SN 0278-0097
EI 1937-416X
DA 2024-04-26
UT WOS:001201838200014
ER

PT C
AU Cerezo, Felipe
   Vela, Belen
BE Tekinerdogan, B
   Spalazzese, R
   Sozer, H
   Bonfanti, S
   Weyns, D
TI Experience of the Architectural Evolution of a Big Data System
SO SOFTWARE ARCHITECTURE: ECSA 2023 TRACKS, WORKSHOPS, AND DOCTORAL
   SYMPOSIUM, ECSA 2023, CASA 2023, AMP 2023, FAACS 2023, DEMESSA 2023,
   QUALIFIER 2023, TWINARCH 2023
SE Lecture Notes in Computer Science
VL 14590
BP 426
EP 437
DI 10.1007/978-3-031-66326-0_26
DT Proceedings Paper
PD 2024
PY 2024
AB This paper presents the evolution of a hybrid Big Data architecture over
   7 years to adapt to changes in user requirements and technological
   evolution. This architecture is developed and used by one of the main
   telco companies in our country. Currently, the result of the presented
   work is used in many projects providing key information for the
   operation of the company.
   We describe the initial architecture, its main shortcomings, and the
   current architecture as well as the challenges we met in the process.
   The main lessons learned are related to the need of modularity and
   flexibility in the architecture of Big Data systems.
CT 17th European Conference on Software Architecture (ECSA)
CY SEP 18-22, 2023
CL Istanbul, TURKEY
SP Springer
RI Vela Sánchez, Belén/H-1561-2015; Cerezo, Felipe/; Sanchez, Belen/H-1561-2015
OI Vela Sánchez, Belén/0000-0003-0604-7312; Cerezo,
   Felipe/0000-0002-0128-0783; 
ZB 0
ZR 0
ZS 0
TC 1
ZA 0
Z8 0
Z9 1
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-66325-3; 978-3-031-66326-0
DA 2024-10-09
UT WOS:001307877700026
ER

PT C
AU Dolhopolov, Anton
   Castelltort, Arnaud
   Laurent, Anne
BE Chbeir, R
   Benslimane, D
   Zervakis, M
   Manolopoulos, Y
   Ngyuen, NT
   Tekli, J
TI Trick or Treat: Centralized Data Lake Vs Decentralized Data Mesh
SO MANAGEMENT OF DIGITAL ECOSYSTEMS, MEDES 2023
SE Communications in Computer and Information Science
VL 2022
BP 303
EP 316
DI 10.1007/978-3-031-51643-6_22
DT Proceedings Paper
PD 2024
PY 2024
AB Over the course of the last few years, the augmentation of processed
   data and an increase in the need for fast product release cycles led to
   the emergence of bottlenecks in information and knowledge flows within
   large organizations. Recent research works attempted to resolve these
   issues from several perspectives, which span from the data platform
   architectures to the storage technologies. In this positional paper, we
   start by comparing the well-established methods of designing analytical
   data platforms and make a review of existing problems inherent to them,
   namely centralization of storage and ownership. It continues by
   analyzing the principles of a data mesh proposal and by providing an
   examination of unresolved challenges, such as metadata centralization.
   We further consider the business domain dependencies and platform
   architecture of our running example. The final section presents our
   vision for solving the identified metadata management issues in large
   enterprises via data decentralization and offers potential directions
   for future work.
CT 15th International Conference on Management of Digital EcoSystems
   (MEDES)
CY MAY 05-07, 2023
CL Heraklion, GREECE
OI Dolhopolov, Anton/0009-0005-6568-0427
ZR 0
Z8 0
ZA 0
ZS 0
TC 1
ZB 0
Z9 1
U1 1
U2 2
SN 1865-0929
EI 1865-0937
BN 978-3-031-51642-9; 978-3-031-51643-6
DA 2024-08-23
UT WOS:001260534100022
ER

PT J
AU Hoi, Lap Man
   Ke, Wei
   Im, Sio Kei
TI Manipulating Data Lakes Intelligently With Java Annotations
SO IEEE ACCESS
VL 12
BP 34903
EP 34917
DI 10.1109/ACCESS.2024.3372618
DT Article
PD 2024
PY 2024
AB Data lakes are typically large data repositories where enterprises store
   data in a variety of data formats. From the perspective of data storage,
   data can be categorized into structured, semi-structured, and
   unstructured data. On the one hand, due to the complexity of data forms
   and transformation procedures, many enterprises simply pour valuable
   data into data lakes without organizing and managing them effectively.
   This can create data silos (or data islands) or even data swamps, with
   the result that some data will be permanently invisible. Although data
   are integrated into a data lake, they are simply physically stored in
   the same environment and cannot be correlated with other data to
   leverage their precious value. On the other hand, processing data from a
   data lake into a desired format is always a difficult and tedious task
   that requires experienced programming skills, such as conversion from
   structured to semi-structured. In this article, a novel software
   framework called Java Annotation for Manipulating Data Lakes (JAMDL)
   that can manage heterogeneous data is proposed. This approach uses Java
   annotations to express the properties of data in metadata (data about
   data) so that the data can be converted into different formats and
   managed efficiently in a data lake. Furthermore, this article suggests
   using artificial intelligence (AI) translation models to generate Data
   Manipulation Language (DML) operations for data manipulation and uses AI
   recommendation models to improve the visibility of data when data
   precipitation occurs.
RI Hoi, Lap Man/; Ke, Wei/LOS-3255-2024; IM, SIO KEI/
OI Hoi, Lap Man/0000-0002-1074-9846; Ke, Wei/0000-0003-0952-0961; IM, SIO
   KEI/0000-0002-5599-4300
ZA 0
Z8 0
TC 1
ZB 0
ZS 0
ZR 0
Z9 1
U1 3
U2 14
SN 2169-3536
DA 2024-03-17
UT WOS:001178919400001
ER

PT J
AU Horvath, Kaleb
   Abid, Mohamed Riduan
   Merino, Thomas
   Zimmerman, Ryan
   Peker, Yesem
   Khan, Shamim
TI Cloud-Based Infrastructure and DevOps for Energy Fault Detection in
   Smart Buildings
SO COMPUTERS
VL 13
IS 1
AR 23
DI 10.3390/computers13010023
DT Article
PD JAN 2024
PY 2024
AB We have designed a real-world smart building energy fault detection
   (SBFD) system on a cloud-based Databricks workspace, a high-performance
   computing (HPC) environment for big-data-intensive applications powered
   by Apache Spark. By avoiding a Smart Building Diagnostics as a Service
   approach and keeping a tightly centralized design, the rapid development
   and deployment of the cloud-based SBFD system was achieved within one
   calendar year. Thanks to Databricks' built-in scheduling interface, a
   continuous pipeline of real-time ingestion, integration, cleaning, and
   analytics workflows capable of energy consumption prediction and anomaly
   detection was implemented and deployed in the cloud. The system
   currently provides fault detection in the form of predictions and
   anomaly detection for 96 buildings on an active military installation.
   The system's various jobs all converge within 14 min on average. It
   facilitates the seamless interaction between our workspace and a cloud
   data lake storage provided for secure and automated initial ingestion of
   raw data provided by a third party via the Secure File Transfer Protocol
   (SFTP) and BLOB (Binary Large Objects) file system secure protocol
   drivers. With a powerful Python binding to the Apache Spark distributed
   computing framework, PySpark, these actions were coded into
   collaborative notebooks and chained into the aforementioned pipeline.
   The pipeline was successfully managed and configured throughout the
   lifetime of the project and is continuing to meet our needs in
   deployment. In this paper, we outline the general architecture and how
   it differs from previous smart building diagnostics initiatives, present
   details surrounding the underlying technology stack of our data
   pipeline, and enumerate some of the necessary configuration steps
   required to maintain and develop this big data analytics application in
   the cloud.
OI Horvath, Kaleb/0009-0004-1161-4777; Kurt Peker,
   Yesem/0000-0002-2801-1962; Merino, Thomas/0009-0002-2731-3237
Z8 0
ZS 0
ZB 0
ZA 0
TC 1
ZR 0
Z9 1
U1 0
U2 7
SN 2073-431X
DA 2024-02-03
UT WOS:001149115200001
ER

PT C
AU Li, Jiabao
   Han, Wei
   Huang, Xiaohui
   Wang, Yuewei
   Long, Ao
   Duan, Rongrong
   Tian, Xiaohua
   Li, Yuqin
BE Song, X
   Feng, R
   Chen, Y
   Li, J
   Min, G
TI Design of Data Management System for Sustainable Development of Urban
   Agglomerations' Ecological Environment Based on Data Lake Architecture
SO WEB AND BIG DATA, PT II, APWEB-WAIM 2023
SE Lecture Notes in Computer Science
VL 14332
BP 16
EP 27
DI 10.1007/978-981-97-2390-4_2
DT Proceedings Paper
PD 2024
PY 2024
AB Research on the ecological environment of urban agglomerations plays a
   crucial role in enhancing environmental quality and ensuring sustainable
   development. In the research of sustainable development-oriented
   monitoring and assessing for ecological environments, the management and
   provision of data have gained significant prominence. However, the
   characteristics of vast data volumes, diverse data types, and
   inconsistent metadata descriptions hinder the comprehensive management
   of ecological environment data in urban agglomerations. This paper aims
   to investigate the data requirements that are necessary for sustainable
   development, with a particular focus on unified data management, online
   data product production and updates, and data service provision. To
   address these challenges, we have designed a metadata model that is
   capable of accommodating various types of datasets to facilitate their
   logical integration. Leveraging the data lake architecture, we have
   achieved semantic-level data governance driven by relational
   associations and proposed a data management solution for heterogeneous
   datasets for the unified management of terabyte-scale datasets. In
   addition, we have conducted the architectural design for the data
   management system. The prototype system is also developed to offer
   comprehensive data services, data product production, and other
   functionalities such as data visualization and analysis. This study
   provides extensive data services for monitoring and evaluating
   activities associated with the Sustainable Development Goals (SDGs) of
   SDG6, SDG11, and SDG15, while also supporting various application
   demonstrations and effectively facilitating the sustainable development
   of urban ecosystems.
CT 7th Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM)
   International Joint Conference on Web and Big Data
CY OCT 06-08, 2023
CL Wuhan, PEOPLES R CHINA
RI Li, Jiabao/; Wang, Yuewei/HZL-0800-2023; Han, Wei/KQU-8833-2024; huang, xiaohui/KRP-2903-2024
OI Li, Jiabao/0009-0004-9425-8981; Han, Wei/0000-0003-3882-1616; 
ZR 0
ZB 0
ZS 0
TC 1
Z8 0
ZA 0
Z9 1
U1 1
U2 5
SN 0302-9743
EI 1611-3349
BN 978-981-97-2389-8; 978-981-97-2390-4
DA 2024-09-15
UT WOS:001285554600002
ER

PT C
AU Liu, Yanmei
   Ma, Pengwei
   Tian, Jiafeng
BE Hu, J
   Min, G
   Wang, G
   Georgalas, N
TI Research on Technology and Industry Situation of Lakehouse
SO 2023 IEEE 22ND INTERNATIONAL CONFERENCE ON TRUST, SECURITY AND PRIVACY
   IN COMPUTING AND COMMUNICATIONS, TRUSTCOM, BIGDATASE, CSE, EUC, ISCI
   2023
SE IEEE International Conference on Trust Security and Privacy in Computing
   and Communications
BP 2198
EP 2203
DI 10.1109/TrustCom60117.2023.00308
DT Proceedings Paper
PD 2024
PY 2024
AB The concept of "Lakehouse" was proposed by Databricks in 2020. Since "
   Lakehouse " was first written into Gartner' s Hype Cycle for Data
   Management in 2021, as a new technology, "Lakehouse" has received
   unprecedented attention from the enterprises who need digital
   transformation. More enterprises believe lakehouse is an important
   infrastructure for digital transformation. Currently, lakehouse is still
   in its early stage of development. It is not merely a technical research
   endeavor, but rather a gradual integration of technologies, representing
   a transitional phase in the evolution of heterogeneous data platform
   towards integration. This paper focuses on the lakehouse technology,
   sorts out the development history of the data platform and the practice
   path of lakehouse technology. It also lists main manufactures and
   products of lakehouse and provides the judgments for the future
   development of lakehouse.
CT IEEE 22nd International Conference on Trust, Security and Privacy in
   Computing and Communications (TrustCom) / BigDataSE Conference / CSE
   Conference / EUC Conference / ISCI Conference
CY NOV 01-03, 2023
CL Exeter, ENGLAND
SP IEEE; IEEE Comp Soc; IEEE Tech Community Scalable Comp
TC 1
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
Z9 1
U1 0
U2 0
SN 2324-898X
BN 979-8-3503-8199-3; 979-8-3503-8200-6
DA 2025-11-20
UT WOS:001239879400284
ER

PT C
AU Nafil, Khalid
   Hennane, Oussama
   Imzagnan, Ilyas
   Lamkhanter, Younes
   Rkik, Fatima Zahra
   Kobbane, Abdellatif
   El Koutbi, Mohammed
BE Arai, K
TI Detection of Organic Tomato Diseases and Monitoring of Climate-Soil
   Through a Combination of IoT, Big Data, and Machine Learning
SO INTELLIGENT SYSTEMS AND APPLICATIONS, VOL 2, INTELLISYS 2024
SE Lecture Notes in Networks and Systems
VL 1066
BP 575
EP 597
DI 10.1007/978-3-031-66428-1_36
DT Proceedings Paper
PD 2024
PY 2024
AB The proposed system integrates various services into a common platform
   for digital agriculture, linking various IoT sensor nodes distributed in
   the field and connected via LoRaWAN technology to collect soil and
   climate data that will be processed using a kappa Big Data architecture
   to display the data collected by the sensors in real-time and provide
   control and monitoring of tomato crop through notifications to the
   farmer via a mobile application. In addition, the system offers the
   ability to detect tomato diseases, using an image-based classification
   model. This model is able to detect leaf diseases with an accuracy of
   86%. The goal is to provide farmers with an accurate view of their crops
   and mitigate disease and environmental damage.
CT Intelligent Systems Conference
CY SEP 05-06, 2024
CL Amsterdam, NETHERLANDS
RI nafil, khalid/HCG-8848-2022; Kobbane, Abdellatif/HLQ-2170-2023
Z8 0
ZA 0
ZB 0
ZR 0
ZS 0
TC 1
Z9 1
U1 1
U2 2
SN 2367-3370
EI 2367-3389
BN 978-3-031-66427-4; 978-3-031-66428-1
DA 2024-11-23
UT WOS:001313755900036
ER

PT C
AU Samonte, Mary Jane C.
   Achacoso, Luke Martin D. L.
   Amper, Alden Christian C.
   Eco, Alesandra Leonina G.
GP ASSOC COMPUTING MACHINERY
TI An In-Depth Analysis on Mitigating Data Breaches in Healthcare Systems
   through Big Data Structure
SO PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON COMPUTER AND
   COMMUNICATIONS MANAGEMENT, ICCCM 2024
BP 139
EP 147
DI 10.1145/3688268.3688289
DT Proceedings Paper
PD 2024
PY 2024
AB Data breaches and cyberattacks in healthcare pose substantial risks to
   patient information privacy, financial integrity, and institutional
   reputation. During rapid digitalization, the healthcare infrastructure
   in the Philippines faces similar vulnerabilities. This study examines
   the prevalence and impact of data breaches, emphasizing the unique
   challenges encountered by Philippine healthcare systems. Leveraging big
   data structures enhances security against such breaches. By emphasizing
   the significance of big data security and the Big Data Structure Model,
   this study analyzes their role in fortifying security measures. By
   scrutinizing strategies and frameworks, these findings may help
   contribute to understanding how big data architecture can effectively
   mitigate cybersecurity threats and safeguard sensitive patient
   information in healthcare settings. Furthermore, the study highlights
   the importance of different privacy-preserving techniques in data
   analytics, being able to reference works such as Gupta and Joshi's
   research on fuzzy association rules hiding in quantitative data,
   Fletcher and Islam's work on measuring information quality for
   privacy-preserving data mining, and Patel et al.'s study on
   privacy-preserving association rule mining throughout evolutionary
   algorithms. The findings that we have on our research is able to provide
   and contribute to the ongoing discourse regarding healthcare
   cybersecurity, being able to provide a foundation for future research
   which are aimed at developing a more robust and scalable security
   solution for the healthcare sector in the Philippines.
CT 12th International Conference on Computer and Communications Management
CY JUL 19-21, 2024
CL JAPAN
OI Samonte, Mary Jane/0000-0001-9867-8722; Amper, Alden
   Christian/0009-0000-9477-8013; Achacoso, Luke/0009-0000-7447-0302
ZR 0
ZA 0
Z8 0
TC 1
ZS 0
ZB 0
Z9 1
U1 0
U2 1
BN 979-8-4007-1803-8
DA 2025-04-05
UT WOS:001441968400021
ER

PT C
AU Sore, Safiatou
   Ouedraogo, Frederic T.
   Bikienga, Moustapha
   Traore, Yaya
GP Assoc Computing Machinery
TI Towards a More Generic and Elastic Metadata Management Model in a Data
   Lake Environment
SO 2024 16TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING,
   ICMLC 2024
BP 44
EP 51
DI 10.1145/3651671.3651773
DT Proceedings Paper
PD 2024
PY 2024
AB The evolution of the vast amount of heterogeneous data sources is
   leading to the emergence of several new concepts. One of the best-known
   concepts that is emerging as a new and trending topic in the big data
   space is the data lake. This is a central repository that stores
   heterogeneous data sources in their native format, without any
   predefined schema. In the absence of an enforced schema, effective
   metadata management based on metadata models remains an active research
   topic to address the problems associated with the data lake: the "data
   swamp". The analysis of existing metadata models shows that there is no
   comprehensive model among them. In this paper, we present a generic and
   scalable metadata model, which refers to the ability to dynamically
   provision computing resources based on demand and to resize resources as
   needed during metadata integration. Our approach will be based on a
   functional architecture of the data lake, along with a set of features
   that promote the generality of the metadata model.
CT 16th International Conference on Machine Learning and Computing (ICMLC)
CY FEB 02-05, 2024
CL Shenzhen, PEOPLES R CHINA
SP Shenzhen Univ, Natl Engn Lab Big Data Syst Comp Technol
OI Ouéaogo, Tounwendyam Frédéric/0000-0002-7340-1150; BIKIENGA,
   Moustapha/0009-0006-2505-7937; , Yaya TRAORE/0009-0004-5890-5838; Sanou,
   Safiatou/0009-0009-8327-2899
ZR 0
ZS 0
ZB 0
ZA 0
Z8 0
TC 1
Z9 1
U1 1
U2 2
BN 979-8-4007-0923-4
DA 2024-10-16
UT WOS:001251222100007
ER

PT J
AU Oliveira e Sa, Jorge
   Rebelo, Francisco
   Silva, Diogo
   Teles, Gabriel
   Ramos, Diogo
   Romeu, Jose
TI A Big Data System Architecture to Support the Monitoring of Paved Roads
SO INFRASTRUCTURES
VL 8
IS 12
AR 167
DI 10.3390/infrastructures8120167
DT Article
PD DEC 2023
PY 2023
AB Today, everything is connected, including the exchange of data and the
   generation of new information. As a result, large amounts of data are
   being collected at an ever-increasing rate and in a variety of forms, a
   phenomenon now known as Big Data. Recent developments in information and
   communication technologies are driving the generation of significant
   amounts of data from multiple sources, namely sensors. In response to
   these technological advances and data challenges, this paper proposes a
   Big Data system architecture for paved road monitoring and implements
   part of this architecture on a section of road in Portugal as a case
   study. The challenge in the case study architecture is to collect and
   process sensor data in real time, at a rate of 500 records per second,
   producing 15 GBytes of data per day, using a real-time data stream for
   real-time monitoring and a batch data stream for deeper analysis. This
   allows users to obtain instant updates on road conditions such as the
   number of vehicles, loads, weather, and pavement temperatures on the
   road. They can monitor what is happening on the road in real time,
   receive alerts, and even gain insight into historical data, such as
   analysing the condition of structures or identifying traffic patterns.
RI Rebelo, Francisco/; Oliveira e Sá, Jorge/B-7176-2012
OI Rebelo, Francisco/0000-0002-5111-1056; Oliveira e Sá,
   Jorge/0000-0003-4095-3431
ZB 0
TC 0
ZA 0
ZS 0
ZR 0
Z8 0
Z9 1
U1 0
U2 4
EI 2412-3811
DA 2024-01-11
UT WOS:001131276100001
ER

PT J
AU Kulkarni, Apurva
   Ramanathan, Chandrashekar
   Venugopal, Vinu E.
TI Semantics-Aware Document Retrieval for Government Administrative Data
SO INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING
VL 17
IS 03
BP 477
EP 491
DI 10.1142/S1793351X23300017
EA JUL 2023
DT Article
PD SEP 2023
PY 2023
AB The process of data analytics on large-scale government administrative
   data - that belong to various domains like education, transport, energy,
   and health - can be enhanced by retrieving pertinent documents from
   diverse data sources. Without a supporting framework of metadata, big
   data analytics can be daunting. Even though statistical algorithms can
   perform extensive analyses on a variety of data with little help from
   metadata, applying these techniques to heterogeneous data may not always
   result in reliable findings. Recently, semantics-aware (or semantic
   search) search techniques received much attention as they utilize
   implicit knowledge to enhance the search. Similarly, traditional search
   engines rely on the inherent linkages within the underlying data model
   to improve their search quality. In the case of general-purpose
   information retrieval systems, to gather information from the internet
   (open access data) or to access open government administrative data, a
   domain agnostic ontology shall be employed to supply background
   knowledge. This paper draws on research undertaken by the authors at
   IIIT Bangalore Center for Open Data Research (CODR) in developing a
   semantics-aware data lake framework to host and analyze government
   administrative data. In this study, we present an ontology-based
   document retrieval solution where an ontology serves as an intermediary
   to close the gap between what the user seeks and what the search
   retrieves. Although our study settings are based on the Government of
   Karnataka (GoK, India), we believe the findings have wider resonance.
   Our experimental results based on agricultural data from the GoK look
   promising.
ZS 0
TC 1
ZR 0
Z8 0
ZA 0
ZB 0
Z9 1
U1 0
U2 5
SN 1793-351X
EI 1793-7108
DA 2023-08-21
UT WOS:001040105300001
ER

PT J
AU Wang, Hongya
   Adenutsi, Caspar Daniel
   Wang, Can
   Sun, Zheng
   Zhang, Yue
   Li, Yuxin
   Zhang, Yiping
   Wang, Jiahuan
TI Construction and Application of a Big Data System for Regional Lakes in
   Coalbed Methane Development
SO ACS OMEGA
VL 8
IS 20
BP 18323
EP 18331
DI 10.1021/acsomega.3c02367
DT Article
PD MAY 11 2023
PY 2023
AB With the rapid development and widespread applicationof big dataand
   artificial intelligence, the upgrading of digital and
   intelligentindustries has been rapidly popularized in the oil and gas
   industry.First, based on the theory of & DPRIME;regional data lake &
   DPRIME;,the digital nature of the CBM governance system is analyzed, and
   theoptimization model of CBM governance for different data types is
   established.Second, considering the geological characteristics and
   developmentmode of the CBM reservoir, the regional data lake expansion
   modelis established. Third, a theoretical model of coupling &
   DPRIME;on-sitedata, laboratory data, management data, and data
   management system & DPRIME;has been established. The research shows the
   following: (a) The CBMgovernance system based on the regional data lake
   can be divided intofour parts: basic support, data life cycle, core
   governance areas,and governance strategy support. (b) The coupling of
   the coalbed methanegovernance model with the BP neural network model in
   this articlehas good application results. (c) The computational
   efficiency ofthis model has been improved by 12%, which has broad
   application prospects.
ZB 0
TC 0
ZR 0
ZS 0
ZA 0
Z8 1
Z9 1
U1 1
U2 7
SN 2470-1343
DA 2023-07-03
UT WOS:001012300800001
PM 37251117
ER

PT J
AU Chen, Hui
   Song, Zhao
   Yang, Feng
TI RETRACTED: Storage Method for Medical and Health Big Data Based on
   Distributed Sensor Network (Retracted Article)
SO JOURNAL OF SENSORS
VL 2023
AR 8506485
DI 10.1155/2023/8506485
DT Article; Retracted Publication
PD FEB 3 2023
PY 2023
AB Monitoring and collecting medical data using embedded medical diagnostic
   devices with multiple sensors and sending these actual measured data to
   the corresponding health monitoring centers using multipurpose wireless
   networks to take necessary measures to coordinate with family medical
   service centers and regional medical service departments is a popular
   medical big data architecture. However, healthcare big data is
   characterized by large data volume, fast growth, multimodality, high
   value and privacy, etc. How to organize and manage it in a unified and
   efficient way is an important research direction at present. In response
   to the problems of low balance and poor security in the storage of data
   collected by distributed sensor networks in healthcare systems, we
   propose a distributed storage algorithm for big data in healthcare
   systems. The platform adopts Hadoop distributed file system and
   distributed file storage framework as the healthcare big data storage
   solution, and implements data integration, multidimensional data query
   and analysis mining components based on Spark-SQL data query tool, Spark
   machine learning algorithm library and its mining and analysis pipeline
   development, respectively. The distributed storage model of big data and
   three data storage levels are constructed using cloud storage
   architecture, and the data storage intensity as well as levels are
   calculated by high data access in the upper level, data connection in
   the middle level, and data archiving in the lower level according to the
   set known data granularity, odds, and elasticity to realize big data
   storage. It is experimentally verified that the above algorithm has high
   distribution balance and low load balance in the storage process.
ZR 0
ZS 0
Z8 0
ZB 0
TC 1
ZA 0
Z9 1
U1 1
U2 14
SN 1687-725X
EI 1687-7268
DA 2023-03-05
UT WOS:000929503500001
ER

PT C
AU Abdelhedi, Fatma
   Jemmali, Rym
   Zurfluh, Gilles
GP IEEE
TI Medical data lake query assistance
SO 2023 20TH ACS/IEEE INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS AND
   APPLICATIONS, AICCSA
SE International Conference on Computer Systems and Applications
DI 10.1109/AICCSA59173.2023.10479336
DT Proceedings Paper
PD 2023
PY 2023
AB In today's world, there is a growing need to analyze data stored in a
   Data Lake, which is a collection of large, heterogeneous databases. Our
   work is part of a medical application that aims to help healthcare
   professionals analyze complex data for decision-making. We propose
   mechanisms that promote data accessibility. The data are stored in a
   Data Warehouse (DW) that is periodically built from a data lake.
   Depending on the needs of the decision-maker, data are extracted from
   the DW and transferred to a Data Mart (DM) for querying. In this paper,
   we present a schema recommendation system based on the principle of
   collaborative filtering. This system can predict the DM schemas that
   were developed in the past that best match the data need expressed by a
   decision-maker. It does this by comparing the attributes present in the
   schemas with the attributes deduced from the need to propose a list of
   predictions for the most suitable schemas. The technique used is simple,
   while allowing us to solve the problem of periodic updates to the source
   data. An experiment was conducted for a medical application.
CT 20th ACS/IEEE International Conference on Computer Systems and
   Applications (AICCSA)
CY DEC 04-07, 2023
CL Giza, EGYPT
SP IEEE; ACS
RI Abdelhedi, Fatma/AAT-3786-2021
ZB 0
ZR 0
ZA 0
Z8 0
ZS 0
TC 0
Z9 1
U1 0
U2 0
SN 2161-5322
BN 979-8-3503-1943-9
DA 2024-07-06
UT WOS:001222477900102
ER

PT J
AU Chikalanov, Alexander
   Kirilov, Leoneed
   Kovatcheva, Eugenia
   Nikolov, Roumen
   Shoikova, Elena
   Iliev, Alexander
   Gotsev, Ljubomir
TI A MODEL OF BIG DATA ARCHITECTURE ON THE BASE OF FIWARE COMPONENTS
SO COMPTES RENDUS DE L ACADEMIE BULGARE DES SCIENCES
VL 76
IS 9
BP 1393
EP 1401
DI 10.7546/CRABS.2023.09.10
DT Article
PD 2023
PY 2023
AB The intensive development of hardware and software tools provides many
   opportunities for solving various real-world tasks, such as working with
   large data sets. This, in turn, is a powerful incentive and opportunity
   for the de-velopment of different approaches and tools for solving them.
   After discussing several well-known BigData architectures a model of a
   BigData architecture is designed. The architecture is organized in three
   sections: edge section, processing-and-storage section and application
   section. The model will be im-plemented using FIWARE components.
RI Kirilov, Leo/GQP-0652-2022; Kovatcheva, Eugenia/A-1770-2018; Gotsev, Lyubomir/AEC-0181-2022
OI Kovatcheva, Eugenia/0000-0003-0108-1271; Gotsev,
   Lyubomir/0000-0002-5217-5780
ZB 1
ZS 0
Z8 0
ZA 0
TC 1
ZR 0
Z9 1
U1 0
U2 2
SN 1310-1331
DA 2024-01-07
UT WOS:001108757800004
ER

PT C
AU Kulkarni, Apurva
   Bassin, Pooja
   Parasa, Niharika Sri
   Venugopal, Vinu E.
   Srinivasa, Srinath
   Ramanathan, Chandrashekar
BE Sachdeva, S
   Watanobe, Y
   Bhalla, S
TI Ontology Augmented Data Lake System for Policy Support
SO BIG DATA ANALYTICS IN ASTRONOMY, SCIENCE, AND ENGINEERING, BDA 2022
SE Lecture Notes in Computer Science
VL 13830
BP 3
EP 16
DI 10.1007/978-3-031-28350-5_1
DT Proceedings Paper
PD 2023
PY 2023
AB Analytics of Big Data in the absence of an accompanying framework of
   metadata can be a quite daunting task. While it is true that statistical
   algorithms can do large-scale analyses on diverse data with little
   support from metadata, using such methods on widely dispersed, extremely
   diverse, and dynamic data may not necessarily produce trustworthy
   findings. One such task is identifying the impact of indicators for
   various Sustainable Development Goals (SDGs). One of the methods to
   analyze impact is by developing a Bayesian network for the policymaker
   to make informed decisions under uncertainty. It is of key interest to
   policy-makers worldwide to rely on such models to decide the new
   policies of a state or a country (https://sdgs.un.org/2030agenda). The
   accuracy of the models can be improved by considering enriched data -
   often done by incorporating pertinent data from multiple sources.
   However, due to the challenges associated with volume, variety,
   veracity, and the structure of the data, traditional data lake systems
   fall short of identifying information that is syntactically diverse yet
   semantically connected. In this paper, we propose a Data Lake (DL)
   framework that targets ingesting & processing of data like any
   traditional DL, and in addition, is capable of performing data retrieval
   for applications such as Policy Support Systems (where the selection of
   data greatly affect the output interpretations) by using ontologies as
   the intermediary. We discuss the proof of concept for the proposed
   system and the preliminary results (IIITB Data Lake project Website
   link: http://cads.iiitb.ac.in/wordpress/) based on the data collected
   from the agriculture department of the Government of Karnataka (GoK).
CT 10th International Conference on Big Data Analytics (BDA)
CY DEC 05-07, 2022
CL Univ Aizu, ELECTR NETWORK
HO Univ Aizu
SP Natl Inst Technol Delhi; Indian Inst Technol Delhi
RI Ellampallil Venugopal, Vinu/; Srinivasa, Srinath/AAT-8414-2020; Kulkarni, Apurva/; Bassin, Pooja/; Ramanathan, Chanashekar/
OI Ellampallil Venugopal, Vinu/0000-0003-4429-9932; Kulkarni,
   Apurva/0000-0002-9215-2049; Bassin, Pooja/0000-0002-0611-8734;
   Ramanathan, Chanashekar/0000-0002-3330-8365
Z8 0
TC 1
ZR 0
ZA 0
ZB 0
ZS 0
Z9 1
U1 1
U2 4
SN 0302-9743
EI 1611-3349
BN 978-3-031-28349-9; 978-3-031-28350-5
DA 2023-07-13
UT WOS:001004046900001
ER

PT C
AU Lo Huang, Jose Luis
   Emeakaroha, Vincent C.
BE VanSteen, M
   Pahl, C
TI Enabling Quantum Key Distribution on a Multi-Cloud Environment to Secure
   Distributed Data Lakes
SO PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND
   SERVICES SCIENCE, CLOSER 2023
SE CLOSER
BP 78
EP 89
DI 10.5220/0011993200003488
DT Proceedings Paper
PD 2023
PY 2023
AB Each day more and more data is produced by different sources including
   humans and machines, and they are stored mainly in Cloud Service
   Providers (CSP) in huge data storages known as big data or data lakes.
   Many situations warrant users to spread their data in a distributed way
   between the different CSP. When this happens, they have to relay the
   inter-cloud communication security to each cloud vendor. This can cause
   data leakage in the transmission channel thereby compromise information
   security. Quantum computing has shown some promises to address this
   issue. One well known algorithm in quantum cryptography is the Quantum
   Key Distribution (QKD) protocol. This enables the sender and receiver of
   a message to know when a third party eavesdropped any data from the
   insecure quantum channel. There are studies integrating this QKD
   protocol with cloud storage and data transmission inside one CSP.
   However, there is no research that studies the data lake security
   concern for distributed multi-cloud communications taking advantage of
   quantum mechanisms. This research proposal aims to address this gap in
   distributed data lake security by using the QKD protocol in the
   multi-cloud distributed data transmission. The achieved results show
   over 91% detection of eavesdropping cases and over 99% correct
   authorisation detection in multi-cloud environments.
CT 13th International Conference on Cloud Computing and Services Science
   (CLOSER)
CY APR 26-28, 2023
CL Prague, CZECH REPUBLIC
SP INSTICC
OI Lo Huang, Jose Luis/0000-0002-1480-0354
Z8 0
ZB 0
ZA 0
TC 1
ZR 0
ZS 0
Z9 1
U1 1
U2 3
SN 2184-5042
BN 978-989-758-650-7
DA 2024-01-10
UT WOS:001118989400008
ER

PT C
AU Watz, Eric
   Neubauer, Peter
   Shires, Ramona
   May, Jeremy
BE Sottilare, RA
   Schwarz, J
TI Precision Learning Through Data Intelligence
SO ADAPTIVE INSTRUCTIONAL SYSTEMS, AIS 2023
SE Lecture Notes in Computer Science
VL 14044
BP 174
EP 187
DI 10.1007/978-3-031-34735-1_13
DT Proceedings Paper
PD 2023
PY 2023
AB Simulations and learning environments often generate massive amounts of
   human performance data, which can be a challenge to manage and interpret
   in a meaningful manner. Lacking a context that provides meaning to data,
   it exists as information stored on computers, data centers, and cloud
   infrastructure. With the ever-increasing need to understand and act on
   learner performance data, there is a growing interest in tools and
   techniques that can help organizations transform performance data into
   actionable insights.
   The assessment of human performance is critical to providing efficient
   and effective training. Modern training environments, especially live
   and virtual distributed environments, are rich with data representing
   multiple modalities and formats. One foundational challenge involves
   storing this wealth of data in a common format with metadata to support
   queries and analyses such as predictive models and proficiency tracking
   necessary to support managed learning over time.
   This paper presents use cases behind a scalable data lake architecture
   and considerations for a common human performance assessment data
   storage method. In addition, lessons learned in storing, managing, and
   cleaning longitudinal data are presented. These methods provide insights
   into a data-centric solution that offers scalability, flexibility,
   maintainability, and usability.
   Within a precision learning ecosystem, interoperability between
   applications can be achieved through proper application of software
   design methods and architectures, as well as information exchange based
   on established standards for data exchange. To effectively realize a
   precision approach to learning and training competency-based
   assessments, organizations need to be able to collect, integrate, and
   share performance data in a way that is both efficient and effective.
   This includes the use of enterprise-level tools and systems that can
   handle the scale and complexity of the data while also providing the
   necessary analytics and reporting capabilities. These solutions must be
   able to convert and present performance data in ways that are meaningful
   and actionable, such as by visualizing data in charts and graphs, or
   using advanced analytics techniques to uncover deeper insights into
   learner performance. This paper explores software architectures that
   enable rich data exchange and interoperability in a learning and
   training ecosystem and paper includes lessons learned from integrating
   modeling and simulation applications with cloud-native, big data
   handling tools.
   Precision learning and training systems that fuse multiple data sources
   to predict current proficiency and future training needs are essential
   to managing localized and longitudinal learning goals for individuals,
   teams, and teams of team. This is the foundation for adaptive
   proficiency-based training that improves training efficiency and
   effectiveness. One approach to leveraging performance data to inform
   proficiency-based training is through the use of competency-based
   assessments. The process of defining competencies is known as knowledge
   engineering and involves defining the specific skills and knowledge
   required for a particular role or task, then measuring an individual's
   performance against these defined competencies. By capturing and
   analyzing data on competency-based performance, organizations gain
   valuable insights into the strengths and weaknesses of their workforce,
   which can be used to inform training and development initiatives. This
   paper addresses the needs and data tagging requirements that enable
   robust capture and processing of learner performance. These metadata are
   shown as key enablers to unlocking actionable insights from information
   that began as purely data.
CT 5th International Conference of the Adaptive Instructional Systems (AIS)
   part of the 25th International Conference on Human-Computer Interaction
   (HCI)
CY JUL 23-28, 2023
CL Copenhagen, DENMARK
OI May, Jeremy/0000-0003-3319-0077
Z8 0
ZR 0
ZS 0
ZA 0
ZB 0
TC 1
Z9 1
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-34734-4; 978-3-031-34735-1
DA 2024-09-29
UT WOS:001297273300013
ER

PT J
AU Paladin, Zdravko
   Kapidani, Nexhat
   Scrima, Piero
   Vosinakis, Georgios
   Hajduch, Guillaume
   Moutzouris, Marios
   Bolakis, Christos
   Astyakopoulos, Alkiviadis
TI Maritime information sharing environment deployment using the advanced
   multilayered Data Lake capabilities: EFFECTOR project case study
SO POMORSTVO-SCIENTIFIC JOURNAL OF MARITIME RESEARCH
VL 36
IS 2
BP 291
EP 304
DI 10.31217/p.36.2.13
DT Article
PD DEC 2022
PY 2022
AB Establishing an efficient information sharing network among national
   agencies in maritime domain is of essential importance in enhancing the
   operational performance, increasing the situational awareness and
   enabling interoperability among all involved maritime surveillance
   assets. Based on various data-driven technologies and sources, the EU
   initiative of Common Information Sharing Environment (CISE), enables the
   networked participants to timely exchange information concerning vessel
   traffic, joint SAR & operational missions, emergency situations and
   other events at sea. In order to host and process vast amounts of
   vessels and related maritime data consumed from heterogeneous sources
   (e.g. SAT-AIS, UAV, radar, METOC), the deployment of big data
   repositories in the form of Data Lakes is of great added value. The
   different layers in the Data Lakes with capabilities for aggregating,
   fusing, routing and harmonizing data are assisted by decision support
   tools with combined reasoning modules with semantics aiming at providing
   a more accurate Common Operational Picture (COP) among maritime
   agencies. Based on these technologies, the aim of this paper is to
   present an end -to-end interoperability framework for maritime
   situational awareness in strategic and tactical operations at sea,
   developed in EFFECTOR EU-funded project, focusing on the multilayered
   Data Lake capabilities. Specifically, a case study presents the
   important sources and processing blocks, such as the SAT-AIS, CMEMS, UAV
   components, enabling maritime information exchange in CISE format and
   communication patterns. Finally, the technical solution is validated in
   the project's recently implemented maritime operational trials and the
   respective results are documented.
RI Kapidani, Nexhat/ABE-8074-2021; Astyakopoulos, Alkis/; Vosinakis, Georgios/
OI Kapidani, Nexhat/0000-0002-1017-8019; Astyakopoulos,
   Alkis/0000-0002-5345-1003; Vosinakis, Georgios/0000-0003-2029-8409
ZB 0
TC 1
ZR 0
ZS 0
Z8 0
ZA 0
Z9 1
U1 2
U2 3
SN 1332-0718
EI 1846-8438
DA 2023-06-10
UT WOS:000992572400012
ER

PT J
AU Shang, Jiling
   Liang, Chaohui
TI Application of Clustering Algorithm in English Proficiency Evaluation
   under the Framework of Big Data
SO MATHEMATICAL PROBLEMS IN ENGINEERING
VL 2022
AR 2463926
DI 10.1155/2022/2463926
DT Article
PD JUL 13 2022
PY 2022
AB With the advancement of science and technology as well as the continual
   improvement of big data analysis technology, the accuracy of traditional
   data information classification has declined, making it impossible to
   assess English ability effectively. A competency evaluation model for
   college English teaching vacancies is built using this information and
   the big data architecture. The ability of the big data information model
   is evaluated and feature information of ability constraints is extracted
   using the predefined constraint parameter index analysis model.
   Simultaneously, the K-means clustering algorithm is used to cluster and
   integrate a series of index parameters of English ability using big
   data, and the English teaching resource allocation plan is completed in
   accordance with this, allowing for the scientific evaluation of English
   teaching ability. The results of the studies show that the clustering
   method utilized in the context of big data can aid in the evaluation of
   English competence. In the experiment, four test cycles of English
   teaching skills were set up, and the effectiveness of the English
   evaluation techniques described in this paper and two classical cluster
   evaluation methods were compared and tested. The research shows that
   using the method described in this paper to evaluate English teaching
   skills can significantly improve the full utilization of data.
OI Shang, Jiling/0000-0002-3170-9018
ZB 0
Z8 0
TC 0
ZS 0
ZA 0
ZR 0
Z9 1
U1 0
U2 9
SN 1024-123X
EI 1563-5147
DA 2022-10-05
UT WOS:000860750600002
ER

PT J
AU Santamaría, Edgar M.
   Espitia-Lopez, José V.
   Marriaga-Barroso, Angie P.
   Paez-Logreira, Heyder D.
TI Sistema de control y monitoreo para el proceso de secado de alimentos:
   caso polen apícola
X1 Remote control of critical variables in the food drying process: a bee
   pollen case study
SO Información tecnológica
VL 33
IS 3
BP 137
EP 148
DT research-article
PD 2022-06
PY 2022
AB Abstract: The present study aims to design a control and monitoring
   system for measuring variables in a pollen drying oven. To avoid bee
   pollen decomposition, an efficient drying process must be applied to
   improve its quality and its capacity to preserve nutritional properties.
   The oven is built by applying the V methodology (V-model) to develop the
   hardware and software systems and to ensure that critical control
   variables are met during real-time remote monitoring. The results show
   that the oven has an efficient and uniform heat distribution. It allows
   drying large bee pollen quantities while yielding a high quality
   product. In conclusion, it is feasible to technologically upgrade bee
   pollen drying processes and improve data architecture processes in the
   apiculture sector by including new technologies such as big data and
   artificial intelligence.
X4 Resumen: En este estudio se propone un sistema de control y monitoreo de
   variables para un horno de secado de polen automatizado. Para evitar la
   descomposición del polen, se debe aplicar un proceso de secado eficiente
   que mejore su calidad y capacidad para conservar sus propiedades
   nutricionales. La construcción del horno se realiza con la metodología V
   (V-Model) para el desarrollo de sistemas hardware y software,
   garantizando las características de control de variables críticas,
   monitoreo remoto y en tiempo real. Los resultados muestran una
   distribución del calor más eficiente y uniforme, permitiendo procesos de
   secado con mayor cantidad y calidad del producto. Se concluye que es
   posible tecnificar el proceso de secado de polen utilizando una
   arquitectura de datos de este proceso que permite la inclusión de nuevas
   tecnologías como big data e inteligencia artificial.
ZS 0
ZB 0
ZA 0
Z8 0
TC 1
ZR 0
Z9 1
U1 0
U2 2
SN 0718-0764
DA 2022-07-15
UT SCIELO:S0718-07642022000300137
ER

PT C
AU Chaves, Pedro
   Fonseca, Tiago
   Ferreira, Luis Lino
   Cabral, Bernardo
   Sousa, Orlando
   Oliveira, Andre
   Landeck, Jorge
GP IEEE
TI An IoT Cloud and Big Data Architecture for the Maintenance of Home
   Appliances
SO IECON 2022 - 48TH ANNUAL CONFERENCE OF THE IEEE INDUSTRIAL ELECTRONICS
   SOCIETY
SE IEEE Industrial Electronics Society
DI 10.1109/IECON49645.2022.9968580
DT Proceedings Paper
PD 2022
PY 2022
AB Billions of interconnected Internet of Things (IoT) sensors and devices
   collect tremendous amounts of data from real-world scenarios. Big data
   is generating increasing interest in a wide range of industries. Once
   data is analyzed through compute-intensive Machine Learning (ML)
   methods, it can derive critical business value for organizations.
   Powerful platforms are essential to handle and process such massive
   collections of information cost-effectively and conveniently. This work
   introduces a distributed and scalable platform architecture that can be
   deployed for efficient real-world big data collection and analytics. The
   proposed system was tested with a case study for Predictive Maintenance
   of Home Appliances, where current and vibration sensors with high
   acquisition frequency were connected to washing machines and
   refrigerators. The introduced platform was used to collect, store, and
   analyze the data. The experimental results demonstrated that the
   presented system could be advantageous for tackling real-world IoT
   scenarios in a cost-effective and local approach.
CT 48th Conference of the Industrial Electronics Society-IECON-Annual
CY OCT 17-20, 2022
CL Brussels, BELGIUM
Z8 0
ZR 0
ZS 0
ZA 0
TC 1
ZB 0
Z9 1
U1 1
U2 1
SN 1553-572X
BN 978-1-6654-8025-3
DA 2022-01-01
UT WOS:001504976200257
ER

PT C
AU Banton, Matthew
   Bowles, Juliana
   Silvina, Agastya
   Webber, Thais
BE Moschoyiannis, S
   Penaloza, R
   Vanthienen, J
   Soylu, A
   Roman, D
TI Conflict-Free Access Rules for Sharing Smart Patient Health Records
SO RULES AND REASONING, RULEML+RR 2021
SE Lecture Notes in Computer Science
VL 12851
BP 33
EP 47
DI 10.1007/978-3-030-91167-6_3
DT Proceedings Paper
PD 2021
PY 2021
AB With an increasing trend in personalised healthcare provision across
   Europe, we need solutions to enable the secure transnational sharing of
   medical records, establishing granular access rights to personal patient
   data. Access rules can establish what should be accessible by whom for
   how long, and comply with collective regulatory frameworks, such as the
   European General Data Protection Regulation (GDPR). The challenge is to
   design and implement such systems integrating novel technologies like
   Blockchain and Data Lake to enhance security and access control. The
   blockchain module must deal with adequate policies and algorithms to
   guarantee that no data leaks occur when authorising data retrieval
   requests. The data lake module tackles the need for an efficient way to
   retrieve potential granular data from heterogeneous data sources. In
   this paper, we define a patient-centric authorisation approach,
   incorporating a structured format for composing access rules that enable
   secure data retrieval and automatic rules conflict checking.
CT 5th International Joint Conference on Rules and Reasoning (RuleML+RR)
CY SEP 13-15, 2021
CL KU Leuven, Leuven, BELGIUM
HO KU Leuven
SP Elsevier, Artificial Intelligence Journal; Springer, Lecture Notes Comp
   Sci; Leuven AI; Decis Management Community; KU Leuven; Univ Surrey; Univ
   Milano Bicocca; RuleML Inc; RR Assoc
RI Webber, Thais/C-6099-2012; Banton, Matthew/JEO-8455-2023; Bowles, Juliana/
OI Webber, Thais/0000-0002-8091-6021; Banton, Matthew/0000-0001-8170-3899;
   Bowles, Juliana/0000-0002-5918-9114
ZR 0
ZA 0
ZB 0
ZS 0
TC 0
Z8 0
Z9 1
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-030-91167-6; 978-3-030-91166-9
DA 2021-01-01
UT WOS:000917009400003
ER

PT C
AU Barbierato, Enrico
   Gribaudo, Marco
   Serazzi, Giuseppe
   Tanca, Letizia
BE Ballarini, P
   Castel, H
   Dimitriou, I
   Iacono, M
   PhungDuc, T
   Walraevens, J
TI Performance Evaluation of a Data Lake Architecture via Modeling
   Techniques
SO PERFORMANCE ENGINEERING AND STOCHASTIC MODELING
SE Lecture Notes in Computer Science
VL 13104
BP 115
EP 130
DI 10.1007/978-3-030-91825-5_7
DT Proceedings Paper
PD 2021
PY 2021
AB Data Lake is a term denoting a repository storing heterogeneous data,
   both structured and unstructured, resulting in a flexible organization
   that allows Data Lake users to reorganize and integrate dynamically the
   information they need according to the required query or analysis. The
   success of its implementation depends on many factors, notably the
   distributed storage, the kind of media deployed, the data access
   protocols and the network used. However, flaws in the design might
   become evident only in a later phase of the system development, causing
   significant delays in complex projects. This article presents an
   application of queuing networks modeling technique to detect significant
   issues, such as bottlenecks and performance degradation, for different
   workload scenarios.
CT 17th European Workshop on Computer Performance Engineering (EPEW)
CY DEC 09-10, 2021
CL ELECTR NETWORK
RI GRIBAUDO, MARCO/AAI-5402-2021; Barbierato, Enrico/AAC-6349-2021; TANCA, LETIZIA/
OI GRIBAUDO, MARCO/0000-0002-1415-5287; TANCA, LETIZIA/0000-0003-2607-3171
ZB 0
ZA 0
ZR 0
TC 1
Z8 0
ZS 0
Z9 1
U1 0
U2 4
SN 0302-9743
EI 1611-3349
BN 978-3-030-91825-5; 978-3-030-91824-8
DA 2022-03-25
UT WOS:000766389200007
ER

PT C
AU Barry, Mariam
   Bifet, Albert
   Chiky, Raja
   Montiel, Jacob
   Tran, Vinh-Thuy
BE Srirama, SN
   Lin, JCW
   Bhatnagar, R
   Agarwal, S
   Reddy, PK
TI Challenges of Machine Learning for Data Streams in the Banking Industry
SO 9TH INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS, BDA 2021
SE Lecture Notes in Computer Science
VL 13147
BP 106
EP 118
DI 10.1007/978-3-030-93620-4_9
DT Proceedings Paper
PD 2021
PY 2021
AB Banking Information Systems continuously generate large quantities of
   data as inter-connected streams (transactions, events logs, time series,
   metrics, graphs, process, etc.). Such data streams need to be processed
   online to deal with critical business applications such as real-time
   fraud detection, network security attack prevention or predictive
   maintenance on information system infrastructure. Many algorithms have
   been proposed for data stream learning, however, most of them do not
   deal with the important challenges and constraints imposed by real-world
   applications. In particular, when we need to train models incrementally
   from heterogeneous data mining and deployment them within complex big
   data architecture. Based on banking applications and lessons learned in
   production environments of BNP Paribas - a major international banking
   group and leader in the Eurozone - we identified the most important
   current challenges for mining IT data streams. Our goal is to highlight
   the key challenges faced by data scientists and data engineers within
   complex industry settings for building or deploying models for real word
   streaming applications. We provide future research directions on Stream
   Learning that will accelerate the adoption of online learning models for
   solving real-word problems. Therefore bridging the gap between research
   and industry communities. Finally, we provide some recommendations to
   tackle some of these challenges.
CT 9th International Conference on Big Data Analytics (BDA)
CY DEC 15-18, 2021
CL ELECTR NETWORK
SP Indian Inst Informat Tech
RI Bifet, Albert/ISA-9610-2023; Montiel, Jacob/AAH-8641-2020
OI Bifet, Albert/0000-0002-8339-7773; 
ZR 0
Z8 0
ZB 0
ZA 0
ZS 0
TC 1
Z9 1
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-030-93619-8; 978-3-030-93620-4
DA 2024-11-01
UT WOS:001308354700009
ER

PT J
AU Bohar, Balazs
   Fazekas, David
   Madgwick, Matthew
   Csabai, Luca
   Olbei, Marton
   Korcsmaros, Tamas
   Szalay-Beko, Mate
TI Sherlock: an open-source data platform to store, analyze and integrate
   Big Data for biology.
SO F1000Research
VL 10
BP 409
EP 409
DI 10.12688/f1000research.52791.1
DT Journal Article
PD 2021
PY 2021
AB In the era of Big Data, data collection underpins biological research
   more so than ever before. In many cases this can be as time-consuming as
   the analysis itself, requiring downloading multiple different public
   databases, with different data structures, and in general, spending days
   before answering any biological questions. To solve this problem, we
   introduce an open-source, cloud-based big data platform, called Sherlock
   ( https://earlham-sherlock.github.io/). Sherlock provides a gap-filling
   way for biologists to store, convert, query, share and generate biology
   data, while ultimately streamlining bioinformatics data management. The
   Sherlock platform provides a simple interface to leverage big data
   technologies, such as Docker and PrestoDB. Sherlock is designed to
   analyse, process, query and extract the information from extremely
   complex and large data sets. Furthermore, Sherlock is capable of
   handling different structured data (interaction, localization, or
   genomic sequence) from several sources and converting them to a common
   optimized storage format, for example to the Optimized Row Columnar
   (ORC). This format facilitates Sherlock's ability to quickly and easily
   execute distributed analytical queries on extremely large data files as
   well as share datasets between teams. The Sherlock platform is freely
   available on Github, and contains specific loader scripts for structured
   data sources of genomics, interaction and expression databases. With
   these loader scripts, users are able to easily and quickly create and
   work with the specific file formats, such as JavaScript Object Notation
   (JSON) or ORC. For computational biology and large-scale bioinformatics
   projects, Sherlock provides an open-source platform empowering data
   management, data analytics, data integration and collaboration through
   modern big data technologies.
TC 1
ZA 0
ZR 0
ZB 1
Z8 0
ZS 0
Z9 1
U1 0
U2 0
EI 2046-1402
DA 2022-12-20
UT MEDLINE:36533093
PM 36533093
ER

PT C
AU Brazhenenko, Maksym
   Petrivskyi, Volodymyr
   Bychkov, Oleksiy
   Sinitcyn, Igor
   Shevchenko, Victor
GP IEEE
TI Enabling Big Data Query with Modern CAD Systems Redundant Data Stores
SO 2021 IEEE 16TH INTERNATIONAL CONFERENCE ON THE EXPERIENCE OF DESIGNING
   AND APPLICATION OF CAD SYSTEMS (CADSM)
SE Experience of Designing and Application of CAD Systems in
   Microelectronics-CADSM
DI 10.1109/CADSM52681.2021.9385265
DT Proceedings Paper
PD 2021
PY 2021
AB This paper review trends in Modern Computer-Aided Design Systems design
   enterprise integration and bring light to some of the adoption
   challenges aspects. The research aims to lay out the consequences of the
   industry-wide move of Computer-Aided Design Systems into the Cloud
   environment and building a roadmap of cross-cloud enterprise systems
   integration. We adopt an approach of building data redundancy storages
   as one of the possible ways to reduce dependability of enterprise
   software systems, improve performance and reduce maintenance costs for
   software systems that integrate with Computer-Aided Design Systems
   solutions.
CT IEEE 16th International Conference on the Experience of Designing and
   Application of CAD Systems (CADSM)
CY FEB 22-26, 2021
CL Lviv, UKRAINE
SP IEEE; IEEE Ukraine Sect W MTT ED AP EP SSC Soc Joint Chapter; Lviv
   Polytechn Natl Univ, Inst Comp Sci & Informat Technologies, Dept Comp
   Aided Design; Lodz Univ Technol, Dept Microelectron & Comp Sci; IEEE
   Ukraine Sect
RI Bychkov, Oleksii/G-5358-2019; BRAZHENENKO, MAKSYM/ABA-5669-2021; Shevchenko, Viktor/R-3688-2018; Petrivskyi, Volodymyr/AAU-9006-2021
OI Bychkov, Oleksii/0000-0002-9378-9535; Petrivskyi,
   Volodymyr/0000-0001-9298-8244
Z8 0
ZS 0
ZB 0
TC 0
ZA 0
ZR 0
Z9 1
U1 0
U2 12
SN 2572-7583
EI 2572-7591
BN 978-1-6654-3894-0
DA 2021-08-11
UT WOS:000668942500051
ER

PT C
AU Melchor-Uceda, Iran A.
   Olivares-Rojas, Juan C.
   Gutierrez-Gnecchi, Jose A.
   Garcia-Ramirez, Maria C.
   Reyes-Archundia, Enrique
   Tellez-Anguiano, Adriana C.
GP IEEE
TI Data Ingestion System for Interoperability and Integration of Hospital
   Data Online and in Real Time
SO 2021 MEXICAN INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE (ENC 2021)
DI 10.1109/ENC53357.2021.9534795
DT Proceedings Paper
PD 2021
PY 2021
AB Since very remote times, the Mexican health sector has been
   characterized by being segmented and containing various actors, which
   generates problems in the quality of care within the users due to the
   lack of data integration and interoperability. Recently, there have been
   modifications and updates in health matters derived from increased of
   use information and communication technologies in the health area. All
   of this has brought new challenges and opportunities. One of the new
   opportunities is the growing generation of further information on
   patients' health through wearable and highly interconnected biomedical
   devices that allow the health status of patients to be continuous
   checked. However, this also represents a considerable challenge in
   achieving the integration and interoperability of health data from
   various media. That is why in this paper, the authors pro-pose a
   methodology to integrate hospital data from multiple sources that can be
   homogenized through the storage of a data lake. The results suggest that
   the integration and interoperability of hospital data in real-time and
   online are possible through a data ingestion system.
CT International Mexican International Conference on Computer Science (ENC)
CY AUG 09-11, 2021
CL ELECTR NETWORK
SP IEEE; Soc Mexicana Ciencia Computac A C; Fac Ciencias Fis Matematicas;
   Univ Michoacana San Nicolas Hidalgo, Fac Ingenieria Electrica; Univ Nacl
   Autonoma Mexico, Escuela Nacl Estudios Superiores; Inst Tecnologico
   Morelia, Tecnologico Nacl Mexico; Inst Tecnologico Culiacan; Centro
   Investigac Matematicas; Soc Mexicana Ciencia Computac A C; IEEE Comp
   Soc; IEEE Computat Intelligence Soc; IEEE Informat Theory Soc; RA;
   Cenidet; Secretaria Educac Publica
RI Olivares Rojas, Juan Carlos/D-7134-2015; Téllez-Anguiano, Aiana del Carmen/JVE-2208-2024; Reyes-Archundia, Enrique/C-8659-2018
OI Olivares Rojas, Juan Carlos/0000-0001-5302-1786; Téllez-Anguiano, Aiana
   del Carmen/0000-0002-0945-2076; Reyes-Archundia,
   Enrique/0000-0003-3374-0059
ZA 0
ZB 0
TC 1
ZS 0
ZR 0
Z8 0
Z9 1
U1 1
U2 6
BN 978-1-6654-2612-1
DA 2022-10-05
UT WOS:000858934700011
ER

PT C
AU Sawadogo, Pegdwende N.
   Darmont, Jerome
BE Golfarelli, M
   Wrembel, R
   Kotsis, G
   Tjoa, AM
   Khalil, I
TI Benchmarking Data Lakes Featuring Structured and Unstructured Data with
   DLBench
SO BIG DATA ANALYTICS AND KNOWLEDGE DISCOVERY (DAWAK 2021)
SE Lecture Notes in Computer Science
VL 12925
BP 15
EP 26
DI 10.1007/978-3-030-86534-4_2
DT Proceedings Paper
PD 2021
PY 2021
AB In the last few years, the concept of data lake has become trendy for
   data storage and analysis. Thus, several approaches have been proposed
   to build data lake systems. However, these proposals are difficult to
   evaluate as there are no commonly shared criteria for comparing data
   lake systems. Thus, we introduce DLBench, a benchmark to evaluate and
   compare data lake implementations that support textual and/or tabular
   contents. More concretely, we propose a data model made of both textual
   and CSV documents, a workload model composed of a set of various tasks,
   as well as a set of performance-based metrics, all relevant to the
   context of data lakes. As a proof of concept, we use DLBench to evaluate
   an open source data lake system we previously developed.
CT 23rd International Conference on Big Data Analytics and Knowledge
   Discovery (DaWaK)
CY SEP 27-30, 2021
CL ELECTR NETWORK
SP Software Competence Ctr Hagenberg; JKU Inst Telecooperat; Web Applicat
   Soc
OI Sawadogo, Pegdwendé N/0000-0001-6180-5476
TC 1
ZA 0
Z8 0
ZB 0
ZR 0
ZS 0
Z9 1
U1 0
U2 5
SN 0302-9743
EI 1611-3349
BN 978-3-030-86534-4; 978-3-030-86533-7
DA 2022-12-04
UT WOS:000886498500002
ER

PT J
AU Hachad, Tarik
   Sadiq, Abdelalim
   Ghanimi, Fadoua
TI A New Big Data Architecture for Real-Time Student Attention Detection
   and Analysis
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 11
IS 8
BP 241
EP 247
DT Article
PD AUG 2020
PY 2020
AB Big Data technologies and their analytical methods can help improve the
   quality of education. They can be used to process and analyze classroom
   video streams to predict student attention, this would greatly improve
   the learning-teaching experience. With the increasing number of students
   and the expansion of educational institutions, processing and analyzing
   video streams in real-time become a complicated issue. In this paper, we
   have reviewed the existing systems of student attention detection,
   open-source real-time data stream processing technologies, and the two
   major data stream processing architectures. We also proposed a new Big
   Data architecture for real-time student attention detection.
OI HACHAD, TARIK/0000-0001-9230-8982
Z8 0
ZS 0
TC 0
ZB 0
ZA 0
ZR 0
Z9 1
U1 1
U2 7
SN 2158-107X
EI 2156-5570
DA 2020-09-28
UT WOS:000568849600032
ER

PT J
AU Jarschel, Tim
   Laroque, Christoph
   Maschke, Ronny
   Hartmann, Peter
TI Practical Classification and Evaluation of Optically Recorded Food Data
   by Using Various Big-Data Analysis Technologies
SO MACHINES
VL 8
IS 2
AR 34
DI 10.3390/machines8020034
DT Article
PD JUN 2020
PY 2020
AB An increasing shortening of product life cycles, as well as the trend
   towards highly individualized food products, force manufacturers to
   digitize their own production chains. Especially the collection,
   monitoring, and evaluation of food data will have a major impact in the
   future on how the manufacturers will satisfy constantly growing customer
   demands. For this purpose, an automated system for collecting and
   analyzing food data was set up to promote advanced production
   technologies in the food industry. Based on the technique of laser
   triangulation, various types of food were measured three-dimensionally
   and examined for their chromatic composition. The raw data can be
   divided into individual data groups using clustering technologies.
   Subsequent indexing of the data in a big data architecture set the
   ground for setting up real-time data visualizations. The cluster-based
   back-end system for data processing can also be used as an
   organization-wide communication network for more efficient monitoring of
   companies' production data flows. The results not only describe the
   procedure for digitization of food data, they also provide deep insights
   into the practical application of big data analytics while helping
   especially small- and medium-sized enterprises to find a good
   introduction to this field of research.
OI Laroque, Christoph/0000-0002-3076-1754
TC 0
ZA 0
ZS 0
Z8 0
ZB 0
ZR 0
Z9 1
U1 0
U2 13
EI 2075-1702
DA 2020-08-04
UT WOS:000551246200011
ER

PT J
AU Vorobev, A. V.
   Vorobeva, G. R.
TI APPROACH TO IMPROVING THE PERFORMANCE OF SOFTWARE PROCESSES FOR
   PROCESSING AND STORING LARGE VOLUMES OF GEOMAGNETIC DATA
SO VESTNIK TOMSKOGO GOSUDARSTVENNOGO UNIVERSITETA-UPRAVLENIE
   VYCHISLITELNAJA TEHNIKA I INFORMATIKA-TOMSK STATE UNIVERSITY JOURNAL OF
   CONTROL AND COMPUTER SCIENCE
IS 50
BP 23
EP 30
DI 10.17223/19988605/50/3
DT Article
PD MAR 2020
PY 2020
AB The issues of increasing the computational speed of software processes
   for the analytical processing of large volumes of geomagnetic data,
   which are the result of continuous monitoring of the parameters of the
   geomagnetic field by a great number of distributed ground magnetic
   stations and observatories, are discussed. A comparative review of the
   existing geomagnetic data architecture (presented in the framework of
   the specified IAGA-2002 format provided by International Association of
   Geomagnetism and Aeronomy), as well as popular data formats is given,
   and arguments are presented in favor of the need to improve the approach
   to organizing the results of geomagnetic observations.
   To solve this problem, a new hybrid format for long-term storage of
   geomagnetic data is presented, represented by a set of three
   interrelated components and characterized in that it uses the rules of
   referential integrity to combine relational, hierarchical and columnar
   data models used to describe metadata and geomagnetic data, and also
   sets POSIX-component addressing structure and implements a combination
   of textual and binary formats for presenting information. The main
   purpose of the proposed architecture is to increase the reactivity of
   software tools for analytic processing of geomagnetic data, on the one
   hand, and reducing the cost of the required amount of physical memory,
   on the other hand.
   The results of the comparison of the proposed hybrid format for
   presenting geomagnetic data with the existing approach to describing
   geomagnetic observation data (IAGA-2002), as well as other common
   formats for presenting large volumes of structured and semi-structured
   data (XML, JSON, Avro, etc.) are presented. In this case, the criteria
   for evaluating the effectiveness of a hybrid format for storing
   geomagnetic data determined the reactivity of software data processing
   and the amount of required disk space for their placement. The results
   of the experiment showed that the proposed format provides a significant
   increase in computing performance (about 4 times), conducted in relation
   to sets of heterogeneous geomagnetic data, and also significantly
   reduces the computational costs associated with their physical storage
   (approximately 5 times).
RI Vorobeva, Gulnara/L-5384-2016; Vorobev, Anei/ABC-8106-2021
OI Vorobeva, Gulnara/0000-0001-7878-9724; Vorobev, Anei/0000-0002-9680-5609
ZB 0
ZA 0
Z8 0
ZR 0
TC 1
ZS 0
Z9 1
U1 0
U2 2
SN 1998-8605
EI 2311-2085
DA 2020-07-07
UT WOS:000542142600003
ER

PT C
AU Bianchini, Devis
   De Antonellis, Valeria
   Garda, Massimiliano
   Melchiori, Michele
BE Hartmann, S
   Kung, J
   Kotsis, G
   Tjoa, AM
   Khalil, I
TI Contextual Preferences to Personalise Semantic Data Lake Exploration
SO DATABASE AND EXPERT SYSTEMS APPLICATIONS, DEXA 2020, PT II
SE Lecture Notes in Computer Science
VL 12392
BP 322
EP 332
DI 10.1007/978-3-030-59051-2_22
DT Proceedings Paper
PD 2020
PY 2020
AB In the latest years, the availability of data collected within Smart
   Cities is enabling citizens to take decisions about their daily life in
   an autonomous way. In this landscape, data aggregation according to
   different analysis dimensions may help users to take decisions,
   leveraging indicators as powerful tools for meaningful exploration.
   However, due to the volume and heterogeneity of Smart City data, data
   lakes have to be used as flexible repositories for enabling data storage
   and organisation. Despite they are usually based on centralisation of
   data storage, data lakes compel to consider pay-as-you-go or on-demand
   solutions, where integration is progressively carried out, to cope with
   the cumbersome nature of Big Data. Given the variety of interested
   users, their goals and preferences on available data, personalised data
   access, as well as representation and use of preferences, are required
   and need to be adapted to the unique characteristics of data lakes. In
   this paper, we describe an approach to model preferences on Smart City
   indicators built on top of a data lake. Preferences are used for
   personalised data exploration. Main contributions of this paper concern:
   (a) the definition of users' preferences and preference constructors
   over the semantic representation of indicators; (b) the definition of
   users' contexts and contextual preferences; (c) preference-based
   personalised exploration of Smart City data.
CT 31st International Conference on Database and Expert Systems
   Applications (DEXA)
CY SEP 14-17, 2020
CL Comenius Univ Bratislava, ELECTR NETWORK
HO Comenius Univ Bratislava
SP Software Competence Ctr Hagenberg; JVU, Inst Telecooperaat; Informat
   Integrat & Web Based Applicat & Serv
RI Melchiori, Michele/AAH-3714-2019; Garda, Massimiliano/
OI Garda, Massimiliano/0009-0006-5823-6595
Z8 0
ZR 0
ZA 0
ZB 0
TC 1
ZS 0
Z9 1
U1 0
U2 3
SN 0302-9743
EI 1611-3349
BN 978-3-030-59051-2; 978-3-030-59050-5
DA 2021-11-21
UT WOS:000716716900022
ER

PT C
AU Cardenas, Irvin Steve
   Paladugula, Pradeep Kumar
   Kim, Jong-Hoon
BE Chakrabarti, S
   Paul, R
   Gill, B
   Gangopadhyay, M
   Poddar, S
TI Large Scale Distributed Data Processing for a Network of Humanoid
   Telepresence Robots
SO 2020 IEEE INTERNATIONAL IOT, ELECTRONICS AND MECHATRONICS CONFERENCE
   (IEMTRONICS 2020)
BP 136
EP 144
DT Proceedings Paper
PD 2020
PY 2020
AB We present an open-source data lake architecture implemented to store
   and process data from robotic systems at large scale. In particular, we
   leverage our architecture for the use case of processing data from a
   network of humanoid telepresence robotic avatars that are controlled by
   human operators wearing immersive telepresence control suits. Our
   architecture leverages well-established open-source technologies and
   integrates into existing robot frameworks and middleware such as Robot
   Operating System (ROS) and Data Distribution Service (DDS).
CT IEEE International IOT, Electronics and Mechatronics Conference
   (IEMTRONICS)
CY SEP 09-12, 2020
CL Vancouver, INDIA
SP IEEE; Inst Engn & Management; IEEE Vancouver Sect; SMART; Univ Engn &
   Management
TC 1
ZA 0
ZR 0
ZS 0
Z8 0
ZB 0
Z9 1
U1 0
U2 0
BN 978-1-7281-9615-2
DA 2021-06-16
UT WOS:000655001800025
ER

PT C
AU Monteiro Joaquim, Joao Luiz
   Mello, Ronaldo dos Santos
BE Indrawan-Santiago, M
   Pardede, E
   Salvadori, IL
   Steinbauer, M
   Khalil, I
   Kotsis, G
TI An Analysis of Confidentiality Issues in Data Lakes
SO 22ND INTERNATIONAL CONFERENCE ON INFORMATION INTEGRATION AND WEB-BASED
   APPLICATIONS & SERVICES (IIWAS2020)
BP 168
EP 177
DI 10.1145/3428757.3429109
DT Proceedings Paper
PD 2020
PY 2020
AB A data lake is a relatively recent technology to maintain and allow
   access to voluminous and heterogeneous data sources. Governments, large
   corporations and startups have increasingly considered it for storing
   useful data and obtain valuable business trends. However, there is still
   a long evolutionary path related to data lake management, where data
   security is an open issue. In this paper we investigate confidentiality
   issues in the context of data lakes, with a focus on authentication and
   authorization. We apply a systematic review methodology focusing on
   approaches that provide some technology for authentication and
   authorization management. In the following, we compare the selected
   studies w.r.t. the used technologies and we also analyze how they are
   positioned w.r.t. a reference architecture for a data lake management
   system. This is the first paper that presents such a kind of analysis
   for data lakes.
CT 22nd Annual International Conference on Information Integration and
   Web-Based Applications and Services (IIWAS)
CY NOV 30-DEC 02, 2020
CL ELECTR NETWORK
SP Assoc Comp Machinery
Z8 0
ZB 0
ZR 0
ZS 0
TC 1
ZA 0
Z9 1
U1 0
U2 3
BN 978-1-4503-8922-8
DA 2020-01-01
UT WOS:000869922700025
ER

PT C
AU Zhang Xiaodong
   Li Ping
   Ma Xiaoning
GP ACM
TI Research on Technical Architecture and Overall Scheme of Railway Block
   Chain Service Platform
SO 2020 THE 3RD INTERNATIONAL CONFERENCE ON BLOCKCHAIN TECHNOLOGY AND
   APPLICATIONS, ICBTA 2020
BP 55
EP 60
DI 10.1145/3446983.3446990
DT Proceedings Paper
PD 2020
PY 2020
AB The block chain has many unique characteristics, like decentralization,
   stability, openness and transparency. We can apply block chain in many
   fields of railway to improve the comprehensive efficiency and benefit,
   such as railway passenger and freight service, engineering construction,
   big data sharing and so on. With the gradual completion of data service
   platform, big data lake and cloud computing in China Railway, these
   foundations can support the railway block chain application. The paper
   analyzed the development status and existing problems of block chain at
   home and abroad. Through the multidimensional analysis for the
   mainstream block chain platform architecture, the thesis gets the
   architecture selection of the railway block chain platform. Based on the
   hyperledger fabric, we propose the technical architecture and overall
   scheme for the railway block chain service platform respectively. The
   paper also designs the implementation path for the platform application
   construction. This research plays an important role and significance in
   promoting the block chain technology applying and developing in the
   railway field.
CT 3rd International Conference on Blockchain Technology and Applications
   (ICBTA)
CY DEC 14-16, 2020
CL ELECTR NETWORK
ZA 0
Z8 0
ZB 0
ZR 0
TC 0
ZS 0
Z9 1
U1 1
U2 13
BN 978-1-4503-8896-2
DA 2020-01-01
UT WOS:001086200100010
ER

PT C
AU Garcia, Ivan
   Martinez-Prieto, Miguel A.
   Bregon, Anibal
   Alvarez, Pedro C.
   Diaz, Fernando
BE Bernadino, J
   Quix, C
   Filipe, J
TI Towards a Scalable Architecture for Flight Data Management
SO PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON DATA SCIENCE,
   TECHNOLOGY AND APPLICATIONS (DATA)
BP 263
EP 268
DI 10.5220/0006473402630268
DT Proceedings Paper
PD 2017
PY 2017
AB The dramatic growth in the air traffic levels witnessed during the last
   two decades has increased the interest for optimizing the Air Traffic
   Management (ATM) systems. The main objective is being able to cope with
   the sustained air traffic growth under safe, economic, efficient and
   environmental friendly working conditions. The ADS-B (Automatic
   Dependent Surveillance - Broadcast) system is part of the new air
   traffic control systems, since it allows to substitute the secondary
   radar with cheaper ground stations that, at the same time, provide more
   accurate real-time positioning information. However, this system
   generates a large volume of data that, when combined with other
   flight-related data, such as flight plans or weather reports, faces
   scalability issues. This paper introduces an (on-going) Data Lake based
   architecture which allows the full ADS-B data life-cycle to be supported
   in a scalable and cost-effective way using technologies from the Apache
   Hadoop ecosystem.
CT 6th International Conference on Data Science, Technology and
   Applications (DATA)
CY JUL 24-26, 2017
CL Madrid, SPAIN
SP INSTICC; Univ Reu Juan Carlos
RI Martínez-Prieto, Miguel A/A-4891-2011; Diaz-Gomez, Fernando/; Diaz, Fernando/Y-6802-2019; Bregon, Anibal/R-6700-2018
OI Martínez-Prieto, Miguel A/0000-0003-4418-561X; Diaz-Gomez,
   Fernando/0000-0002-1767-5130; Bregon, Anibal/0000-0001-6752-1159
TC 1
ZB 0
ZR 0
ZA 0
ZS 0
Z8 0
Z9 1
U1 0
U2 1
BN 978-989-758-255-4
DA 2017-01-01
UT WOS:000852726200030
ER

PT C
AU Hou, Jie
   Zhang, Xi-kun
   Wang, Yao-gang
BE Cai, N
TI SVM-Based Clinical Decision Support Algorithm under Medical Big Data
   Architecture
SO COMPUTER SCIENCE AND TECHNOLOGY (CST2016)
BP 936
EP 945
DT Proceedings Paper
PD 2017
PY 2017
AB Big Data technology has experienced rapid development in recent years
   and has demonstrated its effectiveness when implemented inclinical
   diagnostic decision support. Current research of big data focuses
   largely on data storage architecture, the exploration on big clinical
   data mining and decision making strategy is relatively less. This paper
   discusses the parallel big medical data mining method under Spark
   architecture and proposes the optimized support vector machine (PCSVM)
   algorithm to provide qualitative data for clinical diagnostic decision
   support.
CT International Conference on Computer Science and Technology (CST)
CY JAN 08-10, 2016
CL Shenzhen, PEOPLES R CHINA
TC 1
ZA 0
ZR 0
ZS 0
Z8 0
ZB 0
Z9 1
U1 1
U2 6
BN 978-981-314-642-6; 978-981-3146-41-9
DA 2018-09-12
UT WOS:000443429900105
ER

PT J
AU Nupairoj, Natawut
   Tangsatjatham, Pittayut
TI Hybrid Big Data Architecture for High-Speed Log Anomaly Detection
SO JOURNAL OF INTERNET TECHNOLOGY
VL 18
IS 7
BP 1681
EP 1688
DI 10.6138/JIT.2017.18.7.20170419d
DT Article; Proceedings Paper
PD 2017
PY 2017
AB Anomaly detection in network traffic can be very challenging, especially
   for environments with high-speed networks and lots of servers. In these
   environments, log data of network traffic is usually large, coming at
   highspeed, and have various formats, the classic case of big data
   problem. This makes anomaly detection very difficult due to the fact
   that to get good accuracy, large amount of data must be processed in
   real-time. To solve this problem, this paper proposes a hybrid
   architecture for network traffic anomaly detection using popular big
   data framework including Apache Spark and Apache Flume. To demonstrate
   the capabilities of our proposed solution, we implement a SARIMA-based
   anomaly detection as a case study. The experimental results clearly
   indicated that our proposed architecture allows anomaly detection with
   good accuracy in large-scale environment effectively.
CT 13th International Joint Conference on Computer Science and Software
   Engineering (JCSSE)
CY JUL 13-15, 2016
CL Khon Kaen, THAILAND
ZS 0
ZB 0
ZR 0
TC 1
Z8 0
ZA 0
Z9 1
U1 0
U2 15
SN 1607-9264
EI 2079-4029
DA 2018-02-08
UT WOS:000423459000021
ER

PT C
AU Shlyuger, Gregory
BE Ternovskiy, IV
   Chin, P
TI Apply analytical grid processing to sensor data collections
SO CYBER SENSING 2017
SE Proceedings of SPIE
VL 10185
AR UNSP 101850G
DI 10.1117/12.2269513
DT Proceedings Paper
PD 2017
PY 2017
AB Computer security, information security and event management (SIEM) and
   non-event based raw data (NERD) is a feed activity for modern cyber
   domain network architecture. Each type of cyber domain such as Software
   Defined Networks, Virtualization, Service Orchestration or Cloud/Elastic
   computers, essential carryover characteristics. Each cyber domain might
   have slightly different properties. Enrichment NERD and SIEM models with
   Raw Activity Event Data allowed transformation the raw sensor flowing
   through the system into enriched data elements that are both descriptive
   and predictive in nature. This paper detail some scenarios for evidence
   collection, parsing, enrichment, the implementation k-Nearest Neighbor
   (kNN) classifier as a proof of concept (POC) for Apache Metron cyber
   security framework. For anomaly detection on Hadoop, utilizing Data
   Lake, data science and machine learning algorithm indicate this is a
   viable approach towards collecting, analyzing sensor data and analytical
   grid processing in a complex and ambiguous environment.
CT Conference on Cyber Sensing
CY APR 11, 2017
CL Anaheim, CA
SP SPIE
ZS 0
ZA 0
Z8 0
ZR 0
TC 1
ZB 0
Z9 1
U1 0
U2 3
SN 0277-786X
EI 1996-756X
BN 978-1-5106-0871-9; 978-1-5106-0872-6
DA 2017-12-25
UT WOS:000417464000013
ER

PT C
AU Coneglian, Caio Saraiva
   Fusco, Elvis
   Santarem Segundo, Jose Eduardo
BE Ramachandran, M
   Wills, G
   Walters, R
   Munoz, VM
   Chang, V
TI Semantic Agent in the Context of Big Data Usage in Ontological
   Information Retrieval in Scientific Research
SO IOTBD: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTERNET OF THINGS
   AND BIG DATA
BP 324
EP 330
DI 10.5220/0005875703240330
DT Proceedings Paper
PD 2016
PY 2016
AB The evolution of information technology caused an expansion in the
   amount of data available on the internet. Moreover, such developments
   demanded that new tools were created to allow processing at high
   velocity, trying various informational sources. In this context, in
   flocking to the three V (Velocity, Variety and Volume), emerged the
   phenomenon called Big Data. From the emergence of this phenomenon, the
   need to generate new architectures that allow that users, enjoy the high
   volume of data spread throughout the Web. One way to improve the
   processes carried out, insert the question of semantic information
   processing, in which the use of domain ontologies can expand as
   computational agents interpret the meaning of the data. Thus, this paper
   aims to present a proposal for architecture that places the elements of
   Big Data and semantic, seeking to insert a model that is adapted to the
   current computing needs. As proof of concept performed the
   implementation of the architecture, exploring the question of scientific
   research, where a user is assisted to find relevant information in
   academic databases. Through the implementation, it was found that the
   use ontologies in a Big Data architecture, significantly improves the
   recovery of information performed by computational agents.
CT International Conference on Internet of Things and Big Data (IoTBD)
CY APR 23-25, 2016
CL Rome, ITALY
RI Santarem Segundo, José Eduardo/F-6688-2012
OI Santarem Segundo, José Eduardo/0000-0003-3360-7872
TC 1
Z8 0
ZB 0
ZA 0
ZS 0
ZR 0
Z9 1
U1 0
U2 6
BN 978-989-758-183-0
DA 2016-01-01
UT WOS:000391100400035
ER

PT C
AU Morales, Antonio
   Canovas-Segura, Bernardo
   Campos, Manuel
   Juarez, Jose M.
   Palacios, Francisco
BE Luaces, O
   Gamez, JA
   Barrenechea, E
   Troncoso, A
   Galar, M
   Quintian, H
   Corchado, E
TI Proposal of a Big Data Platform for Intelligent Antibiotic Surveillance
   in a Hospital
SO ADVANCES IN ARTIFICIAL INTELLIGENCE, CAEPIA 2016
SE Lecture Notes in Artificial Intelligence
VL 9868
BP 261
EP 270
DI 10.1007/978-3-319-44636-3_24
DT Proceedings Paper
PD 2016
PY 2016
AB From a technological point of view two kinds of requirements must be
   taken into account when implementing Clinical Decision Support Systems
   (CDSSs) for antibiotic surveillance in a hospital. First, Artificial
   Intelligence (AI) technologies are usually applied to represent and
   reason about existing clinical knowledge, but also to discover new one
   from raw data. Second, at a global decision level, representative
   applications of Business Intelligence (BI) must be also considered. The
   present work introduces the design and implementation of a CDSS platform
   that integrates both AI and BI technologies to assist clinicians in the
   rational use of antibiotics in a hospital. The choice of a Hadoop based
   Big Data architecture provides a suitable solution for the problem of
   integrating, processing and analysing large sets of clinical data. The
   platform facilitates the daily follow-up of antibiotic therapies and
   infections while offering various decision support modules at both
   patient and global level. The system is being tested and evaluated in a
   university hospital.
CT 17th Conference of the Spanish-Association-for-Artificial-Intelligence
   (CAEPIA)
CY SEP 14-16, 2016
CL Salamanca, SPAIN
SP Spanish Assoc Artificial Intelligence; BISITE; Univ Salamanca; Springer
   Team; AEPIA
RI Cánovas-Segura, Bernardo/GWB-9323-2022; MORALES NICOLÁS, ANTONIO/; Juarez, Jose/K-8042-2017; Palacios, Francisco/AAW-5563-2021; Campos, Manuel/H-5226-2015
OI Cánovas-Segura, Bernardo/0000-0002-0777-0441; MORALES NICOLÁS,
   ANTONIO/0000-0002-0872-5351; Campos, Manuel/0000-0002-5233-3769
ZB 1
ZS 0
TC 1
ZR 0
Z8 0
ZA 0
Z9 1
U1 0
U2 5
SN 0302-9743
EI 1611-3349
BN 978-3-319-44636-3; 978-3-319-44635-6
DA 2016-12-07
UT WOS:000387750600024
ER

PT C
AU Rathore, M. Mazhar
   Ahmad, Awais
   Paul, Anand
   Daniel, Alfred
GP IEEE
TI Hadoop Based Real-Time Big Data Architecture for Remote Sensing Earth
   Observatory System
SO 2015 6TH INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND
   NETWORKING TECHNOLOGIES (ICCCNT)
BP 204
EP 210
DT Proceedings Paper
PD 2015
PY 2015
AB Recently, Big Data analytics emerged as a hot topic because of the
   incredible growth of the information and communication technology. One
   of the exceedingly anticipated key contributors of the Big Data is
   real-time Earth Observatory System (EOS). Although the data generated by
   the individual satellite in EOS may not be significant, the overall data
   generated across numerous satellites may yield to the significant amount
   of the Big Data. Thus, extracting the useful information in an efficient
   manner leads a system towards major computational challenges in EOS,
   such as, to analyze, to aggregate, and to store, where data is remotely
   collected. Therefore, the paper proposes a set of requirements for
   achieving pervasive, integrated information system of EOS and associated
   services (real-time and offline data processing). The Big Data
   Architecture is also proposed to address all the aspect of the Big Data
   ecosystem and includes the following components: Data Acquisition Unit,
   Data Processing Unit, Data Storage Unit, and Data Analysis and Decision
   Unit. The proposed architecture is termed as Holistic as it considers
   the flow of data from satellites to services, which is designed for
   efficiently process and analyze the Big Data. Finally, a detailed
   analysis of remotely sensed earth observatory Big Data for Land and Sea
   area are provided using UBUNTU 14.04 LTS core (TM) i5 machine with 3.2
   GHz processor and 4 GB memory. The results show that the proposed
   network architecture efficiently process EOS data at a real-time as well
   as offline.
CT 6th International Conference on Computing, Communication and Networking
   Technologies (ICCCNT)
CY JUL 13-15, 2015
CL Denton, TX
SP IEEE Comp Soc; Univ N Texas
RI Ahmad, Awais/AAA-4504-2019; Paul, Anand/V-6724-2017; Daniel, Alfred/O-1875-2017
OI Daniel, Alfred/0000-0003-0602-3425
ZR 0
ZA 0
TC 1
Z8 0
ZB 0
ZS 0
Z9 1
U1 0
U2 2
BN 978-1-4799-7983-7
DA 2016-09-28
UT WOS:000381568500080
ER

PT C
AU Yan, Yin-Zhen
   Liu, Ren-Hao
   Yang, Chao-Tung
   Chen, Shuo-Tsung
GP IEEE
TI Cloud City Traffic State Assessment System Using a Novel Architecture of
   Big Data
SO 2015 INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND BIG DATA (CCBD)
SE International Conference on Cloud Computing and Big Data-CCBD
BP 252
EP 259
DI 10.1109/CCBD.2015.58
DT Proceedings Paper
PD 2015
PY 2015
AB Recently, big data are widely applied to different field. This work
   presents a cloud city traffic state assessment system using a novel
   architecture of big data. The proposed system provides the real-time bus
   location and real-time traffic situation, especially the real-time
   traffic situation nearby, through open data, GPS, GPRS and cloud
   technologies. With the high-scalability cloud technologies, Hadoop and
   Spark, the proposed system architecture is first implemented
   successfully and efficiently. Next, we utilize three clustering methods,
   DBSCAN, K-Means, and Fuzzy C-Means to find the area of traffic jam in
   Taichung city and moving average to find the area of traffic jam in
   Taiwan Boulevard which is the main road in Taichung city. Finally,
   experimental results show the effectiveness and efficiency of the
   proposed system services via an advanced web technology. In addition,
   some experimental results indicate that the computing ability of Spark
   is better than that of Hadoop.
CT International Conference on Cloud Computing and Big Data
CY NOV 04-06, 2015
CL Shanghai, PEOPLES R CHINA
SP IEEE Comp Soc; Chinese I Elect; Chinese I Elect Cloud Comp Ass Taiwan; E
   China Normal U; Cloud Comp Experts Committee Chinese I Elect; China Big
   Data Experts Committee; China Cloud Comp Tech Ind Alliance
RI Yang, Chao-Tung/B-4562-2009; Chen, Shuo-Tsung/LMP-9707-2024
OI Yang, Chao-Tung/0000-0002-9579-4426; 
ZS 0
Z8 0
ZA 0
TC 0
ZR 0
ZB 0
Z9 1
U1 0
U2 2
SN 2378-3680
BN 978-1-4673-8350-9
DA 2016-09-13
UT WOS:000380552500038
ER

PT C
AU Madkour, Amgad
   Aref, Walid G.
   Basalamah, Saleh
BE Hu, X
   Lin, TY
   Raghavan, V
   Wah, B
   BaezaYates, R
   Fox, G
   Shahabi, C
   Smith, M
   Yang, Q
   Ghani, R
   Fan, W
   Lempel, R
   Nambiar, R
TI Knowledge Cubes - A Proposal for Scalable and Semantically-Guided
   Management of Big Data
SO 2013 IEEE INTERNATIONAL CONFERENCE ON BIG DATA
SE IEEE International Conference on Big Data
DT Proceedings Paper
PD 2013
PY 2013
AB A Knowledge Cube, or cube for short, is an intelligent and adaptive
   database instance capable of storing, analyzing, and searching data.
   Each cube is established based on semantic aspects, e.g., (1) Topical,
   (2) Contextual, (3) Spatial, or (4) Temporal. A cube specializes in
   handling data that is only relevant to the cube's semantics. Knowledge
   cubes are inspired by two prime architectures: (1) Dataspaces that
   provides an abstraction for data management where heterogeneous data
   sources can co-exist and it requires no prespecified unifying schema,
   and (2) Linked Data that provides best practices for publishing and
   interlinking structured data on the web. A knowledge cube uses Linked
   Data as its main building block for its data layer and encompasses some
   of the data integration abstractions defined by Dataspaces. In this
   paper, knowledge cubes are proposed as a semantically-guided data
   management architecture, where data management is influenced by the data
   semantics rather than by a predefined scheme. Knowledge cubes support
   the five pillars of Big Data also known as the five V's, namely Volume,
   Velocity, Veracity, Variety, and Value. Interesting opportunities can be
   leveraged when learning the semantics of the data. This paper highlights
   these opportunities and proposes a strawman design for knowledge cubes
   along with the research challenges that arise when realizing them.
CT IEEE International Conference on Big Data (Big Data)
CY OCT 06-09, 2013
CL Santa Clara, CA
SP IEEE; IEEE Comp Soc; CCF; Yahoo Labs; CISCO
RI Aref, Walid/D-4403-2019
ZA 0
ZR 0
Z8 0
ZB 0
TC 1
ZS 0
Z9 1
U1 0
U2 5
SN 2639-1589
BN 978-1-4799-1292-6; 978-1-4799-1293-3
DA 2014-03-19
UT WOS:000330831300256
ER

PT C
AU Qiu, Robin G.
   Lee, Doris
BE Babu, MSP
   Wenzheng, L
TI Transformative Education Web 2.0 Systems for Enriching High School STEM
   Education
SO PROCEEDINGS OF 2013 IEEE 4TH INTERNATIONAL CONFERENCE ON SOFTWARE
   ENGINEERING AND SERVICE SCIENCE (ICSESS)
SE International Conference on Software Engineering and Service Science
BP 352
EP 356
DT Proceedings Paper
PD 2012
PY 2012
AB There are currently over a hundred STEM (Science, Technology,
   Engineering, and Mathematics) programs in the U. S., aimed at
   encouraging students to enter a STEM-related career. Of the fields
   encompassed by STEM, mathematics is the foundation. However, American
   teens have demonstrated less ability and aptitude for mathematics than
   the teens in many other countries. Efforts to reduce this ability gap
   have focused on building a better communal structure within schools,
   creating more cohesive math curricula, offering smaller classes,
   encouraging cooperative learning, and helping students overcome math
   anxiety at school. There is very little published literature on the
   potential of guided cyber-based self-learning in STEM education,
   although there are a variety of online STEM learning web sites available
   for students to access at or after school. In particular, no scientific
   approach exists to address STEM education challenges in an
   outside-school setting. The paper briefly unveil an approach to
   transformatively addressing how mathematics learning, outside of a
   formal high school setting, could contribute to students' math aptitude.
   A Web 2.0 system has been deployed, focusing on promoting students'
   collaborative learning. A big data architecture is applied to addressing
   voluminous data, which ensures that unstructured Web 2.0 data are always
   ready for analysis in real time.
CT 4th IEEE International Conference on Software Engineering and Service
   Science (ICSESS)
CY MAY 23-25, 2013
CL Beijing, PEOPLES R CHINA
SP IEEE
TC 1
ZR 0
ZB 0
ZS 0
Z8 0
ZA 0
Z9 1
U1 0
U2 7
SN 2327-0594
BN 978-1-4673-5000-6
DA 2012-01-01
UT WOS:000346301300080
ER

PT J
AU Iglesias, Felix
   Ros, Frederic
   Thuy, Lynh Hoang Vy
   Gourcy, Laurence
   Moquet, Jean-Sebastien
   Daele, Veronique
   Dupraz, Sebastien
TI A conceptual architecture for AI-assisted Digital Twins in natural
   resource management
SO ECOLOGICAL INFORMATICS
VL 94
AR 103635
DI 10.1016/j.ecoinf.2026.103635
EA JAN 2026
DT Article
PD MAR 2026
PY 2026
AB The management of natural resources is increasingly critical and
   challenging due to complex interactions among environmental, industrial,
   and societal processes. Traditional approaches often fail to integrate
   heterogeneous data, limiting predictive and decision-support
   capabilities. This study presents a conceptual architecture for an
   Artificial Intelligence (AI)-assisted Digital Twin (DT) of the
   Centre-Val de Loire region, designed to unify time-dependent
   multi-source data. Based on the ENVRI Reference Model, it covers
   Science, Information, Computational, Engineering, and Technology layers,
   defining standardized data exchange, communication protocols, and
   prototype functionalities. A proof of concept FIWARE implementation
   supports ingestion, monitoring and analytical services for piezometric
   and meteorological data, exemplified through groundwater dynamics in the
   Beauce aquifer. It integrates daily observations from 53 piezometric
   stations over more than five years, managing approximately 2.8 million
   records in a containerized environment. Results show that the proposed
   DT architecture can enhance sustainability-oriented decision making,
   integrating heterogeneous data and predictive analyses while enabling
   collaboration across scientific and technical domains. Its modular
   design offers a replicable template for future AI-assisted environmental
   DTs, scalable to larger regions. Hence, this work illustrates how DTs
   can improve environmental monitoring and understanding, providing a
   pathway toward resilient, data-driven management of natural resources.
RI ROS, Frédéric/V-2884-2019; Gourcy, Laurence/AAJ-6717-2020
Z8 0
ZR 0
ZB 0
TC 0
ZA 0
ZS 0
Z9 0
U1 0
U2 0
SN 1574-9541
EI 1878-0512
DA 2026-02-13
UT WOS:001681983600001
ER

PT P
AU SAHAY S
   KUMAR S
   LALLY M
   SAHAY P K
   REGE A
TI Method for determining account holder identities            for
   collected event data of credit events in e.g.            lenders, by
   using e.g. credit data system, involves            associating inverted
   personal identifier with            identified event data in merged set
   associated with            unique user
PN US2026030279-A1
AE EXPERIAN INFORMATION SOLUTIONS INC
AB 
   NOVELTY - The method (1200) involves receiving event                data
   that comprises heterogeneous data structures                from
   multiple data sources by one or multiple                computing
   devices comprising one or multiple                hardware processors
   and specific                computer-executable instructions (1202).    
   Source-specific parameters defining identifiers                available
   from a corresponding data source for each                event data are
   accessed. One or multiple                identifiers are extracted
   (1204) from the event                data based on the source-specific
   parameters.                Multiple hashes are calculated by evaluating 
   multiple hash functions associated with combination                of
   identifiers for each unique identity. The unique               
   identities are selectively grouped (1210) into sets                of
   unique identities associated with common hashed                based on
   whether the unique identities share common                hash
   calculated with a common hash function.
   USE - Method for determining account holder                identities
   for collected event data of credit                events in financial
   institutions e.g. lenders, by                using a computing system
   e.g. credit data system,                and a high volume data analyzing
   system. Uses                include but are not limited to credit card  
                providers, banks, car dealers and brokers.
   ADVANTAGE - The method enables efficiently organizing               
   heterogeneous data elements associated with the                users at
   a massive scale, providing real-time                access to historical
   data elements of the users                that has not previously been
   available, safely                entering into business transaction
   based on credit                scores, providing credit data based on
   smart and                efficient credit data architecture, reducing   
   dimensionality of the unique identities with                multiple
   dimensionality reduction processes, and                performing a
   batch indexing process to reduce or                eliminate significant
   computing resource overhead                associated with extract,
   transform and load (ETL)                of heterogeneous formats to cut
   down processing                overhead.
   DETAILED DESCRIPTION - One or multiple match rules including criteria   
   for comparing the unique identities within the set                are
   applied (1212) for each set of unique                identities. A
   matching set of unique identities is                determined when
   satisfying the match rules. The                matching sets of unique
   identities including the                common unique identity are
   merged (1214) to provide                the merged sets comprising no
   unique identity in                common with other merged sets by
   repeating until                the matching sets are merged. An inverted
   personal                identifier is determined for each merged set.
   The                inverted personal identifier is associated (1218)    
   to each unique identity in the merged set to create                an
   inverted personal identifier map. Event data                associated
   with the combinations of identifiers                associated with the
   unique identity is identified                for each unique identity
   using the inverted                personal identifier map. The inverted
   personal                identifier is associated (1220) with the
   identified                event data in the merged set associated with a
   unique user.The identifiers are selected from first name,               
   last name, middle initial, middle name, date of                birth,
   social security number, taxpayer identifiers                or national
   identifiers.The hash functions comprise locality sensitive              
   hashing.The event data comprises data about a credit               
   event.An INDEPENDENT CLAIM is included for                non-transitory
   computer-readable storage media                including instructions
   for determining account                holder identities for collected
   event data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for determining account holder
   identities                for collected event data of credit events in  
   financial institutions by using a computing               
   system.1200Method for determining account holder               
   identities for collected event data of credit                events in
   financial institutions by using computing               
   system1202Receiving event data that comprises               
   heterogeneous data structures from multiple data                sources
   by one or multiple computing devices                comprising one or
   multiple hardware processors and                specific
   computer-executable instructions1204Extracting one or multiple
   identifiers                from event data based on source-specific     
   parameters1210Selectively grouping unique identities                into
   sets of unique identities associated with                common hashed
   based on whether unique identities                share common hash
   calculated with common hash                function1212Applying one or
   multiple match rules                including criteria for comparing
   unique identities                within set for each set of unique
   identities1214Merging matching sets of unique                identities
   including common unique identity to                provide merged sets
   comprising no unique identity                in common with other merged
   sets by repeating until                matching sets are
   merged1218Associating inverted personal identifier                to
   each unique identity in merged set to create                inverted
   personal identifier map1220Associating inverted personal identifier     
   with identified event data in merged set associated                with
   unique user
Z9 0
U1 0
U2 0
DA 2026-02-08
UT DIIDW:202611383P
ER

PT P
AU BAO S
   XIAO J
   ZHENG Z
   YANG S
   LI W
TI Method for quickly evaluating mineral resource            potential
   based on large data, involves outputting            comprehensive
   mineral resource potential evaluation            graph to visual
   interaction platform, and providing            uncertainty quantization
   index and data traceability            path of evaluation result
PN CN121413916-A
AE QINGHAI NONFERROUS NO 3 GEOLOGICAL EXPLO
AB 
   NOVELTY - The method involves obtaining original mineral               
   data from different data sources by a multi-source                data
   access interface module. The original mineral                data is
   provided with geological map scanning file,                remote
   sensing satellite image grid data,                geophysical
   exploration curve data, geophysical                sampling point
   position data and mine history                production recording table
   data. Intelligent format                identification and structured
   analysis are                performed on the original mineral data.
   Structured                and analyzed multi-source data is input to a  
   distributed data lake storage layer. A                comprehensive
   mineral resource potential evaluation                graph is output to
   a visual interaction platform. A                user is supported
   according to mine, potential                level and space range for
   dynamic screening and                three-dimensional sectioning
   display. An                uncertainty quantization index and data      
     traceability path of an evaluation result are                provided.
   USE - Method for quickly evaluating mineral resource               
   potential based on large data.
   ADVANTAGE - The evaluation period is shortened from               
   several weeks to several hours, the data processing               
   accuracy is improved to more than 97 percent, the               
   evaluation result in the known mine bed area hit                rate
   reaches 89 percent, which improves the mining                exploration
   efficiency and decision                scientificity.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a mineral   
   resource potential fast evaluation system based on                big
   data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for quickly evaluating mineral
   resource                potential based on large data. (Drawing includes
                  non-English language text).
Z9 0
U1 0
U2 0
DA 2026-02-15
UT DIIDW:2026127703
ER

PT P
AU HE J
   SUN Y
   WEI L
   GENG N
   LI S
   HUANG C
TI Hydropower station power production management            system based
   on industrial digitization, has decision            support layer for
   generating scheduling solution and            risk pre-warning based on
   artificial intelligence and            knowledge map
PN CN121414170-A
AE CWE ELECTRIC POWER DEV CO LTD; CHINA INT WATER & ELECTRIC CORP
AB 
   NOVELTY - The system has a service application layer               
   provided with an investment financing management                module,
   an intelligent building module, an item                management module
   and an intelligent operation and                maintenance module. The
   intelligent operation and                maintenance module is provided
   with a device state                monitoring sub-module, an AI fault
   prediction                sub-module and an energy efficiency
   optimization                sub-module. A data collecting layer is used
   for                collecting operation, construction and operation     
   and maintenance data of a hydropower station in                real
   time. A data management and standard layer is                used for
   establishing a uniform data lake and data                platform,
   cleaning, converting, storing and                authority managing
   multi-source heterogeneous data,                and realizing homologous
   connection of the data. A                decision support layer
   generates scheduling                solution and risk pre-warning based
   on AI and                knowledge map. A security and network layer is
   used                for ensuring security and stability of              
    transnational data transmission.
   USE - System for managing power production of a               
   hydropower station based on industrial                digitization.
   ADVANTAGE - The system realizes inter-department and               
   inter-link data interconnection and                intercommunication by
   establishing uniform data                center and standard governing
   system. The system                combines the BIM, GIS and IoT device
   to construct a                digital twin model and introduce AI
   intelligent                prediction and dynamic scheduling mechanism,
   which                improves the automation and refinement level of    
   construction and operation and maintenance. The                decision
   supporting layer is integrated with a                knowledge map and a
   risk pre-warning model to                realize the dynamic monitoring
   and timely response                to the project progress, cost and
   safety risk in                the overseas complex environment so as to
   obviously                improve the operation efficiency, safety and
   risk                controllability of the hydropower station.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   hydropower station power production management                method
   based on industrial digitization.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   hydropower station power production management                system
   based on industrial digitization. (Drawing                includes
   non-English language text).
Z9 0
U1 0
U2 0
DA 2026-02-15
UT DIIDW:202612914F
ER

PT P
AU SHAO C
   FANG J
   WENG Q
   LIU C
   CHEN Y
   WU H
   YANG K
TI Method for collecting, storing and managing            all-element data
   of offshore wind power station,            involves constructing graph
   database that is connected            with data convergence management
   platform and mass            heterogeneous data lake storage system
PN CN121412210-A
AE GUANGDONG BANGXIN DATA TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves constructing a data               
   convergence management platform. The data                convergence
   management platform is used for                uniformly managing data
   source access, data storage                and data analysis. The data
   source of the ith type                is configured. The original data
   is pre-processed.                The tag set is received. The label set
   is received.                The identification data source is received.
   The                data type, time stamp and data attribute are         
   received. The real-time data stream is received.                The real
   time data stream is received. The time                stamp is received.
   The real-time data stream                satisfying the preset condition
   is selected. The                mass heterogeneous data lake storage
   system is used                for data access and data inquiry. The mass
   heterogeneous data lake storage system is connected                with
   the data convergence management platform. The                graph
   database is constructed. The graph database                is connected
   with the data convergence management                platform and the
   mass heterogeneous data lake                storage system.
   USE - Method for collecting, storing and managing               
   all-element data of offshore wind power station.                Uses
   include but are not limited to operation                monitoring,
   environment observation, power output,                operation and
   maintenance ship and submarine cable                state.
   ADVANTAGE - The method enables realizing uniform access,               
   storage and analysis of multi-source heterogeneous                data
   of offshore wind power station, and solving                the problem
   that the traditional database and                storage solution are
   difficult to deal with mass                real-time data.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a marine    
   wind power station full-factor data aggregation                storage
   management system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method  
   for managing all-element data aggregation storage                of
   offshore wind power station. (Drawing includes               
   non-English language text).
Z9 0
U1 0
U2 0
DA 2026-02-15
UT DIIDW:202613121G
ER

PT J
AU Trujillo, Roxana
   Solar, Mauricio
TI Incremental Data Cube Architecture for Sentinel-2 Time Series:
   Multi-Cube Approaches to Dynamic Baseline Construction
SO REMOTE SENSING
VL 18
IS 2
AR 260
DI 10.3390/rs18020260
DT Article
PD JAN 14 2026
PY 2026
AB Highlights What are the main findings? We introduce a Multi-Cube
   architecture for Sentinel-2 time series that integrates spatial hashing,
   dynamic baseline policies, intelligent parallelism and automated
   subdivision of large areas into independent, temporally consistent cubes
   using Zarr-based storage. This design enables fully incremental and
   scalable data cube construction. Applied to 83,755 Sentinel-2 Level-2A
   images (2016-2024), the framework achieves 5.4x faster end-to-end
   processing and over two orders of magnitude less disk Input/Output than
   a conventional sequential pipeline, thanks to its incremental update
   engine and optimized multi-tile handling strategy. What are the
   implications of the main finding? The framework provides interoperable
   cubes that grow incrementally and remain independent of analytical
   methods, allowing flexible integration with diverse workflows. This
   design offers a future oriented backbone for cloud-native EO systems,
   enabling analysts to adopt or replace algorithms without restructuring
   the data architecture and supporting reproducibility, long-term
   maintainability and seamless integration while also benefiting from
   shorter processing times. Beyond incremental ingestion, the architecture
   addresses the challenge of preserving temporal consistency across
   multi-tile areas by automatically generating stable and hash indexed
   units with coherent temporal baselines. This reduces mosaicking
   artifacts and configuration overhead, enabling reliable multi-year
   monitoring with minimal manual intervention.Highlights What are the main
   findings? We introduce a Multi-Cube architecture for Sentinel-2 time
   series that integrates spatial hashing, dynamic baseline policies,
   intelligent parallelism and automated subdivision of large areas into
   independent, temporally consistent cubes using Zarr-based storage. This
   design enables fully incremental and scalable data cube construction.
   Applied to 83,755 Sentinel-2 Level-2A images (2016-2024), the framework
   achieves 5.4x faster end-to-end processing and over two orders of
   magnitude less disk Input/Output than a conventional sequential
   pipeline, thanks to its incremental update engine and optimized
   multi-tile handling strategy. What are the implications of the main
   finding? The framework provides interoperable cubes that grow
   incrementally and remain independent of analytical methods, allowing
   flexible integration with diverse workflows. This design offers a future
   oriented backbone for cloud-native EO systems, enabling analysts to
   adopt or replace algorithms without restructuring the data architecture
   and supporting reproducibility, long-term maintainability and seamless
   integration while also benefiting from shorter processing times. Beyond
   incremental ingestion, the architecture addresses the challenge of
   preserving temporal consistency across multi-tile areas by automatically
   generating stable and hash indexed units with coherent temporal
   baselines. This reduces mosaicking artifacts and configuration overhead,
   enabling reliable multi-year monitoring with minimal manual
   intervention.Abstract Incremental computing is becoming increasingly
   important for processing large-scale datasets. In satellite imagery,
   spatial resolution, temporal depth, and large files pose significant
   computational challenges, requiring efficient architectures to manage
   processing time and resource usage. Accordingly, in this study, we
   propose a dynamic architecture, termed Multi-Cube, for optical satellite
   time series.
   The framework introduces a modular and baseline-aware approach that
   enables scalable subdivision, incremental growth, and consistent
   management of spatiotemporal data. Built on NetCDF, xarray, and Zarr,
   Multi-Cube automatically constructs stable multidimensional data cubes
   while minimizing redundant reprocessing, formalizing automated internal
   decisions governing cube subdivision, baseline reuse, and incremental
   updates to support recurrent monitoring workflows. Its performance was
   evaluated using more than 83,000 Sentinel-2 images (covering 2016-2024)
   across multiple areas of interest. The proposed approach achieved a 5.4x
   reduction in end-to-end runtime, decreasing execution time from 53 h to
   9 h, while disk I/O requirements were reduced by more than two orders of
   magnitude compared with a traditional sequential reprocessing pipeline.
   The framework supports parallel execution and on-demand sub-cube
   extraction for responsive large-area monitoring while internally
   handling incremental updates and adaptive cube management without
   requiring manual intervention. The results demonstrate that the
   Multi-Cube architecture provides a decision-driven foundation for
   integrating dynamic Earth observation workflows with analytical modules.
ZA 0
Z8 0
ZS 0
TC 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
EI 2072-4292
DA 2026-02-01
UT WOS:001671514200001
ER

PT P
AU LIU Y
   MA Y
   SUN G
   TIAN H
   TIAN F
TI Method for managing human resource data based on            big data,
   involves inputting target characteristic            vector into trained
   departure risk prediction model,            and outputting prediction
   result
PN CN121329354-A
AE JINAN TIANGONG HUMAN RESOURCES SERVICE
AB 
   NOVELTY - The method involves obtaining target data and               
   history data. A human resource data lake is                constructed
   using the history data. Entity relation                extraction is
   performed on unstructured data in the                human resource data
   lake. A staff capability-post                demand map containing
   matching weights is generated                based on the entity
   relation. Dominant feature is                extracted from the
   structured data in the human                resource data lake.
   Recessive feature is extracted                from the non-structured
   data. A matching weight is                obtained. An input feature
   vector added with a                departure probability label is used
   for training a                departure risk prediction model according
   to the                dominant feature based on a federal learning
   frame.                A target characteristic vector is constructed
   based                on the target data. The target characteristic      
   vector is input into the trained departure risk               
   prediction model. A prediction result is                output.
   USE - Method for managing human resource data based                on
   big data.
   ADVANTAGE - The method enables effectively excavating the               
   information in the human resource management data                to more
   accurately support human resource                decision.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a human     
          resource data management system based on big                data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a human 
   resource data management method based on big data.               
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2026-02-15
UT DIIDW:202613935Y
ER

PT P
AU LUO J
   CHEN H
   WU Z
   YANG H
TI Method for processing report, involves            synchronizing report
   source data corresponding to            report inquiry request to shared
   area lake table of            data lake, and caching report basic data
   obtained by            processing report source data in shared lake
   table to            first private lake table of data lake
PN CN121326935-A
AE IND & COMML BANK CHINA LTD
AB 
   NOVELTY - The method involves synchronizing report                source
   data corresponding to a report inquiry                request to a
   shared area lake table of a data lake.                Report basic data
   obtained by processing the report                source data is cached
   in the shared area lake table                to a first private area
   lake table of the data                lake. The report basic data in the
   first private                region lake table is pre-processed to
   obtain target                report data, where the pre-processing
   comprises                inquiring the report basic data according to   
   inquiry fields set by service attributes of                different
   types of report, and summarizing the                cache.
   USE - Method for processing a report in a big data                field.
   ADVANTAGE - The method enables synchronizing the report               
   source data corresponding to the report inquiry                request
   to the shared area lake table of the data                lake, thus
   improving the efficiency of the report                processing.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   report processing device; (2) an electronic device;                (3) a
   computer-readable storage medium; (4) a                computer program
   product.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a report  
   processing method. (Drawing includes non-English                language
   text).
Z9 0
U1 0
U2 0
DA 2026-02-01
UT DIIDW:202607679L
ER

PT P
AU HAJIMIRSADEGHI S H
   MORI G P
   CAO Y
   NAWHAL M
   DURAND T
   HE J
   GONG Y
TI Machine learning model architecture system, has            data receiver
   configured to extract mask data structure            from each data set
   of the data sets representative of            modalities are observed
   and modalities are            unobserved
PN US2026010763-A1
AE ROYAL BANK CANADA
AB 
   NOVELTY - The system has a processor configured to               
   provide a data receiver adapted to receive multiple                data
   sets representative of observed data. A data                receiver is
   configured to extract a mask data                structure from each
   data set of the data sets                representative of modalities
   are observed and                modalities are unobserved. A machine
   learning data                architecture engine is adapted to maintain
   an                attributive proposal network (106) for processing     
   the data sets. The attributive proposal network is                used
   for including a set of individual encoders.                The processor
   maintains a collective proposal                network (108) for
   processing the corresponding mask                data structure. The
   collective proposal network                includes a collective encoder
   corresponding to all                of the unobserved modalities. The
   mask data                structure is utilized for conditional selection
   of                a proposal distribution for an unobserved modality.   
   The processor maintains a first generative network               
   including a first set of multiple decoders                (112).
   USE - Machine learning model architecture system                trained
   for conducting machine learning using                partially-observed
   data i.e. heterogeneous data or                multimodal data sets by
   using a variational                selective auto-encoder (VSAE) machine
   learning                model framework used in a variety of
   applications                and platforms from healthcare and finance to
   social                networks and manufacturing systems.
   ADVANTAGE - The system trains the machine learning               
   architecture over training iterations to provide a               
   trained model to be stored for later usage, or                deployed
   for generating predicted inputs, using                generation of
   imputed data to reduce an overall                cost for generating
   labels or annotations where the                label is fairly time
   intensive or costly to                prepare, and provides improved
   deep latent variable                models to efficiently learn from
   partially-observed                heterogeneous data, improves
   computational                accuracy, and allows for compatibility with
   more                variations of incomplete input data sets.
   DETAILED DESCRIPTION - Each decoder of the first set of the multiple    
   decoders is configured to generate output estimated                data
   proposed by the attributive proposal network                and the
   collective proposal network. Expectation                over collective
   observation from the collective                proposal network is
   applied as a corresponding                proposal distribution as an
   approximation of a true                posterior distribution based on
   the mask data                structure such that a joint distribution of
   all                attributes and mask data structure can be learned    
   from the partially-observed data.INDEPENDENT CLAIMS are included for:
   (1) a                method for training a machine learning model       
   architecture for conducting machine learning using               
   partially-observed data by using a variational                selective
   auto-encoder machine learning model                framework; and(2) a
   non-transitory computer readable medium                storing a set of
   instructions for training a                machine learning model
   architecture for conducting                machine learning using
   partially-observed data by                using a variational selective
   auto-encoder machine                learning model framework.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   machine learning model architecture system.102Input
   data104Mask106Attributive proposal network108Collective proposal
   network112Decoders
Z9 0
U1 0
U2 0
DA 2026-01-22
UT DIIDW:202603505N
ER

PT P
AU LV H
   YING J
   JI G
   YAO X
   CHENG W
TI Method for monitoring and pre-warning finished oil            based on
   artificial intelligence and large data fusion,            involves using
   laser point cloud scanning for            constructing oil station
   three-dimensional model for            mapping device state and
   operation data
PN CN121281227-A
AE INSPUR SMART CITY TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves constructing a real-time               
   monitoring network. A millimeter wave radar liquid                level
   instrument is deployed to track the change of                the liquid
   level of the oil tank in real time. The                heterogeneous
   data source is integrated. The                inter-departmental data
   real-time synchronization                is performed with DataX through
   Kafka(RTM: Network                communications software). The
   monitoring data lake                is constructed. The collected data
   is                pre-processed. The structured data system covering    
   the full chain of                purchase-sale-storage-transportation is
   formed. The                fusion multimodal AI model identifies the    
   non-periodic abnormality of the oil tank liquid                level and
   the temperature according to the                pre-processed structured
   data. The laser point                cloud scanning is used for
   constructing a 1:1 oil                station three-dimensional model
   for mapping the                device state and operation data. The
   fault area is                automatically highlighted and the full-flow
   visual                display is performed when the early warning is    
              triggered.
   USE - Method for monitoring and pre-warning finished                oil
   based on artificial intelligence and large data                fusion.
   ADVANTAGE - The method effectively shortens the abnormal               
   discovery time, improves the warning accuracy,                reduces
   the occurrence rate of serious accident,                and reduces the
   device maintenance cost, forms a                reproducible energy
   intelligent monitoring model,                which promotes the industry
   to transform from                experience management to data drive.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   finished oil monitoring pre-warning system based on                AI
   and big data fusion.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating
   a                method for monitoring and pre-warning finished oil     
   based on artificial intelligence and large data                fusion.
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2026-01-29
UT DIIDW:2026046517
ER

PT P
AU YUAN Q
   KANG W
   DING M
   ZHU J
   GONG J
TI Intelligent public service management system based            on large
   data, has feedback collecting unit for            integrating feedback
   data transmitted by cooperation            module, and iteration
   optimizing unit for automatically            adjusting system parameter
   according to explanation            report
PN CN121278750-A
AE ZHEJIANG BENTONG DIGITAL INTELLIGENCE
AB 
   NOVELTY - The system has a federal fusion module. The               
   data quality control unit is used for filtering the                low
   quality data and marking the data reliability                level. The
   dynamic desensitization unit is used for                dynamically
   adjusting the desensitization                granularity of the
   sensitive field. The federal                data lake management unit is
   used for storing the                pre-processing data and reserving
   the ownership                mark. The display-hidden association mining
   unit is                used for hierarchically mining the display-hidden
   association of different time sequence periods. The                scene
   adapting label unit is used for adding the                scene type
   label and the area adapting label for                the characteristic
   vector to generate the                characteristic vector with label.
   The feedback                collecting unit is used for integrating the 
        feedback data transmitted by the cooperation                module.
   USE - Intelligent public service management system                based
   on large data for use in intelligent city.                Uses include
   but are not limited to transportation,                medical treatment,
   community service and community                pension service.
   ADVANTAGE - The system realizes inter-department               
   characteristic alignment without transmitting the               
   original data. The system guarantees the usability                of the
   data lake, and maximizes the data value                while strictly
   protecting the privacy.
   DETAILED DESCRIPTION - The method involves using the reason and         
   explanation reporting unit for analyzing the                location
   deviation source. The iteration optimizing                unit is used
   for automatically adjusting the system                parameter
   according to the explanation                report.An INDEPENDENT CLAIM
   is included for an                intelligent public service management
   method based                on big data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of an        
   intelligent public service management system based                on
   large data. (Drawing includes non-English                language text).
Z9 0
U1 0
U2 0
DA 2026-02-06
UT DIIDW:202611443K
ER

PT P
AU CAI X
   ZHANG Y
TI Method for realizing digital delivery based on            ground
   engineering management flow and management node,            involves
   retrieving related verified data from central            data lake
   according to view generation rule, and            generating dynamic
   data view and sending dynamic data            view to user terminal
PN CN121258409-A
AE SICHUAN CHENGRONG INFORMATION TECHNOLOGY
AB 
   NOVELTY - The method involves constructing a dynamic                data
   ontology base for a ground engineering field.                A
   heterogeneous data of a design system, a                construction
   system and a purchasing system is                obtained through a
   semantic analysis device. A                metadata label and context
   information of the                heterogeneous data are extracted. The
   metadata                label and the context information are matched
   with                a standard semantic definition in the dynamic data  
   ontology base. An intermediate data stream is input                to a
   stream-type checking device. A data access                range and a
   view generation rule are determined                according to a user
   identity identifier and a                current engineering management
   node identifier. A                related verified data is retrieved
   from a central                data lake according to the view generation
   rule. A                dynamic data view is generated and sent to a user
                  terminal.
   USE - Method for digital delivery based on ground               
   engineering management flow and management                node.
   ADVANTAGE - The method enables effectively breaking the               
   data island between each department in the ground               
   engineering management by constructing a dynamic                data
   main body library and a dual heterogeneous                checking
   mechanism, thus realizing semantic                unification and high
   quality integration of                multi-source heterogeneous data.
   The method                improves the accuracy, consistency and
   reliability                of the data and provides a solid data base
   for the                engineering decision. The data is deeply fused
   with                the three-dimensional model, which realizes the     
   precision and visual delivery of the information                and
   improves the management cooperation efficiency,                risk
   identification ability and decision support                level.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of    
   system module interaction of the digital delivery                method
   based on the ground engineering management                flow and the
   management node. (Drawing includes                non-English language
   text).
Z9 0
U1 0
U2 0
DA 2026-01-26
UT DIIDW:202605824G
ER

PT C
AU Ibatullin, Evgeniy
   Khvatov, Valery
   Bogdanov, Alexander
   Shchegoleva, Nadezhda
BE Gervasi, O
   Murgante, B
   Garau, C
   Karaca, Y
   Lago, MNF
   Scorza, F
   Braga, AC
TI Hierarchical Virtual Storage
SO COMPUTATIONAL SCIENCE AND ITS APPLICATIONS-ICCSA 2025 WORKSHOPS, PT IX
SE Lecture Notes in Computer Science
VL 15894
BP 231
EP 248
DI 10.1007/978-3-031-97648-3_16
DT Proceedings Paper
PD 2026
PY 2026
AB The increasing complexity of data management and storage systems,
   coupled with the growing demand for flexible and efficient solutions,
   has led to the emergence of data virtualization technologies. This study
   investigates the potential for enhancing data storage methodologies
   through virtualization approaches, particularly focusing on the
   integration of hierarchical storage systems. Additionally, the research
   explores methods for organizing the management of such systems. The
   principles, methodologies, and architectures underlying distributed
   storage systems employed for handling big data tasks are analyzed.
   Experimental validation was conducted through the physical
   implementation of a prototype system on hardware. The results
   demonstrate a hierarchical data storage system leveraging
   virtualization, facilitating seamless data access and integration from
   disparate sources independently of their structure or storage method.
   Furthermore, a management approach based on reinforcement learning is
   proposed for controlling the developed storage system.
CT 25th International Conference on Computational Science and
   Applications-ICCSA-Annual
CY JUN 30-JUL 03, 2025
CL Galatasaray University, Istanbul, TURKIYE
HO Galatasaray University
SP Institute of Electrical and Electronics Engineers Inc; Springer New
   Zealand; University of Massachusetts Inc; University of Perugia;
   University of Basilicata; Monash University; Kyushu Sangyo University;
   Universidade do Minho
RI Bogdanov, Alexander/H-9947-2013; Khvatov, Valery/KIJ-9773-2024
ZB 0
ZR 0
ZA 0
ZS 0
Z8 0
TC 0
Z9 0
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-97647-6; 978-3-031-97648-3
DA 2025-11-05
UT WOS:001563948300016
ER

PT C
AU Otto, Tim
   Rawald, Christopher
   Dessloch, Stefan
BE Chrysanthis, PK
   Norvag, K
   Stefanidis, K
   Zhang, Z
   Quintarelli, E
   Zumpano, E
TI CoDD: A Constraint-Based Dataset Discovery Tool for Open Data Lakes
SO NEW TRENDS IN DATABASE AND INFORMATION SYSTEMS, ADBIS 2025
SE Communications in Computer and Information Science
VL 2676
BP 24
EP 31
DI 10.1007/978-3-032-05727-3-3
DT Proceedings Paper
PD 2026
PY 2026
AB Data lakes offer the flexibility to store large volumes of heterogeneous
   data with minimal curation. However, this flexibility comes at a cost:
   traditional keyword-based dataset discovery methods require reliable
   metadata such as table names or column headers, and become ineffective
   when this metadata is either missing or incomplete. This issue is
   especially pronounced in open or poorly maintained data lakes, where the
   quality of metadata cannot be guaranteed. In this paper, we present
   CoDD, a system for constraint-based dataset discovery in open data
   lakes. Instead of querying metadata (query-by-metadata), CoDD profiles
   datasets by extracting structured facts directly from the data using
   modular, user-definable components. Users can perform
   query-by-constraint searches by specifying constraints over the profiled
   facts in an interactive, question-driven interface. Early results from
   our user study show that CoDD enables users to find relevant datasets
   when traditional keywordbased approaches fail due to insufficient or
   misleading metadata. Furthermore, CoDD performs comparably well even
   when accurate metadata is available, demonstrating that
   query-by-constraint is a robust and scalable alternative for dataset
   discovery in open data lake environments.
CT 2025 European Conference on New Trends in Databases and Information
   Systems-ADBIS
CY SEP 23-26, 2025
CL Tampere, FINLAND
SP Tampere University
ZA 0
ZB 0
ZS 0
ZR 0
TC 0
Z8 0
Z9 0
U1 0
U2 0
SN 1865-0929
EI 1865-0937
BN 978-3-032-05726-6; 978-3-032-05727-3
DA 2025-12-10
UT WOS:001595536700003
ER

PT C
AU Wrembel, Robert
BE Leung, CK
   Dignos, A
   Kotsis, G
   Tjoa, AM
   Khalil, I
TI Data Integration in the AI Era: Research Trends and Still Open Issues
SO BIG DATA ANALYTICS AND KNOWLEDGE DISCOVERY, DAWAK 2025
SE Lecture Notes in Computer Science
VL 16048
BP 21
EP 36
DI 10.1007/978-3-032-02215-8_2
DT Proceedings Paper
PD 2026
PY 2026
AB Data integration (DI) has been an area for intensive research for
   decades, which resulted in a few acknowledged reference architectures.
   The architectures can be categorized as supporting: (1) virtual
   integration (federated and mediated), (2) physical integration (data
   warehouse), and (3) hybrid (data lake, data lakehouse, data mesh).
   Regardless of their specific type, all these architectures rely on a
   complex integration layer. The layer is implemented by a sophisticated
   software, for designing, orchestrating, and running the so-called DI
   processes. On the one hand, in all business domains, large volumes of
   highly heterogeneous data are produced, e.g., medical systems, smart
   cities, smart agriculture, which require further advancements in the
   data integration technologies. On the other hand, the widespread
   adoption of artificial intelligence (AI) solutions is now extending
   towards DI, offering alternative solutions, opening new research paths,
   and generating new open problems.
   In this talk, I will share my perspective on the application and
   potential of AI solutions for selected DI problems. I will also
   highlight still unresolved issues within the field of DI. The talk will
   be structured into three main parts: (1) an overview of data integration
   architectures, (2) selected AI techniques for DI (like data wrangling,
   data quality, schema matching, optimization of systems, and code
   generation), and (3) still open problems in DI. The findings presented
   in the talk are based on my experience in running research and
   development DI projects for various business entities. It offers a
   concise overview of common DI challenges and potential solutions,
   serving as a quick-start guide for further exploration.
CT 27th International Conference on Big Data Analytics and Knowledge
   Discovery-DAWAK-Annual
CY AUG 25-27, 2025
CL THAILAND
RI Wrembel, Robert/F-7482-2014
OI Wrembel, Robert/0000-0001-6037-5718
ZR 0
ZB 0
ZS 0
ZA 0
Z8 0
TC 0
Z9 0
U1 4
U2 4
SN 0302-9743
EI 1611-3349
BN 978-3-032-02214-1; 978-3-032-02215-8
DA 2025-11-26
UT WOS:001579468600002
ER

PT P
AU MELLOR D
TI Tangible, non-transitory, machine-readable medium            for
   identifying and requesting use of cost-effective            machines to
   process jobs related to big data, has set            of instructions for
   submitting cloud-based data            processing job to cloud service
PN US12511171-B1
AE NBCUNIVERSAL MEDIA LLC
AB 
   NOVELTY - The medium has a set of instructions for               
   generating an ordered cluster options list that               
   prioritizes an optimal cluster option and                equivalent
   cluster options based upon long-term                interruption rates,
   short-term availability (210)                and expected pricing.
   Execution of a data                processing job is caused by a cloud
   service by                providing an electronic request to the cloud  
   service based on the options list, where the                electronic
   request indicates request to allocate a                cluster to
   implement the data processing job. The                cloud-based data
   processing job is submitted to the                cloud service to
   trigger execution of the                cloud-based data processing job
   at the allocated                cluster of the cloud service by a
   processor.
   USE - Tangible, non-transitory, machine-readable                medium
   for identifying and requesting use of                cost-effective
   machines to process jobs related to                big data. Uses
   include but are not limited to the                data from advertising
   data, marketing data,                business data, research and
   development data,                government data, healthcare data, and
   Internet                browsing data.
   ADVANTAGE - The method enables increasing the efficient               
   use of available machines and resources that would               
   otherwise be running idle, and providing efficient               
   execution of data processing to reduce processing                costs
   while increasing utilization of                under-utilized machines
   of a cloud-based data                processing service. The requesting
   process on                interruptible machines may allow for the      
   enterprises to utilize a cost-effective option by               
   requesting on machines that were not purchases                through
   the on-demand market. The method enables                allowing for
   optimization of cost for big data                processing by
   allocating clusters in a                cost-effective and efficient
   manner. Two or more                jobs are selected for parallel
   processing to                maximize resources and minimize time spent 
                 processing the data of a data lake.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   system for identifying and requesting use of               
   cost-effective machines to process jobs related to                big
   data; and (2) a method for resource                allocation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram       
   representing implementation of process to identify                and
   allocate clusters based upon prioritized                cluster
   options.200Optimal machine set202Equivalent machine sets204Cluster
   options206API response208Prioritized cluster options list210Short-term
   availability
Z9 0
U1 0
U2 0
DA 2026-01-14
UT DIIDW:2025C4750N
ER

PT P
AU DENG Y
   FANG R
   LI L
   FU Z
   SHI S
   YANG C
   WU Q
   LIN W
   CHEN W
   ZHENG Z
   CHEN Y
TI Intelligent pre-warning method for marketing            service based on
   large data analysis, involves            combining abnormal condition in
   marketing data and            customer loss probability, and realizing
   intelligent            early warning based on intelligent early warning 
             algorithm
PN CN121213158-A
AE STATE GRID INFO-TELECOM GREAT POWER SCI
AB 
   NOVELTY - The method involves obtaining internal data                and
   external data related to marketing service. The               
   pre-processed data is stored into a uniform model                by
   multi-source data through a data lake, where the                uniform
   model uses a theme domain modeling mode. A                multi-layer
   abnormal detection model is                constructed. The abnormal
   condition in the                marketing data is identified. A client
   loss                prediction model is constructed. The client loss    
   probability is predicted based on the pre-processed                data.
   A high risk client group is identified. The                abnormal
   condition in the marketing data and the                customer loss
   probability are combined. The                intelligent early warning
   is realized based on the                intelligent early warning
   algorithm.
   USE - Intelligent pre-warning method for marketing               
   service based on large data analysis.
   ADVANTAGE - The method enables effectively providing the               
   reliability of marketing service management.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   marketing service intelligent                pre-warning system based on
   big data                analysis;(2) a computer storage medium.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the       
   intelligent pre-warning method for marketing                service
   based on large data analysis. (Drawing                includes
   non-English language text).
Z9 0
U1 0
U2 0
DA 2026-01-29
UT DIIDW:202603480K
ER

PT J
AU Bahmutsky, Sofia
   Turner, Ian
   Arulnathan, Vivek
   Pelletier, Nathan
TI Advancing life cycle assessment through data science: A critical review
   of algorithms, tools, and data challenges
SO SUSTAINABLE PRODUCTION AND CONSUMPTION
VL 61
BP 25
EP 36
DI 10.1016/j.spc.2025.10.007
EA DEC 2025
DT Article
PD DEC 2025
PY 2025
AB A well-executed life cycle assessment requires thorough data collection
   across all relevant processes, combined with advanced data analysis.
   Common data-related issues in life cycle assessment research include the
   absence of necessary data, low data quality, inconsistencies,
   uncertainty, and failure to account for variations over time and
   location. In this context, data science, the discipline of extracting
   meaningful insights from data, has the potential to address these
   challenges. While the integration of data science with life cycle
   assessment holds significant potential, best use cases depend on the
   goal of the study, as well as the data type and volume required,
   underscoring the necessity of reviewing the intersection of data science
   and life cycle assessment. This study used the Preferred Reporting Items
   for Systematic Reviews and Meta-Analysis (PRISMA) method to identify
   literature addressing the use of data science elements to support life
   cycle assessment. It evaluated which data science techniques are
   appropriate for specific life cycle assessment stages or problem areas
   and the strengths and weaknesses of current data science applications in
   life cycle assessment. Key opportunities identified revolve around
   solutions for dealing with missing or poor-quality data,
   expensive/prohibitive data collection, and improving the accuracy of
   life cycle assessment results. The currently most feasible pathways
   appear to involve use of machine learning techniques, as these types of
   studies were the most conducted and generated tangible results. Extreme
   gradient boosting, random forest, and artificial neural networks were
   particularly prominent algorithm choices. Data collection and
   transferability using ontologies and semantic tools were also
   highlighted as important strategies for improving data flow in life
   cycle assessment, including the integration of a wide variety of
   databases and non-life cycle assessment data.
RI Pelletier, Nathan/U-9312-2019
Z8 0
ZS 0
ZA 0
TC 0
ZR 0
ZB 0
Z9 0
U1 14
U2 14
SN 2352-5509
DA 2025-11-09
UT WOS:001608191100001
ER

PT J
AU Elouataoui, Widad
   Gahi, Youssef
TI Empirical Evaluation of Big Data Stacks: Performance and Design Analysis
   of Hadoop, Modern, and Cloud Architectures
SO BIG DATA AND COGNITIVE COMPUTING
VL 10
IS 1
AR 7
DI 10.3390/bdcc10010007
DT Article
PD DEC 24 2025
PY 2025
AB The proliferation of big data applications across various industries has
   led to a paradigm shift in data architecture, with traditional
   approaches giving way to more agile and scalable frameworks. The
   evolution of big data architecture began with the emergence of the
   Hadoop-based data stack, leveraging technologies like Hadoop Distributed
   File System (HDFS) and Apache Spark for efficient data processing.
   However, recent years have seen a shift towards modern data stacks,
   offering flexibility and diverse toolsets tailored to specific use
   cases. Concurrently, cloud computing has revolutionized big data
   management, providing unparalleled scalability and integration
   capabilities. Despite their benefits, navigating these data stack
   paradigms can be challenging. While existing literature offers valuable
   insights into individual data stack paradigms, there remains a dearth of
   studies that offer practical, in-depth comparisons of these paradigms
   across the entire big data value chain. To address this gap in the
   field, this paper examines three main big data stack paradigms: the
   Hadoop data stack, modern data stack, and cloud-based data stack.
   Indeed, we conduct in this study an exhaustive architectural comparison
   of these stacks covering the entire big data value chain from data
   acquisition to exposition. Moreover, this study extends beyond
   architectural considerations to include end-to-end use case
   implementations for a comprehensive evaluation of each stack. Using a
   large dataset of Amazon reviews, different data stack scenarios are
   implemented and compared. Furthermore, the paper explores critical
   factors such as data integration, implementation costs, and ease of
   deployment to provide researchers and practitioners with a relevant and
   up-to-date reference for navigating the complex landscape of big data
   technologies and making informed decisions about data strategies.
RI GAHI, Youssef/AEW-2987-2022
TC 0
ZR 0
ZA 0
Z8 0
ZB 0
ZS 0
Z9 0
U1 0
U2 0
EI 2504-2289
DA 2026-01-31
UT WOS:001670688100001
ER

PT P
AU MAYABHATE A
   MATHIAS O
   HALDAR S
   SMITH C A
TI Method for maintaining data integrity of e.g.            database,
   involves monitoring upstream source using            changed data
   capture by processing circuits, and            determining current
   schema of unstructured data by            processing circuits
PN US12505089-B1
AE FANNIEMAE
AB 
   NOVELTY - The method (200) involves monitoring an               
   upstream source using changed data capture (CDC) by               
   processing circuits, where the CDC identifies an                update,
   insertion, or deletion of unstructured data                stored in the
   upstream source. A current schema of                the unstructured
   data is determined (220) by the                processing circuits using
   a function identified                based on a correspondence between
   the unstructured                data and a previous schema, where the
   function                comprises one of an inferring function or a     
   predefined transformation function. Determination                is made
   to check whether the current schema                comprises performing
   of the function using one of                pattern recognition or the
   previous schema.                &#8195;A divergence between the current
   schema                and the previous schema of the unstructured data
   is                determined (230) by the processing circuits based     
   on comparing the current schema to the previous                schema of
   the unstructured data.
   USE - Method for maintaining data integrity of a                database
   and a data lake.
   ADVANTAGE - The method enables realizing reduction in data              
   redundancy, thus enhancing performance of data                retrieval
   operations, with queries yielding faster                and accurate
   results due to decreased dataset size.                The method enables
   ensuring data entries that are                unique, thus improving
   data integrity and                contributing to overall effectiveness
   and                efficiency of computational operations. The method   
   enables reducing data redundancy, leading to                efficient
   use of storage resources and reducing                costs associated
   with data maintenance. The method                enables allowing the
   system by the CDC to                prioritize processing current or
   modified data                rather than reprocessing an entire dataset,
   thus                significantly reducing time and computational       
   resources required for data updates. The method                enables
   improving system's capability to                efficiently manage
   growth without constantly and                extensively performing data
   re-evaluation or                reprocessing increasing system's
   effectiveness in                processing continuous data expansion and
   complexity. The method enables enhancing                adaptability for
   improving computational efficiency                in environments where
   data requirements evolve                rapidly, ensuring that the
   system can process                diverse data types without performance
   degradation.                The method enables avoiding computational
   costs of                scanning an entire data source/dataset by a
   change                detection system to determine changes and increase
   efficiency and performance of computing devices               
   implementing the change detection system.
   DETAILED DESCRIPTION - Structured data comprising pointers to the       
   unstructured data is generated by the processing                circuits
   based on performing an in-flight                transformation before
   storing the structured data                in the database, where the
   in-flight transformation                causes the processing circuits
   to apply the current                schema to the unstructured data to
   generate the                structured data. The structured data in the 
   database is stored by the processing circuits,                where the
   database comprises an established data                channel between
   the database and a downstream                source, and the structured
   data is a unique                representation within the database of
   the update,                insertion, or deletion of the unstructured   
   data.INDEPENDENT CLAIMS are included for:(1) a system for maintaining
   data integrity of                a database and a data lake;(2) a
   non-transitory computer-readable media                (CRM) comprising a
   set of instructions for                maintaining data integrity of a
   database and a data                lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for maintaining data integrity of a
   database and a data lake.200Method for maintaining data integrity of    
   database and data lake210Identifying unstructured data220Determining
   current schema of unstructured                data230Determining
   divergence between current                schema and previous schema of
   unstructured                data240Generating structured data comprising
        pointers to unstructured data250Storing structured data in database
Z9 0
U1 0
U2 0
DA 2026-01-14
UT DIIDW:2025C2003U
ER

PT P
AU CHEN G
   QIAO P
   WU Z
TI Lake warehouse integrated system, has federated            computing
   service layer for receiving query request            forwarded by
   unified service interface layer, obtaining            corresponding data
   fragment from intelligent storage            engine layer according to
   query request, and feeding            back calculation result
PN CN121166764-A
AE SHENZHEN YINXING INTELLIGENT ELECTRICAL
AB 
   NOVELTY - The system has a data source access layer for               
   accessing multiple heterogeneous data sources in                real
   time. A dynamic metadata management layer                receives
   original data input by the data source                access layer,
   obtains corresponding triple metadata                according to the
   original data, and synchronizes                the triple metadata
   between a data lake and a data                warehouse in real time. An
   intelligent storage                engine layer receives the original
   data input by                the data source access layer and performs  
   persistent storage according to a metadata rule. A               
   federated computing service layer receives a query               
   request forwarded by a unified service interface                layer,
   obtains a corresponding data fragment from                the
   intelligent storage engine layer according to                the query
   request, calculates the data fragment                according to
   computing logic of the query request,                and feeds back a
   calculation result to the unified                service interface
   layer.
   USE - Lake warehouse integrated system for use in a                lake
   cabin integrated system.
   ADVANTAGE - The system solves the problems that the               
   cross-system data processing is complex, the                metadata is
   inconsistent, the query performance                bottleneck, the
   storage cost and efficiency                contradiction and the
   metadata redundancy.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a lake 
   depot integrated system. (Drawing includes                non-English
   language text).10Data source access layer20Dynamic metadata management
   layer30Intelligent storage engine layer40Federation computing service
   layer50Unified service interface layer
Z9 0
U1 0
U2 0
DA 2026-01-17
UT DIIDW:2025C53235
ER

PT P
AU LI H
   XIN L
   HU Z
   SUN J
TI Method for managing memory resident data based on            large data
   architecture, involves calculating judging            index when memory
   occupancy rate is lower than pre-set            safety threshold value,
   and comparing judging index            with pre-set value
PN CN121166574-A
AE JIANGSU TOTA INTELLIGENT TECHNOLOGY CO
AB 
   NOVELTY - The method involves establishing metadata for                a
   cache data block in a memory of a big data                computing
   frame. A global cache list is maintained.                Timing cleaning
   operation is performed to the                global cache list to delete
   the cache data block                exceeding preset survival time.
   Memory occupancy                rate of the big data calculation frame
   is                monitored. Forced cleaning operation is executed      
   based on the global cache list when reaching a                preset
   gradient threshold value. The memory                occupancy rate is
   re-detected after cleaning. A                judging index is calculated
   when the memory                occupancy rate is lower than a pre-set
   safety                threshold value. The judging index is compared
   with                a pre-set judging threshold value to determine      
         whether to enter a safety area or trigger an                alarm.
   USE - Method for managing memory resident data based                on
   big data architecture.
   ADVANTAGE - The method enables dynamically adapting the               
   system load when the task amount is increased                suddenly or
   the memory pressure is fluctuated, and                avoiding a
   resource idle or the memory overflow,                and improves the
   cache retention efficiency through                the multi-level
   cleaning strategy, and reduces the                error deleting risk.
   The method enables introducing                an intelligent alarm
   mechanism to ensure operation                stability of the system in
   the high pressure                environment and improve safety and
   intelligent                level of the whole memory management.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for managing memory resident data
   based on                big data architecture. (Drawing includes        
          non-English language text).
Z9 0
U1 0
U2 0
DA 2026-01-22
UT DIIDW:2025C4962N
ER

PT P
AU LIU H
TI Cloud computing based intelligent campus large            data
   acquisition management system, has real-time            computing engine
   that communicates in two directions,            and plug-in expansion
   mechanism butted with data middle            platform layer and visual
   platform
PN CN121166796-A
AE FOSHAN MICANG NETWORK TECHNOLOGY CO LTD
AB 
   NOVELTY - The system has a data collecting layer for               
   collecting multi-source heterogeneous data                including
   teaching data, student management data,                one-card data,
   campus IoT device data and personnel                management data of
   an intelligent campus. A data                middle platform layer
   constructs a data lake and a                data warehouse based on a
   cloud computing                structure. A real-time calculation engine
   performs                millisecond calculation to real-time stream data
   output by the data middle platform layer to                generate
   scene pre-warning information. An                intelligent analysis
   engine integrates a cluster                analysis model, an
   association rule mining model                and a machine learning
   model. A visualization                platform displays the scene
   pre-warning                information. An authority and security module
   distributes user authority based on an access                control
   model of a role. A plug-in extension                mechanism is butted
   with the data middle platform                layer and the visual
   platform.
   USE - Cloud computing based Intelligent campus large                data
   acquisition management system for multiple                scenes of
   educational institutions such as colleges                and
   universities.
   ADVANTAGE - The system breaks data island, improves data               
   processing efficiency and intelligent level,                reduces
   operation and maintenance cost.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an          
   intelligent campus big data acquisition management                method
   based on cloud computing.
   DESCRIPTION Of DRAWING(S) - The drawing shows a composition diagram of
   an                intelligent campus large data acquisition             
   management system based on cloud computing.                (Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2026-01-22
UT DIIDW:2025C5551S
ER

PT P
AU LIU X
   CHEN J
   ZHANG Y
   LIN G
TI Large data real-time monitoring system for use in            fields e.g.
   internet of things, has storage inquiry            module for writing
   detail, aggregation and alarm record            into time sequence
   storage and edition data lake, and            visualization and
   operation maintenance module for            displaying real-time index
PN CN121166478-A
AE SHANGHAI LIANGHUA SENLIN TECHNOLOGY CO
AB 
   NOVELTY - The system has a data access module for               
   collecting multi-source heterogeneous data from a                message
   queue, a log agent or an internet of things                protocol. A
   pre-processing standard module removes                weight, cleaning
   and desensitization of the                collected data and correlates
   with a dimension                table. A flow calculating module
   executes window                aggregation, state updating and complex
   event                processing under an event time semanteme. A rule   
   model engine module executes threshold value rule               
   judgment and online model reasoning to a                polymerization
   result to generate primary alarm. An                alarm arranging and
   processing module combines the                primary alarm, inhibits
   and silence, and triggers                automatic processing according
   to a strategy. A                storage inquiry module writes detail,
   aggregation                and alarm record into a time sequence storage
   and                an edition data lake, and supports history playback  
                and audit.
        USE - System for monitoring large data in                real-time.
   ADVANTAGE - The system uses self-adaptive water level               
   line, the double-layer window is fused with the                dynamic
   threshold gating to realize the high aging                and
   consistency monitoring, support the edition                playback,
   approximate calculation cost reduction,                and have the
   automatic degradation and self-healing                ability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a big       
           data real-time monitoring method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a large
   data real-time monitoring system. (Drawing includes               
   non-English language text).
Z9 0
U1 0
U2 0
DA 2026-02-15
UT DIIDW:2025C48882
ER

PT P
AU MA J
TI Method for realizing port data comprehensive            application
   based on big data, involves generating            control instruction
   based on comprehensive application            result, and automatically
   sending control instruction            to corresponding port operation
   device control system            or personnel terminal
PN CN121168981-A
AE JIANGSU MARITIME INST
AB 
   NOVELTY - The method involves collecting multi-source               
   heterogeneous data of port operation through an                internet
   of things interface, an application                programming interface
   data interface and a manual                recording interface. The
   multi-source heterogeneous                data is standardized, cleaned,
   de-noised,                associated and fused to generate a port data
   lake.                A port operation knowledge map is constructed. An  
   entity and relation in the port data lake are                extracted
   and mapped based on a preset port service                main body
   model. A dynamic knowledge map describing                a device, a
   ship, goods, personnel, an event and a                multi-dimensional
   relation is formed. A control                instruction is generated
   based on a comprehensive                application result. The control
   instruction is                automatically sent to a corresponding port
   operation device control system or a personnel                terminal.
   A closed-loop control of                decision-execution is formed.
   USE - Method for comprehensive application of port                data
   based on big data.
   ADVANTAGE - The method enables realizing deep fusion of               
   the port data and global knowledge, so that                real-time
   operation scheduling optimization uses                the enhanced
   learning algorithm, which can                dynamically respond to the
   complex change of the                port field, thus realizing the
   global real-time                optimal solution of the resource
   distribution, from                sensing to executing the intelligent
   closed-loop                automation. The method enables improving the 
   response speed and the operation efficiency, and               
   continuously collecting the effect data after the               
   decision execution by the unique feedback learning               
   module, and is used for increment learning and                iteration
   optimization of the model.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a port      
    data comprehensive application system based on big                data.
Z9 0
U1 0
U2 0
DA 2026-01-29
UT DIIDW:2025C54575
ER

PT P
AU ZHAO Z
   LI J
   LIN H
   YANG W
   CHEN P
   RUAN X
   LI Q
   ZENG C
   SHAO C
TI Method for integrating heterogeneous data lake            snapshot of
   marine wind power station for supporting            dynamic
   backtracking, involves monitoring structure            change event of
   data source, and dynamically            overwriting history snapshot
   query statement based on            backward compatibility rule
PN CN121144281-A
AE GUANGZHOU NATIVEAI TECHNOLOGY CO LTD; UNIV GUANGDONG TECHNOLOGY
AB 
   NOVELTY - The method involves receiving (S1) real-time               
   monitoring data stream and offline survey data of                an
   offshore wind farm through a distributed message                queue. A
   dynamic metadata analyzer is invoked to                extract a data
   source format characteristic. A                corresponding data mode
   adapter is activated based                on a characteristic matching
   result. A uniform data                object is generated. A continuous
   time window is                divided (S2) according to a data time
   stamp. A                snapshot is triggered to generate an event when 
   each window is closed. A unique version identifier                is
   generated (S3) for a current snapshot segment. A               
   bidirectional version pointer is established based                on a
   storage path of a previous snapshot. A                historical state
   inquiry request submitted by a                user is received (S4). A
   column file of a target                snapshot segment set is loaded
   (S5).
   USE - Method for integrating heterogeneous data lake               
   snapshot of marine wind power station for                supporting
   dynamic backtracking.
   ADVANTAGE - The method enables reducing operation and               
   maintenance complexity of an offshore wind electric                field
   data lake, realizing efficient integration of               
   heterogeneous data and accurate backtracking of a                history
   state, and effectively improving data                usability of a fan
   fault diagnosis.
   DETAILED DESCRIPTION - The method involves monitoring (S6) a            
   structure change event of a data source. A mode                change
   log is recorded in a newly added snapshot. A                history
   snapshot query statement is dynamically                overwritten based
   on a backward compatibility                rule.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating
   a                method for integrating heterogeneous data lake         
   snapshot of marine wind power station for                supporting
   dynamic backtracking. (Drawing includes                non-English
   language text).S1Receiving real-time monitoring data stream             
   and offline survey data of offshore wind farm                through
   distributed message queueS2Dividing continuous time window according    
   to data time stampS3Generating unique version identifier for            
   current snapshot segmentS4Receiving historical state inquiry request    
   submitted by userS5Loading column file of target snapshot               
   segment set
Z9 0
U1 0
U2 0
DA 2026-01-14
UT DIIDW:202600482B
ER

PT P
AU LI X
   SUN J
   SHAO M
TI System for processing and analyzing big data for           
   microorganism culture, has processing analysis module            for
   analyzing and processing obtained culture            monitoring data
   according to processing analysis path            information, and
   obtaining culture full-period analysis            data
PN CN121116629-A
AE UNIV XIAN MEDICAL
AB 
   NOVELTY - The system has a culture monitoring module for               
   recording corresponding culture item information,                and
   obtaining corresponding culture monitoring data                in a
   corresponding microorganism culture process                according to
   the culture item information. A data                storage module
   collects federal microorganism                culture data, and obtains
   corresponding                microorganism shape data and microorganism 
   characteristic data according to the federal               
   microorganism culture data for storage management.                A
   monitoring processing module pre-evaluates the                obtained
   culture monitoring data in real time                according to the
   culture project information and                the microorganism storage
   data lake to obtain the                corresponding classification
   evaluation result. A                processing analysis module analyzes
   and processes                the obtained culture monitoring data
   according to                the corresponding processing analysis path  
   information, and obtains the culture full-period                analysis
   data corresponding to the corresponding                culture item
   information.
   USE - System for processing and analyzing big data                for
   microorganism culture.
   ADVANTAGE - The system improves the accuracy and               
   efficiency of the data processing process.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the
   system for processing and analyzing big data for               
   microorganism culture. (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2026-01-14
UT DIIDW:2025C20725
ER

PT P
AU LUO X
   HU X
   LUO B
   WU J
   XIE X
   YANG T
   SUN D
   HE J
   WANG H
   ZHAO L
   WANG J
   PENG J
   LI C
TI Method for processing disaster monitoring            multi-source
   disaster data fusion based on big data,            involves calling
   abnormal knowledge map to perform            abnormal tracking
   monitoring on abnormal monitoring            area, and predicting
   disaster spreading position            according to abnormal tracking
   monitoring result
PN CN121117512-A
AE SICHUAN HUADI CONSTR PROJECTS CO LTD; MIANYANG VOCATIONAL & TECH COLLEGE
AB 
   NOVELTY - The method involves obtaining (S1) disaster               
   monitoring records, and obtaining associated               
   environmental data and change trend curves at the               
   beginning and end of each disaster occurrence based                on
   the disaster monitoring records, so as to                establish
   abnormal knowledge maps of each disaster.                Multiple
   mapping nodes are set (S2) in the                monitoring scene, and a
   visual monitoring scene is                established according to the
   real-time                environmental data collected by each mapping
   node,                and multiple monitoring areas are divided in the   
   visual monitoring scene. The real-time                environmental data
   and basic attributes carried by                each monitoring area are
   matched (S3) with the                abnormal data lake, and the
   monitoring area is                recorded as an abnormal monitoring
   area if it is                determined that the real-time environmental
   data is                matched with the abnormal data lake, and the     
   abnormal knowledge map is called to perform                abnormal
   tracking monitoring on the abnormal                monitoring area.
   USE - Method for processing disaster monitoring               
   multi-source disaster data fusion based on big data                in
   geological disaster monitoring field. Uses                include but
   are not limited to landslides, debris                flows, rainstorms,
   and earthquake.
   ADVANTAGE - The system improves the accuracy of disaster               
   monitoring.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   disaster monitoring multi-source disaster data                fusion
   processing system based on big data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for processing disaster monitoring 
   multi-source disaster data fusion based on big                data.
   (Drawing includes non-English language                text).S1Obtaining
   disaster monitoring records, and                obtaining associated
   environmental data and change                trend curves at beginning
   and end of each disaster                occurrence based on disaster
   monitoring records, so                as to establish abnormal knowledge
   maps of each                disasterS2Setting multiple mapping nodes in 
   monitoring sceneS3Matching real-time environmental data and             
   basic attributes carried by each monitoring area                with
   abnormal data lake
Z9 0
U1 0
U2 0
DA 2026-01-14
UT DIIDW:2025C22243
ER

PT J
AU Barros, Julio
   Silva, Nuno
   Goncalves, Joao N. C.
   Cortez, Paulo
   Carvalho, M. Sameiro
   Santos, Maribel Y.
   Costa, Carlos
TI A machine learning-based framework for predicting supply delay risk
   using big data technology
SO INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS
VL 21
IS 1
AR 71
DI 10.1007/s41060-025-00969-8
DT Article
PD DEC 9 2025
PY 2025
AB The growing complexity of products and business processes is pushing
   companies toward an integrated and data-driven supply chain management.
   Companies are increasingly adopting big data analytics (BDA) and machine
   learning (ML) approaches as a way to manage uncertainty factors and to
   soften their effects on supply chain performance. In this paper, we
   present a data-driven framework that combines BDA techniques and ML
   models for estimating the risk of supply delay at a multinational
   automotive electronics manufacturer. The framework is based on a big
   data architecture so as to facilitate the integration of the proposed
   models into real practical contexts driven by large volumes of data. The
   framework developed was empirically tested with real data. We evaluated
   the results obtained from the predictive models not only in terms of
   error metrics, but also in terms of the financial impact of
   misclassification on inventory management performance. Our results could
   be used to promote the adoption of modeling strategies that relax common
   assumptions in the supply chain literature, such as considering that
   supply lead time is constant or that its distribution is known.
RI Carvalho, Maria/E-6812-2012; Barros, Júlio/IQW-9532-2023; Cortez, Paulo/A-2674-2008; Costa, Carlos/P-3314-2019; Santos, Maribel/M-5214-2013; Gonçalves, João N C/GOE-5856-2022
OI Gonçalves, João N C/0000-0002-0933-1995
ZA 0
Z8 0
ZB 0
TC 0
ZS 0
ZR 0
Z9 0
U1 0
U2 0
SN 2364-415X
EI 2364-4168
DA 2025-12-16
UT WOS:001634670300001
ER

PT P
AU CHEN W
TI Method for processing data based on artificial            intelligence
   and intelligent car park, involves using            online learning or
   increment learning technology for            periodically re-training
   each artificial intelligence            model and adjusting parameter
PN CN121096164-A
AE TIANJIN RUIYI TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves collecting multi-source               
   heterogeneous data in real time by multiple                internet of
   things sensing devices arranged in a                car park. The data
   is cleaned, formatted and                space-time aligned to form a
   uniform data lake. An                artificial intelligent algorithm is
   used to perform                depth analysis and model training based
   on the data                lake. An intelligent decision model library
   is                constructed. An analysis result is converted into a   
   specific application service and control                instruction
   based on the intelligent decision model                library. Actual
   effect data of the application                service is taken as
   feedback. Online learning or                increment learning
   technology is used for                periodically re-training each AI
   model and                adjusting a parameter such that prediction     
   precision and decision-making ability of the model                are
   continuously improved along with accumulation                of the data
   to form a closed-loop system from data                analysis to
   application to feedback to                optimization.
   USE - Method for processing data based on artificial               
   intelligence and intelligent car park.
   ADVANTAGE - The method enables periodically performing               
   re-training and parameter optimization on each AI                model,
   so that the prediction precision and                decision ability of
   the model are continuously                improved along with the
   accumulation of the data,                thus forming a               
   data-analysis-application-feedback-optimized closed                loop
   system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for processing data based on
   artificial                intelligence and intelligent car park.
   (Drawing                includes non-English language text).
Z9 0
U1 0
U2 0
DA 2026-01-10
UT DIIDW:2025C1698L
ER

PT P
AU BEULAH M E
   JEHAN C
   SARAVANAN R
TI Big data-powered internet of things (IoT) platform            for
   implementing intelligent predictive maintenance and           
   optimization method (IPMOM), has edge-cloud            collaborative
   framework that enables low latency            anomaly detection and
   cloud-based model training
PN IN202541112952-A
AE VELTECH MULTI TECH RANGARAJAN SAKUNTHALA
AB 
   NOVELTY - The platform has an IoT sensing layer, an edge               
   analytics layer, a big data processing engine, a               
   predictive maintenance module, a process                optimization
   engine, and a feedback control                integrator. 2. The
   predictive maintenance module                utilizes the hybrid big
   data-driven predictive                analytics technique (HBD-PAT)
   comprising                convolutional neural network (CNN), long
   short-term                memory (LSTM) and ensemble models. The process
   optimization engine employs reinforcement learning                for
   real-time adjustment of industrial parameters.                An
   edge-cloud collaborative framework enables low                latency
   anomaly detection and cloud-based model                training. The
   sensing layer includes heterogeneous                sensors such as
   vibration, acoustic, thermal,                current, voltage, pressure,
   humidity, and optical                sensors. The IoT gateway and edge
   analytics layer                performs local data aggregation and
   filtering ,                noise removal and feature extraction.
   USE - Big data-powered internet of things (IoT)                platform
   for implementing intelligent predictive                maintenance and
   optimization method (IPMOM).
   ADVANTAGE - The platform enables real-time condition               
   monitoring, failure prediction, and intelligent                process
   optimization, develops a unified                IoT-enabled industrial
   platform for continuous,                high-frequency data acquisition,
   provides a                scalable big data architecture supporting     
   high-volume, high-velocity data, introduces                intelligent
   predictive maintenance and optimization                method (IPMOM)
   for intelligent, real-time                predictive maintenance and
   optimization, introduces                hybrid big data-driven
   predictive analytics                technique (HBD-PAT) for accurate
   failure prediction                and RUL estimation, enables autonomous
   decision-making for process adjustment and                optimization,
   reduces downtime, improves safety,                and increases
   productivity and supports hybrid                edge-cloud deployment
   for efficient                computation.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
          for predictive maintenance and optimization                IPMOM.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a method
        for predictive maintenance and optimization                (IPMOM).
Z9 0
U1 0
U2 0
DA 2025-12-29
UT DIIDW:2025B9645N
ER

PT P
AU JOSHI U
   ZHANG Z
   WANG Y
   CHERUKUPALLI V
   YORDANOV V D
   WADHWA A L
   IRANI D
   SANKURATRI R
   RAO S H N
   YADAV N
TI Method for facilitating attack generation on data            lake e.g.
   set of object blobs, involves implementing            data gathering
   phase of attack that gathers data about            other objects,
   attributes, and relationships in data            lake repository
PN US2025373641-A1
AE THEOM INC
AB 
   NOVELTY - The method (300) involves providing an attack               
   generation mimicry tool for a data lake repository.                A
   reconnaissance phase attack generation is                implemented
   (302). An infiltration phase attack                generation is
   implemented (304) on the data lake                repository. Hiding and
   data intelligence collection                phase of the attack are
   implemented (306) by hiding                from any monitoring or
   notification system of the                lake repository and surveying
   the data lake                repository to determine what data is worth
   abusing                or exfiltrating from the data lake repository.
   Data                gathering phase of attack that gathers data about   
   other objects, attributes, and relationships in the                data
   lake repository is implemented (308).                Exfiltration of
   data or abuse of data is                implemented (310).
   USE - Method for facilitating attack generation on a                data
   lake i.e. a computerized repository of data                stored in its
   natural and raw format, and set of                object blobs (all
   claimed). Uses include but are                not limited to data lake
   can include structured                data from relational databases,
   semi-structured                data, unstructured data and binary data
   e.g.                images, audio, and video.
   ADVANTAGE - The method enables increased security               
   robustness and attack detection capabilities based                on the
   adage that you can't fix what you cannot                measure, and
   providing strategies to help                enterprises assess their
   current security posture,                find gaps in overall security
   strategy, and                monitor, measure, detect, and fix issues.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for facilitating attack generation
   on a                data lake.300Facilitating attack generation on a
   data                lake302Implementing reconnaissance phase attack     
   generation304Implementing infiltration phase attack               
   generation on the data lake repository306Implementing hiding and data
   intelligence                collection phase of the attack by hiding
   from any                monitoring or notification system of the lake   
   repository and surveying the data lake repository                to
   determine what data is worth abusing or                exfiltrating from
   the data lake repository308Implementing data gathering phase of         
   attack that gathers data about other objects,                attributes,
   and relationships in the data lake               
   repository310Implementing exfiltration of data or abuse               
   of data
Z9 0
U1 0
U2 0
DA 2026-01-14
UT DIIDW:2025B7145X
ER

PT P
AU MONTALVO A
TI System for dynamically fusing physical sensor data            to create
   new dataset for sensor management and            utilization in
   autonomous transportation of i.e. car,            by using computing
   device e.g. mobile phone, has            computer processor operable to
   instruct vehicle to            brake based on new data set
PN US2025371110-A1
AE DIGITAL GLOBAL SYSTEMS INC
AB 
   NOVELTY - The system has a computer processor (860)               
   operable to analyze first distance measurement and                second
   distance measurement. A curation engine is                operable to
   curate second distance measurement by                categorizing the
   second distance measurement into                second distance property
   and/or second distance                sub-property, where the second
   distance property                and the second distance sub-property
   include a                first data point of a vehicle (300) and/or a
   second                data point of the vehicle. A fusion engine is     
   operable to fuse the first distance measurement and                the
   second distance measurement, where the fusion                engine
   creates a new data set. The computer                processor is
   operable to instruct the vehicle to                brake based on the
   new data set.
   USE - System for dynamically fusing physical sensor                data
   to create a new dataset for sensor management                and
   utilization in autonomous transportation of a                vehicle
   i.e. car (from drawings) by using a                computing device.
   Uses include but are not limited                to a server, blade
   server, mainframe, mobile phone,                personal digital
   assistant (PDA), smartphone,                desktop computer, netbook
   computer, tablet                computer, workstation, laptop, and other
   similar                computing devices.
   ADVANTAGE - The system effectively fuses deep features of               
   multi-spectral images, enhances feature expression                of
   pedestrian areas, suppresses irrelevant                background noise
   features during the fusion                process, and realizes accurate
   pedestrian                detection. The system increases sensor data
   fusion                accuracy while reducing computational processing  
   requirements and storage demands. The system allows                the
   user to generate a query, and processes the                query against
   the stored data to find correlations                between the query
   and the indexed data before                producing results, requires
   less computational                power than an unstructured data lake,
   and requires                problematic amount of power and/or computing
   time.The system is paramount to ensure a camera                and/or a
   light detection and ranging (LiDAR) sensor                are accurately
   monitoring and/or predicting traffic                patterns and
   monitoring and/or predicting whether                an object is likely
   to enter the road while an                autonomous vehicle is
   transporting passengers in                autonomous driving. The system
   minimizes storage                requirements because fused data is
   stored, reduces                computational requirements because sensor
   data is                curated before and/or after a computer processor 
   receives a user and/or computer query, and enhances               
   sensor accuracy by providing additional actionable                data.
   The system is operable to enhance zero-trust                data
   principles using artificial intelligence                (AI)/machine
   learning (ML), where each sensor data                source and fusion
   is continuously validated against                a dynamic model that
   reflects the current state of                the electromagnetic
   environment and assigned a                confidence interval and/or
   reliability score.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for dynamically fusing physical sensor data to                create a
   new dataset for sensor management and                utilization in
   autonomous transportation of a                vehicle by using a
   computing device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of a   
               vehicle.300Vehicle301LiDAR sensor302Radar sensor
Z9 0
U1 0
U2 0
DA 2025-12-21
UT DIIDW:2025B57122
ER

PT P
AU MONTALVO A
TI System for dynamically fusing physical sensors            measuring data
   to create new dataset and curate gear            sensor data into
   property of torque of robotic            component on other joints of
   surgical robot by            computing device, has fusion engine fusing
   distance            measurements
PN US2025371109-A1
AE DIGITAL GLOBAL SYSTEMS INC
AB 
   NOVELTY - The system has a computer processor including                a
   memory, a fusion engine and an inference engine.                A first
   distance sensor captures a first distance                measurement. A
   second distance sensor is operable                to capture a second
   distance measurement, where the                computer processor
   analyzes the first and second                distance measurements. The
   fusion engine fuses the                distance measurements, and the
   inference engine                determines an inference from the
   measurement and                determines another inference. A
   validation engine                validates the inference when a
   comparison between                the inference and the latter inference
   exceeds a                predefined threshold. The computer processor   
   instructs a vehicle based on the inference being               
   validated.
   USE - System for dynamically fusing physical sensors               
   measuring data to create a new dataset and curate                gear
   sensor data into a property of torque of a                robotic
   component on other joints of surgical robot                and a
   sub-property of a location of the robotic                component based
   on gear position, and detecting                living beings like
   pedestrians or animals using                infrared cameras/sensors by
   a computing device.                Uses include but are not limited to a
   server, a                blade server, a mainframe, a mobile phone, a   
   personal digital assistant (PDA), a smartphone, a                desktop
   computer, a netbook computer, a tablet                computer, a
   workstation and a laptop.
   ADVANTAGE - The system can effectively fuse the deep               
   features of multi-spectral images, enhance the                feature
   expression of pedestrian areas and suppress                irrelevant
   background noise features during the                fusion process, and
   can realize accurate pedestrian                detection. The system
   increases sensor data fusion                accuracy while reducing
   computational processing                requirements and storage
   demands. The system                establishes reliable information from
   incoming                sensor data without excessive power,
   computational,                and/or storage requirements. The user
   generates a                query, the system processes the query against
   the                stored data to find correlations between the query   
   and the indexed data before producing results,                requiring
   less computational power than an                unstructured data lake,
   but still requiring a                problematic amount of power and/or
   computing time.                The system ensures a camera and/or a
   LiDAR sensor                are accurately monitoring and/or predicting
   traffic                patterns and monitoring and/or predicting whether
   an object is likely to enter the road while an                autonomous
   vehicle is transporting passengers. The                system realizes
   uniquely provide and enable the                system to compare
   captured sensor data to a set of                known sensor data, link
   the captured sensor data                and the set of known sensor
   data, fusing the                captured sensor data to the set of known
   sensor                data, thereby creating a new unique set of data,  
   infer statistical relevance of the new unique set                of
   data, and mathematically validate the fused                data, allows
   the system to unwind the fusion,                linking, and/or curation
   process, correcting errors                and ensuring accuracy. The
   system is operable to                enhance zero-trust data principles
   using AI/ML,                where each sensor data source and fusion is 
   continuously validated against a dynamic model that               
   reflects the current state of the electromagnetic               
   environment and assigned a confidence interval                and/or
   reliability score.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for sensor data fusion for sensor management and               
   utilization.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a     
   system for dynamically fusing physical sensors                measuring
   data.
Z9 0
U1 0
U2 0
DA 2025-12-21
UT DIIDW:2025B68981
ER

PT J
AU Ait Errami, Soukaina
   Hajji, Hicham
   Ait El Kadi, Kenza
   Badir, Hassan
TI A spatial big data architecture for vehicle data analytics: a focus on
   spatial data storage and query optimization
SO INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS
VL 21
IS 1
AR 40
DI 10.1007/s41060-025-00948-z
DT Article
PD DEC 2 2025
PY 2025
AB Effective management of vehicle mobility data is critical for advancing
   smart urban mobility and transportation analytics. Beyond data
   collection, the field requires integrated, end-to-end systems to handle
   large, heterogeneous datasets generated by vehicle sensors. Challenges
   include managing extensive data volumes, addressing data heterogeneity,
   ensuring robust storage and scalable processing, and complying with
   strict data protection regulations for sensitive location-based
   information. Additionally, managing the computational complexity of
   spatial data and efficiently handling metadata is vital for meaningful
   analysis. To address these challenges, this paper introduces a
   comprehensive Big Data architecture designed for large-scale,
   heterogeneous vehicle trajectory data processing. It evaluates
   architectural and storage design choices, focusing on high-dimensional
   spatial query optimization. Techniques such as space-filling curves
   (Z-order and Hilbert) and bucketing methods (H3 and S2 indexing)
   significantly improved query performance, reducing range query times by
   over 50% and join query times by up to 80%. Further enhancements were
   achieved with Cost-Based Optimization, Adaptive Query Execution, and
   Dynamic File Pruning. This paper outlines the key elements in building
   an end-to-end solution for spatial data analytics within the domain of
   vehicle mobility data analytics. It highlights the critical aspects and
   challenges of this field, offering insights into the development of an
   effective data management system tailored to the unique demands of smart
   urban mobility.
RI Hassan, BADIR/R-6226-2019
ZS 0
TC 0
ZB 0
ZR 0
ZA 0
Z8 0
Z9 0
U1 2
U2 2
SN 2364-415X
EI 2364-4168
DA 2025-12-08
UT WOS:001629432400008
ER

PT J
AU Cuzzocrea, Alfredo
   Soufargi, Selim
TI AB-DOM: An Algorithmic Framework for Supporting Privacy-Preserving Big
   Data Publishing in Big Data Lakes
SO IEEE TRANSACTIONS ON BIG DATA
VL 11
IS 6
BP 3029
EP 3046
DI 10.1109/TBDATA.2025.3570081
DT Article
PD DEC 2025
PY 2025
AB With the emergence of new technologies that extend the capabilities of
   actual data collection methods, healthcare data are more and more
   amassed in the purpose of being later analyzed to serve the ultimate,
   well-known, goal of 4P medicine (Predictive, Preventive, Personalized,
   Participative). Given the sensitive nature of healthcare data, and in a
   matter of compliance with data protection and privacy regulations, there
   is a need to make data publishing more secure. This is one of the main
   goals of the EU H2020 QUALITOP research project, with particular regards
   to the issue of defining a big health data smart digital platform and
   the shared data lake. In this context, we design, implement and
   experimentally assess an innovative algorithmic framework called
   <bold>A</bold>dvanced Privacy-Preserving <bold>B</bold>ig Data
   Publishing in Hierarchical <bold>DOM</bold>ains (AB-DOM). AB-DOM is
   based on state-of-the-art anonymization techniques mixed with a graph
   coloring algorithm and an integrated data sampling method to guarantee
   that sensitive data are highly secured.
RI Cuzzocrea, Alfredo/B-6374-2015
ZA 0
ZS 0
ZB 0
ZR 0
Z8 0
TC 0
Z9 0
U1 0
U2 0
SN 2332-7790
DA 2025-12-05
UT WOS:001626457200029
ER

PT J
AU Xiong, Runqun
   Zhao, Shiyuan
   Chen, Ciyuan
   Xu, Zhuqing
TI Optimizing Multimodal Data Queries in Data Lakes
SO TSINGHUA SCIENCE AND TECHNOLOGY
VL 30
IS 6
BP 2625
EP 2637
DI 10.26599/TST.2025.9010022
DT Article
PD DEC 2025
PY 2025
AB This paper addresses the challenge of efficiently querying multimodal
   related data in data lakes, a large-scale storage and management system
   that supports heterogeneous data formats, including structured,
   semi-structured, and unstructured data. Multimodal data queries are
   crucial because they enable seamless retrieval of related data across
   modalities, such as tables, images, and text, which has applications in
   fields like e-commerce, healthcare, and education. However, existing
   methods primarily focus on single-modality queries, such as joinable or
   unionable table discovery, and struggle to handle the heterogeneity and
   lack of metadata in data lakes while balancing accuracy and efficiency.
   To tackle these challenges, we propose a Multimodal data Query mechanism
   for Data Lakes (MQDL), which employs a modality-adaptive indexing
   mechanism raleted and contrastive learning based embeddings to unify
   representations across modalities. Additionally, we introduce product
   quantization to optimize candidate verification during queries, reducing
   computational overhead while maintaining precision. We evaluate MQDL
   using a table-image dataset across multiple business scenarios,
   measuring metrics such as precision, recall, and F1-score. Results show
   that MQDL achieves an accuracy rate of approximately 90%, while
   demonstrating strong scalability and reduced query response time
   compared to traditional methods. These findings highlight MQDL's
   potential to enhance multimodal data retrieval in complex data lake
   environments.
RI Xu, Zhuqing/MFZ-9549-2025; Chen, Ciyuan/ORK-1384-2025
ZR 0
TC 0
ZA 0
ZS 0
Z8 0
ZB 0
Z9 0
U1 5
U2 8
SN 1007-0214
EI 1878-7606
DA 2025-07-11
UT WOS:001523490700012
ER

PT P
AU JI H
TI Medical information management system for use in            modern
   medical health system, has trusted audit            tracking module for
   synchronously recording all            operation logs of blockchain core
   layer module, and            system management console module for
   dynamically            monitoring resource load state of each module
PN CN121034661-A
AE JIANGSU HEYOU TECHNOLOGY CO LTD
AB 
   NOVELTY - The system has a heterogeneous data access               
   module for outputting medical data after converting                the
   medical data through a standardized protocol. A               
   blockchain core layer module establishes                bidirectional
   communication with the distributed                data lake module. A
   patient sovereignty portal                module provides a data sharing
   strategy                configuration interface. A dynamic authority    
   control module responds to an instruction sent by                an
   intelligent contract engine unit of the                blockchain core
   layer module according to an                authorization rule. A
   clinical decision support                module is connected with a
   large data analysis                central module for generating a
   structured clinical                pre-warning signal based on an
   analysis result                output by the large data analysis central
   module. A                trusted audit tracking module for synchronously
   recording all operation logs of blockchain core                layer
   module, and system management console module                for
   dynamically monitoring resource load state of                each
   module.
   USE - Medical information management system for use                in a
   medical health system. Uses include but are                not limited
   to a hospital information system (HIS),                a laboratory
   information system (LIS), an image                archiving and
   communication system (PACS) and an                electronic health
   record (EHR) system.
   ADVANTAGE - The data desensitization engine automatically               
   filters the sensitive field so as to provide the               
   technical foundation for the cooperation of the               
   classification diagnosis and treatment and the                medical
   combination.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
            medical data sharing method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of the medical information management                system.(Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2026-01-04
UT DIIDW:2025C4081T
ER

PT P
AU LUO T
   CHEN Y
   SHANG G
   QI G
   GUO X
TI Method for inquiring heterogeneous data source            such as
   relational database, based on Doris database,            involves
   screening global optimal query plan from            multiple candidate
   global logic plans by using cost            model, and executing by
   Doris federal query function to            obtain target data
PN CN121029963-A
AE INSPUR YUNZHOU IND INTERNET CO LTD
AB 
   NOVELTY - The method involves obtaining (S1) a query                text
   submitted by the user. The query parameter of                the query
   text is analyzed. The metadata of each                data source is
   obtained (S2). The multiple                candidate global logic plans
   are generated based on                the metadata and the query
   parameter. The global                optimal query plan is screened (S3)
   from multiple                candidate global logic plans by using the
   cost                model. The global optimal query plan is converted   
   (S4) into a federal query statement supported by                the
   Doris. The Doris federal query function is                executed to
   obtain the target data.
   USE - Method for inquiring heterogeneous data source                such
   as relational database, NoSQL, data lake and                file system,
   based on Doris database.
   ADVANTAGE - The method automatically generates multiple               
   candidate global logic plans based on the                heterogeneous
   data source metadata and the query                parameter, and
   accurately selects the optimal plan                by using the cost
   model, which obviously improves                the execution efficiency
   of the cross-source query                and avoids the low-efficiency
   scanning and network                transmission bottleneck.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a heterogeneous data source query system based               
   on doris database;a device; anda computer readable storage medium
   storing                program for inquiring heterogeneous data         
         source.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for inquiring heterogeneous data source                based on
   Doris database. (Drawing includes                non-English language
   text)S1Step for obtaining a query text submitted                by the
   userS2Step for obtaining metadata of each data               
   sourceS3Step for screening the global optimal query                plan
   from multiple candidate global logic plans by                using the
   cost modelS4Step for converting the global optimal                query
   plan into a federal query statement supported                by the
   Doris
Z9 0
U1 0
U2 0
DA 2026-01-04
UT DIIDW:2025B7870W
ER

PT P
AU ILLOUZ A
   SHEMESH E
   RISE L
   WEINTRAUB G
TI Method for performing integrity verification of            data obtained
   from cloud data lake, involves            calculating combined hash
   value based on partition hash            values, and calculating
   verified combined hash value            based on hash values obtained
   from metadata table            corresponding to each of partitions
PN US2025363089-A1; US12547606-B2
AE INT BUSINESS MACHINES CORP
AB 
   NOVELTY - The method involves transmitting (602) a               
   request for a data set to the cloud data lake, and               
   receiving (604) multiple file names from the cloud                data
   lake in response to the request. Multiple                partitions of
   the cloud data lake that stores a set                of files that
   satisfy the request are extracted                (606) from the file
   names. A partition hash value                for each of the partitions
   is calculated (608). A                metadata table created by a data
   owner of the set                of files is obtained (610), and the
   metadata table                is verified based on a digital signature
   of the                metadata table corresponding to the data owner.
   The                verified partition hash values are obtained (612)    
   from the metadata table. A combined hash value is               
   calculated based on the partition hash values, and                a
   verified combined hash value is calculated based                on hash
   values obtained from the metadata table                corresponding to
   each of the partitions.
   USE - Method for performing integrity verification                of
   data obtained from cloud data lake used as                primary
   sources for analytics and machine learning                models for
   data-driven decision-making.
   ADVANTAGE - The cloud data lake is a cloud-hosted               
   centralized repository that provides nearly                unlimited
   capacity and scalability for storing                large-scale
   structured and unstructured data. The                control functions
   and the forwarding functions of                network module are
   performed on physically separate                devices, such that the
   control functions manage                multiple different network
   hardware devices.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   computing system having a memory having                computer readable
   instructions and multiple                processors for executing the
   computer readable                instructions;(2) a computer program
   product comprising a                computer readable storage medium
   having program                instructions.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a method
   for performing integrity verification of data                obtained
   from cloud data lake.602Transmitting a request for a data set to        
   the cloud data lake604Receiving multiple file names from the            
   cloud data lake in response to the request606Extracting multiple
   partitions of the                cloud data lake that stores a set of
   files that                satisfy the request from the file
   names608Calculating partition hash value for each                of the
   partitions610Obtaining metadata table created by a data               
   owner of the set of files612Obtaining verified partition hash values    
              from the metadata table
Z9 0
U1 0
U2 0
DA 2025-12-16
UT DIIDW:2025B22079
ER

PT J
AU Wei, Luyang
   Fang, Huan
TI Lakehouse storage architecture design methodology for station-city
   integrated cyberspace
SO CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS
VL 29
IS 1
AR 44
DI 10.1007/s10586-025-05832-w
DT Article
PD NOV 20 2025
PY 2025
AB Data operation and maintenance in station-city integrated cyberspace
   represent a cross-domain application scenario that spans smart
   transportation and smart cities. This scenario involves multi-source
   heterogeneous data from diverse contexts, including sensor-collected
   data, Building Information Modeling (BIM), and City Information Modeling
   (CIM). However, challenges remain in managing massive multi-source
   heterogeneous data storage, eliminating information silos, and enhancing
   data fusion efficiency within such integrated frameworks. To address
   these challenges, this study proposes a Data Lakehouse architecture
   tailored for the intelligent operation and maintenance of Shenzhen North
   Railway Station's station-city integrated cyberspace. Additionally, this
   paper defines domain-specific storage and query meta-models for five
   critical operation and maintenance scenarios: structure, environment,
   human flow, events, and energy consumption. By integrating ubiquitous
   multi-dimensional state perception, intelligent evaluation, and
   emergency response simulations, it ensures data-driven implementation of
   intelligent operation and maintenance systems for station-city
   integrated cyberspace.
Z8 0
ZS 0
ZA 0
TC 0
ZR 0
ZB 0
Z9 0
U1 8
U2 8
SN 1386-7857
EI 1573-7543
DA 2025-11-30
UT WOS:001621261600001
ER

PT P
AU CI Q
TI Method for optimizing data lake storage for            enterprise
   service integration in data processing            field, involves
   obtaining prior storage result,            verifying prior storage
   result, and storing real-time            service data based on prior
   storage result if            verification is passed
PN CN120973835-A
AE JIANGSU XINDAI INFORMATION TECHNOLOGY CO
AB 
   NOVELTY - The method involves traversing a service               
   multi-source heterogeneous data set to perform                explicit
   implicit information joint modeling. An                explicit implicit
   information joint association                result set is obtained,
   where association result                set is provided with a service
   multi-source                heterogeneous explicit information set and a
   service multi-source heterogeneous implicit                information
   set. Iterative memory storage                optimization analysis is
   performed based on the                explicit implicit information
   joint association                result set. Data lake storage is
   performed on the                heterogeneous data set according to a
   target                storage optimization strategy, and storage        
   optimization memory library is generated. Prior                storage
   identification is performed based on the                storage
   optimization memory base when receiving the                real-time
   service data. A prior storage result is                obtained, and the
   prior storage result is verified.                The real-time service
   data is stored based on the                prior storage result if the
   verification is                passed.
   USE - Method for optimizing data lake storage for               
   enterprise service integration in data processing                field.
   ADVANTAGE - The method solves the technical problem that               
   the enterprise business integration data storage               
   efficiency is low in the existing technology, and                reaches
   the technical effect of improving the data                lake storage
   efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a data      
   lake storage optimization platform for enterprise                service
   integration.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the method for optimizing data lake storage
   for                enterprise service integration. (Drawing includes    
              non-English language text).
Z9 0
U1 0
U2 0
DA 2025-12-29
UT DIIDW:2025B4786P
ER

PT P
AU WANG C
   HU L
   KANG Z
   FENG Y
   WU Z
   DAI Y
   ZHU Q
TI Blockchain-data blood margin tracking and            management method,
   involves utilizing graph database to            construct collected
   multi-source heterogeneous data to            data blood relation graph,
   where visualization            technology is used to display processing
   process and            blood relationship of heterogeneous data
PN CN120892421-A
AE CHINA NAT BUILDING MATERIAL XINYUN ZHILI; CHINA NAT BUILDING MATERIALS
   TECHNOLOGIC; CNBM XINYUN ZHILIAN TECHNOLOGY CO LTD; CNBM TECHNOLOGY CORP
   LTD
AB 
   NOVELTY - The method involves utilizing a distributed               
   storage engine for butt-jointing multi-source               
   heterogeneous data through a data access gateway                and
   storing the data partition. The flow path and                the change
   information of the multi-source                heterogeneous data are
   recorded in each processing                link in the data lake
   framework based on the                intelligent contract designed by
   the alliance chain                framework. A graph database is
   utilized to                construct the collected multi-source
   heterogeneous                data to a data blood relation graph. A     
   visualization technology is used to display the               
   processing process and blood relationship of                multi-source
   heterogeneous data.
   USE - Blockchain-data blood margin tracking and               
   management method.
   ADVANTAGE - The visual technology is used for displaying               
   the processing process and blood relationship of                the
   multi-source heterogeneous data so as to solve                the
   problem that the data cannot be effectively                monitored and
   managed in the flowing process of the                whole life period.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   data blood edge tracking and management system                based on
   blockchain; (2) a computer device; (3) a                computer
   readable storage medium.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a         
   blockchain-data blood margin tracking and                management
   method. (Drawing includes non-English                language text).
Z9 0
U1 0
U2 0
DA 2025-12-03
UT DIIDW:2025B0911H
ER

PT J
AU Tu, Xiudong
   Liu, Yan
TI Construction and Experimental Validation of an AI Animation Teaching
   Platform Driven by Corpus Analysis and Character Animation Generation
SO INTERNET TECHNOLOGY LETTERS
VL 8
IS 6
AR e70164
DI 10.1002/itl2.70164
DT Article
PD NOV 3 2025
PY 2025
AB With the rise of the animation industry, animation courses in
   universities are becoming increasingly popular. However, traditional
   teaching focuses too much on software operation and neglects animation
   principles (such as the 3D creation process), resulting in insufficient
   practical ability of students and an urgent need for reform. To
   cultivate students' comprehensive practical ability in animation design,
   this article constructs a teaching platform based on artificial
   intelligence technology, integrates multiple environmental resources,
   and promotes the transformation of the teaching mode toward ability
   cultivation. In addition, the platform adopts a modular design to
   construct a corpus module with a high-frequency word filtering effect,
   and proposes a character animation generation module based on the
   ontology model library to automatically generate scripts. Experimental
   verification shows that the PSNR value has increased by about 12 dB, the
   SSIM average is 0.92, the user evaluation average score is 4.6 points
   (visual quality), and the PCK keypoint accuracy has an average of 93.9%
   (such as wrist 97.7%), which is significantly better than the comparison
   method. The platform constructed effectively enhances students'
   practical abilities, supports teaching innovation, and will expand big
   data architecture to optimize applications in the future.
Z8 0
ZR 0
ZS 0
TC 0
ZB 0
ZA 0
Z9 0
U1 2
U2 2
EI 2476-1508
DA 2025-11-28
UT WOS:001607471100001
ER

PT P
AU LIU Y
   FU Y
   YU T
   LI D
   YUAN X
   CHEN J
TI Method for multi-source heterogeneous data            hierarchical
   aggregation used in military            reconnaissance field, involves
   obtaining hierarchy in            data pool by dividing based on
   hierarchy relationship            of data collecting device in unmanned
   helicopter
PN CN120873008-A
AE CHINA RONGTONG SCI RES INST GROUP CO LTD
AB 
   NOVELTY - The method involves obtaining (110) the               
   multi-source heterogeneous data in the flying                process of
   the unmanned helicopter. The , heat                dimension and the
   time dimension are used (120),                based on the data format
   dimension. Several types                of information contained in the
   multi-source                heterogeneous data are determined. The
   multi-source                heterogeneous data is stored (130) to the
   storage                layer of the data lake. The label information of 
   the multi-source heterogeneous data is stored to                the
   corresponding hierarchy in different data                pools, based on
   several types of information. The                different data pools
   comprise a structured data                pool constructed in the data
   lake, a                semi-structured data pool, an unstructured data  
   pool, a hot data pool, a cold data pool and a time               
   sequence data pool. The hierarchy in the data pool                is
   obtained by dividing based on the hierarchy                relationship
   of the data collecting device in the                unmanned helicopter.
   USE - Method for multi-source heterogeneous data               
   hierarchical aggregation used in military                reconnaissance,
   environmental monitoringand                logistics transportation
   field.
   ADVANTAGE - The invalid scanning of other original data in              
   the data lake is avoided, so as to obviously                improve the
   inquiry speed and efficiency. The                system construction and
   knowledge map construction                provide powerful support,
   which greatly improves                the management efficiency and
   application value of                the data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a multi-source heterogeneous data hierarchical               
   aggregation device;an electronic device; anda non-transient state
   computer readable                storage medium storing computer program
   for                multi-source heterogeneous data hierarchical         
         aggregation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   multi-source heterogeneous data hierarchical                aggregation
   method. (Drawing includes non-English                language
   text)110Step for obtaining the multi-source                heterogeneous
   data in the flying process of the                unmanned
   helicopter120Step for using the heat dimension and the               
   time dimension130Step for storing the multi-source               
   heterogeneous data to the storage layer of the data                lake
Z9 0
U1 0
U2 0
DA 2025-12-03
UT DIIDW:2025A8348M
ER

PT J
AU Santos-Dominguez, Martin
   Hernandez Flores, Nicasio
   Parra-Ramirez, Isaac Alberto
   Arroyo-Figueroa, Gustavo
TI AI-Big Data Analytics Platform for Energy Forecasting in Modern Power
   Systems
SO BIG DATA AND COGNITIVE COMPUTING
VL 9
IS 11
AR 272
DI 10.3390/bdcc9110272
DT Article
PD OCT 31 2025
PY 2025
AB Big Data Analytics is vital for power grids, as it empowers informed
   decision-making, anticipates potential operational and maintenance
   issues, optimizes grid management, supports renewable energy
   integration, ultimately reduces costs, improves customer service,
   monitors consumer behavior, and offers new services. This paper
   describes the AI-Big Data Analytics Architecture based on a data lake
   architecture that uses a reduced and customized set of Hadoop and Spark
   as a cost-effective, on-premises alternative for advanced data analytics
   in power systems. As a case study, a comparative analysis of electricity
   price forecasting models in the day-ahead market for nodes of the
   Mexican national electrical system using statistical, machine learning,
   and deep learning models, is presented. To build and select the best
   forecasting model, a data science and machine learning methodology is
   used. The results show that the Gradient Boosting and Support Vector
   Regression models presented the best performance, with a Mean Absolute
   Percentage Error (MAPE) between 1% and 4% for five-day-ahead electricity
   price forecasting. The implementation of the best forecasting model into
   the Big Data Analytics Platform allows the automation of the calculation
   of the local electricity price forecast per node (every 24, 72, or 120
   h) and its display in a comparative dashboard with actual and forecasted
   data for decision-making on demand. The proposed architecture is a
   valuable tool that allows the future implementation of intelligent
   energy forecasting models in power grids, such as load demand, fuel
   prices, power generation, and consumption, among others.
RI Arroyo Figueroa, Gustavo/O-4911-2016
OI Arroyo Figueroa, Gustavo/0000-0003-0764-045X
ZA 0
Z8 0
ZS 0
ZR 0
ZB 0
TC 0
Z9 0
U1 1
U2 1
EI 2504-2289
DA 2025-12-01
UT WOS:001624040600001
ER

PT J
AU Ait Errami, Soukaina
   Hajji, Hicham
   Ait El Kadi, Kenza
   Badir, Hassan
TI Leveraging Space Filling Curves for Efficient Storage and Processing of
   Spatial Data in the Data LakeHouse
SO TRANSACTIONS IN GIS
VL 29
IS 7
AR e70137
DI 10.1111/tgis.70137
DT Article
PD OCT 28 2025
PY 2025
AB The rapid growth of data-driven decision making has led to an increasing
   need for efficient and scalable data processing architectures. Data
   Lakehouse has emerged as a solution for nowadays data workload needs.
   It's a paradigm that combines the benefits of data lakes and data
   warehouses and provides a unified platform for a variety of operational
   workloads, including business intelligence, analytics, data science, and
   AI. In this paper, we present the concept of space filling curves,
   particularly the three most known curves, namely: Z-order, Hilbert, and
   Gray code curves. We investigate how they improve both data locality and
   access patterns in the context of spatial big data in Data Lakehouse.
   This improvement has a direct impact on storage optimization and query
   performance. As a practical use case, the paper covers the
   implementation of the Z-order curve, along with a discussion of the main
   algorithm's components and how it fits into the current Data Lakehouse
   system. We show that these optimizations significantly enhance the
   capabilities of Data Lakehouse architectures, ensuring better resource
   utilization across various spatial workloads.
RI Hassan, BADIR/R-6226-2019
ZA 0
Z8 0
ZS 0
ZB 0
ZR 0
TC 0
Z9 0
U1 2
U2 2
SN 1361-1682
EI 1467-9671
DA 2025-11-03
UT WOS:001602062200001
ER

PT P
AU LI D
   MENG Y
   LIU J
   AN D
   CHENG Y
   WEI Y
TI Method for converging village industry data based            on
   multimodal fusion, involves constructing multi-level            data
   architecture, where architecture is provided with            data
   acquisition layer, data processing layer and data            gathering
   layer
PN CN120849401-A
AE UNIV CHINA AGRICULTURAL
AB 
   NOVELTY - The method involves constructing a multi-level               
   data architecture, where the architecture is                provided
   with a data acquisition layer, a data                processing layer
   and a data gathering layer. The                data collecting layer
   obtains heterogeneous data                from multiple sources. A
   database is constructed. A                data processing mechanism
   pre-processes the data. A                text is accurately divided
   according to semantic                and grammar rules through a text
   slicer. A                knowledge map and a knowledge structure are    
   constructed. Data of the multi-modal fusion model                MIXNet
   fusion text, an image and a sensor are used                as an input
   to output a prediction result supported                by a decision.
   USE - Method for converging village industry data                based
   on multimodal fusion.
   ADVANTAGE - The system effectively improves the element               
   configuration efficiency of the rural                industry.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   computer readable storage medium;(2) an electronic device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a         
   multimodal data convergence rural social service               
   platform. (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-12-02
UT DIIDW:2025A7286K
ER

PT P
AU ZHANG Z
   REN L
   LI X
   CHENG S
   JIN C
TI Data lake warehouse integrated management method,            involves
   performing semantic correction and fusion on            multi-source
   heterogeneous traffic data in data lake            and data warehouse
   switching flow based on alignment            strategy, and outputting
   lake warehouse fusion data            with semantic alignment
PN CN120832388-A; CN120832388-B
AE LIANYUNGANG ELECTRONICS PORT INFORMATION
AB 
   NOVELTY - The method involves collecting (S1) the               
   multi-source heterogeneous traffic data, and                obtaining
   the pre-processed multi-source                heterogeneous traffic
   data. The semantic feature                extraction and hierarchical
   relationship analysis                are performed (S2) on the
   pre-processed traffic                heterogeneous data. A semantic
   mapping rule set is                constructed (S3) based on the
   semantic feature set.                The historical semantic drift
   accumulation                deviation analysis is performed (S4) on the 
   semantic feature set, and semantic drift correction               
   parameter is generated. The initial mapping rule                and the
   semantic drift correction parameter are                combined (S5) to
   construct a dynamic semantic                alignment and correction
   mechanism to generate an                alignment strategy. The semantic
   correction and                fusion are performed (S6) on the
   multi-source                heterogeneous traffic data in the data lake
   and                data warehouse switching flow based on the           
   alignment strategy, and the lake warehouse fusion                data is
   outputted with semantic alignment.
   USE - Data lake warehouse integrated management                method
   based on port traffic.
   ADVANTAGE - The accuracy of the lake cabin cooperative               
   management and the consistency of data analysis are               
   improved. The data lake warehouse integrated                management
   method and system based on port traffic,                specifically
   relating to the technical field of                data management is
   provided.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a data      
   lake warehouse integrated management system based                on port
   traffic.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a data lake
   warehouse integrated management method based on                port
   traffic. (Drawing includes non-English                language
   text).S1Step for collecting the multi-source               
   heterogeneous traffic dataS2Step for performing semantic feature        
   extraction and hierarchical relationship analysis                on the
   pre-processed traffic heterogeneous                dataS3Step for
   constructing a semantic mapping                rule setS4Step for
   performing historical semantic                drift accumulation
   deviation analysis on the                semantic feature setS5Step for
   combining the initial mapping rule                and the semantic drift
   correction parameterS6Step for performing semantic correction and       
   fusion on the multi-source heterogeneous traffic                data in
   the data lake and data warehouse switching                flow
Z9 0
U1 0
U2 0
DA 2025-11-30
UT DIIDW:2025A5158K
ER

PT P
AU LIU Z
   SU Z
   YAO H
TI Intelligent factory full-flow online monitoring            and
   pre-warning system with large data enabling            comprises
   multimodal pre-warning module used for            integrating
   statistical process control threshold            pre-warning and long
   short-term memory neural network            prediction model
PN CN120802753-A
AE ZHONGZHI CLOUD PLATFORM HENAN CO LTD
AB 
   NOVELTY - Intelligent factory full-flow online                monitoring
   and pre-warning system with large data                enabling comprises
   a multi-source heterogeneous                data fusion module used for
   accessing at least 11                types of data sources e.g.
   programmable logic                controller, manufacturing execution
   system and                sensor through industrial physical connection 
   gateway, realizing millisecond level data                collection by
   using time sequence database                (InfluxDB), satisfying
   collection period                &#916;t is &#8804; 10 ms. An
   edge-cloud-end                collaborative architecture used for edge
   node                performs data pre-processing and reduces data       
   transmission amount, and cloud data center adopts                data
   lake storage. An intelligent monitoring engine                used for
   real-time calculating dynamic index based                on Spark(Big
   data processing general engine) frame                and driving
   three-dimensional digital twinning                board. A multimodal
   pre-warning module used for                integrating statistical
   process control threshold                pre-warning and long short-term
   memory neural                network prediction model.
   USE - Used as intelligent factory full-flow online               
   monitoring and pre-warning system with large data               
   enabling.
   ADVANTAGE - The system solves the problems of the               
   traditional factory monitoring system e.g. data                fusion
   bottleneck, limited calculation structure                and intelligent
   analysis defect, and realizes the                whole flow monitoring
   and pre-warning by the                multi-source heterogeneous data
   fusion module,                edge-cloud cooperative structure,
   intelligent                monitoring engine and multi-mode pre-warning 
   module, realizes millisecond-level data collection,               
   reduces the data transmission amount, combines with                the
   Spark (Big data processing general engine)                frame to drive
   the three-dimensional digital twin                signboard, integrates
   index weighted moving average                control diagram and long
   short-term memory neural                network to perform multi-mode
   pre-warning, realizes                fault diagnosis and energy
   consumption abnormality                detection, breaks through the
   data fusion                bottleneck, reduces the transmission storage
   cost,                and improves the monitoring pre-warning            
      accuracy.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the       
   intelligent factory full-flow online monitoring and               
   pre-warning system with large data enabling                (Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-11-23
UT DIIDW:2025A7736W
ER

PT P
AU WANG X
   ZHANG B
   FANG D
   WANG Y
   LI Z
   YUAN G
   ZHANG X
TI Intelligent security early warning system used in            production
   safety management field, has universal video            access unit for
   accessing video monitoring system of            third party at any time
   to obtain corresponding video            monitoring information, and
   sensor access unit for            obtaining data of sensor
PN CN120808524-A
AE BEIJING ZHIHUI HULIAN POWER CO LTD
AB 
   NOVELTY - The system has a technical framework unit that               
   adopts micro-service framework. The network unit               
   optimizes the network performance and security                through
   the network monitoring tool and the regular                security
   evaluation. The data unit stores the                original data from
   various sources and supports the                standardization and
   thematic storage of the data.                The data frame unit uses
   Apache Hadoopand                ApacheSpark data processing frame. The
   application                unit is provided with the functions of data  
   analysis, report generation and real-time                monitoring. The
   display unit supports the user to                deeply analyze and
   generate the customized report.                The universal video
   access unit accesses the video                monitoring system of the
   third party at any time to                obtain the corresponding video
   monitoring                information. The sensor access unit obtains
   the                data of the corresponding sensor at any time.
   USE - Intelligent security early warning system used                in
   production safety management field.
   ADVANTAGE - The network unit optimizes the network               
   performance and security through the network                monitoring
   tool and the regular security                evaluation, at the same
   time, it also can optimize                the security and efficiency of
   the data                transmission. The system breaks the device
   protocol                barrier, supports plug-and-play access of       
   industrial device, sensor, and video monitoring,               
   constructs space-time aligned data lake, uniformly               
   processes 1kHz vibration data, 30fps video stream                and so
   on heterogeneous data to realize data                real-time
   transmission and timely linkage, and                shortens the
   response time, improves the response                efficiency, and
   improves the accuracy of safety                early warning.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a  
   monitoring picture of an intelligent security early               
   warning system. (Drawing includes non-English                language
   text)
Z9 0
U1 0
U2 0
DA 2025-11-23
UT DIIDW:2025A4563M
ER

PT P
AU MONTALVO A
TI System for realizing sensor data fusion for sensor            management
   and utilization in satellite command and            control, has
   computer processor including memory, and            fusion engine for
   creating new data set, where computer            processor is operable
   to instruct satellite to reorient            based on new data set
PN US2025321544-A1; US12488067-B2
AE DIGITAL GLOBAL SYSTEMS INC
AB 
   NOVELTY - The system has a computer processor including                a
   memory. A first radio frequency (RF) power sensor                is
   operable to capture first power measurement of                an RF
   signal received by a satellite. A second RF                power sensor
   is operable to capture second power                measurement of the RF
   signal received by the                satellite. The computer processor
   is operable to                analyze the first power measurement and
   the second                power measurement. A fusion engine is operable
   to                fuse the first power measurement and the second       
   power measurement. The fusion engine creates a new                data
   set. The computer processor is operable to                instruct the
   satellite to reorient based on the new                data set.
   USE - System for realizing sensor data fusion for                sensor
   management and utilization in satellite                command and
   control.
   ADVANTAGE - The system minimizes storage requirements when              
   fused data is stored, reduces computational                requirements
   when sensor data is curated before                and/or after the
   computer processor receives a user                and/or computer query
   and enhances sensor accuracy                by providing additional
   actionable data. The system                can ensure a camera and/or a
   sensor accurately                monitor and/or predict traffic patterns
   and monitor                and/or predict whether an object enters a
   road. The                system processes query against the stored data
   to                find correlations between the query and indexed       
   data before producing results requiring less               
   computational power than an unstructured data lake                to
   effectively fuse deep features of multi-spectral                images
   so as to enhance feature expression of                pedestrian areas,
   suppress irrelevant background                noise features during
   fusion process and realize                accurate pedestrian detection.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for sensor data fusion for sensor management and               
   utilization in satellite command and control.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a     
   robotics sensor data fusion system.201Surgical robot202Robotic
   component203Patient`s Body
Z9 0
U1 0
U2 0
DA 2025-11-05
UT DIIDW:2025A0804N
ER

PT P
AU SHEN M
TI Large data based data intelligent management            service platform
   system, has closed-loop optimization            center for realizing
   global model update and system            dynamic expansion through
   federal learning and            micro-service framework
PN CN120723969-A
AE SHANGHAI LANGYU INFORMATION TECHNOLOGY
AB 
   NOVELTY - The system has a dynamic data lake                constructing
   module used for accessing multi-source                heterogeneous data
   in real time and calculating                data value coefficient and
   realizing data layered                storage. A semantic map generating
   module                establishes a multi-dimensional semantic network  
   based on knowledge map technology. A self-adaptive                label
   engine generates and updates the user label                through a
   graph neural network (GNN) and the time                attenuation
   factor. A reinforced learning pushing                module adopts
   multi-arm tiger machine model and A/B                test frame
   optimization pushing strategy. A                closed-loop optimization
   center realizes global                model update and system dynamic
   expansion through                federal learning and micro-service
   framework.
   USE - Large data based data intelligent management               
   service platform system.
   ADVANTAGE - The system solves problems that the               
   traditional central station system data is layered                and
   rigid, tag generation is static and strategy                optimization
   is lagged, and realizes closed-loop                system of data
   intelligent management, precise                service pushing and
   system self-adaptive                optimization.
   DESCRIPTION Of DRAWING(S) - The drawing shows a system block diagram of
   a                large data based data intelligent management           
   service platform system. (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2025-11-13
UT DIIDW:2025996391
ER

PT C
AU Puertas, Enrique
   Bemposta, Sergio
   Monsalve, Borja
   Lopez, Jose M.
   Corrales-Paredes, Ana
TI Big Data System for Traffic Monitoring and Management at Roundabouts
   using Drones and Artificial Intelligence
SO IFAC PAPERSONLINE
VL 59
IS 10
BP 1534
EP 1539
DI 10.1016/j.ifacol.2025.09.258
EA SEP 2025
DT Proceedings Paper
PD 2025
PY 2025
AB This paper proposes a vehicle detection system for roundabouts based on
   images captured by a drone. This system runs on a Big Data architecture
   to ensure scalability and real-time processing. The system architecture
   is divided into two parts: a detection part, based on drones and
   computer vision, and a communication and processing part, based on a Big
   Data architecture deployed in the cloud. The system is able to
   accurately detect both roundabouts and the vehicles driving on them,
   providing valuable information on traffic conditions. The Big Data
   architecture allows real-time traffic information to be processed and
   analyzed, facilitating informed decision-making to improve traffic flow
   and safety. The evaluation of the system, carried out through
   simulations, has demonstrated its robustness and ability to handle large
   volumes of data in real time. Copyright (C) 2020 The Authors. Published
   by Elsevier B.V. This is an open access article under the CC BY-NC-ND
   license (http://creativecommons.org/licenses/by-nc-nd/4.0/)
CT 11th IFAC Conference on Manufacturing Modelling, Management and Control
   (MIM)
CY JUN 30-JUL 03, 2025
CL Trondheim, NORWAY
SP Int Federat Automat Control, TC 5 2 Management & Control Mfg & Logist;
   Int Federat Automat Control, TC 1 3 Discrete Event & Hybrid Syst; Int
   Federat Automat Control, TC 3 2 Computat Intelligence Control; Int
   Federat Automat Control, TC 5 1 Mfg Plant Control; Int Federat Automat
   Control, TC 7 4 Transportat Syst; Int Federat Automat Control, TC 9 1
   Econ, Business, & Financial Syst
RI Puertas, Enrique/L-5656-2014; Corrales Paredes, Ana/AAD-4733-2022
Z8 0
ZR 0
ZS 0
ZA 0
TC 0
ZB 0
Z9 0
U1 2
U2 2
SN 2405-8963
DA 2025-11-28
UT WOS:001583825700257
ER

PT P
AU ZHANG L
   LI Y
   QIU H
   WANG H
   LOU J
TI System for dynamically monitoring and analyzing            land
   transactions based on multi-source data fusion,            has dynamic
   monitoring and decision support unit which            is used to
   quantify market fluctuations using heat            index model and
   output idle land warning signal
PN CN120707288-A
AE JIANGSU JINMAO INT E-COMMERCE CO LTD
AB 
   NOVELTY - The system has a real-time verification unit               
   which is used to establish a transaction process                rule
   library based on the Flink stream processing                engine,
   execute the real-time verification rules                generated by a
   knowledge graph construction unit,                and perform compliance
   filtering on the transaction                process. A trusted
   transaction verification unit is                used to verify the
   rationality of the transaction                price by using an improved
   isolation forest                algorithm, verify the relevance of the
   transaction                subjects in combination with a graph
   convolutional                network, conduct a secondary verification
   of the                transaction that passes the real-time verification
   unit, and generate a trusted certificate for the               
   compliant transaction based on blockchain                technology. A
   dynamic monitoring and decision                support unit is used to
   quantify market                fluctuations using a heat index model and
   output an                idle land warning signal.
   USE - System for dynamically monitoring and                analyzing
   land transactions based on multi-source                data fusion.
   ADVANTAGE - The system integrates heterogeneous data               
   through multi-modal data lake architecture and                performs
   space-time standard unification, uses                knowledge map
   technology to construct transaction                element association
   network to generate dynamic                verification rule, combines
   Flink flow processing                engine to realize transaction flow
   realtime                compliance filtering, improves the isolated
   forest                algorithm and the graph convolution network. The  
   transaction price rationality verification and the                main
   body relevance analysis are finished, and the                evidence is
   stored based on the blockchain                technology, so as to
   realize the full-flow                intelligent control of the national
   land                transaction data, the precise identification and    
   prevention and control of the risk, and the               
   high-efficiency and datamation support of the                monitoring
   decision.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the
   system for dynamically monitoring and analyzing                land
   transactions based on multi-source data                fusion. (Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-11-05
UT DIIDW:2025A0541U
ER

PT J
AU Lagos-Obando, Juan
   Aillapan, Gabriel
   Fenner-Lopez, Julio
   Bustamante-Mora, Ana
   Burgos-Lopez, Maria
TI A Secure-by-Design Approach to Big Data Analytics Using Databricks and
   Format-Preserving Encryption
SO APPLIED SCIENCES-BASEL
VL 15
IS 19
AR 10356
DI 10.3390/app151910356
DT Article
PD SEP 24 2025
PY 2025
AB Managing and analyzing data in data lakes for big data environments
   requires robust protocols to ensure security, scalability, and
   compliance with privacy regulations. The increasing need to process
   sensitive data emphasizes the relevance of secure-by-design approaches
   that integrate encryption techniques and governance frameworks to
   protect personal and confidential information. This study proposes a
   protocol that combines the capabilities of Databricks and
   format-preserving encryption to improve data security and accessibility
   in data lakes without compromising usability or structure. The protocol
   was developed using a design science methodology, incorporating findings
   from a systematic literature review and validated through expert
   feedback and proof-of-concept experiments in banking environments. The
   proposed solution integrates multiple layers, data ingestion,
   persistence, access, and consumption, leveraging the processing
   capabilities of Databricks and format-preserving encryption to enable
   secure data management and governance. Validation results indicate the
   protocol is effectiveness in protecting sensitive data, with promising
   applicability in regulated industries. This work contributes to
   addressing key challenges in big data security and lays the groundwork
   for future developments in data governance and encryption techniques.
RI Fenner Lopez, Julio Ernesto/I-5504-2014; Bustamante-Mora, Ana/; Lagos Obando, Juan Manuel/; Fenner Lopez, Julio/I-5504-2014; Burgos López, María Yolanda/
OI Fenner Lopez, Julio Ernesto/0000-0002-6953-062X; Bustamante-Mora,
   Ana/0009-0003-6096-0783; Lagos Obando, Juan Manuel/0000-0001-8378-9091;
   Burgos López, María Yolanda/0009-0007-9958-1763
ZB 0
ZR 0
Z8 0
ZA 0
TC 0
ZS 0
Z9 0
U1 0
U2 0
EI 2076-3417
DA 2025-10-20
UT WOS:001593447600001
ER

PT P
AU TIAN J
   DENG Z
   JIANG B
   LI W
   LIU Z
   ZHANG J
TI Method for automatically classifying and grading            sensitive
   data in oil-gas exploration and development            using computing
   device, involves performing data            feature engineering
   processing to multi-source            heterogeneous data stored in data
   lake
PN CN120687870-A
AE BEIJING RES CENT CNOOC CHINA LTD; CNOOC CHINA CO LTD
AB 
   NOVELTY - The method involves performing data feature               
   engineering processing on multi-source                heterogeneous data
   stored in a data lake to extract                name feature, service
   range, data feature and                initial security grading
   information. A data                feature vector is constructed. A
   feature similarity                of the data feature vector is
   calculated based on                the marked data sample. A relation
   network between                data items is constructed. Label
   transmission and                security level evaluation is performed.
   The known                security level information is transmitted to an
   unmarked data item to realize automatic grading                process.
   A data security level classification                result and a user
   feedback iteration optimization                security classification
   result are output. The                label diffusion model re-training
   is performed                using the feedback data. The classification
   result                is obtained.
   USE - Method for automatically classifying and                grading
   sensitive data in oil-gas exploration and                development
   using a computing device                (claimed).
   ADVANTAGE - The method automatically evaluates the               
   security level of the multi-type data by the label               
   transmission algorithm based on the data                association
   relation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:A system for automatically classifying and                grading
   sensitive data in oil-gas exploration and                development;
   andA computer-readable storage medium comprising                a set of
   instructions for automatically classifying                and grading
   sensitive data in oil-gas exploration                and development.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the method
   for automatically classifying and grading sensitive                data
   in oil-gas exploration and development using a                computing
   device (Drawing includes non-English                language text).
Z9 0
U1 0
U2 0
DA 2025-11-03
UT DIIDW:2025968615
ER

PT P
AU AN J
   ZHANG C
   XIE J
   SONG G
   WANG H
   ZHENG H
TI Locomotive big data base architecture in software,            has data
   processing layer which is configured to            collect and store
   locomotive data based on data lake            technology, data service
   layer is configured to            encapsulate locomotive data into
   application            programming interface (API)
PN CN120670500-A
AE CRRC QISHUYAN LOCOMOTIVE CO LTD; CRRC TECHNOLOGY INNOVATION BEIJING CO
AB 
   NOVELTY - The architecture has a data processing layer               
   (10) which is configured to collect and store                locomotive
   data based on data lake technology. A                data service layer
   (20) is configured to                encapsulate the locomotive data
   into an application                programming interface (API), and load
   and output                the locomotive data when the API is called.
   The                data collection and cleansing layer is configured    
   to collect locomotive metadata and cleanse the                locomotive
   metadata based on data lake access                standards, and the
   data storage layer is configured                to deploy a data storage
   policy and store the                cleansed locomotive metadata in a
   corresponding                first data warehouse based on the data
   storage                policy.
   USE - Locomotive big data base architecture used in               
   software.
   ADVANTAGE - The architecture focuses multisource,               
   heterogeneous and mass locomotive data collection,               
   management and service, provides locomotive large                data
   base framework, realizes high reusability of                locomotive
   data, ensures reasonable utilization of                data resource,
   and lays a solid data base for the                construction of
   subsequent platform.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
      for operating a locomotive big data base                architecture.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of the locomotive big data base architecture.                (Drawing
   includes non-English language                text).10Data processing
   layer20Data service layer
Z9 0
U1 0
U2 0
DA 2025-11-15
UT DIIDW:202594873G
ER

PT P
AU WANG F
   LI Y
   GUO Y
   FAN X
   WANG J
TI Method for storing computer data, involves adding            version
   identifier to checked power drawing, where            version identifier
   is generated by combination of            design tool type, time stamp
   and hash value, and            synchronously recorded to block chain
   index table
PN CN120653711-A
AE UNIV YANTAI NANSHAN
AB 
   NOVELTY - The method involves receiving a mixed input               
   stream of a CAD file including power drawing and a               
   structured log by a heterogeneous data access                module, and
   triggering a multi-mode classification                module to extract
   characteristics of the input                data. The graphic
   characteristic extracting unit                identifies the vector
   graphic element in the input                data based on the
   pre-trained convolution neural                network model, where the
   text characteristic                extracting unit analyzes the text
   symbol in the                data. The power drawing is routed to the
   isolation                storage pool according to the pattern type     
   identification in the feature vector. A dynamic                term
   mapping module is arranged at the entrance of                the
   isolation storage pool, and performs the term                consistency
   check on the written power drawing. A                version identifier
   is added to the checked power                drawing, where the version
   identifier is generated                by combination of design tool
   type, time stamp and                hash value, and synchronously
   recorded to the block                chain index table.
   USE - Method for storing computer data, useful for               
   integrating ammeter reading, equipment log, design               
   drawing and multivariate data in a power grid data                lake
   of a power system.
   ADVANTAGE - The method allows the system to push the               
   manual check mark to the appointed terminal through                the
   asynchronous message queue.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   computer data storage system, comprising set of               
   instructions for storing computer data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for storing computer data (Drawing includes               
   non-English language text).
Z9 0
U1 0
U2 0
DA 2025-10-27
UT DIIDW:202594249P
ER

PT P
AU XIAO D
   XU J
   LIN X
   SONG Z
   WU S
   ZHUANG X
TI Method for migrating frequently accessed data from            big data
   storage system e.g. data lake and data middle            station,
   involves determining a migration decision            matrix, and
   performing data migration based on            migration decision matrix
   based on access frequency            predictions and data storage cost
   parameters
PN CN120653199-A
AE XIAMEN MY EANT INFORMATION TECHNOLOGY CO
AB 
   NOVELTY - The method involves obtaining (S210) data               
   access logs and pre-processing the data access logs                to
   obtain a time window statistical matrix. The                statistical
   features of the multiple data access                counts are extracted
   (S230) to obtain a                multidimensional feature matrix based
   on the                weighted access frequency vector. The feature     
   vectors of each time window slice in the                multidimensional
   feature matrix is clustered (S240)                using a Gaussian
   mixture model to obtain a cluster                label vector. The
   cluster label vector is input                (S250) into an access
   frequency prediction model.                Each access frequency
   prediction value is                associated with a confidence
   interval. A migration                decision matrix is determined
   (S260), and data                migration is performed based on the
   migration                decision matrix based on the access frequency  
   predictions and data storage cost parameters. The                data
   storage locations are adjusted by the                migration decision
   matrix to balance access speed                and storage cost.
   USE - Method for migrating frequently accessed data                from
   big data storage system e.g. data lake and                data middle
   station.
   ADVANTAGE - The method reduces data migration complexity               
   and balance access speed and storage cost.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a frequency access data storage migration               
   device for large data;an electronic device;a computer-readable storage
   medium storing                program for migrating frequently accessed
   data from                a big data storage system; anda computer
   program product for migrating                frequently accessed data
   from big data storage                system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating
   a                method for migrating frequently accessed data from     
   a big data storage system. (Drawing includes                non-English
   language text).S210Step for obtaining data access logs and              
   pre-processing the data access logs to obtain a                time
   window statistical matrixS230Step for extracting statistical features   
   of the multiple data access countsS240Step for Clustering the feature
   vectors                of each time window slice in the multidimensional
   feature matrixS250Step for inputting the cluster label               
   vector into an access frequency prediction                modelS260Step
   for determining migration decision                matrix
Z9 0
U1 0
U2 0
DA 2025-10-27
UT DIIDW:202594968S
ER

PT P
AU YUAN J
   MA X
TI Method for realizing industrial templated modeling            and
   dynamic access controlling process based on data            lake,
   involves constructing distributed intelligent            data lake
   storage structure, and displaying safety            situation and
   pre-warning information through visual            interface
PN CN120639473-A
AE THIRD RES INST MIN PUBLIC SECURITY
AB 
   NOVELTY - The method involves collecting multi-source               
   heterogeneous data of a data center in real time.                Data
   verification, key field extraction and                metadata
   extraction process is performed through a                parser to
   complete data source classification                process. The
   currently obtained data matching                industry classification
   template is matched with an                industrial classification
   template according to a                data classification
   classification key point. A                protection level is
   dynamically adjusted according                to data dynamic access
   control rule, sensitivity                and service importance. A
   distributed intelligent                data lake storage structure is
   constructed. The                current obtained data is stored in a
   data lake                according to grading result stored in the data 
   lake. Safety situation and pre-warning information                are
   displayed through a visual interface.
   USE - Method for realizing industrial templated                modeling
   and dynamic access controlling process                based on data
   lake.
   ADVANTAGE - The method enables collecting the multi-source              
   heterogeneous data of the data center in real-time,               
   checking the data, extracting the key field and               
   extracting the metadata by the analyzer, where               
   classification protection is performed to the                currently
   obtained data matching industry                classification template
   according to the data                classification classification
   classification key                point, and thus enables to effectively
   solve the                limitation of traditional data lake modeling.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   system for realizing industrial templated modeling                and
   dynamic access controlling process based on                data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for realizing industrial templated modeling                and
   dynamic access controlling process based on                data lake
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-10-20
UT DIIDW:202592609H
ER

PT J
AU Pourramezani, Kiarash
   Baghaee, HR.
   Gharehpetian, Gevork. B.
TI Data-driven management for cyber-physical-social distribution networks:
   A comprehensive review and complex system-of-systems data analytics
   framework
SO ENERGY REPORTS
VL 14
BP 2170
EP 2190
DI 10.1016/j.egyr.2025.08.049
EA SEP 2025
DT Review
PD DEC 2025
PY 2025
AB The electricity industry is facing escalating energy demands, heightened
   market competition, and pressing environmental concerns, including air
   pollution and its reliance on fossil fuels. The advent of the smart
   grid, a next-generation power system enabling bidirectional flows of
   electricity and data, presents a transformative solution to these
   challenges. This evolution is underpinned by advancements in equipment,
   including sensors, smart meters, and Phasor Measurement Units (PMUs).
   However, integrating smart homes, Electric Vehicles (EVs), and smart
   cities into this infrastructure introduces unprecedented complexities
   that the traditional grid cannot effectively address. The smart grid
   relies on sophisticated data acquisition and processing systems,
   including Advanced Metering Infrastructure (AMI) and Intelligent
   Electronic Devices (IEDs), to achieve enhanced visibility and control.
   Data analytics frameworks must efficiently manage diverse data types and
   purposes, utilizing secure telecommunication networks and robust
   cybersecurity protocols. A critical challenge is minimizing data volume
   during preprocessing to mitigate cyberattack risks while ensuring system
   reliability. This balance between robust data-driven management and
   system stability is pivotal for efficiently operating cyberphysical
   Distribution Networks (DNs) in a complex system-of-systems architecture.
   This review explicitly highlights the joint role of Artificial
   Intelligence and Machine Learning (AI/ML) in conjunction with
   cybersecurity, enabling the secure and data-driven operation of
   cyber-physical-social DNs. We synthesize AI/ML methods for DN analytics
   and map the cybersecurity requirements, threats, and standards that must
   be codesigned with the data architecture.
RI Pourramezani, Kiarash/; B. Gharehpetian, Gevork/M-3759-2018
OI Pourramezani, Kiarash/0009-0002-9638-3212; 
ZR 0
TC 0
ZA 0
ZS 0
ZB 0
Z8 0
Z9 0
U1 5
U2 5
SN 2352-4847
DA 2025-09-17
UT WOS:001567632600001
ER

PT P
AU LU N
   CAI W
   CUI J
   LI S
   WU Z
   LI A
   LIU Y
   SHANG J
TI Method for processing data, involves constructing            contrastive
   learning model based on minimizing            similarities among modal
   features, and utilizing            contrastive learning model to manage
   multimodal data            stored in data lake-warehouse integrated
   system in            distributed manner
PN CN120578723-A
AE CHINA MOBILE INFORMATION TECHNOLOGY CO; CHINA MOBILE COMMUNICATIONS
   GROUP CO LTD
AB 
   NOVELTY - The method involves obtaining (11)                preprocessed
   multimodal data. The multimodal data                is stored in a data
   lake-warehouse integrated                system. The data lake-warehouse
   integrated system                is obtained by integrating a data
   warehouse with a                data lake cluster. The modal features
   are extracted                (12) from the multimodal data based on deep
   learning technology. The feature vectors of the                modal
   features are generated. The feature vectors                are mapped
   into an isometric space, and                similarities among the modal
   features are                calculated. The contrastive learning model
   is                constructed based on minimizing the similarities      
   among the modal features. The contrastive learning                model
   is utilized to manage the multimodal data                stored in the
   data lake-warehouse integrated system                in a distributed
   manner.
   USE - Method for processing data for optimizing               
   heterogeneous data queries in data lake-warehouse               
   executed by electronic device (claimed).
   ADVANTAGE - The multimodal data is managed in a               
   distributed manner according to the results of the               
   contrastive learning model, by using deep learning               
   technology to construct a contrastive learning                model
   based on modal features, thus enabling                efficient
   heterogeneous data query.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:1. a data processing device;2. an electronic device;3. a
   readable storage medium for storing                program for
   processing data; and4. a computer program product for processing        
          data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for processing data. (Drawing includes               
   non-English language text)11Step for obtaining preprocessed multimodal  
   data12Step for extracting modal features from                multimodal
   data
Z9 0
U1 0
U2 0
DA 2025-10-13
UT DIIDW:202590905V
ER

PT P
AU PENG P
   WANG Y
   LIN S
   HAO Y
TI Method for evaluating penetration based on deep            learning data
   architecture analysis in big data field,            involves performing
   standard evaluation to large data            system framework according
   to evaluation result and            dynamic analysis result
PN CN120562416-A; CN120562416-B
AE SHANGHAI COMPASS INFORMATION TECHNOLOGY
AB 
   NOVELTY - The method involves obtaining an                identification
   large model based on deep learning                process training. A
   large data system framework is                identified and analyzed
   through the identification                big model to obtain framework
   change data of a                large data framework. A monitoring index
   of the big                data framework is obtained. A dynamic analysis
   result is obtained according to the monitoring                index. A
   standard evaluation is performed on the                large-data system
   framework according to an                evaluation result and the
   dynamic analysis                result.
   USE - Method for evaluating penetration based on                deep
   learning data architecture analysis in big                data field.
   ADVANTAGE - The automatic monitoring process is realized               
   so as to capture and analyze the framework change                of the
   large data system in real time, which                obviously improves
   the monitoring efficiency,                reduces the time and resource
   needed by the manual                monitoring, and provides timely
   feedback for the                decision maker by evaluating whether
   these changes                are in accordance with the standard.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   system for evaluating penetration based on deep                learning
   data architecture analysis in big data                field.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for evaluating penetration based on deep               
   learning data architecture analysis in big data                field
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-10-05
UT DIIDW:202590279R
ER

PT P
AU ZHENG Y
   ZHAO L
   DUN X
TI Method for smartly managing regional-level            distributed
   photovoltaic power station based on            cloud-mist edge-end
   architecture, involves predicting            key indexes e.g. future
   power generation, device fault            rate and cost expenditure of
   power station
PN CN120528097-A
AE SHENYANG HUAYAN ELECTRIC TECHNOLOGY CO
AB 
   NOVELTY - The method involves deploying high-performance               
   micro-servers based on advanced ARM architecture at                an
   edge node close to a distributed photovoltaic                power
   station. A super-large-scale data lake is                built based on
   big data technology. An innovative                storage framework
   combined by a distributed file                system and a determinant
   storage database is                adopted to realize efficient storage
   and quick                retrieval of mass power station operation data.
   A                route planning algorithm is used for planning an       
   optimal inspection route for operation and                maintenance
   personnel to improve inspection                efficiency according to
   the distribution and                operation state of the power station
   device. Key                indexes e.g. future power generation, device
   fault                rate and cost expenditure of the power station are 
   predicted by using a time sequence analysis                algorithm and
   a machine learning algorithm.
   USE - Method for smartly managing regional-level               
   distributed photovoltaic power station based on               
   cloud-mist edge-end architecture used in energy               
   management and information technology cross.
   ADVANTAGE - The method calculates the indexes e.g. average              
   power generation efficiency, and also uses the time               
   sequence analysis and data normalization processing                for
   multidimensional comparison, so that the device               
   performance abnormality is found more timely and               
   accurately.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the method
   for smartly managing regional-level distributed               
   photovoltaic power station based on cloud-mist                edge-end
   architecture used in energy management and                information
   technology cross (Drawing includes                non-English language
   text).
Z9 0
U1 0
U2 0
DA 2025-09-29
UT DIIDW:2025871779
ER

PT P
AU LI Y
   LU D
   GAO Z
   LI X
   DI B
   PEI R
TI Highway incident handling method based on big            data, involves
   generating phased handling plans based            on real-time traffic
   density and congestion propagation            rate, and recording key
   nodes of handling process            through private blockchain
PN CN120510721-A
AE ANHUI WANTONG TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves performing (1) real-time               
   multi-source data collection and standardization.               
   Real-time data within a 10-minute window is cached                (2)
   using the Kafka streaming platform. The                historical data
   is stored in Parquet format on                Hadoop distributed file
   system (HDFS), and a                traffic data lake is constructed to
   support                multimodal queries. Vehicle speed variance, lane 
   occupancy, and meteorological visibility features                are
   classified (3) using a random forest algorithm,                and
   labels for accidents, congestion, and faults                are
   generated. A priority score is calculated by                combining a
   logistic time-window penalty factor.                The priority score
   is utilized for dynamic                scheduling of rescue resources.
   The higher score                indicates higher event handling
   priority. Phased                handling plans are generated (4) based
   on real-time                traffic density and congestion propagation
   rate.                The key nodes of the handling process are recorded 
                 through a private blockchain.
   USE - Highway incident handling method based on big                data.
   ADVANTAGE - The delays in highway incident response, rigid              
   resource allocation, and data silo issues are               
   systematically resolved, by integrating                multi-source
   data, dynamic priority evaluation,                phased and precise
   handling, and blockchain-based                record-keeping technology.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the highway incident handling method based on big                data.
   (Drawing includes non-English language                text)1Step for
   performing real-time multi-source                data collection and
   standardization2Step for caching real-time data within a               
   10-minute window3Step for classifying vehicle speed variance,           
   lane occupancy, and meteorological visibility                features
   using random forest algorithm4Step for generating phased handling       
           plans
Z9 0
U1 0
U2 0
DA 2025-09-29
UT DIIDW:202585911W
ER

PT P
AU TANG F
   LI H
   GAO J
   WANG L
   LI J
   CHENG W
TI Convergence management system of multi-source            remote sensing
   large data, has data storage layer that            is used for storing
   entity data and metadata governing            label set so that data
   identification (ID) in metadata            governing label set is
   associated with entity            data
PN CN120448445-A
AE GEOVIS DIGITAL EARTH HEFEI CO LTD
AB 
   NOVELTY - The system has a data gathering layer that is               
   used for gathering the multi-source remote sensing                large
   data through the interface and checking the                data
   integrity. A data management layer is used for               
   normalizing the multi-source remote sensing large                data to
   generate a standard entity data and                metadata management
   label set. A data storage layer                is used for storing the
   entity data and the                metadata governing label set so that
   the data ID in                the metadata governing label set is
   associated with                the entity data. A data sharing layer is
   used for                locating the target entity data based on the    
   metadata treatment label set and realizing the                target
   entity data security sharing.
   USE - Convergence management system of multi-source               
   remote sensing large data.
   ADVANTAGE - The system effectively solves the problem of               
   standardization and standardization of multi-source               
   heterogeneous data in the management process of                remote
   sensing large data, gathering and                integrating mass
   multi-source remote sensing data,                cleaning, checking,
   processing and quality checking                the data to form a clean,
   complete and consistent                data lake; establishing a uniform
   data management                rule and platform, rationally
   distributing the data                in the lake, realizing the data
   sharing channel to                be safe and controllable, and reducing
   the data                management cost of the individual and           
       enterprise.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the
   convergence management system of multi-source                remote
   sensing large data. (Drawing includes                non-English
   language text)
Z9 0
U1 0
U2 0
DA 2025-09-21
UT DIIDW:2025826738
ER

PT P
AU MONTALVO A
TI System for sensor data fusion for sensor            management and
   utilization in autonomous            transportation, has fusion engine
   operable to fuse            first distance measurement and second
   distance            measurement, and fusion engine created new data     
         set
PN US2025252157-A1; US12499182-B2
AE DIGITAL GLOBAL SYSTEMS INC
AB 
   NOVELTY - The system (800) has a curation engine                operable
   to curate a first distance measurement by                categorizing
   the first distance measurement into                first distance
   property and/or first distance                sub-property. The curation
   engine is operable to                curate a second distance
   measurement by                categorizing the second distance
   measurement into                second distance property and/or second
   distance                sub-property. A curation engine operable to
   filter                first distance measurement and second distance    
   measurement based on first distance property and/or                first
   distance sub-property and the second distance                property. A
   fusion engine is operable for fusing                the first distance
   measurements. The fusion engine                creates a data set. A
   computer processor instructs                a vehicle based on the data
   set.
   USE - System for sensor data fusion for sensor                management
   and utilization in utilization in                autonomous
   transportation.
   ADVANTAGE - The system increases sensor data fusion               
   accuracy while reducing computational processing               
   requirements and storage demands. once a user                generates a
   query, the system processes the query                against the stored
   data to find correlations                between the query and the
   indexed data before                producing results, requiring less
   computational                power than an unstructured data lake, but
   still                requiring a problematic amount of power and/or     
   computing time, ensures a camera and/or a LiDAR                sensor
   are accurately monitoring and/or predicting                traffic
   patterns and monitoring and/or predicting                whether an
   object is likely to enter the road while                an autonomous
   vehicle is transporting passengers,                minimizes storage
   requirements because fused data                is stored, reduces
   computational requirements                because sensor data is curated
   before and/or after                a computer processor receives a user
   and/or                computer query, and enhances sensor accuracy by   
   providing additional actionable data, allows the                system
   to unwind the fusion, linking, and/or                curation process,
   correcting errors and ensuring                accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for sensor data fusion for sensor management and               
   utilization in autonomous transportation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for sensor data fusion for sensor management and  
   utilization in utilization in autonomous               
   transportation.800System810Network812Wireless communication
   antenna850server852Operating system
Z9 0
U1 0
U2 0
DA 2025-08-20
UT DIIDW:202579029A
ER

PT P
AU GAO Y
TI Comprehensive energy cooperative controlling            method, involves
   storing standardized data stream to            preset multi-modal data
   lake, and obtaining            comprehensive energy scheduling data
   according to            standardized data stream in multi-modal data
   lake
PN CN120430465-A
AE SHANDONG INSPUR INTELLIGENT TERMINAL TEC
AB 
   NOVELTY - The method involves performing semantic                mapping
   on preset comprehensive energy source                association
   relation in a node-side manner to                construct an energy
   source field main body library.                Comprehensive energy
   parameter data of an edge side                is converted into
   JSON(RTM: Computer data format)                data through a protocol
   conversion device arranged                at the edge side. Semantic
   labeling is performed on                the JSON(RTM: Computer data
   format) data according                to a preset metadata label system.
   A uniform data                stream is generated. Data of the uniform
   data                stream is cleaned according to the energy source    
   field main body library to generate a standard data               
   stream. Standardized data stream is stored to a                preset
   multi-modal data lake. Comprehensive energy                scheduling
   data is obtained according to the                standardized data
   stream in the multi-modal data                lake.
   USE - Comprehensive energy cooperative controlling               
   method.
   ADVANTAGE - The method enables processing multiple               
   multi-source heterogeneous data through Kafka +                Flink,
   and improving overall response speed of the                comprehensive
   energy system.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   comprehensive energy cooperative                controlling device;
   and(2) an integrated energy cooperative control                storage
   medium for storing a set of instructions                for controlling
   comprehensive energy                cooperative.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a comprehensive energy cooperative
   controlling                method. (Drawing includes non-English
   language                text).
Z9 0
U1 0
U2 0
DA 2025-09-16
UT DIIDW:2025805121
ER

PT J
AU Saxena, Deepika
   Singh, Ashutosh Kumar
   Lindenstruth, Volker
TI QuAd-caching management model for heterogeneous data lake environments
SO EXPERT SYSTEMS WITH APPLICATIONS
VL 296
AR 129133
DI 10.1016/j.eswa.2025.129133
EA AUG 2025
PN D
DT Article
PD JAN 15 2026
PY 2026
AB Heterogeneous and multi-structured data, stored at distributed
   geographical locations leads to latency in user query processing and
   unavailability of demanded data. The existing caching schemes based on
   the duration of the web page stay within the cache, lag while dealing
   with the heterogeneity of content's demand, and fail to provision
   dynamic caching automatically. In this context, this paper proposes a
   novel dynamic and automatic cache management model named QuAd-Caching.
   It integrates diverse learning with the computational efficiency of
   Quantum machine learning and optimal solution-finding capability of Adam
   optimization for proactive estimation of caching contents. Specifically,
   three distinct QuAd estimators for cache size prediction, eviction, and
   entry, are employed to capture all-inclusive dynamic cache management in
   diverse data lake environments. The simulation and performance
   evaluation of the proposed QuAd caching using a benchmark dataset
   confirms its efficiency and potency in dynamic cache management while
   reducing average data access time up to 100.826 nsec as compared with
   optimal case reporting 100 nsec, and minimizing average delay up to 99%
   over without QuAd-caching. Further, the number of cache hits is improved
   up to 52.7% over the existing caching approaches.
OI Lindenstruth, Volker/0009-0006-7301-988X
ZS 0
ZA 0
TC 0
ZB 0
ZR 0
Z8 0
Z9 0
U1 2
U2 2
SN 0957-4174
EI 1873-6793
DA 2025-09-23
UT WOS:001545699100011
ER

PT P
AU GUO S
TI Method for optimizing device data collection and            query based
   on large-scale model-enabled enterprise            resource
   planning-monitoring management system            (ERP-MES) system used
   in industrial production field,            involves optimizing
   performance of big model by            learning new equipment data and
   query            requirements
PN CN120316134-A
AE SUZHOU LISHENGXIN INFORMATION TECHNOLOGY CO              LTD
AB 
   NOVELTY - The method involves deploying (S1) lightweight               
   noise detection model on edge nodes to identify               
   electromagnetic interference patterns in sensor                data
   collected by devices in real time. A                multimodal
   large-scale model is used (S2) to map                device's
   pre-processed data into a unified vector                space. The
   real-time status of equipment is                monitored (S3) and
   predicted by big model. The                determination is made to
   check whether the                equipment is operating healthily. A
   virtual data                lake interface is built (S4). The big model
   is used                to analyze index query characteristics of        
   heterogeneous data sources. The performance of big                model
   is optimized (S5) by learning new equipment                data and
   query requirements. A feedback mechanism                is established
   to provide feedback on accuracy and                efficiency of query
   results to the big model for                self-adjustment and
   optimization. The equipment                anomalies found in query
   results are pushed to the                ERP system in real time, and
   the collaboration is                triggered with the MES system.
   USE - Method for optimizing device data collection                and
   query based on large-scale model-enabled                ERP-MES system
   used in industrial production                field.
   ADVANTAGE - The method uses the edge node light noise               
   detection model combined with the environment data               
   compensation correction and uses the BERT large                model
   analysis protocol document design middleware                to realize
   the precise collection of the device                dynamic
   self-adaptive data and high-efficiency                processing
   function. The pre-processing data is                mapped and fused to
   form a uniform data warehouse                through the multimode large
   model, and the mapping                strategy is optimized by
   strengthening learning,                the heterogeneous data is fused,
   the data quality                of the device is improved, and the data
   fusion and                utilization efficiency are enhanced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a computer device; anda computer readable storage medium
   storing                program for optimizing device data collection and
   query based on a large-scale model-enabled ERP-MES               
   system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating 
   the method for optimizing device data collection                and
   query based on a large-scale model-enabled                ERP-MES
   system. (Drawing includes non-English                language
   text).S1Step for deploying lightweight noise                detection
   model on edge nodes to identify                electromagnetic
   interference patterns in sensor                data collected by devices
   in real timeS2Step for using multimodal large-scale model               
   to map the device's preprocessed data into a                unified
   vector spaceS3Step for monitoring and predicting the               
   real-time status of equipment by the big                modelS4Step for
   building virtual data lake                interfaceS5Step for
   continuously optimizing the                performance of big model
Z9 0
U1 0
U2 0
DA 2025-09-08
UT DIIDW:202575031A
ER

PT P
AU ZHANG P
   OUYANG M
   YANG F
   LI M Z
   ZHAO W
   MA W
   DOU Y
   ZHANG B
   HAN P
   MENG H
   ZHANG S
   LEI P
   OUYANG G
   NING Y
   QIAO Y
   XUE Z
   ZHAO H
   MA C
   LI J
   LI L
   ZHANG J
   HAN C
   REN H
   LIU A
   HOU G
   WU H
TI Method for building mine big data processing            system, involves
   obtaining service data and log data            from service system in
   real time, and using Hudi for            stepping real-time data in Hive
   and building lake cabin            integrated structure in layers
PN CN120316816-A
AE TIANDI SCI & TECHNOLOGY CO LTD; CCTEG COAL MINING RES INST
AB 
   NOVELTY - The method involves obtaining service data and               
   log data from a service system in real time. Hudi                is used
   for stepping real-time data in Hive and                building a lake
   cabin integrated structure in                layers to obtain a big data
   processing system. The                service data is captured in real
   time by using                Flink Change Data Capture (CDC). Flume and
   Kafka                are used to collect log data in real time. Hudi    
   related dependency is introduced in the Flink.                HudiSink
   is configured to ensure that the result                data generated by
   the Flink calculation effectively                written into a Hudi
   data lake. A HiveCatalog of the                Flink is configured to
   ensure that an effective                connection that is established
   between the Flink                and the Hive.
   USE - Method for building mine big data processing               
   system.
   ADVANTAGE - The method enables combining the data lake and              
   the data warehouse to establish the mine large data               
   processing system based on the lake warehouse               
   integration, selecting the proper storage engine               
   according to the different characteristics of the                data
   and the service requirement, improving the                flexibility of
   data and making the system more                suitable for diversified
   data and service scene,                and realizing better data
   isolation and safety                control. The data of different
   business departments                can be independently managed, which
   improves the                privacy and safety of data. The
   lake-warehouse                integrated system can generally support
   real-time                calculation and analysis, and can better adapt
   to                the business scene with high real time requirement    
   in combination with the original performance of                data
   lake.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   building device of mine big data                processing system;(2) a
   computer readable storage medium for                storing a set of
   instructions for building mine big                data processing
   system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for building mine big data
   processing                system. (Drawing includes non-English language
                  text).
Z9 0
U1 0
U2 0
DA 2025-09-08
UT DIIDW:202573866J
ER

PT P
AU ZENG P
   LI D
   LIU Y
   REQUEST N N B P
   LI Y
   LAN D
TI Unstructured data lake hierarchical storage and            verification
   system, has metadata management layer for            supporting
   multi-dimensional search, and verification            service layer for
   providing data verification service            to ensure integrity of
   data
PN CN120296224-A
AE SHENYANG AUTOMATION INST CAS
AB 
   NOVELTY - The system has an application layer that               
   interacts with a data lake interface layer and                realizes
   the access and operation of data by                calling the service
   provided by the data Lake                interface layer. A metadata
   management layer is                used to store the identification (ID)
   of the user                file, the storage path, the check sum,
   storage                type, the access frequency and the service label,
   and supports the multi-dimensional search according                to
   the label, the attribute and the time range. An               
   unstructured data storage layer is configured to                store a
   user file transmitted by the data Lake                interface layers,
   support multiple storage mediums                and storage solutions,
   and automatically distribute                the storage medium based on
   a preset rule. A                verification service layer provides the
   data                verification service to ensure the integrity and    
              accuracy of the data.
   USE - Unstructured data lake hierarchical storage                and
   verification system for use in an                enterprise.
   ADVANTAGE - The system realizes automatic scheduling,               
   uniform access and multi-dimensional check                management of
   cold and hot layered storage in                unstructured data lake.
   The system allows the user                file to be transmitted by the
   data lake interface                layer, thus supporting multiple
   storage mediums and                storage solutions, and automatically
   distributing                the storage medium based on the preset rule.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an     
   unstructured data lake hierarchical storage and               
   verification method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a structural block diagram
   of an unstructured data lake hierarchical storage                and
   verification system (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2025-09-08
UT DIIDW:2025728812
ER

PT P
AU SHANG H
   GUAN W
   LU M
   WANG Y
   CAO Z
   PU A
   YANG H
   DU R
TI Method for publishing online data interface            configuration,
   involves performing interface            configuration encapsulation
   process based on online            configuration table and query engines
   corresponding to            storage components to obtain target
   interface, and            scheduling workflow of target interface
PN CN120234058-A
AE CHINA EVERBRIGHT BANK CO LTD CREDIT CARD
AB 
   NOVELTY - The method involves displaying an application               
   page through a human-computer interaction interface                in
   response to an online interface application                request,
   where the application page is utilized for                configuring an
   interface parameter required by an                online interface. An
   online configuration table is                determined corresponding to
   the online interface                based on the interface parameter in
   response to a                submission request for the application
   page. A                checking result of the online configuration table
   is obtained. Authorization and resource allocation               
   processes are performed on an access tenant of the                online
   interface if the checking result is passed.                Interface
   configuration encapsulation process is                performed based on
   the online configuration table                and a preset rule
   expression engine, a                configuration constraint frame, a
   micro-service                framework and query engines corresponding
   to                different storage components to obtain a target       
   interface. A workflow of the target interface is               
   scheduled.
   USE - Method for publishing an online data interface               
   configuration.
   ADVANTAGE - The method enables improving publishing               
   efficiency and flexibility of an online data                interface.
   DETAILED DESCRIPTION - The application page comprises an interface      
   use, an access tenant, a cluster type, a cluster                number,
   a library, a table, an inquiry engine, a                common inquiry
   condition, an inquiry field, a                processing logic and an
   access user amount. The                storage components comprise HBase
   (RTM:                open-source, column-oriented distributed database  
   system in a Hadoop environment), Elasticsearch                (RTM:
   real-time distributed and open source                full-text search
   and analytics engine), CLICKHOUSE                (RTM: open-source
   column-oriented database                management system), Kudu (RTM:
   distributed columnar                storage engine), Hadoop (RTM: open
   source                framework) distributed file storage system (HDFS) 
   /Hive (RTM: data warehouse system), data Paimon                (RTM:
   open-source Lakehouse storage framework)/Hudi                (RTM: open
   data lakehouse platform), StarRocks                (RTM: open-source
   high performing analytical                database)/Doris (RTM: modern
   data warehouse for                real-time analytics), distributed
   cache Redis (RTM:                Database management system), relational
   database                storing MySQL (RTM: relational database
   management                system), Oracle (RTM: relational database     
   management system) and EverDB (RTM: embedded                database
   system). The query engines comprise                Phoenix java database
   connectivity (JDBC) (RTM:                application program interface)
   query encapsulation,                HBase (RTM: open-source,
   column-oriented                distributed database system in a Hadoop  
   environment) application programming interface                (API)
   encapsulation, Elasticsearch (RTM: real-time                distributed
   and open source full-text search and                analytics engine)
   API interface query                encapsulation, CLICKHOUSE (RTM:
   open-source                column-oriented database management system)  
   interface encapsulation, Impala interface                encapsulation,
   Presto (RTM: distributed query                engine for big data using
   the SQL query                language)/Trino (RTM: highly parallel and  
   distributed query engine) interface encapsulation,               
   StarRocks (RTM: open-source high performing                analytical
   database)/Doris (RTM: modern data                warehouse for real-time
   analytics)universal                interface encapsulation, MySQL (RTM:
   relational                database management system) universal
   interface                encapsulation, Oracle (RTM: relational database
   management system) universal interface                encapsulation,
   EverDB (RTM: embedded database                system) universal
   interface encapsulation and                distributed cache Redis
   universal interface                encapsulation. The micro-service
   framework is a                Spring-Boot (RTM: microservice-based
   framework) for                packaging the target interface into an
   executable                Java archive (JAR) file (RTM: package file
   format)                file.INDEPENDENT CLAIMS are included for: (1) an 
   online data interface configuration publishing                device;
   (2) an electronic device comprising a                storage medium and
   a processor for executing                machine-readable instructions
   for performing a                process of publishing an online data
   interface                configuration; and (3) a computer-readable
   storage                medium storing a computer program for executing a
   process of publishing an online data interface               
   configuration by a processor.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for publishing an online data
   interface                configuration. (Drawing includes non-English   
               language text).
Z9 0
U1 0
U2 0
DA 2025-08-12
UT DIIDW:202569537C
ER

PT P
AU UDDIHAL G
   GHOSH S
   BADDAM V R
   THUNUGUNTLA S S
TI Method for batch materialization of incremental            change data
   capture change set in computer system for            online analytical
   processing services over            communication network, involves
   combining baseline            changed data frame with baseline unmatched
   data frame            to produce final changed baseline data table
PN US12346331-B1
AE INTUIT INC
AB 
   NOVELTY - The method involves receiving an incremental               
   change data capture (CDC) change set comprising a                set of
   primary keys associated with corresponding                data changes
   with one of additions, updates, and                deletes, and
   extracting primary keys from the                incremental CDC change
   set. Extracted primary keys                are added to a Bloom filter.
   A baseline data table                (132) is filtered from a data lake
   based on the                extracted primary keys in a broadcast Bloom
   filter                to produce a baseline match data frame and a      
   baseline unmatched data frame. A different subset                of
   incremental CDC change set is provided to each                executor.
   Changes in a received subset of the                incremental CDC
   change set are applied to the                baseline match data frame
   to produce a baseline                changed data frame. The baseline
   changed data frame                is combined with the baseline
   unmatched data frame                to produce a final changed baseline
   data table. The                final changed baseline data table is
   stored in the                data lake.
   USE - Method for batch materialization of an                incremental
   CDC change set in a computer system                (claimed) for online
   transaction processing (OLTP)                or online analytical
   processing (OLAP) services                over a communication network.
   Uses include but are                not limited to enterprise resource
   planning                services, big data analytics servers for
   advanced                analytics, data warehousing services, and
   business                intelligence environments over a telephone
   network,                a local area network (LAN), a wide area network 
                 (WAN), and an Internet.
   ADVANTAGE - The method enables broadcasting the extracted               
   primary keys so as to eliminate a need for a                shuffle
   operation to complete the merge operation                with the CDC
   change set, thus reducing resources                and time required to
   complete batch materialization                operations in efficient
   manner without requiring                expensive or complex
   modifications to a batch                materialization architecture,
   and improving                function of the computer system and a
   database                technology y using a commodity hardware, which
   can                be readily available, inexpensive, and easily        
          interchangeable with other hardware.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
       computer system configured for batch                materialization.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of a network environment.132Baseline data table101 Computing device112
   Interface130 Database150 Network
Z9 0
U1 0
U2 0
DA 2025-07-18
UT DIIDW:2025659219
ER

PT J
AU Yang, Ruoxi
   Gao, Jie
TI Enhancing Financial Data Security in Big Data Environments Using
   AES-Blowfish and Cloud-Aided Encryption Techniques
SO JOURNAL OF CIRCUITS SYSTEMS AND COMPUTERS
VL 34
IS 17
DI 10.1142/S0218126625503396
EA JUN 2025
DT Article
PD NOV 30 2025
PY 2025
AB Big data analyzes various variables, such as conventional financial
   data, consumer behavior and sentiment from social media, to enable
   predictive modeling. Financial companies may now forecast future
   performance and recognize dangers posed by outside variables like
   political unrest. Another important advantage is real-time risk
   assessment, which enables ongoing observation and flexible
   modifications. The field of financial risk management is seeing a
   profound transformation due to the transformative influence of big data
   analytics. Ensuring secure communication channels among financial
   institutions and clients is crucial in the Fintech industry to safeguard
   confidential data, including private financial data, contact details and
   credentials. Encryption is essential to keep the sensitive and private
   information shared across these channels private. This research aims to
   protect intellectual property and private information due to the dynamic
   nature of the Internet. The major part of this study is to determine and
   evaluate potential solutions for data protection at each stage of the
   data life cycle, from data collecting to data processing. The report
   highlights the necessity for strong security procedures even when
   handling massive data volumes while acknowledging the potential of big
   data. This study proposes a method that combines the MapReduce Big data
   procedure with the encryption and decryption algorithms for AES and
   blowfish to improve big data with cloud-aided financial data protection.
   The first step of data protection was carried out using AES-Blowfish,
   and then the significant data architecture was implemented to improve
   the data security in cloud environments. Performance tests are conducted
   for the following criteria: throughput, compressed ratio, encryption
   time, decryption time and information loss, and the suggested strategy
   for these procedures is implemented using R programming.
ZR 0
Z8 0
ZA 0
ZB 0
ZS 0
TC 0
Z9 0
U1 3
U2 3
SN 0218-1266
EI 1793-6454
DA 2025-07-05
UT WOS:001519572900001
ER

PT P
AU FRY T W
   PANDE A A
   SHIRAN T
   NADEAU J
TI Non-transitory computer-readable medium storing            program for
   automatically caching and updating cached            data at local
   cluster for caching by data system, has            set of instructions
   for returning second query result            including cached data
   object read from storage            location
PN US2025181582-A1
AE DREMIO CORP
AB 
   NOVELTY - The medium has a set of instruction for               
   obtaining data object from an external data source.                The
   data object is cached in a storage location of                a data
   system as a cached data object. A unit of                hashing
   corresponding to an output of a hash                algorithm is
   generated based on an input indicative                of the data
   objects. The cached data objects are                mapped to the
   external data sources in accordance                with the unit of the
   hashing. A query is received                that is configured for
   reading data stored at the                external source. A first query
   result that                satisfies the query includes the data. A
   second                query result including cached data object read is 
                 returned from the storage location.
   USE - Non-transitory computer-readable medium                storing
   program for automatically caching and                updating cached
   data at local cluster of nodes to                accelerate read
   requests for data at external                systems mapped to local
   cluster for caching by a                data system with execution
   engines e.g. virtual                execution engines in public cloud   
               deployments.
   ADVANTAGE - The method enables automatically caching and               
   updating cached data at a local cluster of nodes to               
   accelerate read requests for data at external                systems
   mapped to the local cluster. The                traditional tools such
   as SQL databases are unable                to effectively store or
   process the larger volumes                of data due to cost and
   technology limitations. The                large-scale datasets analyzes
   to utilize SQL-on-Big                Data tools that provide users with
   SQL-based access                to data stored in a data lake. The
   caching                architecture is designed to flexibly support any 
   type of external data source, runtime changes to                cluster
   configuration due to a node removal or                inclusion, data
   changes on external systems which                are outside the
   platform, multiple execution                engines in a single cluster,
   high performance by                sending compute to nodes data is
   cached, and high                concurrency by creating multiple cached
   copies of                the same data on multiple nodes when highly    
   accessed. The users can experience higher                performance
   because the data system can                automatically identify
   commonly accessed data and                store on nodes of the
   platform. The method improves                performance but creates a
   consistent experience                across different environments and
   storage services.                The medium prevents the need to
   transmit data                across nodes within the platform cluster
   and                improves performance while always reading locally    
   from cache. The performance and simplification are               
   improved. The load balancer can decide to enable                multiple
   rings for that file, so that copies are                cached locally.
   The load balancer can map requests                for that file to
   multiple nodes with additional                compute resources.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
       computing system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for providing a query request.100Automatically caching and
   updating cached                data at local cluster of nodes to
   accelerate read                requests102Resulting query plan includes
   query                fragments divided into distinct phases for node    
   cluster104-AMapping fragments that read from                external
   data source to target nodes104-BSending query fragments to targeted     
   nodes for execution in accordance with               
   mapping106Performing target node during                execution
Z9 0
U1 0
U2 0
DA 2025-06-22
UT DIIDW:2025579919
ER

PT P
AU PENG D
   XIONG M
   GUO C
   SHI M
   HUANG C
TI Hot upgrading method, involves controlling target            process to
   pull up new process according to            configuration information,
   where target process sends            upgrading data to new process to
   thermally upgrade            target process
PN CN120085895-A
AE BEIJING DIDIWUXIAN TECHNOLOGY DEV CO LTD
AB 
   NOVELTY - The method involves receiving (S110) a thermal               
   upgrade request of a target mounting point. A               
   corresponding target process thermal upgrading task                is
   triggered (S120) in response to receiving a                thermal
   upgrading request of a target mounting                point. The target
   process to pull up a new process                is controlled (S130)
   according to the configuration                information. The target
   process sends (S140) the                upgrading data to the new
   process to carry out hot                upgrading on the target process.
   USE - Method for performing thermal upgrading of                target
   process e.g. Posix log disc storage client                process, in
   cloud native data lake storage system                for storing
   unstructured data e.g. picture, video                and log, in
   electronic device.
   ADVANTAGE - The target process sends the upgrading data to              
   the new process to thermally upgrade the target                process,
   so that new process is pulled up without                manual
   intervention to realize the thermal                upgrading, thus
   improving thermal upgrading                efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:(1) a thermal upgrading device;(2) an electronic device;
   and(3) a computer readable storage medium storing                program
   for hot upgrading.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the hot   
   upgrade method. (Drawing includes non-English                language
   text).S110Step for receiving a thermal upgrade                request of
   a target mounting pointS120Step for triggering a corresponding          
   target process thermal upgrading task in response                to
   receiving a thermal upgrading request of a                target
   mounting pointS130Step for controlling the target process               
   to pull up a new process according to the                configuration
   informationS140Step for sending the upgrading data to                the
   new process to carry out hot upgrading on the                target
   process
Z9 0
U1 0
U2 0
DA 2025-07-18
UT DIIDW:2025585755
ER

PT J
AU ORDÓÑEZ PALACIOS, LUIS EDUARDO
   BUCHELI GUERRERO, VÍCTOR
   CAICEDO BRAVO, EDUARDO
TI E-solar: una herramienta para la evaluación del recurso solar basada en
   una arquitectura big data sobre un ambiente PySpark
X1 E-solar: a tool for solar resource assessment based on a Big Data
   architecture in a PySpark environment
SO Ingeniería y Desarrollo
VL 43
IS 1
BP 6
EP 23
DI 10.14482/inde.43.01.456.089
DT research-article
PD 2025-06
PY 2025
AB Abstract Over time, diverse researchers have created mathematical,
   statistical, and predictive models to evaluate solar resources. However,
   their implementation in technical tools restricts their usability for
   non-technical users. Additionally, data processing to estimate solar
   radiation often necessitates powerful hardware. This study introduces a
   Big Data based tool that employs flat files and satellite images to
   estimate solar radiation in Colombia. A model was developed using
   machine learning techniques and various programming languages. It
   operates within MapR, a distribution of the Hadoop ecosystem with an
   extensive array of Big Data capabilities and utilizes the PySpark API
   for parallel data processing within a computer cluster. The E-Solar
   tool, deployed on a web server, underwent assessment by professionals
   within the energy sector. Usability was analyzed, compliance with recent
   programming standards was confirmed, and profiles of interested users
   were identified. The solar radiation data generated by the tool are
   pivotal for solar projects. Furthermore, the tool lends support to
   researchers and organizations in decision-making for the implementation
   of photovoltaic systems, as it offers pertinent information regarding
   the behavior of solar resources in Colombia.
X4 Resumen Con el tiempo, diversos investigadores han creado modelos
   matemáticos, estadísticos y predictivos para evaluar el recurso solar.
   Sin embargo, su implementación en herramientas técnicas limita su
   utilización por usuarios no técnicos. Además, el procesamiento de datos
   para estimar la radiación solar suele requerir hardware potente. Este
   estudio presenta una herramienta basada en Big data que utiliza archivos
   planos e imágenes de satélite para estimar la radiación solar en
   Colombia. Se desarrolló un modelo con técnicas de aprendizaje automático
   y varios lenguajes de programación. Se ejecuta en MapR, una distribución
   del ecosistema Hadoop con un amplio conjunto de capacidades big data y
   emplea la API de PySpark para procesar datos en paralelo en un clúster
   de computadoras. La herramienta E-solar implementada en un servidor web
   fue evaluada por profesionales del sector energético. Se analizó la
   usabilidad, se verificó la conformidad con estándares de programación
   recientes y se identificaron perfiles de usuarios interesados. Los datos
   de radiación solar generados por la herramienta son fundamentales para
   proyectos solares. Además, la herramienta proporciona apoyo a
   investigadores y organizaciones; y facilita la toma de decisiones en la
   implementación de sistemas fotovol-taicos al ofrecer información
   relevante sobre el comportamiento del recurso solar en Colombia.
ZS 0
TC 0
ZA 0
ZB 0
ZR 0
Z8 0
Z9 0
U1 0
U2 0
SN 2145-9371
DA 2025-08-21
UT SCIELO:S0122-34612025000100006
ER

PT P
AU VUYYURU S R
TI Artificial intelligence-friendly data            infrastructure for
   unifying retail information through            central data lake, has
   infrastructure body for            integrating diverse data types from
   retail            point-of-sale terminals and e-commerce platforms along
              with CRM programs
PN IN202541044444-A
AE VUYYURU S R
AB 
   NOVELTY - The infrastructure has an infrastructure body               
   for integrating diverse data types from retail               
   point-of-sale terminals and e-commerce platforms                along
   with CRM programs and inventory management                platforms and
   IoT sensor networks. A real-time data                ingestion layer
   incorporates streaming and batch                processing technologies
   to establish a continuous                capture for diverse retail
   endpoint data that                routes streams into centralized data
   repository. A                data architecture contains additional      
   functionalities for encompassing a data processing                and
   transformation layer which performs ETL/ELT                operations,
   feature engineering, normalization, and                enrichment for
   analytics and machine learning                applications.
   USE - Artificial intelligence-friendly data               
   infrastructure for unifying retail information                through a
   central data lake.
   ADVANTAGE - The system utilizes cloud-native solutions in               
   combination with distributed pipelines and                independent
   microservice design to achieve system                flexibility and
   continuous operation capabilities.                The system combines
   features from big data                management with elements from
   cloud computing and                deployments of artificial
   intelligence (AI) models                along with retail technology to
   construct advanced                data architecture that facilitates
   data movement                while improving operational efficiency and
   enabling                smart retail experiences. The system handles    
   massive amounts of fast-moving data insertion while               
   maintaining data quality standards and providing               
   straightforward AI model development                capabilities.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an    
       artificial intelligence-friendly data                infrastructure.
Z9 0
U1 0
U2 0
DA 2025-06-29
UT DIIDW:2025592473
ER

PT P
AU JYOTHY C R
   ABRAHAM S
   RAVEENDRANATH R
   GOPIKA G
   SHIRIN A
TI Artificial intelligence-driven decision support            system for
   digital transformation in traditional            manufacturing
   enterprises, has secure deployment            architecture that supports
   on-premise or cloud-based            environments
PN IN202541041001-A
AE MANGALAM ENG COLLEGE
AB 
   NOVELTY - The system has a data integration layer for               
   collecting operational and enterprise data from               
   enterprise resource planning (ERP), manufacturing               
   execution system (MES), supervisory control and                data
   acquisition (SCADA) and Internet of things                (IoT) systems.
   An artificial intelligence (AI)                engine performs
   predictive, prescriptive and                diagnostic analytics on
   manufacturing data. A                modular user interface (UI) is
   provided with                role-based dashboards and recommendation   
   visualization. A digital twin simulation module is               
   provided for scenario testing. A secure deployment               
   architecture supports on-premise or cloud-based               
   environments.
   USE - AI-driven decision support system for digital               
   transformation in traditional manufacturing                enterprises.
   ADVANTAGE - The system bridges the gap between traditional              
   manufacturing systems and advanced digital                technologies
   using an incremental and modular                approach, avoids a
   complete overhaul of existing                systems, preserves capital
   investment, provides                AI-powered insights to allow good
   decision-making                across maintenance, quality control,
   energy                management and inventory planning, scales across  
   industries and plant sizes, is cloud compatible,                tailored
   with key performance indicators (KPIs)                relevant to roles
   and deployed in low-connectivity                environments using edge
   computing strategies,                empowers mid-sized manufacturers to
   harness digital                tools for competitiveness, flexibility,
   and                sustainable growth, combines predictive analytics,   
   real-time monitoring, and data visualization to                assist
   plant managers, engineers, and executives in                process
   optimization, machine utilization, and                operational
   forecasting, contributes to the                practical adoption of
   Industry 4.0 technologies                within brownfield industrial
   ecosystems, operates                on legacy systems with limited
   connectivity,                fragmented data sources and outdated       
   decision-making processes, increases data centric               
   economy, guides traditional manufacturing                enterprises
   through the process of digital                transformation, enhances
   existing enterprise                software e.g. ERP, MES, and SCADA
   systems using AI                for analytics, forecasting and
   optimization,                aggregates structured and unstructured data
   from                multiple operational silos comprising machine logs, 
   energy meters, maintenance schedules and supply                chain
   flows, processes data to identify                inefficiencies,
   suggests process adjustments,                predicts machine failures,
   offers role specific                dashboards and actionable
   recommendations,                simulates operational scenarios for
   risk-free                experimentation, supports local deployment for 
   data-sensitive industries and secure cloud                deployment or
   on- premise hosting and uses data                encryption and access
   control mechanisms for                cybersecurity compliance, achieves
   modularity,                ensures that manufacturers adopts components 
   incrementally without overhauling their                infrastructure,
   reduces the barrier to Industry 4.0                adoption by merging
   real-time intelligence,                historical context, and strategic
   insight into a                unified decision support system, empowers 
   manufacturers to remain competitive and agile,                augments
   industrial operations without disrupting                core processes,
   collects and aggregates operational                data from IoT
   sensors, machine controllers, energy                meters, and
   maintenance logs in real time into a                central data lake,
   pre-processes data using                techniques e.g. normalization,
   anomaly detection                and imputation to ensure quality,
   identifies                operational bottlenecks, predicts component
   wear,                classifies machine efficiency zones, generates     
   prescriptive suggestions e.g. optimal machine                scheduling,
   energy saving strategies or proactive                inventory ordering,
   features interactive dashboards                for plant managers,
   supervisors and executives and                allows simulation of
   scenarios before implementing                real-world changes.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
              AI-driven decision support system.
Z9 0
U1 0
U2 0
DA 2025-06-21
UT DIIDW:202557041V
ER

PT P
AU ROY P
TI Artificial intelligence (AI)-based retail supply            chain system
   for monitoring shipments and automating            order placement in
   retail operations, has procurement            engine that is configured
   to automate order placement            based on predictive analytics
PN IN202531046634-A
AE ROY P
AB 
   NOVELTY - The AI-based retail supply chain system has a               
   centralized Data Lake-house that integrates                structured
   and unstructured data. A forecasting                engine utilizes
   linear time series machine,                transformer models, and
   Langraph RAG for demand                prediction. The
   Internet-of-things (IoT) sensor                provides real-time
   tracking of inventory and                shipments. A procurement engine
   automates order                placement based on predictive analytics.
   The Data                Lake-house stores historical sales data,
   supplier                logs, and social sentiment data. The AI
   forecasting                engine utilizes multi-variable time-series   
   forecasting and sentiment analysis for dynamic                demand
   prediction.
   USE - Artificial intelligence (AI)-based retail                supply
   chain system for monitoring shipments and                automating
   order placement in retail                operations.
   ADVANTAGE - The system significantly enhances inventory               
   accuracy, improves fulfillment efficiency, and                reduces
   logistics risks. The AI model achieves                forecast-accuracy,
   cut stock outs and lowered                inventory costs. The system
   ensures continuity in                decentralized retail networks,
   improves system                robustness and minimizes operational     
             downtime.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
              for retail supply chain optimization.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
         AI-powered retail supply chain optimization                system.
Z9 0
U1 0
U2 0
DA 2025-06-19
UT DIIDW:202557024A
ER

PT P
AU ERDMANN C A
   BRUSH R A
   SUTARIYA B B
TI Non-transitory media for storing program for            providing
   closed-loop intelligence in financial fields            to capture and
   analyze vast streams of transactional,            experimental and other
   data, includes projecting at            least one condition onto first
   data model by hardware            processors to produce second data
   model
PN US2025165438-A1
AE CERNER INNOVATION INC
AB 
   NOVELTY - The non-transitory media includes receiving a               
   selection of a subset of information from a set of               
   information in a data structure at a cloud-based               
   computing platform. The set of information is                provided
   corresponding to multiple sets of data                provided by
   multiple disparate data sources, and                multiple disparate
   data sources are associated with                multiple data generating
   entities. The portion of                the set of information is
   transformed based on the                subset of information and based
   on a                standardization format to generate a transformed    
   portion of the set of information. At least one                condition
   is projected onto the first data model by                the hardware
   processors to produce a second data                model.
   USE - Non-transitory media for storing program for               
   providing closed-loop intelligence in financial,               
   scientific, medical and other fields to capture and               
   analyze vast streams of transactional,                experimental, and
   other data.
   ADVANTAGE - The method enables providing an efficient and               
   convenient system that enables structured views of                data
   to be aggregated into a big data architecture,                to train
   and test predictive tools on an entire                population of
   data, to push insights gained from                the predictive tools
   into a clinical workflow, and                to provide access and use
   to predictive tools by                other organizations. The method
   trains and tests                predictive tools on an entire population
   of data,                and pushes insights gained from the predictive  
   tools on the entire population of data into a                clinical
   workflow, and also provides access and                uses to these
   predictive tools by other                organizations. The method
   enables a user to take a                very complex data set and turns
   it into much                simpler tables that provides greater
   efficiencies                and reduces costs associated with using the 
   closed-loop intelligence system. The method                corrects
   misclassifications from previous boosting                iterations.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of a computing environment for providing                closed-loop
   intelligence.100Computing environment102Control server104Data
   store106Computer network108Remote computer
Z9 0
U1 0
U2 0
DA 2025-06-13
UT DIIDW:2025543727
ER

PT P
AU ERDMANN C A
   BRUSH R A
   SUTARIYA B B
TI Non-transitory medium for providing closed-loop            intelligence
   in e.g. financial field, to capture and            analyze vast streams
   of transactional, experimental,            and other data, involves
   providing data structure by            disparate data sources that are
   associated with data            generating entities
PN US2025165439-A1
AE CERNER INNOVATION INC
AB 
   NOVELTY - The medium has a set of instructions for               
   building a first data model at a cloud-based                computing
   platform through hardware processors                based on a
   transformed portion of the set of                information. A
   selection of a condition is received                from a set
   conditions at the cloud- based computing                platform. The
   condition is projected onto the first                model through the
   hardware processors to produce a                second data model. The
   second data model is                electronically written to one or
   both of a data                structure and the platform through the
   processors.                The data structure is provided by disparate
   data                sources that are associated with data generating    
              entities.
   USE - Non-transitory medium for providing                closed-loop
   intelligence in financial, scientific,                medical and other
   fields to capture and analyze                vast streams of
   transactional, experimental, and                other data.
   ADVANTAGE - The method enables providing an efficient and               
   convenient system that enables structured views of                data
   to be aggregated into a big data architecture                to train
   and test predictive tools on an entire                population of
   data, to push insights gained from                the predictive tools
   into a clinical workflow, and                to provide access and use
   to predictive tools by                other organizations, and allowing
   a user to take a                very complex data set and turn it into
   much simpler                tables that provides greater efficiencies
   and                reduces costs associated with using a closed-loop    
              intelligence system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   computing environment.100Computing environment102Control server104Data
   store106Computer networks108Remote computers
Z9 0
U1 0
U2 0
DA 2025-06-04
UT DIIDW:2025543588
ER

PT P
AU VATTIKONDA N
   NARRA B
   POLU A R
   PATCHIPULUSU H H S
   GUPTA A K
   BUDDULA D V K R
TI System for scalable and secure data ingestion,            analysis, and
   visualization across distributed edge and            cloud computing
   environments, has visualization            interface that generates
   real-time insights and            interactive dashboards
PN IN202541037889-A
AE VATTIKONDA N; NARRA B; POLU A R; PATCHIPULUSU H H S; GUPTA A K; BUDDULA
   D V K R
AB 
   NOVELTY - The system has set of edge nodes configured to               
   perform initial data processing and artificial               
   intelligence (AI) inference. A cloud-based                analytics
   engine is configured to perform batch and                real-time
   processing. A data ingestion engine                utilizes AI
   algorithms for adaptive routing, schema                inference, and
   deduplication. A data fabric layer                enables synchronized
   data sharing between edge and                cloud nodes. A secure data
   governance module                enforcies access control, encryption,
   and                compliance policies. A visualization interface       
   generates real-time insights and interactive                dashboards.
   The data ingestion engine dynamically                adjusts ingestion
   pipelines based on data volume,                type, and network
   conditions using reinforcement                learning.
   USE - System for facilitating scalable and secure                data
   ingestion, analysis and visualization across               
   heterogeneous cloud and edge computing                environments.
   ADVANTAGE - The system dynamically scales and provides               
   low-latency insights, and ensures compliance with                privacy
   and security policies across distributed                environments.
   The system provides a unified,                intelligent, and secure
   data architecture that                enables real-time ingestion,
   analysis, and                visualization across distributed
   infrastructures                while being adaptable to various data
   volumes,                velocities, and varieties.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   method for realizing real-time big data processing                in
   distributed computing environments.
Z9 0
U1 0
U2 0
DA 2025-06-04
UT DIIDW:202554420Q
ER

PT P
AU GUTTULA S C
   PATEL H
   KAUL A
TI Method for selecting and ordering application of            data quality
   rules during ingestion of raw data by data            lake or other data
   repository in enterprises, involves            generating order of
   executing subset of selected data            quality rules
PN US2025156385-A1; US12353375-B2
AE INT BUSINESS MACHINES CORP
AB 
   NOVELTY - The method (200) involves generating (202) a               
   snapshot of a table-formatted dataset. The snapshot               
   provides a sample comprises a reduced number of                rows of
   the table-formatted dataset such that each                column
   variation of the table-formatted dataset is                included in
   the snapshot. A predetermined                collection of data quality
   (DQ) rules is executed                (204) on the snapshot. Multiple
   performance                statistics for the DQ rule is determined
   (206). The                performance statistics indicate a likelihood
   that a                DQ rule determines a data quality deficiency. A   
   subset of the DQ rules is generated (208) based on                the
   performance statistics. The DQ rule of the                subset is
   selected based on the likelihood that the                DQ rule
   selected detects a quality deficiency. An                order of
   executing the subset of DQ rules selected                is generated.
   The order generated specifies a                sequence for applying
   each DQ rule of the subset to                the table-formatted
   dataset.
   USE - Method for selecting and ordering application                of
   data quality rules during ingestion of raw data                by a data
   lake or other data repository in                enterprises, such as
   business and governmental                organizations, Uses include but
   are not limited to                information technology (IT) users
   utilize data                contained in large datasets, such as big
   data using                computer or mobile device, such as a desktop  
   computer, laptop computer, tablet computer, smart                phone,
   smart watch or wearable computer, mainframe                computer,
   quantum computer, and accessing a network                or querying a
   database, such as remote                database.
   ADVANTAGE - The method enables prioritizing execution of               
   DQ rules that are more likely to rapidly detect a                data
   deficiency, thus reducing overall processing                time for
   determining which rows should be marked                for cleaning or
   other handling of data                deficiencies.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(a) a system
   has a processor configured to                initiate operations for
   generating a snapshot of a                table formatted dataset;
   and(b) a computer program product has a program               
   instruction collectively stored on the                computer-readable
   storage media.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the       
   method.200Method202Generating a snapshot of a table-formatted           
   dataset, the snapshot providing a sample has a                minimum
   number rows of the table-formatted dataset                such that each
   column variation of the                table-formatted dataset is
   included in the                snapshot204Executing a predetermined
   collection of                data quality rules on the
   snapshot206Determining performance statistics for the                DQ
   rules, the performance statistics indicating the               
   likelihood that a DQ rule determines a data quality               
   deficiency based on the executing208Generating a subset of the DQ rules,
   each                DQ rule of the subset selected based on the         
   likelihood that the DQ rule selected detects a                quality
   deficiency based on the performance                statistics,
Z9 0
U1 0
U2 0
DA 2025-07-01
UT DIIDW:202563616G
ER

PT P
AU JIANG X
TI Smart city planning system based on large data            analysis, for
   governments and urban planners, has urban            planning
   visualization display platform which obtains            urban indicator
   display mode corresponding to key urban            indicators and
   development trend display mode            corresponding to urban
   development trend
PN CN119991381-A
AE BEIJING JOIN-CREATING TECHNOLOGY CO LTD
AB 
   NOVELTY - The system has a city data integration                platform
   (101) which is used to obtain multi-source                city data,
   pre-process the multi-source city data                using a
   distributed computing framework to obtain                pre-processed
   multi-source city data, and store                pre-processed
   multi-source city data in a big data                lake of a
   distributed file system. A data analysis                engine (102) is
   used to perform data mining on all                multi-source city data
   in the big data lake using a                deep learning algorithm to
   determine key city                indicators and city development
   trends. An urban                planning visualization display platform
   (103) is                used to obtain the urban indicator display mode 
   corresponding to the key urban indicators and the               
   development trend display mode corresponding to the                urban
   development trend, and visualize the key                urban indicators
   according to the urban indicator                display mode and the
   urban development trend                according to the development
   trend display mode, so                as to assist urban planners in
   making urban                planning decisions.
   USE - Smart city planning system based on large data               
   analysis, for governments and urban planners.
   ADVANTAGE - The data analysis engine is used to mine all               
   multi-source urban data in the big data lake using                deep
   learning algorithms, so as to deeply explore                the
   potential laws and models of multi-source urban                data.
   Accurate urban potential laws and development                trends help
   improve the accuracy of urban planning.                The urban
   planning visualization display platform                is used to
   visualize key urban indicators according                to the urban
   indicator display mode and urban                development trends
   according to the development                trend display mode.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a smart city planning method based on large               
   data analysis;an electronic device; anda computer readable storage
   medium storing                computer program for smart city planning.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram           
   illustrating the smart city planning system.                (Drawing
   includes non-English language text)101City data integration
   platform102Data analysis engine103Urban planning visualization display  
                platform
Z9 0
U1 0
U2 0
DA 2025-06-29
UT DIIDW:202552416M
ER

PT P
AU LUZZI F
   CHAUDHRY M S A
TI Method for integrating data functions for            disparate
   applications within enterprise, involves            executing online
   analytical processing (OLAP)            application with computing
   instructions operable to            perform operations, where data layer
   is stored in data            store
PN US12298950-B1
AE IP HOLDINGS 2017 LLC
AB 
   NOVELTY - The method involves executing a data lake               
   application with computing instructions that are                operable
   to perform operations, where the raw data                is received.
   The raw data is cleansed using                validation rules to
   generate curated raw                unstructured data, where the
   validation rules are                devoid for enforcing a data schema.
   The curated raw                unstructured data is stored, where an
   online                analytical processing (OLAP) application is       
   executed with computing instructions operable to                perform
   operations. The data layer is stored in a                data store, and
   batch processing is performed using                the data layer. The
   data layer is stored as (SQL)                tables.
   USE - Method for integrating data functions for                disparate
   applications within enterprise.
   ADVANTAGE - The data lake is a system or repository of               
   data stored in its natural or raw format, which                allows
   for the storage of large amounts of raw data                in a
   scalable and cost-effective manner. The online               
   transaction processing (OLTP) supports               
   transaction-oriented applications and is typically               
   characterized by a large number of short online               
   transactions e.g. insert, update, delete with fast                query
   processing. The integration leads to improved                data
   consistency, reduced redundancy, and enhanced                data
   integrity across various applications within                an
   enterprise.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   system with the processors; (2) a non-transitory               
   computer-readable media for storing computing               
   instructions.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic front view of
   a                computer system.100Computer
   system102Chassis104Keyboard106Monitor108Screen110Mouse114Hard drive
Z9 0
U1 0
U2 0
DA 2025-06-01
UT DIIDW:202550307A
ER

PT P
AU KIM T H
   KIM D W
   SON J
   NAM S
   AN D
   AHN D W
   NAM S D
   SONJIHHO
   WOO K D
   KIM T
TI Smart big de-identification and meta-ization            method, involves
   indexing standardized object address            and extracted meta to
   database management system, and            loading processed file on
   object-based storage in data            lake as standardized object
   address
PN US2025148128-A1; KR2025067313-A
AE MISOINFO TECH
AB 
   NOVELTY - The method involves detecting an event from a               
   file server in which clinical data is stored and               
   obtaining a source data file (S10). File meta is               
   extracted (S20) from the source file according to a               
   preset meta format. Unnecessary meta is                de-identified
   (S30) to form a processed file. An                object address
   standardized is generated (S40)                based on a data structure
   preset by a user. The                standardized object address and the
   extracted meta                are indexed (S50) to a database management
   system.                The processed file is loaded (S60) on an         
   object-based storage in a data lake as the                standardized
   object address.
   USE - Smart big de-identification and meta-ization                method
   for building a data lake by automatizing                meta-extraction,
   structured/unstructured data                structuring and loading.
   ADVANTAGE - The method enables easily building the data               
   lake by automatizing meta-extraction,               
   structured/unstructured data structuring and                loading,
   which are necessary for building a                CDW.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   smart big de-identification and meta-ization               
   method.S10Detecting event from file server in which               
   clinical data is stored and obtaining source data               
   fileS20Extracting file meta from source file                according to
   preset meta formatS30De-identifying unnecessary meta to form            
   processed fileS40Generating object address standardized               
   based on data structure preset by userS50Indexing standardized object
   address and                extracted meta to database management
   systemS60Loading processed file on object-based                storage
   in data lake as standardized object                address
Z9 0
U1 0
U2 0
DA 2025-05-27
UT DIIDW:202548946H
ER

PT P
AU MENG X
   WANG X
   SUN B
   WANG Y
   ZHAO Z
   GUO Y
TI Metadata management system based on data lake            universal
   metadata model, has metadata base for            receiving and storing
   taken metadata, and metadata            management module for managing
   metadata and data blood            relation of multi-source
   heterogeneous data in data            lake
PN CN119903216-A
AE UNIV QILU TECHNOLOGY SHANDONG ACAD SCI; SHANDONG COMPUTER SCI CENT
AB 
   NOVELTY - The system has a metadata shooting module for               
   shooting a metadata in a process of multi-source               
   heterogeneous data entering a lake. A modelling                module is
   used for modelling the taken metadata                through the
   metadata model and supporting the                dynamic expansion. A
   metadata base is used for                receiving and storing the taken
   metadata. A                metadata management module is used for
   managing the                metadata and data blood relation of the     
   multi-source heterogeneous data in the data lake                and
   visually displaying in the form of graph. The                taken
   metadata is modeled by the metadata model,                and the
   dynamic expansion is supported. The                metadata is modeled
   in the form of graph. The data                entity and metadata entity
   are represented by                nodes. The self-defined data structure
   comprises                structured data, semi-structured data and      
   non-structured data. The association relationship                between
   the nodes is represented by edges.
   USE - Metadata management system based on data lake               
   universal metadata model.
   ADVANTAGE - The metadata management module manages the               
   metadata and data blood relation of the                multi-source
   heterogeneous data in the data lake                and visually displays
   in the form of graph, where                the taken metadata is modeled
   by the metadata                model, and the dynamic expansion is
   supported, and                thus enables to efficiently support the   
   multi-resource heterogeneous data example in a data                lake.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   method for managing metadata based on data lake                universal
   metadata model.
   DESCRIPTION Of DRAWING(S) - The system supports the multi-source        
          heterogeneous data example in the data lake.
Z9 0
U1 0
U2 0
DA 2025-06-21
UT DIIDW:202547082R
ER

PT P
AU MONTALVO A
TI System for sensor data fusion for sensor            management and
   utilization in autonomous            transportation, has validation
   engine that validates            inference when comparison between
   inference and second            inference exceeds predefined threshold
PN US12282528-B1
AE DIGITAL GLOBAL SYSTEMS INC
AB 
   NOVELTY - The system has a computer processor including                a
   memory. A first distance sensor is operable to                capture a
   first distance measurement from a vehicle                to at least one
   object. A second distance sensor is                operable to capture a
   second distance measurement                from the vehicle to the at
   least one object. The                computer processor is operable to
   analyze the first                distance measurement and the second
   distance                measurement. The computer processor is operable
   to                receive a query. An inference engine is operable to   
   determine a second inference. A validation engine                is
   operable to use artificial intelligence to                compare the
   inference to the second inference. The                validation engine
   validates the inference when the                comparison between the
   inference and the second                inference exceeds a predefined
   threshold. The                computer processor is operable to instruct
   the                vehicle to move based on the inference being         
         validated.
   USE - System for sensor data fusion for sensor                management
   and utilization in autonomous                transportation.
   ADVANTAGE - The method can effectively fuse the deep               
   features of multispectral images, enhance the                feature
   expression of pedestrian areas and suppress                irrelevant
   background noise features during the                fusion process, and
   can realize accurate pedestrian                detection. The system
   establishes reliable                information from incoming sensor
   data without                excessive power, computational, and/or
   storage                requirements. Once a user generates a query, the 
   system processes the query against the stored data                to
   find correlations between the query and the                indexed data
   before producing results, requiring                less computational
   power than an unstructured data                lake, but still requiring
   a problematic amount of                power and/or computing time. The
   system minimizes                storage requirements because only fused
   data is                stored, reduces computational requirements
   because                sensor data is curated before and/or after a     
   computer processor receives a user and/or computer                query,
   and enhances sensor accuracy by providing                additional
   actionable data. The system is allowed                to unwind the
   fusion, linking, and/or curation                process, correcting
   errors and ensuring                accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for sensor data fusion for sensor management and               
   utilization in autonomous transportation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation
   of an autonomous transportation sensor data               
   fusion.300Autonomous vehicle301LiDAR sensor302Radar sensor
Z9 0
U1 0
U2 0
DA 2025-05-05
UT DIIDW:2025421018
ER

PT P
AU WU H
   ZHANG N
   XU W
TI Method for searching and generating large model            information
   based on large batch of documents in data            lake, involves
   triggering file analysis and            segmentation process, and
   performing text slicing and            vectorization processing
PN CN119862155-A
AE TIANFU JIANGXI LAB
AB 
   NOVELTY - The method involves uploading a file to a data               
   lake by a user. Unstructured data is received by a               
   system. File changes are captured by monitoring                newly
   added or modified file events in the data                lake. File
   analysis and segmentation process is                triggered. Text
   slicing and vectorization                processing are performed on
   file content in the                file for subsequent retrieval. A
   query questions                are inputted by the user to the system.
   An answer                is generated and returned to the user. A data
   lake                is built based on distribution, and configured the  
   data lake to support uploading of multi-format                files and
   automatic classification storage. An                event-driven
   architecture is used to manage the                flow of uploaded files
   to ensure that multiple                documents can enter the system
   and be                managed.
   USE - Method for searching and generating large                model
   information based on large batch of documents                in data
   lake.
   ADVANTAGE - The method enables improving data processing               
   capacity, retrieval precision, professional                knowledge
   application capacity and user interaction                experience, and
   realizes more efficient and                intelligent document
   processing and question                answering service.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for searching and generating large model               
   information based on large batch of documents in                data
   lake (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-06-08
UT DIIDW:202544952G
ER

PT P
AU WANG H
   LI W
   MIAO X
   LIN C
   ZHANG Z
   YAO S
   ZHU C
   WANG Y
   SUN S
   GUO P
   DONG R
   WU Y
   TIAN Y
TI Method for storing heterogeneous energy data based            on
   lake-warehouse integration in power industry,            involves
   collecting heterogeneous energy source data            and
   pre-processing data, and performing fuzzy            clustering on
   processed data by FCM algorithm to            extract characteristic key
   word
PN CN119759996-A
AE GUIZHOU POWER GRID CO LTD
AB 
   NOVELTY - The method involves collecting heterogeneous               
   energy source data and pre-processing the data.                Fuzzy
   clustering is performed on the pre-processed                data by an
   FCM algorithm to extract the                characteristic key word. The
   source data in a HDFS                component is loaded into a
   warehouse table created                based on a Hive component. The
   heterogeneous energy                data is provided with power industry
   data, other                energy data and external support data. The
   source                data is loaded into a lake bin table established  
                based on the Hive component.
   USE - Method for storing heterogeneous energy data                based
   on lake-warehouse integration in power                industry.
   ADVANTAGE - The method enables unifying the heterogeneous               
   energy source data by the FCM clustering and ETL                tool,
   optimizing the data loading and inquiring                efficiency, and
   reducing the processing time                consumption. The method
   enables providing a                lake-cabin integrated structure to
   enhance the data                management and cross-domain sharing and
   improve the                system expansibility and flexibility. The
   method                enables improving the data inquiry and execution  
   efficiency, and effectively reducing the problem                that the
   data island is difficult to-be uniformly                managed.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   computer-readable storage medium has a set of               
   instructions for storing heterogeneous energy data                based
   on lake-warehouse integration in power                industry.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of an integrated architecture of heterogeneous data                lake
   based on a heterogeneous energy data storage                method
   integrated with lake (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2025-05-18
UT DIIDW:202539463P
ER

PT P
AU ZHAO H
   JIA Z
   ZHANG H
   TAN Y
   ZHANG B
TI Method for processing data based on data lake,            involves
   obtaining source data to be stored in data            lake, and
   binarizing unstructured data to obtain binary            data
   corresponding to source data and storing binary            data and tag
   data in data lake
PN CN119760177-A
AE BEIJING HORIZON INFORMATION TECHNOLOGY
AB 
   NOVELTY - The method involves obtaining source data to                be
   stored in a data lake. The non-structured data                is
   analyzed to determine label data corresponding                to the
   non- Structured data in response to the                source data being
   non-structure data. The binarized                data is binarize to
   obtain binary data that                corresponds to the data of the
   source. The binary                data and the label data are stored in
   the data                lake, where the binary data is a binary binary  
                data.
   USE - Method for processing data such as image,                video,
   and point of point data, based on a data                lake for
   automatic driving process such as                auxiliary driving and
   unmanned driving.
   ADVANTAGE - The method enables realizing to uniformly               
   store the unstructured data and the tag data in the                data
   lake, which can improve the retrieval speed                and the
   reading speed of the stored data, and                ensure the use of
   the data stored in data                lake.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:a data processing device based on data                lake;a data
   management system; anda computer readable storage medium comprising     
             a set of instructions for processing data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for processing data (Drawing includes                Non-English
   language text).
Z9 0
U1 0
U2 0
DA 2025-05-18
UT DIIDW:2025401302
ER

PT P
AU LI H
   LI Y
   SONG G
   GAO Y
   TIAN Y
   LIU W
   ZHANG L
   DONG J
   LIU J
   SHAN B
   YU H
   HU H
   ZHANG X
   JI Z
   SHE M
TI Real-time vehicle use warning method based on big            data,
   involves building big data real-time warning            platform,
   decoupling from business platform, and            building offline
   computing engine that supports big            data batch computing,
   distributing analysis data, and            setting data analysis rules
PN CN119669953-A
AE STATE GRID INFORMATION & TELECOM GROUP; TIANJIN RICHSOFT ELECTRIC POWER
   INFORMAT
AB 
   NOVELTY - The method involves building a big data               
   platform architecture, selecting the Kappa                architecture
   as the big data architecture of the                vehicle management
   platform. The data collection,                calculation and storage
   are performed, data is                collected from multiple data
   sources using                different methods, and the data is stored
   in a big                data warehouse through custom calculation logic.
   A                big data real-time warning platform is built, and      
   decoupled it from the business platform, and a                one-stop
   real-time big data analysis computing                engine is built for
   streaming computing. An offline                computing engine is built
   that supports big data                batch computing, the analysis data
   is distributed,                and setting data analysis rules are set.
   The Kappa                architecture is selected based on the real-time
   warehouse by using tools such as message queues,               
   stream-batch integrated calculation engines, and                query
   engines to improve the calculation and query                efficiency
   of the vehicle management                platform.
   USE - Real-time vehicle use warning method based on                big
   data.
   ADVANTAGE - The method improves the stability of the               
   vehicle management platform, reduces the overall                R&D and
   operation and maintenance costs of the                vehicle platform,
   meets the diverse warning                decision analysis needs of
   users, and help users to                make continuous improvements in
   vehicle                management.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of    
   real-time vehicle use warning method based on big                data.
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-05-03
UT DIIDW:202535085Q
ER

PT P
AU WANG H
   LIN C
   WANG Y
   ZHANG Z
   ZHU C
   SUN S
   LI W
   MIAO X
   CHEN H
   YAO S
   WU Y
   DONG R
   TIAN Y
TI Method for entering multi-source heterogeneous            energy data
   into data lake, involves obtaining            multi-source heterogeneous
   data from internal and            external systems of energy enterprise,
   preprocessing            lake-entering data, and executing lake data
   filing,            deleting and transferring processes
PN CN119645983-A; CN119645983-B
AE GUIZHOU POWER GRID CO LTD
AB 
   NOVELTY - The method involves obtaining multi-source               
   heterogeneous data from internal and external                systems of
   an energy enterprise. The lake-entering                data is
   preprocessed. The multi-source                heterogeneous data is
   entered into the lake through                different entering
   controllers. An Apache Flink                (RTM: open source stream
   processing framework) is                used as the uniform data
   entering engine. Lake data                filing, deleting and
   transferring processes are                executed, where the
   multi-source heterogeneous data                comprises energy
   production, transmission, storage,                consumption and energy
   market transaction full link                full chain data. A
   SeaTunnel(RTM: open-source big                data integration tool) is
   used to provide data                cleaning. A Spark SQLfunction is
   used to define                pre-processing operator.
   USE - Method for entering multi-source heterogeneous               
   energy data into data lake.
   ADVANTAGE - The method enables using the technical               
   framework of Flink (RTM: open source stream                processing
   framework) + Iceberg(RTM: high                performance open-source
   format for large analytic                tables) to realize real-time
   lake entry of                multi-source heterogeneous energy data,
   supporting                data access from different types of data
   source,                ensuring diversification and comprehensiveness of
   data source, adapting multiple data formats, and               
   satisfying the real-time performance and                flexibility
   requirement of energy service                processing.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   computer device comprising a memory and a processor                for
   executing a set of instructions for entering                multi-source
   heterogeneous energy data into data                lake; (2) a
   computer-readable storage medium for                storing a set of
   instructions for entering                multi-source heterogeneous
   energy data into data                lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for entering multi-source
   heterogeneous                energy data into data lake. (Drawing
   includes                non-English language text).
Z9 0
U1 0
U2 0
DA 2025-04-26
UT DIIDW:2025329080
ER

PT P
AU ZHANG Z
   ZHANG Y
   LI J
   GUO Z
   CHEN M
   WANG Z
   SONG Z
   ZHANG M
   JIANG T
TI Method for performing distribution network cloud            platform
   data fusion, involves evaluating effect of            data fusion,
   adjusting fusion strategy and algorithm            according to
   evaluation result, and optimizing data            fusion process
PN CN119621823-A
AE STATE GRID LIAONING ELECTRIC POWER SUPPL
AB 
   NOVELTY - The method involves identifying needed data               
   type, format, quality and data updating frequency.                A data
   access interface is designed to ensure that                data of
   different service sub-systems can be                effectively accessed
   by a cloud platform. The data                is collected by timing
   collection and event                triggering collection to ensure
   timiness and                integrity of the data. A distributed
   database or a                data lake is utilized for storing a large
   amount of                fused data. The fused data is utilized to
   analyze                and combine an artificial intelligence algorithm
   to                predict. An effect of data fusion is periodically     
   evaluated. A fusion strategy and algorithm are                adjusted
   according to an evaluation result to                optimize data fusion
   process.
   USE - Method for performing distribution network                cloud
   platform data fusion in power distribution                network
   operation monitoring, power failure                management, fault
   emergency repair command and                service level of power grid.
   ADVANTAGE - The method enables designing a data access               
   interface to ensure that the data of different                service
   sub-systems can be effectively accessed by                the cloud
   platform. The data is collected by timing                collection and
   event triggering collection to                ensure the timeliness and
   integrity of the data.                The method allows the power
   distribution network                cloud platform to effectively
   integrate the                multi-source heterogeneous data and provide
   strong                data support for the intelligent management and   
               decision of the network.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for performing distribution network cloud                platform
   data fusion (Drawing includes non-English                language
   text).S1Step for analyzing the existing data                source:
   identifying the needed data type, format,                quality and
   data updating frequencyS2Step for data acquisition and access:          
   designing a data access interface to ensure that                the data
   of different service sub-systems can be                effectively
   accessed by the cloud platformS3Step for cleaning the data, eliminating 
   invalid, wrong or incomplete data recordS4Step for classifying the data
   and                establishing a data association modelS5Step for
   realizing the data fusion                algorithm, comprehensively
   processing the                multi-source dataS6Step for processing the
   space-time data to                ensure the consistency of the data in
   the                space-time dimensionalityS7Step for distributing
   database or a data                lake is used for storing a large
   amount of fused                dataS8Step for using the fused data to
   analyze                and combining AI algorithm to predictS9Step for
   periodically evaluating the effect                of the data fusion
Z9 0
U1 0
U2 0
DA 2025-04-17
UT DIIDW:202534936B
ER

PT P
AU SHEN H
   RAO X
TI Data management platform comprises data access            module that is
   used for collecting metadata of            heterogeneous data source,
   and data processing module            is used for processing metadata
   through distributed            computing frame and data lake
PN CN119597975-A
AE GUANGZHOU GUANGHA COMMUNICATION CO LTD
AB 
   NOVELTY - The data management platform comprises a data               
   access module that is used for collecting the                metadata of
   a heterogeneous data source. The                metadata is provided
   with the unstructured data,                structured data, and
   semi-structured data. The data                processing module is used
   for processing the                metadata through a distributed
   computing frame and                a data lake. A data storage module is
   used for                storing the processed metadata. A capacity      
   management module is used for real-time monitoring                the
   data resource use condition of the current                cluster in the
   data management platform through the                integrated
   monitoring system, and adjusting the                data storage
   capacity according to the data                resource use condition of
   the current                cluster.
                       USE - Data management platform.
   ADVANTAGE - The data management platform effectively               
   manages the heterogeneous data from different                sources and
   effectively ensures the safety and                flexibility of the
   data.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a data      
   management method applied to the data management               
   platform.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the
   data                management platform. (Drawing includes non-English  
                language text).
Z9 0
U1 0
U2 0
DA 2025-04-14
UT DIIDW:202528072Y
ER

PT P
AU WANG Q
   PANG J
   LUO Y
   WEI W
TI Data lake management system for use in data lake           
   construction, has query calculation engine management            module
   that is used for configuring appointed query            calculation
   engine in data lake, and establishing            resident resource pool
   through query calculation            engine
PN CN119597771-A
AE SHUGUANG CLOUD COMPUTING GROUP CO LTD
AB 
   NOVELTY - The system has a metadata management module               
   (101) that is used for providing a metadata                management
   interface, and sending the database                table structure
   change request of the data lake                triggered by the object
   through the metadata                management interface to inquiry
   calculation engine                management module. A big data
   processing module                (102) is used for providing a big data
   processing                interface and sending the data processing
   request                of data lake triggered by object. A query        
   calculation engine management module (103) is used                for
   configuring the appointed query calculation                engine in the
   data lake, and establishing a                resident resource pool
   through query calculation                engine. The target resident
   session in the resident                resource pool is started through
   the query                calculation engine after receiving the library 
   table structure change request or the data                processing
   request through the resident resource                pool, where the
   corresponding request is processed                through the target
   resident session.
   USE - Data lake management system for use in data                lake
   construction.
   ADVANTAGE - The method improves the efficiency and               
   flexibility of data lake management. The efficient               
   resource scheduling mechanism enables the system to                deal
   with different types of requests and reduces                the time for
   processing the request by the data                lake.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a data lake management method;a data lake management device;
   andan electronic device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of the data lake management method. (Drawing                includes
   non-English language text)101Metadata management module102Big data
   processing module103Query calculation engine management               
   module
Z9 0
U1 0
U2 0
DA 2025-04-14
UT DIIDW:202528719H
ER

PT P
AU GARG H
   DEBASHISH K
   DE SUPRIYA K
   SAHU S
   PODDAR H
   INGLE S
   SAXENA G
   KUSHWAHA A
   CHAUDHARY S
   SONI S
   VISHWAKARMA D K
   GURBANI G
   TELGOTE K
   KUMAR Y
   GANVEER C
   RAJANI M
   SAHU K
   MEENA S
   KUMAR G
   KUMAR R
   KISHORE J
   KHADE A
   KUMAR K V
   BHUSHAN S
   KUMAR D
   GAYKI V
   BHANWRIA M
   SAROHI M
   MURARKA A
   BHATNAGAR P K
   BHATNAGAR A
TI Method for performing on-demand network            performance
   management by using user equipment,            involves receiving
   generated modified network            performance data from computing
   nodes, and analyzing            received modified performance data to
   generate view of            performance data
PN IN202321059356-A
AE JIO PLATFORMS LTD
AB 
   NOVELTY - The method involves receiving a request               
   comprising parameters and tasks from a user                equipment
   (UE) by a receiving unit. Network                performance data is
   collected from data sources                based on the parameters by a
   processing engine.                Each of the received tasks is split
   into sub-tasks                by the processing engine. Each sub-task is
   assigned                along with the collected network performance
   data                across computing nodes, where each computing node   
   is selected based on defined conditions and is                configured
   to perform the assigned sub- tasks to                generate a modified
   network performance (NPD) data.                The generated modified
   NPD data is received from                each of the computing nodes.
   USE - Method for performing on-demand network                performance
   management by using a user equipment                (Claimed).
   ADVANTAGE - The method enables optimizing use of computing              
   resources, and improving performance, and enhancing               
   scalability and reliability, and allowing the data                lake
   for scalable data storage and efficient                querying often
   supporting big data and analytics                use cases.
   DETAILED DESCRIPTION - An independent claim is also included for a      
   system for performing on-demand network performance               
   management by using a user equipment.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for performing on-demand network performance      
   management by using a user equipment.102User104Computing
   device106Network108System for performing on-demand network              
    performance management by using a user                equipment
Z9 0
U1 0
U2 0
DA 2025-04-26
UT DIIDW:202530771T
ER

PT P
AU LI K
   DING C
   LIU X
   ZENG Z
   FANG Y
   CHEN M
TI Method for data collection storage, modeling            development and
   sharing service covering information            technology (IT) and OT
   scenes, involves performing data            development on structured
   data, measurement data and            unstructured data through batch
   processing and stream            processing, respectively sharing data
PN CN119513167-A; CN119513167-B
AE CHINA THREE GORGES CORP
AB 
   NOVELTY - The method involves respectively collecting                the
   IT data and OT data according to three data                types of
   structured data, measuring data and                non-structured data.
   The structured data is                collected by the extract,
   transform, and load (ETL)                tool, and collected by the
   periodic collecting way.                The physical collection
   comprises entity file and                metadata synchronous
   collection, the virtual                collection comprises collection
   metadata. The data                storage and modelling are performed
   based on the                full data base, the distributed frame and
   the                layered data model. An integrated data lake and a    
   data warehouse are constructed based on the storage               
   component and the calculation engine in the                distributed
   system basic framework. The data                development is performed
   on the structured data,                the measurement data and the
   unstructured data                through batch processing and stream
   processing. The                data is respectively shared according to
   two scenes                of non-real-time data sharing and real-time
   data                sharing.
   USE - Method for data collection storage, modeling               
   development and sharing service covering                information
   technology (IT) and OT scenes.
   ADVANTAGE - The method can realize uniform storage of IT               
   and OT data, build integrated data lake and data               
   warehouse, and lay a foundation for fusion                application of
   two types of data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:the method for data collection;the method for data acquisition
   storage;                anda data acquisition storage for covering IT
   and                OT scenes for the method for data acquisition        
          storage.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a  
   method for data collection storage, modeling                development
   and sharing service covering IT and OT                scene.(Drawing
   includes non-English language                text)
Z9 0
U1 0
U2 0
DA 2025-03-28
UT DIIDW:202523037U
ER

PT P
AU PRIYA P
   SARANYADURAI A
   KRISHNAPRIYA S
   SUKUMAR C
TI System for cloud-based artificial            intelligence-powered data
   lake for scalable data            management, has data ingestion module,
   metadata            management unit, artificial intelligence-driven data
   processing engine, distributed storage unit, and query           
   optimization module
PN IN202541013196-A
AE ANNAPOORANA ENG COLLEGE AUTONOMOUS
AB 
   NOVELTY - The system has a data ingestion module (102),               
   metadata management unit (104), artificial               
   intelligence-driven data processing engine (106),               
   distributed storage unit (108), and query                optimization
   module (110). A data ingestion module                collecting data
   from multiple sources and                transferring the data to a
   distributed storage                unit. A metadata management unit
   maintains metadata                for indexing, searching, and
   categorization of                data. An artificial intelligence
   (AI)-driven data                processing engine applies machine
   learning                techniques for data cleansing, transformation,
   and                feature extraction. A query optimization module      
   enhances data retrieval performance. A security and               
   governance module (112) enforces access controls,               
   compliance, and data integrity. An analytics and               
   visualization unit (114) provides insights,                reports, and
   dashboards for end-users.
   USE - System for cloud-based artificial               
   intelligence-powered data lake for scalable data               
   management including internet of things (IoT)                devices,
   business applications, web services, and                user
   interactions.
   ADVANTAGE - The system facilitates high-speed data               
   processing, automated feature extraction, and                real-time
   analytics. The distributed storage unit                efficiently
   stores large-scale structured and                unstructured data. The
   query optimization module                enhances data retrieval
   performance. The security                and governance module enforces
   access controls,                compliance, and data integrity. The
   analytics and                visualization unit provides insights,
   reports, and                dashboards for end-users.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   method of cloud-based artificial                intelligence-powered
   data lake for scalable data                management.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a     
   system for cloud-based artificial                intelligence-powered
   data lake for scalable data                management.102Data ingestion
   module104Metadata management unit106Artificial intelligence-driven data 
   processing engine108Distributed storage unit110Query optimization
   module112Security and governance module114Analytics and visualization
   unit
Z9 0
U1 0
U2 0
DA 2025-03-28
UT DIIDW:2025222614
ER

PT P
AU PANG N
   LIU Q
   LIU D
   WANG X
   RU M
   WANG P
   GAO Y
TI System for adaptively computing electric network            measurement
   for use in a computer device, has storage            layer that supports
   cross-source heterogeneous data            storage and uniform metadata
   storage to ensure storage            and management of data
PN CN119477602-A
AE NARI GROUP CORP; NANJING NARI RUIZHONG DATA CO LTD
AB 
   NOVELTY - The system has an engine core service unit               
   provided with a service layer and an engine layer.                A
   storage layer is supported cross-source                heterogeneous
   data storage and uniform metadata                storage. A data
   platform, a data lake, a relation                database, a time
   sequence database, a metadata                management engine and a
   metadata storage are                ensured storage and management of
   data. The data                standardization management module is
   generated a                uniform data cataloged and provided cataloged
   management function. The calculation layer is                integrated
   with a calculation engine, an                interactive inquiry engine,
   a stream batch                calculation engine and the real-time
   calculation                engine to provide efficient data calculation
   and                analysis service.
   USE - System for adaptively computing an electric                network
   measurement for use in a computer device                (claimed) used
   for power grid measurement, instant                inquiry, fusion
   calculation, off-line calculation,                real-timely
   calculating multi-element scene,                satisfying different
   service inquiry calculation                requirements in dimensions
   i.e. aging and data                amount, selecting data source and
   planning                execution path, and realizing service Structured
              Query Language(SQL)automatic routing                function.
   ADVANTAGE - The system improves searching and using               
   efficiency of the electric network measuring data                and
   reduces the application development cost.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:a method for adaptively computing an electric                network
   measurement for use in a computer device;                anda
   computer-readable storage medium for storing                a set of
   instructions for adaptively computing an                electric network
   measurement for use in a computer                device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for adaptively computing an electric network      
   measurement for use in a computer device (Drawing               
   includes non-English language text).
Z9 0
U1 0
U2 0
DA 2025-03-22
UT DIIDW:202520638V
ER

PT P
AU ERDMANN C A
   BRUSH R A
   SUTARIYA B B
TI Non-transitory media storing program for providing           
   closed-loop intelligence in financial fields to capture            and
   analyze vast streams of transactional data, has set            of
   instructions for projecting condition onto first            data model
   through hardware processors to produce            second data model
PN US2025045253-A1
AE CERNER INNOVATION INC
AB 
   NOVELTY - The non-transitory media has instructions                that,
   when executed by multiple hardware                processors, cause
   multiple hardware processors to                facilitate multiple
   operations which includes                receiving a selection of a
   subset of information                from a set of information in a data
   structure, at a                cloud-based computing platform. The set
   of                information corresponds to multiple sets of data      
   provided by multiple disparate data sources, and                multiple
   disparate data sources are associated with                multiple data
   generating entities. A portion of set                of information is
   transformed based on subset of                information and based on a
   standardization format                to generate a transformed portion
   of set of                information. A first data model is built at    
   cloud-based computing platform through multiple                hardware
   processors based on transformed portion of                set of
   information. The second data model is                electronically
   written through multiple hardware                processors to one or
   both of the data structure and                the cloud-based computing
   platform.
   USE - Non-transitory media storing program for                providing
   closed-loop intelligence in financial,                scientific,
   medical and other fields to capture and                analyze vast
   streams of transactional, experimental                and other data.
   ADVANTAGE - The method enables providing an efficient and               
   convenient system that enables structured views of                data
   to be aggregated into a big data architecture,                to train
   and test predictive tools on an entire                population of
   data, to push insights gained from                the predictive tools
   into a clinical workflow, and                to provide access and use
   to predictive tools by                other organizations. Ad-hoc data
   subsets can be                created by selecting data from data store
   for                analysis, this enables a user to take a very         
   complex data set and turn it into much simpler                tables
   that provides greater efficiencies and                reduces costs
   associated with using the closed-loop                intelligence
   system.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a method
   for providing closed-loop                intelligence in financial
   field;(2) a system for providing closed-loop                intelligence
   in financial field.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of        
   computing environment.100Computing environment102Control server104Data
   store106Computer networks108Remote computers
Z9 0
U1 0
U2 0
DA 2025-02-24
UT DIIDW:202513352S
ER

PT P
AU LIU Q
   JIA J
   WEI L
TI Method for achieving data blood-bearing used in            big data
   field, involves acquiring data blood-edge            information of
   uplink service system, and details of            integration task from
   data lake to standard layer, and            acquiring data blood-edge
   information of downlink            service system
PN CN119357184-A
AE SHANGHAI YUNXI TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves establishing batch task,               
   and acquiring data blood-edge information of an                uplink
   service system. The details of the                integration task are
   acquired from the data lake to                the standard layer. Data
   blood-edge information of                the downlink service system is
   acquired. An                execution script of the batch task is
   constructed                after the field mapping is completed, the
   execution                script is sent to a batch platform to execute
   the                complete data circulation, and the constructed task  
   information is deleted, if the task execution                fails. A
   joint query screening is carried out on                the data queried
   by the two tables, where the joint                query screening
   comprises matching and screening                task states of source
   table identification (ID) and                target table IDs which are
   mutually corresponding                in the two tables. An association
   inquiry is                carried out with inquiry data of an uplink
   system                through table names to obtain the whole blood     
   relationship from the service source to the topic               
   library.
   USE - Method for achieving data blood-bearing used                in big
   data field.
   ADVANTAGE - The blood relationship is obtained through               
   multi-table joint association inquiry according to                the
   task ID and the source library table ID. The                data blood
   edges follow the establishment of the                batch task and are
   displayed in real time. The                process of data generation
   and extinction is better                managed.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device    
              for realizing data blood border.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a method
   for achieving data blood-bearing. (Drawing includes               
   non-English language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202512871T
ER

PT P
AU BHATNAGAR A
   MURARKA A
   KOLARIYA J K
   KUMAR G
   SAHU K
   VERMA R
   MEENA S
   GURBANI G
   CHAUDHARY S
   GANVEER C K
   DE S
   DEBASHISH K
   MEHUL T
   KUMAR Y
   TELGOTE K
   PATNAM N
   KUSHWAHA A
   VISHWAKARMA D K
   SRINATH K
   PANDEY V
TI Method for generating reports from database i.e.            data lake,
   involves pulling data from database,            generating report at
   reporting engine based on data            pulled from database by
   processor and storing generated            report along with first
   identifier at caching layer by            processor
PN WO2025017746-A1; IN202321048725-A
AE JIO PLATFORMS LTD
AB 
   NOVELTY - The method (500) involves monitoring (502)               
   historical behavior of users regarding report                generation
   by processor, and determining (504) if                the users require
   report based on monitoring the                historical behavior of the
   users using an                artificial intelligence/machine learning
   (AI/ML)                module by processor. A data is pulled (506) from
   a                database to a reporting engine based on detecting      
   the users require the report by processor, the                report is
   generated (508) at the reporting engine                based on the data
   pulled from the database by                processor and the generated
   report is stored (510)                along with first identifier at a
   caching layer by                processor. The data pulled and reports
   generated                based on ongoing user interactions are
   continuously                adapted by processor. The frequency of
   report                generation by prioritizing reports is adjusted    
   based on their historical frequency of use and                execution
   time.
   USE - Method for generating reports from database                i.e.
   data lake, to store, process, and secure large                amounts of
   structured, semi-structured, and                unstructured data.
   ADVANTAGE - The method for generating reports from the               
   database which is efficient, time-saving, faster                and
   optimizes use of system resources. The method                provides
   optimization of network and computational                resources,
   faster report generation, and saves time                and bandwidth.
   The method provides for storing such                frequently generated
   and accessed reports in the                caching layer for instant
   retrieval and fetching,                thereby obviating the need for
   the whole process of                data retrieval from the database
   into the caching                layer, calculations, etc., thus saving
   on time and                resources. The method provides multiple
   advantages                including optimization of network and
   computational                resources, faster report generation, and
   saves both                time and bandwidth.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a system for generating reports;a non-transitory
   computer-readable medium                storing program for generating
   reports; anda user equipment (UE).
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for automatic report generation
   from a                database.500Method for generating reports from    
   database502Step for monitoring historical behavior of               
   users regarding report generation504Step for determining if the users
   require                report506Step for pulling data from
   dataset508Step for generating report at reporting               
   engine510Step for storing generated report
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2025093066
ER

PT P
AU MA J
TI Big data platform based data association system,            has data
   acquisition module for collecting structured            data and
   unstructured data of vehicle and performing            preprocessing to
   obtain processed data, and big data            platform connected to
   data acquisition module
PN CN119336950-A
AE SHANGHAI YOOCAR NETWORK TECHNOLOGY CO
AB 
   NOVELTY - The platform has a data acquisition module (1)               
   for collecting structured data and unstructured                data of a
   vehicle and perform preprocessing to                obtain processed
   data. A big data platform (2) is                connected to the data
   acquisition module. The big                data platform includes a data
   lake (21) for storing                the processed data. A batch-stream
   integrated                computing module (22) is connected to the data
   lake                for performing batch processing and stream          
   processing on the processed data to obtain                calculated
   data. An online analytical processing                database (23) is
   connected to the batch-stream                integrated computing module
   for associating and                retrieving the calculated data.
      USE - Big data platform based data association                system.
   ADVANTAGE - The system utilizes the big data platform to               
   effectively associate structured data and                unstructured
   data based on data lake technology to                store structured
   data and unstructured data, thus                providing efficient
   information retrieval services,                and satisfying
   comprehensive data utilization                needs.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a big  
   data platform based data association system.                (Drawing
   includes non-English language                text).1Data acquisition
   module2Big data platform21Data lake22Batch-stream integrated computing  
               module23Online analytical processing                database
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202510052S
ER

PT P
AU FANG Y
   LUO Y
   SHEN S
TI Multi-layer data lake based dual-model operation            and
   maintenance management method for information            technology (IT)
   informatization, involves receiving            data by Kafka operation
   and maintenance cluster, when            collecting IT operation and
   maintenance related data            from multiple data sources in real
   time
PN CN119323367-A; CN119323367-B
AE ZHUHAI ZHILIAN SIXUN TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves inputting (S4) the several               
   key performance evaluation state quantity sets into               
   operation and maintenance identification model to               
   identify potential factors that cause system                problems and
   performance bottlenecks. The                information technology (IT)
   resource configuration                is adjusted (S5) through operation
   and maintenance                adjustment model based on potential
   factors to                optimize system performance and improve
   resource                utilization. The identification results and     
   adjustment results are displayed (S6) by                associating
   operation and maintenance                identification model with
   operation and maintenance                adjustment model to support
   operation and                maintenance decision-making. The data is
   received                by Kafka(RTM: network communication platform)   
   operation and maintenance cluster, when collecting                IT
   operation and maintenance related data from                multiple data
   sources, and data receiving groups                are divided and nodes
   are configured to complete                Hadoop distributed file system
   (HDFS)                storage.
   USE - Multi-layer data lake based dual-model                operation
   and maintenance management method for                information
   technology (IT) informatization.
   ADVANTAGE - The method automatically adjusts IT resource               
   configuration based on potential factors through               
   operation and maintenance adjustment model to                optimize
   system performance and improves resource                utilization;
   dynamically display identification                results and adjustment
   results. The method                significantly improves operation and
   maintenance                efficiency and system stability.
   DETAILED DESCRIPTION - The IT operation and maintenance related data    
   is collected (S1) from multiple data sources in                real
   time, and a multi-layer operation and                maintenance data
   network is formed after                correlation processing. The
   multi-layer operation                and maintenance data network is
   integrated (S2)                into the data lake with layered
   technology to form                a multi-layer operation and
   maintenance data lake.                The multi-layer interface of the
   multi-layer                operation and maintenance data lake is used
   to                quickly access and retrieve large-scale operation     
   and maintenance data. The big data analysis is                performed
   (S3) on the retrieved large-scale                operation and
   maintenance data, accurate filtering                of data is
   completed, key features and deep                integration are
   extracted, and several key                performance evaluation state
   quantity sets are                obtained.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of
   multi-layer                data lake based dual-model operation and     
   maintenance management method for IT                informatization.
   (Drawing includes non-English                language text)S1Step for
   collecting the IT operation and                maintenance related data
   from multiple data sources                in real time, and forming a
   multi-layer operation                and maintenance data network after
   correlation                processingS2Step for integrating the
   multi-layer                operation and maintenance data network into
   the                data lake with layered technology to form a          
   multi-layer operation and maintenance data lake,                and
   using the multi-layer interface of the                multi-layer
   operation and maintenance data lake to                quickly access and
   retrieve large-scale operation                and maintenance dataS3Step
   for performing the big data analysis                on the retrieved
   large-scale operation and                maintenance data, completing
   accurate filtering of                data, extracting key features and
   deep integration,                and obtaining several key performance
   evaluation                state quantity setsS4Step for inputting the
   several key                performance evaluation state quantity sets
   into the                operation and maintenance identification model
   to                identify potential factors that may cause system      
   problems and performance bottlenecksS5Step for adjusting the IT resource
   configuration automatically through the operation                and
   maintenance adjustment model based on the                potential
   factors to optimize system performance                and improve
   resource utilizationS6Step for displaying the identification            
   results and adjustment results dynamically by                associating
   the operation and maintenance                identification model with
   the operation and                maintenance adjustment model to support
   operation                and maintenance decision-making
Z9 0
U1 0
U2 0
DA 2025-02-24
UT DIIDW:202512162L
ER

PT P
AU WANG B
TI Method for performing intelligent marketing            analysis and
   decision based on full life cycle data of            client, involves
   loading and analyzing data in data            lake warehouse according
   to different service            requirements to obtain data analysis
   result
PN CN119293022-A
AE EHI CAR RENTAL SERVICE MANAGEMENT SHANGH; SHANGHAI EHI INFORMATION
   TECHNOLOGY SERV; SHANGHAI YIHAI CAR LEASING CO LTD
AB 
   NOVELTY - The method involves collecting set of               
   heterogeneous data source information of a client.                The
   set of heterogeneous data source information is                cleaned.
   A data lake warehouse is established. The                cleaned
   heterogeneous data source information is                stored to the
   data lake warehouse. Data in the data                lake warehouse is
   loaded and analyzed according to                different service
   requirements to obtain a data                analysis result. The
   heterogeneous data source                information is pushed to a
   service end through the                client, where the heterogeneous
   data source                information comprises order data, user data, 
   behavior data, buried log and third party                anonymization
   data.
   USE - Method for performing intelligent marketing               
   analysis and decision based on full life cycle data                of a
   client.
   ADVANTAGE - The method enables ensuring consistency and               
   integrity of the data by cleaning the data, and                realizing
   real-time processing and analysis of the                large-scale data
   so as to satisfy requirement of an                enterprise for
   real-time data processing, thus                improving data processing
   efficiency. The method                enables providing multiple
   multidimensional                analysis tools and individual marketing
   support so                as to determine a demand and behavior of the
   client                deeply, thus realizing accurate marketing, hence  
                improving marketing effect.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic flow diagram  
   illustrating a method for performing intelligent               
   marketing analysis and decision based on full life                cycle
   data of a client. (Drawing includes                non-English language
   text).
Z9 0
U1 0
U2 0
DA 2025-02-22
UT DIIDW:2025080749
ER

PT P
AU SEDRAK F
TI Method for pulling most up-to-date metadata from            metadata
   catalog associated with data lake using            virtual file system,
   involves detecting data change to            data in a data repository,
   identifying metadata of data            change and storing metadata in
   virtual file by            processor
PN WO2025010173-A1; US2025013610-A1; US12411809-B2; US2025371025-A1;
   IN202547106702-A
AE GOOGLE LLC
AB 
   NOVELTY - The method involves detecting a data change to               
   data in a data repository by processor. The                metadata
   (224,228) of the data change is identified                by processor.
   The metadata in a virtual file (242)                is stored by
   processor. The virtual file is in a                data storage format
   that is compatible with data                analysis tools. The user
   request is received to                access metadata of the data in the
   data repository                by processor. The virtual files
   containing metadata                identified in the user request are
   transmitted in                response to the user request. The data
   repository                is a data lake (210) containing both
   structured                data and unstructured data. The data storage
   format                of the virtual file is an open table format. The  
   snapshot of the metadata of the data change is                generated.
   The snapshot in a network-connected                storage medium
   separate from the data repository is                stored.
   USE - Method for pulling most up-to-date metadata                from
   metadata catalog associated with data lake                using virtual
   file system.
   ADVANTAGE - The method provides an advancement for data               
   lakes that increases the ability to provide                security and
   governance for the data, thus                facilitating atomicity
   consistency isolation                durability (ACID) transactions,
   data versioning,                auditing, indexing, caching and
   statistical                analysis. The method avoids the need to wait
   for                exports of the metadata from the metadata catalog    
   to the data lake and ensures that up-to-date                metadata is
   returned in response to user requests.                The method
   provides an improved architecture for                managing data in
   data lakes while ensuring                consistency between the
   collected metadata and the                data actually contained in the
   data lake.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system    
   for pulling most up-to-date metadata from metadata               
   catalog associated with data lake using virtual                file
   system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   virtual file system.200Virtual file system210Data lake224,228Meta
   data230User device242Virtual file
Z9 0
U1 0
U2 0
DA 2025-01-19
UT DIIDW:2025037444
ER

PT P
AU NANDHAKUMAR N
   GANDI S
   ANNIE T
   MOHAN M
   MOHANAPRAKASH T A
   MANIRAJ S P
TI System for storing and analyzing large data in            cloud-based
   data lake as various formats e.g.            structured data, has data
   ingestion module for            ingesting data from multiple sources
   into data lake in            real-time, and analytics module for
   analyzing processed            data using advanced analytics tools
PN IN202441102919-A
AE NANDHAKUMAR N; GANDI S; ANNIE T; MOHAN M; MOHANAPRAKASH T A; MANIRAJ S P
AB 
   NOVELTY - The system has a cloud-based storage module               
   provided for storing large volumes of unprocessed                data in
   various formats such as structured data                i.e. relational
   databases, semi-structured data                i.e. XML files, and
   unstructured data i.e. audio,                video, images, and text. A
   data ingestion module                ingests data from multiple sources
   into a data lake                in real-time. An analytics module
   analyzes the                processed data using advanced analytics
   tools to                extract actionable insights from the data. A    
   visualization module generates visual reports from                the
   analyzed data for decision-making purposes. A                main body
   enables large-scale data analytics using                a cloud-based
   data lake without requirement for                on-premises
   infrastructure.
   USE - System for storing and analyzing large data in                a
   cloud-based data lake as various formats such as               
   structured data, semi-structured data and                unstructured
   data.
   ADVANTAGE - The system utilizes the cloud-based solutions               
   of organizations to extract actionable insights,                provide
   informed decisions, and gain competitive                advantages
   across industries by enabling efficient                large data
   analytics, ensures better data handling                within the data
   lake aligns with legal and                industry-specific
   requirements, enhances                scalability, flexibility, and
   processing power                required to handle the complex data
   environments,                and reduces the latency by enabling
   real-time                decision-making.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for performing large data analytics using a                cloud-based
   data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for storing and analyzing large data in a         
         cloud-based data lake.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2025056345
ER

PT C
AU Azeroual, Otmane
   Fabre, Renaud
   Stoerl, Uta
BE Coenen, F
   Fred, A
   Aveiro, D
   Dietz, J
   Poggi, A
   Gruenwald, L
   Masciari, E
   Bernardino, J
TI Revolutionary Synergy: The Fusion of Data Mesh and Data Fabric for
   Strategy Analytics in GRAPHYP Knowledge Graph
SO KNOWLEDGE DISCOVERY, KNOWLEDGE ENGINEERING AND KNOWLEDGE MANAGEMENT,
   IC3K 2023
SE Communications in Computer and Information Science
VL 2454
BP 277
EP 295
DI 10.1007/978-3-031-87569-4_13
DT Proceedings Paper
PD 2025
PY 2025
AB In today's world, big data has rapidly evolved, presenting organizations
   with a multitude of challenges in managing, integrating, and leveraging
   their most valuable resource. As data volumes and complexity continue to
   grow, the question arises: How can organizations effectively and
   efficiently manage their data to extract valuable insights? Two
   approaches, data mesh and data fabric, have emerged with the potential
   to revolutionize the current state of data architecture and help
   organizations redefine their data strategy. Although data mesh and data
   fabric are often viewed as contrasting approaches, they ultimately share
   the same goal of improving data management. While data mesh emphasizes
   decentralized responsibility and collaboration, data fabric focuses on
   unified infrastructure and data integration. Our paper aims to
   demonstrate how these two concepts challenge traditional paradigms and
   enable organizations to optimize their data utilization. By integrating
   their respective capabilities through "fusion analytics" GRAPHYP KG
   introduces, for the first time, a geometric interactive mapping of
   disputed knowledge categorizations processed from search logs. This
   novel framework gives rise to innovative "challenging options analytics"
   services, embracing web augmentation on mobile devices and facilitating
   innovative strategic management on an unprecedented scale.
CT 15th International Joint Conference on Knowledge Discovery, Knowledge
   Engineering and Knowledge Management-IC3K
CY NOV 13-15, 2023
CL Rome, ITALY
RI Azeroual, Dr. Otmane/B-7260-2018; Fabre, Renaud/
OI Fabre, Renaud/0000-0003-4170-324X
TC 0
ZR 0
ZB 0
ZS 0
Z8 0
ZA 0
Z9 0
U1 0
U2 0
SN 1865-0929
EI 1865-0937
BN 978-3-031-87568-7; 978-3-031-87569-4
DA 2025-09-25
UT WOS:001553035000013
ER

PT R
AU Coetsee, De Wit
TI History of Process Transformation
SO Figshare
DI https://doi.org/10.6084/M9.FIGSHARE.29105411
DT Data set
PD 2025-06-17
PY 2025
AB This flowchart, titled "History of Processes," illustrates the evolution
   of data processing from manual to fully automated systems. It features
   five stages: 1) Manual Process (human ��� manual forms ��� filing
   system, human searchable); 2) Automated with Manual Forms (human ���
   manual forms ��� application system ��� metadata ��� filing system, tech
   searchable); 3) Automated with Digital Forms (human ��� digital forms
   ��� application system ��� metadata ��� e-filing system, tech
   searchable); 4) Automated with E-Forms (human/e-sensing ��� metadata ���
   application system ��� e-filing/big data, algorithmic search/AI); 5)
   Fully Automated (e-sensing ��� unstructured/structured data ��� data
   lake, algorithmic search/AI). The chart shows a shift from human-driven
   to automated processes, with decreasing human intervention,
   transitioning from physical filing to big data, and advancing search
   capabilities. (Created: 04:45 AM SAST, May 20, 2025) Copyright: CC BY
   4.0
Z8 0
ZB 0
TC 0
ZA 0
ZR 0
ZS 0
Z9 0
U1 0
U2 0
DA 2025-09-25
UT DRCI:DATA2025133033517244
ER

PT R
AU Coetsee, De Wit
TI History of Process Transformation
SO Figshare
DI https://doi.org/10.6084/M9.FIGSHARE.29105411.V1
DT Data set
PD 2025-06-17
PY 2025
AB This flowchart, titled "History of Processes," illustrates the evolution
   of data processing from manual to fully automated systems. It features
   five stages: 1) Manual Process (human ��� manual forms ��� filing
   system, human searchable); 2) Automated with Manual Forms (human ���
   manual forms ��� application system ��� metadata ��� filing system, tech
   searchable); 3) Automated with Digital Forms (human ��� digital forms
   ��� application system ��� metadata ��� e-filing system, tech
   searchable); 4) Automated with E-Forms (human/e-sensing ��� metadata ���
   application system ��� e-filing/big data, algorithmic search/AI); 5)
   Fully Automated (e-sensing ��� unstructured/structured data ��� data
   lake, algorithmic search/AI). The chart shows a shift from human-driven
   to automated processes, with decreasing human intervention,
   transitioning from physical filing to big data, and advancing search
   capabilities. (Created: 04:45 AM SAST, May 20, 2025) Copyright: CC BY
   4.0
ZB 0
ZR 0
ZS 0
TC 0
Z8 0
ZA 0
Z9 0
U1 0
U2 0
DA 2025-09-25
UT DRCI:DATA2025133033517245
ER

PT C
AU Diouan, Ahlame
   Ferey, Eric
   Darmont, Jerome
   Loudcher, Sabine
BE Chbeir, R
   Ilarri, S
   Manolopoulos, Y
   Revesz, PZ
   Bernardino, J
   Leung, CK
TI About Relationships in Data Lakes
SO DATABASE ENGINEERED APPLICATIONS, IDEAS 2024
SE Lecture Notes in Computer Science
VL 15511
BP 141
EP 155
DI 10.1007/978-3-031-83472-1_10
DT Proceedings Paper
PD 2025
PY 2025
AB In the era of Big Data, managing voluminous and heterogeneous data
   presents significant challenges for organizations. To tackle these
   challenges, the concept of a data lake has emerged as a promising
   solution, allowing the storage of raw data from diverse sources in their
   original format. An efficient metadata management system plays a crucial
   role in preventing data lake to turn into an unusable data swamp by
   providing a structured framework for organizing, categorizing and
   establishing relationships between data entities. In this paper,
   identify the various relationships from diverse domains found in the
   literature. Then, we categorize the types of relationships and propose a
   relationship typology that classes relationships by similarity,
   containment, grouping and provenance. Eventually, we also aim to check
   whether goldMEDAL, a state-of-the-art generic metadata management model,
   adequately supports all such relationships. This evaluation is
   particularly relevant for Bial-X, which seeks to implement a robust
   metadata management system based on goldMEDAL's concepts.
CT 28th International Symposium on Database Engineered Applications-IDEAS
CY AUG 26-29, 2024
CL Bayonne, FRANCE
SP Association for Computing Machinery
OI Darmont, Jérôme/0000-0003-1491-384X; Loudcher,
   Sabine/0000-0002-0494-0169
TC 0
Z8 0
ZB 0
ZS 0
ZR 0
ZA 0
Z9 0
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-83471-4; 978-3-031-83472-1
DA 2025-08-08
UT WOS:001525027400010
ER

PT C
AU El Falah, Zineb
   Abouchabaka, Jaafar
   Rafalia, Najat
BE Koubaa, A
   Mnaouer, AB
   Boulila, W
   Raghay, S
TI An Intelligent Big Data Analysis Approach for Real-Time Data Processing:
   A Case Study on META Stock Price Prediction Using LSTM Model
SO INTERNATIONAL CONFERENCE ON SMART SYSTEMS AND EMERGING TECHNOLOGIES,
   2024
SE Lecture Notes in Networks and Systems
VL 1401
BP 279
EP 291
DI 10.1007/978-3-031-91235-1_25
DT Proceedings Paper
PD 2025
PY 2025
AB Technological progress has made Big Data a major trend, posing
   significant challenges in managing vast datasets through digital
   technologies. These challenges include integrating diverse data,
   ensuring timely processing, and providing effective analysis and
   visualization. Big Data analytics uses AI, deep learning, and machine
   learning to uncover patterns and trends, improving decision-making and
   predictions. Real-time data processing faces additional difficulties
   related to handling high data velocity and volume, requiring robust and
   scalable infrastructure. This paper proposes an intelligent Big Data
   architecture for real-time data processing using Apache Kafka, the
   Elastic Stack, and Apache Spark. The approach is demonstrated through a
   case study predicting META stock prices using a Long Short-Term Memory
   (LSTM) model. By integrating deep learning, the architecture effectively
   addresses Big Data challenges, improving decision-making efficiency. The
   analysis uses stock data from Yahoo Finance, and the model's performance
   was assessed using R-squared (R-2) and Mean Absolute Error (MAE). The
   LSTM model achieved an R-2 score of 0.99 and an MAE of 0.04, showing
   significant improvement over traditional models, particularly in
   handling volatile stock data.
CT 3rd International Conference on Smart Systems and Emerging
   Technologies-SMARTTECH-Annual
CY NOV 19-21, 2024
CL Cadi Ayyad University of Marrakech, Marrakesh, MOROCCO
HO Cadi Ayyad University of Marrakech
SP Institute of Electrical and Electronics Engineers Inc; Prince Sultan
   University; University of Prince Mugrin; Ibn Tofail University of
   Kenitra
RI ABOUCHABAKA, Jaafar/JTV-1015-2023; EL FALAH, Zineb/KYP-0832-2024
TC 0
ZB 0
ZR 0
ZA 0
Z8 0
ZS 0
Z9 0
U1 0
U2 0
SN 2367-3370
EI 2367-3389
BN 978-3-031-91234-4; 978-3-031-91235-1
DA 2025-12-05
UT WOS:001591611300025
ER

PT C
AU Freitas, Nelson
   Vaqueira, Diogo
   Rocha, Andre Dionisio
   Barata, Jose
   Serrano, Fabio
   Silva, Luis
   Madeira, Manuel
BE Dassisti, M
   Madani, K
   Panetto, H
TI A Novel Pipeline for Data Management and Analysis that Integrates Data
   Lakehouse Architecture into the Aeronautics Industry
SO INNOVATIVE INTELLIGENT INDUSTRIAL PRODUCTION AND LOGISTICS, IN4PL 2024,
   PT I
SE Communications in Computer and Information Science
VL 2372
BP 410
EP 424
DI 10.1007/978-3-031-80760-2_26
DT Proceedings Paper
PD 2025
PY 2025
AB Currently, the majority of processes or systems aim to benefit from the
   data produced by their own or other relevant systems, with the objective
   of increasing efficiency. This is especially true in the field of
   industrial systems, where a multitude of devices attempt to publish
   their metrics and data into the system, often resulting in
   characteristics that can be classified as big data. However, companies
   often struggle with the correct and useful utilization of this harvested
   data. Therefore, this paper focuses on a use case of a data pipeline
   system with a data lakehouse in an airplane parts factory. The developed
   architecture shows that with some adjustments to the classic data
   lakehouse architecture, it is possible to achieve higher parallelism in
   order to simultaneously store data in the data lake and data warehouse.
   Additionally, a visualization tool was developed to highlight how metric
   calculation and outlier detection can be automated or facilitated with
   the utilization of data, as opposed to manual labor.
CT 5th International Conference on Innovative Intelligent Industrial
   Production and Logistics
CY NOV 21-22, 2024
CL PORTUGAL
SP Institute of Electrical and Electronics Engineers Industry Applications
   Society
RI Rocha, Ane Dionisio/O-9800-2017; Freitas, Nelson/
OI Rocha, Ane Dionisio/0000-0003-0874-7099; Freitas,
   Nelson/0000-0003-4616-0723
ZS 0
ZA 0
ZR 0
ZB 0
Z8 0
TC 0
Z9 0
U1 2
U2 3
SN 1865-0929
EI 1865-0937
BN 978-3-031-80759-6; 978-3-031-80760-2
DA 2025-04-25
UT WOS:001457753900026
ER

PT J
AU Gartner, Christopher M.
   Sakhare, Rahul Suryakant
   Desai, Jairaj Chetas
   Sturdevant, James
   Bullock, Darcy M.
TI A Scalable Data Model for Signalized Intersection Performance Measures
SO IEEE ACCESS
VL 13
BP 197851
EP 197863
DI 10.1109/ACCESS.2025.3635007
DT Article
PD 2025
PY 2025
AB Traditionally, traffic signal management has been largely tactical and
   operated from the "bottom-up," focusing on individual intersection
   performances influenced by public suggestions, field observation, and in
   some cases high-resolution controller data. Connected vehicle trajectory
   data now enables a scalable, complementary "top-down" approach for
   system-wide performance analysis, even for locations with no detection
   or communication. This paper proposes a data architecture for derived
   performance measures that can be used for both strategic (top-down) and
   tactical (bottom-up) management of traffic signals. To leverage a
   "top-down" approach, large amounts of data must be processed into
   smaller datasets. For efficiency and interoperability between
   performance measures and the tools used to derive the same, a scalable
   data management architecture is essential. This paper discusses and
   demonstrates a framework developed for the Indiana Department of
   Transportation, which contains the derived information from over 177
   billion records per year for the state, covering more than 2,500
   signalized intersections in just 80,000 or so rows of derived
   performance measures. This framework collects data using a combination
   of a fixed time period, date, movement, approach, and intersection
   identifier. The structured model of derived performance measures
   supports both tactical applications, such as green-time allocation, and
   top-down, strategic management applications such as identifying
   capacity-constrained signals that are candidates for capital
   improvements. This framework is applicable to other tools used to derive
   performance measures, such as high-resolution or LiDAR datasets.
RI Gartner, Christopher/LGY-8902-2024; Bullock, Darcy/; Sakhare, Rahul Suryakant/IVV-7297-2023
OI Gartner, Christopher/0009-0008-8448-2356; Bullock,
   Darcy/0000-0002-7365-1918; Sakhare, Rahul Suryakant/0000-0001-7843-5707
ZA 0
Z8 0
TC 0
ZB 0
ZR 0
ZS 0
Z9 0
U1 0
U2 0
SN 2169-3536
DA 2025-12-10
UT WOS:001626884300009
ER

PT B
AU Hermanus, Danny
Z2  
TI Strategies for Migrating Data Warehouses to Data Lakehouses Using Public
   Cloud Computing
DT Dissertation/Thesis
PD Jan 01 2025
PY 2025
ZA 0
ZR 0
Z8 0
ZS 0
ZB 0
TC 0
Z9 0
U1 1
U2 1
BN 9798315718598
UT PQDT:123519540
ER

PT J
AU Kretzer, Arthur Raulino
   Barreto Vavassori Benitti, Fabiane
   Siqueira, Frank
TI Challenges and Opportunities in Big Data Analytics for Industry 4.0: A
   Systematic Evaluation of Current Architectures
SO IEEE ACCESS
VL 13
BP 183419
EP 183447
DI 10.1109/ACCESS.2025.3624558
DT Article
PD 2025
PY 2025
AB The current efforts to integrate Big Data Analytics (BDA) into Industry
   4.0 manufacturing systems, despite their usefulness for enhancing
   data-driven decision-making, are constrained by the lack of
   architectural standards for data management. This systematic mapping
   study analyzes many BDA architectures proposed in the literature,
   revealing a fragmented landscape in which the proposed architectures are
   largely conceptual with limited industrial validation. Our analysis
   identifies dominant technological patterns, such as Apache Kafka for
   ingestion, Spark for processing, and Hadoop and Hive for storage, with
   the majority of implementations favoring open-source solutions. Despite
   their theoretical importance, real-time analytics capabilities remain
   underutilized in practice. This study synthesizes a unified conceptual
   reference architecture with eight fundamental layers to provide a
   framework for comparative analysis. We document an imbalance in layer
   development: storage and processing receive comprehensive attention
   while querying, infrastructure management, and monitoring layers remain
   underdeveloped. Implementation approaches show distinct patterns in
   deployment strategies and data handling, with structured and
   semi-structured data well supported, whereas unstructured data
   integration presents ongoing challenges. Future research should focus on
   developing standardized modular frameworks, benchmarking methodologies,
   and integrating modern data lakehouse architectures to bridge the gap
   between theoretical proposals and production-ready systems.
RI Siqueira, Frank/ABB-8351-2021; Raulino Kretzer, Arthur/; Benitti, Fabiane/
OI Raulino Kretzer, Arthur/0000-0003-1656-9464; Benitti,
   Fabiane/0000-0003-2747-9931
ZB 0
Z8 0
ZA 0
ZR 0
TC 0
ZS 0
Z9 0
U1 4
U2 4
SN 2169-3536
DA 2025-11-12
UT WOS:001606717700016
ER

PT J
AU Mekled, Mahmoud Hamam
TI DX House: Unified Data Management Architecture Implementation
SO Zenodo
DI https://doi.org/10.5281/ZENODO.17872888
DT Software
PD 2025-12-23
PY 2025
AB Data-X-House is a unified big data management platform that combines
   data lake and data warehouse capabilities using containerized
   microservices. This repository provides a production-ready
   implementation based on the Data Lake Architecture Framework (DLAF),
   featuring Apache Hadoop ecosystem (HDFS, Hive, Spark), Apache Iceberg
   for table format, Trino for query execution, Apache Ranger for security,
   and Apache Airflow for orchestration. The platform supports both
   on-premise and cloud deployments with enterprise-grade security,
   governance, and compliance features suitable for highly regulated
   sectors. Includes complete Docker-based deployment, ingestion pipelines,
   data warehouse implementation, and visualization dashboards. Copyright:
   Apache License 2.0
TC 0
ZB 0
ZR 0
ZS 0
Z8 0
ZA 0
Z9 0
U1 0
U2 0
DA 2026-02-17
UT DRCI:DATA2026016034812558
ER

PT J
AU Mekled, Mahmoud Hamam
TI DX House: Unified Data Management Architecture Implementation
SO Zenodo
DI https://doi.org/10.5281/ZENODO.17872889
DT Software
PD 2025-12-23
PY 2025
AB Data-X-House is a unified big data management platform that combines
   data lake and data warehouse capabilities using containerized
   microservices. This repository provides a production-ready
   implementation based on the Data Lake Architecture Framework (DLAF),
   featuring Apache Hadoop ecosystem (HDFS, Hive, Spark), Apache Iceberg
   for table format, Trino for query execution, Apache Ranger for security,
   and Apache Airflow for orchestration. The platform supports both
   on-premise and cloud deployments with enterprise-grade security,
   governance, and compliance features suitable for highly regulated
   sectors. Includes complete Docker-based deployment, ingestion pipelines,
   data warehouse implementation, and visualization dashboards. Copyright:
   Apache License 2.0
ZS 0
ZB 0
Z8 0
ZA 0
ZR 0
TC 0
Z9 0
U1 0
U2 0
DA 2026-02-17
UT DRCI:DATA2026016034812559
ER

PT C
AU Rossi, A.
   Santos, K.
   Treinta, F.
   Pontes, J.
BE Zimmermann, R
   Rodrigues, JC
   Simoes, A
   Dalmarco, G
TI Proposal for a Decision-Making Dashboard Enhanced by Big Data: An
   Application in the Portuguese Furniture Industry
SO HUMAN-CENTRED TECHNOLOGY MANAGEMENT FOR A SUSTAINABLE FUTURE, VOL 2,
   IAMOT
SE Springer Proceedings in Business and Economics
BP 407
EP 416
DI 10.1007/978-3-031-72494-7_40
DT Proceedings Paper
PD 2025
PY 2025
AB Big data analysis in strategic management significantly aids
   decision-making processes, enhancing their effectiveness. Investigating
   key themes in this area allows for trend identification, process
   optimization, and proactive issue prediction, thereby boosting
   competitiveness. This study aims to develop a visual tool for
   decision-making support regarding sales behaviour in the operations
   department of a service industry, utilizing big data analysis within
   Industry 4.0. The research methodology used was the Design Science,
   identifying 5 main phases to accomplish the objective. It was applied
   the IDC model to centralizes the management of data and establish
   decision-making requirements. Subsequently, the DMN method for
   describing and modeling repeatable decisions within organizations and
   data architecture were applied, facilitating the comprehension of the
   significance and scope of the studied topic, along with its main themes.
   Utilizing Power Query, data analysis and processing were automated, and
   M language codes were programmed. It was concluded that data quality's
   relevance significantly impacts decision-making within strategic
   management, thus enabling the development of digital transformation
   within the organization.
CT 33rd International Association for the Management of Technology
   Conference
CY JUL 08-11, 2024
CL Porto, PORTUGAL
ZB 0
Z8 0
ZS 0
ZA 0
ZR 0
TC 0
Z9 0
U1 0
U2 1
SN 2198-7246
EI 2198-7254
BN 978-3-031-72496-1; 978-3-031-72494-7; 978-3-031-72493-0
DA 2025-04-25
UT WOS:001454289900040
ER

PT C
AU Siddabattula, Venkata Sudheer
   Hachem, Fatima
   Mesiti, Marco
   Leo, Matteo
   Rosa, Giampiero
   Damiani, Maria Luisa
BE Rao, J
   Xie, Y
   Gao, S
   Eftelioglu, E
   Aly, H
   Li, Y
TI Geo-enrichment in a Data Lakehouse: Exploring Challenges and
   Opportunities
SO PROCEEDINGS OF 4TH ACM SIGSPATIAL INTERNATIONAL WORKSHOP ON SPATIAL BIG
   DATA AND AI FOR INDUSTRIAL APPLICATIONS, GEOINDUSTRY 2025
BP 90
EP 98
DI 10.1145/3764919.3770886
DT Proceedings Paper
PD 2025
PY 2025
AB Data lakehouses are modern data architectures designed to support the
   integrated management and analysis of large volumes of heterogeneous,
   business-oriented data on distributed platforms. When such data include
   spatial information, a key question is how to semantically integrate it
   with other sources - an operation referred to as geo-enrichment -
   thereby creating new opportunities for more effective and insightful
   data analysis. Yet, the notion of geo-enrichment has received limited
   attention in the academic literature and is often associated with
   commercial information services. In this paper, we present our vision
   and discuss key challenges, particularly those related to defining a
   data exploration environment that provides geo-enrichment operators and
   tools for both discovering relevant data sources and interacting with
   geo-enrichable data. Our discussion is grounded in a use case involving
   the integration of spatial datasets provided by Eurostat-the statistical
   office of the European Union (EU)-within Apache Sedona on a Spark
   cluster, as adopted in the context of the EU-funded GRINS project.
CT 4th ACM SIGSPATIAL International Workshop on Spatial Big Data and AI for
   Industrial Applications-GeoIndustry
CY NOV 03, 2025
CL Minneapolis, MN
SP Institute of Electrical and Electronics Engineers Inc; ACM Special
   Interest Group on Spatial Information; Apple Inc; Esri Co Ltd; Quantum
   Geometry LLC; Oracle Corp; Springer Nature
RI Mesiti, Marco/LFS-0643-2024
ZR 0
Z8 0
ZA 0
ZB 0
TC 0
ZS 0
Z9 0
U1 0
U2 0
BN 979-8-4007-2182-3
DA 2026-02-11
UT WOS:001661533300010
ER

PT J
AU Silva, Danilo
   Moir, Monika
   Dunaiski, Marcel
   Blanco, Natalia
   Murtala-Ibrahim, Fati
   Baxter, Cheryl
   de Oliveira, Tulio
   Xavier, Joicymara S.
CA INFORM Africa Res Study Grp
TI Review of open-source software for developing heterogeneous data
   management systems for bioinformatics applications
SO BIOINFORMATICS ADVANCES
VL 5
IS 1
AR vbaf168
DI 10.1093/bioadv/vbaf168
DT Review
PD 2025
PY 2025
AB In a world where data drive effective decision-making, bioinformatics
   and health science researchers often encounter difficulties managing
   data efficiently. In these fields, data are typically diverse in format
   and subject. Consequently, challenges in storing, tracking, and
   responsibly sharing valuable data have become increasingly evident over
   the past decades. To address the complexities, some approaches have
   leveraged standard strategies, such as using non-relational databases
   and data warehouses. However, these approaches often fall short in
   providing the flexibility and scalability required for complex projects.
   While the data lake paradigm has emerged to offer flexibility and handle
   large volumes of diverse data, it lacks robust data governance and
   organization. The data lakehouse is a new paradigm that combines the
   flexibility of a data lake with the governance of a data warehouse,
   offering a promising solution for managing heterogeneous data in
   bioinformatics. However, the lakehouse model remains unexplored in
   bioinformatics, with limited discussion in the current literature. In
   this study, we review strategies and tools for developing a data
   lakehouse infrastructure tailored to bioinformatics research. We
   summarize key concepts and assess available open-source and commercial
   solutions for managing data in bioinformatics.Availability and
   implementation Not applicable.
RI de Castro Silva, Danilo/; Santos Xavier, Joicymara/; Moir, Monika/AAU-6520-2021; Dunaiski, Marcel/AAC-9387-2022
OI de Castro Silva, Danilo/0000-0001-5740-3968; Santos Xavier,
   Joicymara/0000-0002-4649-6270; Moir, Monika/0000-0003-1095-1910; 
ZS 0
ZR 0
TC 0
Z8 0
ZB 0
ZA 0
Z9 0
U1 3
U2 3
EI 2635-0041
DA 2025-08-12
UT WOS:001543196800001
PM 40761326
ER

PT C
AU Thorat, Madhu
   Walters, Sarah
GP ACM
TI Empowering Generative AI in Enterprises: Sustainable, High-Performance
   Storage Solutions with Global Data Fabric and Data Lakehouse
SO PROCEEDINGS OF 2025 SUPERCOMPUTING ASIA CONFERENCE, SCA 2025
BP 70
EP 78
DI 10.1145/3718350.3718351
DT Proceedings Paper
PD 2025
PY 2025
AB Rapid growth in unstructured data has triggered enterprises to shift
   from traditional file storage to scalable object storage solutions in
   their infrastructure. There has been a growing need to integrate both
   file and object storage into a unified platform capable of managing the
   complexities of traditional and modern data workloads. Gartner predicts
   that by 2029, over 80% of unstructured data will reside on consolidated
   storage systems-up from 40% in early 2024-highlighting the urgency of
   this transition. As sustainability takes center stage, businesses must
   adopt storage solutions that deliver both high performance and minimal
   environmental impact. This paper presents a storage solution designed
   for Generative AI and High-Performance Computing (HPC), addressing two
   critical requirements:
   1. A unified high-performance storage solution that seamlessly
   integrates both file and object storage access.
   2. A sustainable approach to building infrastructure that reduces energy
   usage and carbon emissions.
   We will explore how sustainable, high-performance storage solutions such
   as global data fabrics and data lakehouse technologies enable this
   vision. Key topics include:
   1. The critical role of efficient storage in accelerating AI workloads.
   2. The advantages of distributed file and object storage for
   enterprise-scale workloads.
   3. Essential sustainability practices that align with environmental
   goals.
   4. A global data fabric use case, leveraging features from reference
   architectures for research data, that incorporates these innovations as
   well as embedded sustainability practices.
   This paper demonstrates how researchers, architects, CIOs, and
   administrators can leverage next-generation storage solutions to improve
   operational efficiency and support sustainability in generative AI use
   cases.
CT 2025 Supercomputing Asia Conference-SCA
CY MAR 10-13, 2025
CL Singapore, SINGAPORE
OI Thorat, Madhu/0000-0003-2068-4609; Walters, Sarah
   Bernadette/0009-0003-2762-3232
ZB 0
TC 0
ZR 0
Z8 0
ZA 0
ZS 0
Z9 0
U1 1
U2 1
BN 979-8-4007-1250-0
DA 2025-09-12
UT WOS:001542473700008
ER

PT J
AU Wang, Gaizhi
TI Intelligent Path for Constructing Financial Risk Monitoring Mechanism
   Under the Big Data Environment
SO INTERNATIONAL JOURNAL OF DECISION SUPPORT SYSTEM TECHNOLOGY
VL 17
IS 1
AR 389193
DI 10.4018/IJDSST.389193
DT Article
PD 2025
PY 2025
AB In the era of big data, corporate financial operations generate a large
   amount of heterogeneous information. Traditional risk monitoring systems
   cannot effectively accommodate complex data flows and real-time risk
   changes, which often leads to false positives and delays. This study
   proposes a framework based on 'big data lake-semantic layer-intelligent
   algorithm' to achieve real-time and interpretable financial risk
   monitoring. Through the multi-modal risk representation combined with
   the hybrid flow-batch pipeline and knowledge graph, the real-time
   synchronization of risk scoring and response strategy is realized by
   using a state machine-driven feedback loop and adaptive threshold
   adjustment. The experimental results show that the accuracy of the
   framework is improved by more than 10%, the false alarm rate is reduced
   to 1.8%, and the response time is shortened to 250 milliseconds. This
   study improves the stability and responsiveness of the system through
   multi-modal learning and an adaptive threshold mechanism.
TC 0
ZA 0
ZS 0
ZB 0
ZR 0
Z8 0
Z9 0
U1 3
U2 3
SN 1941-6296
EI 1941-630X
DA 2025-11-17
UT WOS:001612856000002
ER

PT R
AU Wang, Mingxuan
   Heinz, D Fill
TI Multimodal Interaction Behavior Framework (MIBF) and Visitor Intent
   Recognition Network (VIRNet)
SO Zenodo
DI https://doi.org/10.5281/ZENODO.17440598
DT Data set
PD 2025-11-18
PY 2025
AB Multimodal Interaction Behavior Framework (MIBF) and Visitor Intent
   Recognition Network (VIRNet) A comprehensive implementation framework
   inspired by the paper��User Behavior Modeling and Experience
   Optimization of Smart Museums Based on Multimodal Interaction. ����
   Vision Smart museums are rapidly transforming from static exhibition
   spaces into dynamic, interactive cultural ecosystems.To understand and
   enhance visitor experience, this project introduces a Multimodal
   Interaction Behavior Framework (MIBF) and a Visitor Intent Recognition
   Network (VIRNet) that jointly model, predict, and optimize user behavior
   through visual, auditory, spatial, and tactile modalities. The goal is
   to build an intelligent system capable of recognizing visitor intentions
   and providing adaptive, personalized exhibition experiences that enhance
   immersion, engagement, and learning. ���� Core Concepts 1. Multimodal
   Interaction Behavior Framework (MIBF) The MIBF defines how heterogeneous
   data streams from museum interactions can be semantically represented
   and processed. Modalities: vision (camera/video), audio (speech, ambient
   sound), position (indoor localization), and touch (UI interaction).
   Behavior Semantic Units (BSU): atomic elements of visitor behavior that
   unify events from different modalities into semantic structures. Data
   Representation: BSUs serve as interpretable intermediate representations
   that capture both temporal and contextual meaning. 2. Visitor Intent
   Recognition Network (VIRNet) A deep learning model that performs
   hierarchical intent recognition: Temporal Encoding Layer: Bi-directional
   GRU with attention to model sequential dependencies. Multimodal Fusion:
   Joint encoding of modalities with learned gating weights. Hierarchical
   Attention: Extracts both individual (per-visitor) and collective
   (group-level) behavioral patterns. Output: Visitor intent categories
   such as exploration, learning, social engagement, or disengagement. ����
   Data Architecture Multimodal Data System The dataset integrates: Visual
   stream ��� extracted from surveillance or embedded cameras Audio stream
   ��� voice commands, discussions, environmental cues Spatial data ���
   visitor trajectory and location heatmaps Touch interactions ��� screen
   navigation, object manipulation events Each data record is synchronized,
   normalized, and segmented into Behavior Semantic Units (BSUs) using
   timestamp alignment and context labels. ���� Methodology Overview Data
   Acquisition and SynchronizationCollection of real-world multimodal
   visitor data from museum installations. Behavior Semantic
   ModelingDefinition of BSUs and hierarchical behavior categories for
   semantic interpretation. Model Construction (VIRNet)Multimodal input
   fusion ��� temporal attention encoding ��� intent classification.
   Training and Evaluation Cross-entropy loss for classification. Metrics:
   accuracy, macro-F1, satisfaction rating correlation. Baselines:
   single-modality models and early-fusion neural networks. Scenario-Based
   EvaluationEmpirical validation through controlled experiments in museum
   environments to measure: Immersion Learning effectiveness User
   satisfaction ���� Experimental Highlights Data Scale: 10,000+ multimodal
   interaction segments from actual exhibition sessions. Results: VIRNet
   achieved up to 17% higher accuracy and 12% higher F1-score than
   traditional unimodal methods. Multimodal feedback enhanced visitor
   engagement and perceived immersion during interactive exhibits.
   Qualitative Insight: MIBF enabled interpretable behavior tracking that
   informed exhibit design refinements. ���� Applications Adaptive Exhibit
   PersonalizationAutomatically adjust displayed content based on inferred
   visitor intent. Immersive Learning AnalyticsMeasure attention and
   comprehension through multimodal feedback. Museum Operations
   IntelligencePredict visitor flow and recommend optimal spatial
   arrangements. Cross-Museum CollaborationShared multimodal behavioral
   schema (BSU) for standardized data exchange. ���� Evaluation Metrics
   Metric Description Accuracy Correct classification rate of visitor
   intent Macro F1 Balance of precision/recall across intent classes
   Immersion Score Subjective rating from post-experience surveys Learning
   Gain Difference in pre/post knowledge assessments Satisfaction Index
   Mean Likert-scale user rating of recommendations ���� Technical
   Architecture Data Layer ��� Semantic Modeling ��� VIRNet Inference ���
   Experience Optimization Data Layer: Collects and synchronizes multimodal
   data. Semantic Layer: Transforms raw data into BSUs. Model Layer:
   Encodes temporal and multimodal patterns. Application Layer: Drives
   personalization and feedback visualization. ���� Innovation Highlights
   Theoretical Innovation - Introduces the concept of Behavior Semantic
   Units (BSUs) for unified semantic modeling of visitor behavior.
   Methodological Innovation - Proposes VIRNet, a hierarchical
   attention-based model integrating visual, audio, spatial, and tactile
   modalities. Technical Innovation - Employs temporal attention mechanisms
   to improve the modeling of complex sequential behavior. Practical
   Contribution - Provides data-driven pathways for experience optimization
   and personalized cultural engagement in smart museums. ���� Future Work
   Expanding MIBF to incorporate physiological modalities (e.g., gaze
   tracking, heart rate). Building a cross-domain knowledge layer that
   connects behavioral semantics with exhibit metadata (linking to
   ICMM/CKG). Deploying real-time adaptive recommendation systems within
   museum applications. ���� License & Citation Open for academic and
   non-commercial use under MIT License. If you use or extend this
   framework, please cite: User Behavior Modeling and Experience
   Optimization of Smart Museums Based on Multimodal Interaction (2024) -
   Legend Co. Research Division. Copyright: Creative Commons Attribution
   4.0 International
ZB 0
ZR 0
ZS 0
Z8 0
TC 0
ZA 0
Z9 0
U1 0
U2 0
DA 2026-02-17
UT DRCI:DATA2025210034492416
ER

PT R
AU Wang, Mingxuan
   Heinz, D Fill
TI Multimodal Interaction Behavior Framework (MIBF) and Visitor Intent
   Recognition Network (VIRNet)
SO Zenodo
DI https://doi.org/10.5281/ZENODO.17440597
DT Data set
PD 2025-11-18
PY 2025
AB Multimodal Interaction Behavior Framework (MIBF) and Visitor Intent
   Recognition Network (VIRNet) A comprehensive implementation framework
   inspired by the paper��User Behavior Modeling and Experience
   Optimization of Smart Museums Based on Multimodal Interaction. ����
   Vision Smart museums are rapidly transforming from static exhibition
   spaces into dynamic, interactive cultural ecosystems.To understand and
   enhance visitor experience, this project introduces a Multimodal
   Interaction Behavior Framework (MIBF) and a Visitor Intent Recognition
   Network (VIRNet) that jointly model, predict, and optimize user behavior
   through visual, auditory, spatial, and tactile modalities. The goal is
   to build an intelligent system capable of recognizing visitor intentions
   and providing adaptive, personalized exhibition experiences that enhance
   immersion, engagement, and learning. ���� Core Concepts 1. Multimodal
   Interaction Behavior Framework (MIBF) The MIBF defines how heterogeneous
   data streams from museum interactions can be semantically represented
   and processed. Modalities: vision (camera/video), audio (speech, ambient
   sound), position (indoor localization), and touch (UI interaction).
   Behavior Semantic Units (BSU): atomic elements of visitor behavior that
   unify events from different modalities into semantic structures. Data
   Representation: BSUs serve as interpretable intermediate representations
   that capture both temporal and contextual meaning. 2. Visitor Intent
   Recognition Network (VIRNet) A deep learning model that performs
   hierarchical intent recognition: Temporal Encoding Layer: Bi-directional
   GRU with attention to model sequential dependencies. Multimodal Fusion:
   Joint encoding of modalities with learned gating weights. Hierarchical
   Attention: Extracts both individual (per-visitor) and collective
   (group-level) behavioral patterns. Output: Visitor intent categories
   such as exploration, learning, social engagement, or disengagement. ����
   Data Architecture Multimodal Data System The dataset integrates: Visual
   stream ��� extracted from surveillance or embedded cameras Audio stream
   ��� voice commands, discussions, environmental cues Spatial data ���
   visitor trajectory and location heatmaps Touch interactions ��� screen
   navigation, object manipulation events Each data record is synchronized,
   normalized, and segmented into Behavior Semantic Units (BSUs) using
   timestamp alignment and context labels. ���� Methodology Overview Data
   Acquisition and SynchronizationCollection of real-world multimodal
   visitor data from museum installations. Behavior Semantic
   ModelingDefinition of BSUs and hierarchical behavior categories for
   semantic interpretation. Model Construction (VIRNet)Multimodal input
   fusion ��� temporal attention encoding ��� intent classification.
   Training and Evaluation Cross-entropy loss for classification. Metrics:
   accuracy, macro-F1, satisfaction rating correlation. Baselines:
   single-modality models and early-fusion neural networks. Scenario-Based
   EvaluationEmpirical validation through controlled experiments in museum
   environments to measure: Immersion Learning effectiveness User
   satisfaction ���� Experimental Highlights Data Scale: 10,000+ multimodal
   interaction segments from actual exhibition sessions. Results: VIRNet
   achieved up to 17% higher accuracy and 12% higher F1-score than
   traditional unimodal methods. Multimodal feedback enhanced visitor
   engagement and perceived immersion during interactive exhibits.
   Qualitative Insight: MIBF enabled interpretable behavior tracking that
   informed exhibit design refinements. ���� Applications Adaptive Exhibit
   PersonalizationAutomatically adjust displayed content based on inferred
   visitor intent. Immersive Learning AnalyticsMeasure attention and
   comprehension through multimodal feedback. Museum Operations
   IntelligencePredict visitor flow and recommend optimal spatial
   arrangements. Cross-Museum CollaborationShared multimodal behavioral
   schema (BSU) for standardized data exchange. ���� Evaluation Metrics
   Metric Description Accuracy Correct classification rate of visitor
   intent Macro F1 Balance of precision/recall across intent classes
   Immersion Score Subjective rating from post-experience surveys Learning
   Gain Difference in pre/post knowledge assessments Satisfaction Index
   Mean Likert-scale user rating of recommendations ���� Technical
   Architecture Data Layer ��� Semantic Modeling ��� VIRNet Inference ���
   Experience Optimization Data Layer: Collects and synchronizes multimodal
   data. Semantic Layer: Transforms raw data into BSUs. Model Layer:
   Encodes temporal and multimodal patterns. Application Layer: Drives
   personalization and feedback visualization. ���� Innovation Highlights
   Theoretical Innovation - Introduces the concept of Behavior Semantic
   Units (BSUs) for unified semantic modeling of visitor behavior.
   Methodological Innovation - Proposes VIRNet, a hierarchical
   attention-based model integrating visual, audio, spatial, and tactile
   modalities. Technical Innovation - Employs temporal attention mechanisms
   to improve the modeling of complex sequential behavior. Practical
   Contribution - Provides data-driven pathways for experience optimization
   and personalized cultural engagement in smart museums. ���� Future Work
   Expanding MIBF to incorporate physiological modalities (e.g., gaze
   tracking, heart rate). Building a cross-domain knowledge layer that
   connects behavioral semantics with exhibit metadata (linking to
   ICMM/CKG). Deploying real-time adaptive recommendation systems within
   museum applications. ���� License & Citation Open for academic and
   non-commercial use under MIT License. If you use or extend this
   framework, please cite: User Behavior Modeling and Experience
   Optimization of Smart Museums Based on Multimodal Interaction (2024) -
   Legend Co. Research Division. Copyright: Creative Commons Attribution
   4.0 International
ZA 0
ZR 0
ZB 0
ZS 0
TC 0
Z8 0
Z9 0
U1 0
U2 0
DA 2026-02-17
UT DRCI:DATA2025210034492415
ER

PT B
AU Zhang, Xin
Z2  
TI SynopsisDB: A Distributed Data System Supports In-System Data
   Exploration
DT Dissertation/Thesis
PD Jan 01 2025
PY 2025
TC 0
ZS 0
ZR 0
ZB 0
ZA 0
Z8 0
Z9 0
U1 0
U2 0
BN 9798263308971
UT PQDT:161747741
ER

PT P
AU LIU J
   XU Y
TI Method for synchronizing data lake with            multi-source
   heterogeneous data by using electronic            device, involves
   obtaining data interface            specifications and interactive data
   task tables of each            system, and obtaining original data of
   each            system
PN CN119226401-A
AE PICC INFORMATION TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves obtaining data interface               
   specifications and interactive data task tables of                each
   system (S1). The original data of each system                is obtained
   based on the data interface                specifications and the
   interactive data task tables                of each system (S2). The
   original data of each                system is pre-processed to obtain
   the pre-processed                data of each system (S3). The
   pre-processed data of                each system is synchronized to a
   data lake (S4).                The method further involves determining
   the alarm                rules corresponding to each system. The process
   of                synchronizing the data lake of each system data is    
          monitored and pre-warned based on the alarm                rules.
   USE - Method for synchronizing data lake with               
   multi-source heterogeneous data by using electronic               
   device (claimed).
   ADVANTAGE - The method imports multi-source heterogeneous               
   data into the data lake based on the data interface               
   specifications and the interactive data task tables                of
   each system, reduces the difficulty and cost of               
   multi-source heterogeneous data integration, and                improves
   the consistency and accuracy of the                data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:a device for synchronizing data lake with               
   multi-source heterogeneous data by using electronic               
   device; anda computer storage medium has a set of               
   instructions for synchronizing data lake with               
   multi-source heterogeneous data by using electronic               
   device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for synchronizing data lake with                multi-source
   heterogeneous data (Drawing includes                non-English language
   text).S1Step for obtaining data interface                specifications
   and interactive data task tables of                each systemS2Step for
   obtaining original data of each                system based on data
   interface specifications and                interactive data task tables
   of each systemS3Step for pre-processing original data of               
   each system to obtain pre-processed data of each               
   systemS4Step for synchronizing pre-processed data                of each
   system to data lake
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202503993J
ER

PT P
AU LI W
   WEI C
   GAO L
   LIU H
   WANG Z
   HU Y
   FAN Y
TI Method for distributing patent retrieval based on            homemade
   autonomous controllable environment involves            e.g. inquiring
   matched patent and node of structured            data and position of
   non-structured data based on            searching information and
   searching mode input by            user
PN CN119166718-A
AE INST SOFTWARE CHINESE ACAD SCI
AB 
   NOVELTY - The method involves establishing a distributed               
   patent data architecture comprising a relational                database
   for storing patent structured data, a                NoSQLdatabase and a
   high capacity storage server                for storing patent
   unstructured data (1). The                patent data of the same core
   country is stored in                same node. The main table is used
   for storing core                information of patent. The sub-table is
   used for                storing patent information related to the main  
   table and the main key of the main table. The                retrieval
   engine extracts the key field of each                patent from the
   patent data of each country. The                node where the patent
   data is located and the main                key of the main table where
   the patent data is                located. The main key information of
   the secondary                table, the index of the patent and the
   dictionary                item are constructed (2). The matched patent
   and                the node of the structured data and the position of  
   the non-structured data are inquired based on the               
   searching information and searching mode input by                the
   user (3).
   USE - The method is useful for distributing patent               
   retrieval based on homemade autonomous controllable               
   environment.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for        
   distributed patent retrieval system based on                homemade
   autonomous controllable environment.
   DESCRIPTION Of DRAWING(S) - The drawing shows a general search flow
   chart                of the distributing patent retrieval based on      
   homemade autonomous controllable                environment.(Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2024D6060S
ER

PT J
AU Boukraa, Doulkifli
   Bouraoui, Meriem
   Grine, Chaima
   Ouahab, Racha
TI Megale: A Metadata-Driven Graph-Based System for Data Lake Exploration
SO INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING
VL 24
IS 01
BP 259
EP 295
DI 10.1142/S0219622024500135
EA DEC 2024
DT Article
PD JAN 2025
PY 2025
AB Data lakes are storage repositories that contain large amounts of data
   (big data) in its native format; encompassing structured,
   semi-structured or unstructured. Data lakes are open to a wide range of
   use cases, such as carrying out advanced analytics and extracting
   knowledge patterns. However, the sheer dumping of data into a data lake
   would only lead to a data swamp. To prevent such a situation,
   enterprises can adopt best practices, among which to manage data lake
   metadata. A growing body of research has focused on proposing metadata
   systems and models for data lakes with a special interest on model
   genericness. However, existing models fail to cover all aspects of a
   data lake, due to their static modeling approach. Besides, they do not
   fully cover essential features for an effective metadata management,
   namely governance, visibility and uniform treatment of data lake
   concepts. In this paper, we propose a dynamic modeling approach to meet
   these features, based on two main constructs: data lake concept and data
   lake relationship. We showcase our approach by Megale, a graph-based
   metadata system for NoSQL data lake exploration. We present a
   proof-of-concept implementation of Megale and we show its effectiveness
   and efficiency in exploring the data lake.
RI Boukraa, Doulkifli/A-8548-2011
OI Boukraa, Doulkifli/0000-0002-6065-4183
ZB 0
ZS 0
ZR 0
TC 0
ZA 0
Z8 0
Z9 0
U1 3
U2 6
SN 0219-6220
EI 1793-6845
DA 2024-12-22
UT WOS:001379146900001
ER

PT P
AU ERDMANN C A
   BRUSH R A
   SUTARIYA B B
TI Non-transitory medium for providing closed-loop            intelligence,
   has machine learning model applied to            portion of data to
   generate prediction by accessing            shared model information
   associated with            machine-learning algorithm selected from
   group            including logistic regression, linear regression
PN US12158864-B1
AE CERNER INNOVATION INC
AB 
   NOVELTY - The medium has a set of instructions for               
   receiving a selection of data from a database                including
   internal data of a particular healthcare                organization and
   external data from other                healthcare organizations to
   build a data model. A                machine learning model is applied
   to a portion of                the data to generate a prediction by
   accessing                shared model information associated with a     
   machine-learning algorithm selected from a group               
   including logistic regression, linear regression,               
   decision tree classifier and decision tree                regression.
   Insights gained from the model are                pushed into a clinical
   workflow associated with the                particular organization. A
   common code system is                utilized for the model.
   USE - Non-transitory media storing program for                providing
   closed-loop intelligence for training and                testing
   predictive tools on entire population of                data, push
   insights gained from predictive tools.                Uses include but
   are not limited to financial,                scientific, medical and
   other fields to capture and                analyze vast streams of
   transactional,                experimental, and other data.
   ADVANTAGE - The method enables providing an efficient and               
   convenient system that enables structured views of                data
   to be aggregated into a big data architecture,                to train
   and test predictive tools on an entire                population of
   data, to push insights gained from                the predictive tools
   into a clinical workflow, and                to provide access and use
   to predictive tools by                other organizations.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a method
   involves receiving a selection of                data, from a database
   including internal data of a                particular healthcare
   organization and external                data from one or more other
   healthcare                organizations, to build a data model;(2) a
   system having one or more hardware                processors configured
   to facilitate a set of                operations.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an     
   exemplary computing environment.100Computing environment102Control
   server104Data store106Computer networks108Remote computers
Z9 0
U1 0
U2 0
DA 2024-12-18
UT DIIDW:2024C7619S
ER

PT P
AU HUANG X
TI Statistical analysis based medical channel leakage            quantity
   checking system, has visualization and            monitoring module for
   displaying decision support data            and abnormal alarm data and
   realizing real-time            monitoring and transparent management of
   each supply            chain
PN CN119067678-A; CN119067678-B
AE SHANGHAI PHARMEYES MEDICAL TECHNOLOGY CO
AB 
   NOVELTY - The system has a multi-modal sensor module for               
   collecting multi-dimensional data of medicine in                each
   supply chain link, including environment                state, position
   data and transportation condition,                and generating
   multi-modal sensor data. A data lake                module is
   communicated with the multi-modal sensor                module for
   receiving the multi-modal sensor data,                performing data
   standardization and semantic                hierarchy integration
   operations, and generating                integrated semantic data. An
   intelligent analysis                and verification module comprises a
   heterogeneous                data interaction unit for performing data  
   interaction analysis operation to the integrated                semantic
   data to generate interaction analysis                result data. A
   visualization and monitoring module                displays decision
   support data and abnormal alarm                data and realizes
   real-time monitoring and                transparent management of each
   supply chain.
   USE - Statistical analysis based medical channel                leakage
   quantity checking system.
   ADVANTAGE - The system can identify abnormity by               
   collecting and standardizing the medicine                circulation
   data in real-time and utilizing                intelligent analysis and
   causal reasoning, and                realizes multi-target optimization
   to make better                management strategy, dynamically adjusts
   the                management strategy through a feedback mechanism     
   and application of machine learning and improves                response
   ability and transparency of each supply                chain, ensures
   high-efficiency and precise                management of entire process
   from production of the                medicine to a terminal shop for
   providing reliable                guarantee for safety and efficiency of
   the medical                channel.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   statistical analysis based medical channel leakage               
   quantity checking method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   statistical analysis based medical channel leakage               
   quantity checking system. (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2025-01-11
UT DIIDW:2024C9266K
ER

PT P
AU CHENG Y
   MA Z
   ZHENG X
   WANG Y
   LI H
   XIAO X
   JIANG S
   LI R
   ZHANG L
   CUI X
   WANG G
   LI X
   LIU J
   WANG J
   WANG C
   ZHENG H
TI Data lake warehouse integrated management method            based on
   port traffic, involves searching cold data            with output data
   characteristic in port traffic data            lake, invoking cold data
   to temporary capacity            expansion node, and feeding back
   invoking condition to            administrator page
PN CN119025598-A; CN119025598-B
AE JIANGSU ZHENYUN TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves constructing a port                traffic
   data lake and several port traffic data                warehouses
   interacted with the data lake. The port                traffic data lake
   is used for storing the cold data                in the port monitoring
   data. The port traffic data                warehouse is used for storing
   the hot data in the                port monitoring data. A fixed time
   period is                constructed. The port monitoring data is
   obtained                under each time period. The port monitoring data
   change is invoked under each time period. An                intelligent
   analysis model is constructed. The                function
   characteristic relation is formed between                the port
   traffic data change rate and the port                monitoring data
   call based on the intelligent                analysis model. The data
   characteristic is output.                The cold data with the output
   data characteristic                is searched in the port traffic data
   lake based on                the output data characteristic. The cold
   data is                invoked to the temporary capacity expansion node.
   The invoking condition is fed back to the                administrator
   page.
   USE - Data lake warehouse integrated management                method
   based on port traffic, for the analysis and                storage of
   data such as photo, video, document and                unstructured
   data.
   ADVANTAGE - The method solves the limitation of the data               
   lake based on the lake cabin integrated structure,                stores
   and calculates the port traffic data, ends                the cluster
   layer cutting between the data                warehouse and the data
   lake and makes the data and                the calculation freely flow
   between the two                platforms. The data congestion risk is
   reduced                based on the temporary capacity expansion        
          mode.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a data      
   lake warehouse integrated management system based                on port
   traffic.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the data lake warehouse integrated
   management                method based on port traffic. (Drawing
   includes                non-English language text)
Z9 0
U1 0
U2 0
DA 2024-12-31
UT DIIDW:2024C6078X
ER

PT P
AU SINGH A
TI Method for preventing remote code execution            attacks in web
   applications, involves chaining stack            trace with process to
   generate flow graph, and adding            flow graph to data lake in
   response to determining that            flow graph does not include
   command that accepts user            input
PN US2024386113-A1
AE CISCO TECHNOLOGY INC
AB 
   NOVELTY - The method involves receiving (602) a data                flow
   from a web application, where the data flow                includes a
   stack trace and a process. The stack                trace is chained
   (604) with the process to generate                a flow graph. A data
   lake is accessed (606) to                determine whether the flow
   graph exists in the data                lake. A determination is made
   whether the graph                includes a command that accepts a user
   input. The                process is terminated in response to
   determining                that the graph does include the command that 
   accepts user input. The flow graph is added to the                data
   lake in response to determining that the flow                graph does
   not include a command that accepts user                input. The
   instruction is sent to an extended                detection and response
   service for termination in                response to determining that
   the process includes a                command that accepts user input.
   USE - Method for preventing remote code execution                attacks
   in web applications. Uses include but are                not limited to
   webmail, online calculators or                e-commerce shops.
   ADVANTAGE - The method provides a cloud-native platform               
   built on big data infrastructure to provide                security
   teams with flexibility, scalability, and                opportunities
   for automation. The method allows                organizations to use
   insights gained through                endpoint detection and response
   (EDR) tools to                harden security against future attacks and
   reduce                dwell time for a potential infection, thus        
   providing holistic protection against cyberattacks,               
   unauthorized access and misuse.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   security agent comprising multiple                processors;(2) a set
   of non-transitory computer-readable                media storing
   computer-executable                instructions.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method  
   for preventing remote code execution attacks in web               
   applications.602Receiving a data flow from a web               
   application604Chaining stack trace with the process to               
   generate a flow graph606Accessing a data lake to determine whether      
   the flow graph exists in the data lake608Determining whether flow exists
   in data                lake610Ending process
Z9 0
U1 0
U2 0
DA 2024-12-09
UT DIIDW:2024C2600P
ER

PT J
AU Nyikana, Wandisa
   Iyamu, Tiko
TI Human interaction with structure in designing big data architecture
SO SOUTH AFRICAN JOURNAL OF INFORMATION MANAGEMENT
VL 26
IS 1
AR a1918
DI 10.4102/sajim.v26i1.1918
DT Article
PD NOV 18 2024
PY 2024
AB Background: There is an increasing interest in big data. However,
   challenges shape and affect the gathering, retrieval, use and management
   of big data in many organisations. Some of the challenges are linked to
   a lack of architecture that is specific to big data. Attempts have been
   made from both business and academic fronts, yet the challenges persist.
   The challenges are attributed to a lack of an understanding of the
   factors that influence the design of architecture for big data in an
   organisation. Objectives: The study aims to propose big data
   architecture for enterprises. Method: We employed the qualitative
   method, using document analysis to gather data. Activity theory (AT) was
   employed in the analysis of the data. Results: From the analysis,
   governance, interactions, relationships and allocative were found to be
   the factors that influence the design of big data architecture. An
   interpretation was conducted following the inductive reasoning approach
   to gain a deeper insight of how the factors manifest themselves.
   Conclusion: Big data architecture is proposed. The architecture is
   intended to address some of the challenges encountered in gathering,
   retrieving, using or managing big data in organisations. Contribution:
   This study advances our understanding of the complex interplay of
   factors influencing the architecture of big data. Applying AT, the study
   fortifies our understanding of complex interactions between humans and
   big data including the architecture design.
OI Nyikana, Wandisa/0000-0002-9625-5379; Iyamu, Tiko/0000-0002-4949-094X
Z8 0
TC 0
ZR 0
ZB 0
ZA 0
ZS 0
Z9 0
U1 0
U2 3
SN 2078-1865
EI 1560-683X
DA 2024-12-11
UT WOS:001370583800001
ER

PT P
AU HUANGYI J
   MA J
   WAN G
   XIONG J
   XU Y
TI Method for managing multi-source heterogeneous            data, involves
   expanding analysis data to obtain            internal analysis data and
   external analysis data and            pouring internal analysis data and
   external analysis            data into partitioned data lake and
   building target            blockchain in partitioned data lake
PN CN118939738-A
AE CHINA UNITED NETWORK COMMUNICATIONS CORP
AB 
   NOVELTY - The method involves acquiring (S101)               
   multi-source heterogeneous data of each device and               
   processing and analyzing (S102) the multi-source               
   heterogeneous data based on the partitioned data                lake to
   obtain analysis data. The analysis data is                expanded
   (S103) to obtain internal analysis data                and external
   analysis data. The internal analysis                data and the
   external analysis data are poured                (S104) into the
   partitioned data lake. A target                blockchain is built
   (S105) in the partitioned data                lake. The metadata is
   marked on the preparation                data and the operation data to
   obtain multi-source                heterogeneous data of each device.
   The multi-source                heterogeneous data is extracted based on
   the                ingestion zone to obtain extracted data. The         
   changes in the multi-source heterogeneous data are               
   detected. The extracted data is converted into a               
   standardized format based on the preparation area                to
   obtain standardized format data.
   USE - Method for managing multi-source heterogeneous               
   data.
   ADVANTAGE - The method realizes convenient and efficient               
   management of multi-source heterogeneous data,                reduces
   infrastructure resources and bandwidth                consumption, saves
   time cost of data storage and                management, helps to
   identify correlation and                influence of multiple aspects of
   data, improves                insight and data analysis capability,
   enhances                effectiveness of decision support and promotes
   data                driving decision and innovation development of      
            various industries.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:1. a device for managing multi-source               
   heterogeneous data;2. an electronic device; and3. a computer readable
   storage medium storing                program for managing multi-source
   heterogeneous                data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method  
   for managing multi-source heterogeneous data.                (Drawing
   includes non-English language text)S101Step for acquiring multi-source  
   heterogeneous data of each deviceS102Step for processing and analyzing
   the                multi-source heterogeneous data based on the         
   partitioned data lake to obtain analysis                dataS103Step for
   expanding analysis data to                obtain internal analysis data
   and external analysis                dataS104Step for pouring internal
   analysis data                and the external analysis data into the
   partitioned                data lakeS105Step for building target
   blockchain in                the partitioned data lake
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2024C1093R
ER

PT P
AU ZHOU S
   LI Y
   ZHANG Z
   DU W
   SUO J
TI Heterogeneous data fusion method applied to data            lake
   platform, involves performing data fusion and data            labeling
   on heterogeneous data to be fused after fine            adjustment, to
   obtain target fusion data, and            constructing data view through
   target fusion data, to            perform data display through data view
PN CN118885971-A
AE INSPUR CLOUD INFORMATION TECHNOLOGY CO
AB 
   NOVELTY - The method involves collecting (S11)               
   heterogeneous data of different data sources. The               
   heterogeneous data is pre-processed to obtain the               
   pre-processed heterogeneous data. The local target                data
   fusion model is fine-adjusted (S12) according                to the
   received data fusion task to obtain the                fine-adjusted
   data fusion model. The corresponding                heterogeneous data
   to be fused is extracted from                the pre-processed
   heterogeneous data according to                the data fusion task. The
   data fusion and data                labeling are performed (S13) on the
   heterogeneous                data to be fused according to the data
   fusion model                after fine adjustment, so as to obtain
   target                fusion data. The data view is constructed through 
   the target fusion data, so as to perform data                display
   through the data view.
   USE - Heterogeneous data fusion method applied to                data
   lake platform.
   ADVANTAGE - The method automatically identifies and               
   understands the characteristics of different data               
   sources, realizes the seamless fusion of the               
   heterogeneous data, makes the data more consistent                and
   standardized, and improves the accessibility                and
   usability of the data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a heterogeneous data fusion device;an electronic device; anda
   computer readable storage medium storing                program for
   heterogeneous data fusion.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the heterogeneous data fusion method.
   (Drawing                includes non-English language text)S11Step for
   collecting heterogeneous data of                different data
   sourcesS12Step for fine-adjusting local target data               
   fusion model according to received data fusion task                to
   obtain fine-adjusted data fusion modelS13Step for performing data fusion
   and data                labeling on heterogeneous data to be fused      
   according to data fusion model after fine                adjustment, to
   obtain target fusion data
Z9 0
U1 0
U2 0
DA 2024-12-09
UT DIIDW:2024B8002D
ER

PT P
AU CHEN W
   LI B
   XU J
   SHAN M
   QIAN H
   QIAN R
TI Multi-channel infrared (IR) gas detection system            based on
   large data used in large data processing and            analysis tools,
   has data storage and management module            that is used for
   establishing disaster recovery            mechanism, and encrypting and
   storing data by adopting            encryption technology
PN CN118858203-A; CN118858203-B
AE JIANGSU JIUCHUANG ELECTRICAL TECHNOLOGY
AB 
   NOVELTY - The system has a real-time monitoring and               
   alarming module that is used for continuously                monitoring
   the gas concentration data acquired from                the data
   preprocessing module, and keeping                real-time communication
   with the sensor module. The                abnormal alarming module is
   used for setting a                threshold value of gas concentration,
   judging                whether an alarm condition is reached, and       
   dynamically adjusting according to the requirement.                A
   data storage and management module is used for                storing
   the processed data by adopting an HDFS,                storing the
   structured data by using an Apache(RTM:                Network
   communications software) HBase,                establishing a data lake
   architecture, and storing                the structured data and the
   unstructured data in                the same storage pool. The module is
   used for                implementing a distributed data backup strategy.
   The module is used for storing the data backup in a                set
   of geographic positions, establishing a                disaster recovery
   mechanism, and encrypting and                storing the data by
   adopting an encryption                technology.
   USE - Multi-channel IR gas detection system based on               
   large data used in large data processing and                analysis
   tools.
   ADVANTAGE - The data preprocessing module extracts               
   frequency domain characteristics by utilizing                Fourier
   transformation, and analyzes the spectrum                intensity of
   each channel more accurately, thus                improving the accuracy
   and stability of gas                detection. The mode identification
   and                classification module adopts a support vector        
   machine algorithm, combines a set of feature                vectors to
   train and classify the samples, and                better identify the
   gas types and improve the                performance and generalization
   capability of the                classification model compared with the
   traditional                method. The data storage and management
   module                adopts the techniques of HDFS, apache HBase, and  
   APACHE KAFKA, realizes efficient data storage,                processing
   and backup, and improves the efficiency                and reliability
   of data management.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   multi-channel infrared gas detection method based                on
   large data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a system structure diagram
   of the multi-channel IR gas detection system based                on
   large data. (Drawing includes non-English                language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2024B7050W
ER

PT P
AU MAI J
   ZHANG Z
   ZHANG X
   ZHENG J
TI Multi-source heterogeneous data based            visualization
   presentation method, involves obtaining            visualization
   application, and distributing            visualization applications to
   present visualization            arrangement result of multi-source
   heterogeneous            data
PN CN118796924-A
AE CHINA MOBILE GROUP GUANGDONG CO LTD; CHINA MOBILE COMMUNICATIONS GROUP
   CO LTD
AB 
   NOVELTY - The method involves accessing multiple               
   heterogeneous data sources through a data lake.                Metadata
   of the data sources is obtained and                managed. Multiple
   data tables are obtained. A data                visualization component
   is constructed. Related                information of a data
   visualization components is                set. The data information of
   the related                information is provided with data
   information,                where the data information is used to
   indicate a                data field in the data table defaulted by the
   data                visualization component. A visualization            
   application is obtained. The visualization                applications
   are distributed to present a                visualization arrangement
   result of the                multi-source heterogeneous data.
   USE - Multi-source heterogeneous data based                visualization
   presentation method for analysis,                modelling,
   visualization and machine learning                applications.
   ADVANTAGE - The method enables distributing the visual               
   application to present the visual arrangement                result of
   the multi-source heterogeneous data, so                that the
   visualization presentation based on multi-                source
   heterogeneous metadata is realized, and the                data value is
   realized by the visualization                arrangement technology.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   multi-source heterogeneous data based visualization               
   presentation device; (2) a multi-source                heterogeneous
   data based visualization presentation                system; (3) an
   electronic device comprising a                memory and a processor for
   performing a                multi-source heterogeneous data based
   visualization                presentation method; (4) a computer
   readable                storage medium for storing a set of instruction
   for                performing a multi-source heterogeneous data based   
               visualization presentation method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a         
   multi-source heterogeneous data based visualization               
   presentation method. (Drawing includes non-English               
   language text).
Z9 0
U1 0
U2 0
DA 2024-11-16
UT DIIDW:2024B3025T
ER

PT P
AU ZHAO X
   WU P
   YIN K
   RU J
TI Method for managing data, involves performing            object storage
   on target data according to address            information, and
   determining processing result            corresponding to configuration
   information according to            data table
PN CN118797102-A
AE CHINA MOBILE COMMUNICATION CO LTD RES; CHINA MOBILE COMMUNICATIONS GROUP
   CO LTD
AB 
   NOVELTY - The method involves obtaining (101) first               
   configuration information corresponding to the                target
   data. The first configuration information                comprises first
   address information and data type                of the target data. The
   object storage is performed                (102) on the target data
   according to the first                address information, the first
   index information of                the target data is obtained based on
   the object                storage, and the first related information of
   the                target data is recorded in a data table associated   
   with the data lake under the condition of                determining
   that the target data is unstructured                data based on the
   data type. The first related                information comprises the
   first address                information, the first identification
   information                of the target data and the first index
   information.                The first processing result corresponding to
   the                first configuration information is determined (103)  
                according to the data table.
     USE - Method for managing data applied to first                server.
   ADVANTAGE - The processing result corresponding to the               
   configuration information is determined according                to the
   data table, and the processing result is                determined based
   on the data type of the target                data, so that the data
   management process is                performed in an efficient manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a device for managing data;a system for managing data;a
   computer readable storage medium storing                program for
   managing data;a computer program product for managing data;             
     anda computer device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for managing data. (Drawing includes                non-English
   language text).101Step for obtaining first configuration               
   information corresponding to the target data102Step for performing
   object storage on the                target data according to the first
   address                information, obtaining the first index
   information                of the target data based on the object
   storage, and                recording the first related information of
   the                target data in a data table associated with the      
   data lake under the condition of determining that                the
   target data is unstructured data based on the                data
   type103Step for determining the first processing                result
   corresponding to the first configuration                information
   according to the data table
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2024B2872X
ER

PT P
AU ZHENG Q
   CHEN D
TI Method for improving data mart based on data lake,            involves
   generating verification result table,            comparing differences
   according to verification result            table, locating specific
   reasons, and optimizing tasks            according to specific reasons
PN CN118673330-A
AE JIANGSU SU MERCHANTS BANK CO LTD
AB 
   NOVELTY - The method involves extracting tasks that do               
   not meet mart reference specification, and                modifying the
   tasks that do not meet the mart                reference specification,
   and then deploying                modified tasks to big data platform,
   and obtaining                result data based on running the modified
   tasks.                Result data obtained by running the modified tasks
   and result data obtained by running the tasks                before
   modification are read, and data consistency                analysis and
   time consistency analysis are                performed on the result
   data obtained by running                the modified tasks and the
   result data obtained by                running the tasks before the
   modification. The                modified tasks are standardized and
   result data                comparison is consistent if both the data    
   consistency analysis and the time consistency                analysis
   pass. Otherwise, a verification result                table is
   generated, and differences are compared                according to the
   verification result table, and                specific reasons are
   located, and the tasks are                optimized according to the
   specific reasons.
   USE - Method for improving data mart based on data                lake
   in practice of data construction in                bank.
   ADVANTAGE - The device for improving data mart based on               
   data lake ensures data availability and business               
   continuity, provides a more standard data mart               
   construction method, and unifies the data                source.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device    
              for improving data mart based on data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   device                for improving data mart based on data lake.       
   (Drawing includes non-English language text)1Task scanning module2Task
   improvement module3Analysis module
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2024A3690Y
ER

PT P
AU JUNG B
   JIN W
   ZHANG W
   WANG Y
TI Method for constructing federal data lake            warehouse utilized
   for supporting multi-source            heterogeneous data storage,
   involves connecting data            lakes and reservoirs together to
   form federal data lake            and reservoir and forming unified data
   access and            analysis platform
PN CN118660088-A
AE SHENZHEN SUNTANG DIGITAL TECHNOLOGY CO
AB 
   NOVELTY - The method involves constructing a digital               
   object architecture that comprises a digital object               
   model, a digital object interface protocol, a                digital
   object identification analysis protocol, a                digital object
   warehouse system, a digital object                registration system
   and a digital object                identification analysis system. The
   digital                networking uses a digital object interface
   protocol                and a digital object identification analysis    
   protocol to connect a digital object warehouse                system, a
   digital object registration system and a                digital object
   identification analysis system.                Several data lakes and
   reservoirs are connected                together to form a federal data
   lake and reservoir.                A unified data access and analysis
   platform is                formed.
   USE - Method for constructing federal data lake                warehouse
   utilized for supporting multi-source                heterogeneous data
   storage, batch processing,                stream processing and other
   working loads.
   ADVANTAGE - The method has commonality and expandability,               
   which effectively manages the data, avoids the                formation
   of the data swamp and makes the data have                fluidness.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:(1)a system for constructing federal data               
   lakes; and(2)a computer readable storage medium storing               
   program for constructing federal data lake                warehouse.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a     
   digital object architecture. (Drawing includes               
   non-English language text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2024A1625Q
ER

PT P
AU FOX J C
   IKENE Y
TI System for forming extensible data warehouse, has            electronic
   processor that extracts metadata associated            with additional
   dataset from dataset, creates catalog            of dataset and modifies
   data lake to include additional            data based on dataset and
   catalog with data ingester            application
PN WO2024182891-A1; US2024303248-A1; AU2024231873-A1; SG11202504970-A;
   EP4677455-A1
AE MASTERCARD TECHNOLOGIES CANADA ULC
AB 
   NOVELTY - The system (100) has a server that is provided               
   with an electronic processor (106) and a memory                (110)
   including a data ingestor application (112).                The
   electronic processor receives raw data (116)                with a
   structure, and forms a data lake (114) in                the memory
   using the raw data. Additional raw data                having a set of
   structures is continuously                received. Each structure is
   determined. A dataset                is generated based on the
   additional data and the                set of determined structures.
   Metadata associated                with the additional dataset is
   extracted from the                dataset. A catalog of the dataset is
   created and                the data lake to include additional data is  
   modified based on dataset and catalog with the data               
   ingester application.
         USE - System for forming extensible data                warehouse.
   ADVANTAGE - The system automatically detects the format of              
   data and make the data available in a data                warehousing
   platform for data analytics for data                science with built
   in rigorous data processing and                data security controls to
   ensure that privacy,                integrity, and confidentiality of
   the data access                usage. The data warehouse platform is
   designed to                flexibly accommodate a fast-paced product    
   development environment where data structures may                change
   unexpectedly that combines big data pipeline                concepts
   with the concept of a data lake that                stores data in any
   format.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a method for forming an extensible data               
   warehouse; anda non-transitory computer-readable medium               
   storing a program for forming an extensible data               
   warehouse.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for forming an extensible data warehousing
   platform                with built-in data security
   controls.100Extensible data warehouse formation               
   system106Electronic processor110Memory112Data ingestor
   application114Data lake116Raw data
Z9 0
U1 0
U2 0
DA 2024-09-23
UT DIIDW:202494532J
ER

PT P
AU WANG X
   SUN B
   MENG X
   WANG Y
   ZHAO Z
   GUO Y
TI Hudi data uptake method for multi-source            heterogeneous data,
   involves constructing data resource            catalog of data lake
   according to collected metadata            and related information of
   metadata in data lake
PN CN118503229-A; CN118503229-B
AE UNIV QILU TECHNOLOGY SHANDONG ACAD SCI; SHANDONG COMPUTER SCI CENT
AB 
   NOVELTY - The method involves uploading related               
   information of a lake-entering task. The relevant               
   information is uploaded to issue a lake entry task,                where
   the relevant information includes a task                title, a data
   connection configuration and a data                type. A gateway
   distributes data to the lake entry                tasks. An
   Apache(Company Name) Flink is used as a                data lake entry
   processing engine. The gateway                manages and distributes
   the data entry task. A data                resource directory of the
   data lake is constructed                according to the collected
   metadata and the related                information. The data types
   include MySQL(Database                management system) data,
   JSON(Computer data format)                data, log files, video stream
   data, image data,                audio data, and text documents. Each
   data type is                provided with a unique identification (ID)
   and is                extensible.
   USE - Hudi data acquisition method and system for               
   multi-source heterogeneous data, belonging to                technical
   field of data processing, for use in                university
   intelligent education platform.
   ADVANTAGE - The method enables integrally managing all               
   data streams through the gateway framework,                simplifying
   the data integration process from                different sources, and
   reducing the complexity of                the system and the total cost.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   Hudi data uptake system for multi-source                heterogeneous
   data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   Hudi data uptake method for multi-source                heterogeneous
   data (Drawing includes Non-English                language text).
Z9 0
U1 0
U2 0
DA 2024-09-07
UT DIIDW:2024893120
ER

PT P
AU LIAO T
   GU D
   JIA X
   LIANG J
TI Method for managing security of big data used in            field of
   energy storage management involves converting            energy storage
   knowledge map data into form which can            be processed by rule
   engine, performing constraint            solving, so as to generate
   energy storage scheduling            solution data
PN CN118445277-A
AE DALIAN ZHONGZE RUIXING TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves obtaining the real-time               
   operation data of each sub-system of the energy                storage
   system (S1). The standardized mapping on                the
   energy-stored heterogeneous data is performed.                The
   abnormal data restoration to the energy storage                fusion
   data set is performed (S2). The battery                health state
   evaluation on the key event response                data is performed
   (S3). The primary analysis result                data and the key event
   response data is uploaded                (S4) to the energy storage data
   lake for storage.                The domain knowledge of the data in the
   energy                storage data lake is extracted by using the large 
   data processing frame, so as to generate the domain               
   knowledge data. The energy storage knowledge map                data is
   constructed (S6) according to the field                knowledge data.
   The energy storage knowledge map                data is converted into
   the form which can be                processed by the rule engine. The
   constraint                solving is performed, so as to generate energy
                  storage scheduling solution data.
   USE - Method for managing security of big data used                in
   the field of energy storage management.
   ADVANTAGE - The method combines complex event processing               
   with long-term data analysis, which can balance               
   short-term response and long-term optimization of               
   industrial energy storage system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
            system for managing security of big data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the method
   for managing security of big data used in the field                of
   energy storage management (Drawing includes                non-English
   language text).S1Step for obtaining the real-time operation             
   data of each sub-system of the energy storage               
   systemS2Step for performing abnormal data                restoration to
   the energy storage fusion data                setS3Step for performing
   battery health state                evaluation on the key event response
   dataS4Step for uploading the primary analysis                result data
   and the key event response data to the                energy storage
   data lake for storageS5Step for constructing energy storage             
    knowledge map data according to the field knowledge                data
Z9 0
U1 0
U2 0
DA 2024-09-13
UT DIIDW:202491933Q
ER

PT P
AU WU X
   FU S
   SUN L
   NA Y
   LIU P
   KANG J
   CHEN S
   ZHOU M
   YUE D
   HUANG J
   YANG Q
   LI Y
   LIANG F
   MA Z
TI Data security collection scheduling system has            hash chain
   construction device that iterates hash chain            according to
   hash result, and backtracking device that            backtracks original
   data in data lake by using            position, hash chain and
   reversible hash function when            original data needed to be
   called
PN CN118445338-A; CN118445338-B
AE STATE GRID NINGXIA ELECTRIC POWER CO LTD; STATE GRID SIJI WANGAN
   TECHNOLOGY BEIJIN
AB 
   NOVELTY - The system (1) has a micro-service device (11)               
   that creates a micro-service framework and                integrates
   micro-service items, and the                micro-service items include
   a first micro-service                item and a second micro-service
   item. A hash device                (12) selects a reversible hash
   function after the                second micro-service item extracts
   data from the                data lake, hashes the identifier, and
   overwrites                the identifier in the original data with the  
   obtained first hash value. A hash chain                construction
   device (13) uses an identifier as a                first data node, a
   first hash value as a second                data node, constructs a hash
   chain, creates a                trigger for monitoring an original data
   change,                continues to hash when the trigger monitors that 
   the original data is scheduled, and iterates the                hash
   chain according to a hash result. A                backtracking device
   (14) backtracks the original                data in the data lake by
   using the position, the                hash chain and the reversible
   hash function when                the original data needed to be called.
           USE - Data security collection scheduling                system.
   ADVANTAGE - The original data is backtracked in time, so               
   that the original data is prevented from being                changed in
   the collection and scheduling process,                and the accuracy
   of big data processing is                improved.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the
   data                security collection scheduling system. (Drawing     
   includes non-English language text)1Data security collection scheduling 
   system11Micro-service device12Hash device13Hash chain construction
   device14Backtracking device
Z9 0
U1 0
U2 0
DA 2024-09-13
UT DIIDW:2024917472
ER

PT P
AU LOU Y
   ZHUANG Q
   WANG B
   YANG B
TI Method for processing data, involves determining            execution
   strategy corresponding to operation type            according to
   operation type corresponding to data lake            table operation
   request, and performing updating            operation
PN CN118331960-A
AE IND & COMML BANK CHINA LTD
AB 
   NOVELTY - The method (200) involves determining (S210) a               
   target metadata base from N groups of metadata                bases
   according to target data lake table                information in the
   data lake table operation                request by utilizing a mapping
   table and a state                table in response to a data lake table
   operation                request, and N is an integer greater than or
   equal                to 1. The N groups of metadata bases are           
   distributed and deployed. The mapping table and the                state
   table are stored in any group of metadata                bases in the N
   groups of metadata bases. An                execution strategy
   corresponding to the operation                type is determined (S220)
   according to the                operation type corresponding to the data
   lake table                operation request. The updating operation is  
   performed (S230) on the metadata corresponding to                the
   data lake table operation request in the target                metadata
   base according to the execution                strategy.
   USE - Method for processing data for use in field of                big
   data, and financial technology.
   ADVANTAGE - The target metadata database corresponding to               
   the data lake table operation request is determined                by
   utilizing the mapping table and the state table                so as to
   execute the updating operation                corresponding to the data
   lake table operation                request in the target metadata
   database, so that                the expansion capability of the
   metadata database                is realized, and the development
   requirement of                enterprises is met.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a device for processing data;an electronic device for
   processing                data;a computer-readable storage medium
   storing                program for processing data; anda computer
   program product for processing                data.
   DESCRIPTION Of DRAWING(S) - The drawing shows the schematic flowchart of
   the method for processing data. (Drawing includes               
   non-English language text)200Method for processing dataS210Step for
   determining target metadata                baseS220Step for determining
   execution strategy                corresponding to the operation
   typeS230Step for performing the updating                operation
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2024751088
ER

PT P
AU HOU G
   LI Y
   YUAN Y
   SHEN Y
TI Multi-source data processing method for big data            architecture
   and blockchain, involves establishing            multi-source data
   processing system, where data            processing module in
   multi-source data processing            system collects data sources and
   converts collected            data into data stream with a uniform
   format
PN CN118296065-A
AE HENAN HENGYU ELECTRIC GROUP CO LTD
AB 
   NOVELTY - The method involves establishing a                multi-source
   data processing system, where a data                processing module in
   the multi-source data                processing system collects various
   data sources and                converts the collected data into a data
   stream with                a uniform format. The abnormality judgment is
   performed on the detected data abnormality through                a data
   judgment module. The data stream is                classified and cached
   in a unified format after the                abnormality judgment. The
   export of the data stream                and the import process of the
   data stream to the                transfer destination are executed in
   parallel                through a set of transfer tasks when the
   acquired                data stream is transferred to the blockchain.
   USE - Multi-source data processing method for a big                data
   architecture and a blockchain.
   ADVANTAGE - The state of the data stream transfer               
   real-time synchronous data is obtained by                calculating the
   similarity degree before and after                the data conversion,
   and the accuracy of the                multi-source data transfer
   processing is                improved.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a         
   multi-source data processing method for big data               
   architecture and blockchain. (Drawing includes               
   non-English language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202471051Y
ER

PT P
AU CHANG L
   CUI Y
   LIU X
   YUE X
   CHENG Z
   WANG L
   WU P
   LIU Z
   ZHANG Y
   LI W
TI High-efficiency data lake metadata collection            method used in
   field of data processing and metadata            management involves
   parsing instruction definition            language issued by data lake
   tool intelligently, and            updating metadata information in
   cache according to            results of intelligent parsing
PN CN118193620-A
AE BEIJING RES CENT CNOOC CHINA LTD; CNOOC CHINA CO LTD
AB 
   NOVELTY - The method involves abstracting an adaptation               
   layer that encapsulates a big data platform                interface in
   a data lake tool on the upper layer of                a data lake. The
   adaptation layer is connected with                different types of
   data lake platforms. Metadata                information of data
   structure changes is obtained.                The metadata information
   is written into the cache                of the adaptation layer. The
   instruction definition                language issued by the data lake
   tool is                intelligently parsed, based on the cache of the  
   adaptation layer. The metadata information in the                cache
   is updated according to the results of the                intelligent
   parsing.
   USE - High-efficiency data lake metadata collection               
   method used in field of data processing and                metadata
   management.
   ADVANTAGE - The design of the adaptation layer of the               
   method makes the upper layer data lake tool do not                need
   to greatly change the current data lake                platform, when
   butt jointing different types of                data lake platforms,
   which reduces the invasiveness                and reduces the cost. The
   method reduces the                dependence on the full quantity or
   increment                extraction mode and reduces the operation cost
   by                the mode of the interface adaptation layer. The       
   method intelligently analyzes the instruction                definition
   language issued by the data lake tool at                the adaptation
   layer to realize the real-time                capture of the data lake
   data structure change and                improve the data timeliness.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:1. a high-timeliness data lake metadata               
   collection system;2. a processing device; and3. a computer readable
   storage medium storing                program for high-efficiency data
   lake metadata                collection.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the high-efficiency data lake metadata collection                method
   used in field of data processing and                metadata management.
   (Drawing includes non-English                language text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202465028H
ER

PT P
AU CHENG G
   MEN C
   HAN Y
TI Method for constructing data space based on            knowledge map in
   industrial intelligent factory            building process, involves
   performing node mapping on            heterogeneous data resource view
   and data resource to            be processed, and completing
   construction of data            space
PN CN118193491-A
AE UNIV SUN YAT-SEN
AB 
   NOVELTY - The method involves obtaining original data of               
   multi-source heterogeneous data in an industrial                field.
   The original data is mapped to a                pre-constructed data
   structure. A body element                model is obtained based on
   industrial event                process. A field in a data architecture
   is                connected in a form of attribute of an entity in an   
   entity meta-model to establish a data mapping               
   relationship. An industrial field knowledge map is               
   constructed and acquired based on the entity in the                main
   body meta-model and the attribute. A                heterogeneous data
   resource view is constructed                based on a domain knowledge
   map. Node mapping                process is performed on the
   heterogeneous data                resource view and a data resource to
   be processed.                A construction of a data space is
   completed.
   USE - Method for constructing a data space based on                a
   knowledge map in an industrial intelligent                factory
   building process.
   ADVANTAGE - The data management solution is provided by               
   the application to establish the association                relation of
   the multi-source heterogeneous data                information, to
   construct the uniform heterogeneous                data resource view,
   and to design the universal                industrial field data space
   system frame.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:a system for constructing a data space based                on a
   knowledge map in an industrial intelligent                factory
   building process;an electronic device; anda computer-readable storage
   medium comprising                a set of instructions for constructing
   a data space                based on a knowledge map in an industrial   
               intelligent factory building process.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for constructing a data space based
   on a                knowledge map in an industrial intelligent factory  
   building process. (Drawing includes non-English                language
   text).
Z9 0
U1 0
U2 0
DA 2024-07-17
UT DIIDW:202464619C
ER

PT P
AU KIMDOOYOUNG
   JAE-HEE L
   HAN Y G
TI Unstructured data management system for data            lakes, has
   additional metadata generator that generates            additional
   metadata, and data lake provided with data            reception unit
   that receives basic unstructured data            and additional metadata
   together
PN KR2024083595-A
AE METABUILD CO LTD
AB 
   NOVELTY - The system (100) has an additional metadata               
   generator that generates additional metadata in                response
   to the basic unstructured data in a source                system (110)
   where basic unstructured data is                produced, including an
   unstructured data body and                basic data generated in
   response to the                unstructured data body. A data lake (120)
   is                provided with a data reception unit that receives     
   the basic unstructured data and the additional                metadata
   together. A metadata matching unit                generates matching
   additional metadata by inserting                storage information of
   the basic unstructured data                into the additional metadata.
   A first storage is                provided in which basic unstructured
   data is                stored.
   USE - Unstructured data management system for data                lakes.
   ADVANTAGE - The efficiency of searching and analyzing               
   unstructured data is significantly improved                compared to
   the past. The system checks the video                files to identify
   problems such as construction                site accidents when the
   equipment speed is high,                and to compare site photos to
   analyze the                construction site environment when work
   progress is                high and low.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an          
        unstructured data management method for data                lakes.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram           
   illustrating an unstructured data management system                for
   data lakes. (Drawing includes non-English                language
   text)100Unstructured data management system110Source system120Data
   lake190User terminal
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202462581V
ER

PT P
AU LIU L
   ZHAO N
   JIN Y
   TANG X
   WANG M
   CHEN G
TI Implementation method of streaming data warehouse            of
   multi-source heterogeneous data, involves providing            data
   inquiry and counting service through jdbc in            Apache Doris for
   providing data query and statistical            service
PN CN118093735-A
AE JIANGSU CAS NOR-WEST STAR INFORMATION
AB 
   NOVELTY - The method involves taking the data lake as                ODS
   layer. The javatool packet provided by the                ApachePaimon
   official is used. The multi-source                heterogeneous data is
   leaded into the ODS layer.                The KNN algorithm as the
   user-defined method udf is                registered in the Flink SQL.
   The processing of the                abnormal data is finished in the
   multisource                heterogeneous data in ODS based on the KNN   
   algorithm. The conversion of the Flink SQL is                performed
   through the corresponding operator                translated by the
   Flink engine, based on the ETL                task of the Flink SQL. The
   data fusion and analysis                of the multi-source
   heterogeneous data is realized                in ODS. The data inquiry
   is provided, when using                the data. The service is counted
   through the jdbc                in the ApacheDoris.
   USE - Method for realizing streaming data warehouse                of
   multi-source heterogeneous data such as                enterprise
   transaction, scientific research, social                media
   interaction and so on.
   ADVANTAGE - The development difficulty is reduced to               
   realize the streaming data warehouse of                multi-source
   heterogeneous data. The development                efficiency is
   improved. The expandability and                consistency of the system
   is improved, the error is                reduced, the time for solving
   problem is shortened,                and the cost is saved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a streaming data warehouse system of               
   multi-source heterogeneous data; anda computer-readable storage medium
   storing                program for realizing streaming data warehouse of
                  multi-source heterogeneous data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method  
   for implementing a streaming data warehouse of               
   multi-source heterogeneous data. (Drawing includes               
   non-English language text)
Z9 0
U1 0
U2 0
DA 2024-06-18
UT DIIDW:2024585887
ER

PT P
AU ZHAO T
   ZHAO J
   LIU W
TI Method for processing data in big data and            financial
   technology, involves sending distributed            corrosion protection
   data table to downstream            application of data lake in response
   to detecting that            second data source is restored to normal
PN CN118093698-A
AE IND & COMML BANK CHINA LTD
AB 
   NOVELTY - The method (200) involves sending (S210) a               
   first data table of a first data source to a                downstream
   application of a data lake in response                to detecting that
   a second data source is abnormal,                where the first data
   source is a centralized data                source and the second data
   source is a distributed                data source. The first data and
   second data in the                first data table are updated (S220) to
   a                distributed corrosion-resistant data table, where      
   the first data is historical interaction data                between the
   first data source and the downstream                application, and the
   second data is newly added                interaction data between the
   first data source and                the downstream application during
   the abnormal                period of the second data source. A
   distributed                corrosion protection data table is sent
   (S230) to a                downstream application of the data lake in
   response                to detecting that the second data source is     
             restored to normal.
   USE - Method for processing data in big data and               
   financial technology.
   ADVANTAGE - The data processing method involves sending a               
   data table of a data source to the downstream                application
   of the data lake in response to                detecting that the data
   source is abnormal, where                the data table is a centralized
   data source and a                distributed data source, and thus
   ensures simple                and efficient processing of large data and
                  financial technology.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   data processing device; (2) an electronic device;                (3) a
   computer-readable storage medium storing                program for
   processing data in big data and                financial technology; and
   (4) a computer program                product for processing data in big
   data and                financial technology.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a data processing method. (Drawing includes 
   non-English language text).200Method for processing dataS210Step for
   sending a first data table of a                first data source to a
   downstream application of a                data lakeS220Step for
   updating first data and second                data in the first data
   table to a distributed                corrosion-resistant data
   tableS230Step for sending distributed corrosion               
   protection data table to downstream application of                data
   lake
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202458587U
ER

PT P
AU CHANG F
   WANG Y
   HUANG Y
   CHAI J
   GONG X
   WU J
   LIU Y
   REN W
   LEI Y
   XU Z
TI System for controlling and managing fire-fighting            data based
   on lake-cabinet integrated architecture, has            fire-fighting
   data collecting unit for collecting            original fire-fighting
   data of heterogeneous data            source based on multiple
   communication protocols
PN CN117971988-A
AE JIANGXI TELECOM INFORMATION IND CO LTD
AB 
   NOVELTY - The system has a fire-fighting data collecting               
   unit for collecting original fire-fighting data of                a
   heterogeneous data source based on multiple                communication
   protocols, where the fire-fighting                original data is in
   unstructured data and                structured and semi structured
   service data in a                fire-fighting government affair system.
   A fire                control data storage unit is used in a lake cabin 
   integrated framework to store original fire control                data.
   The lake cabin integrated framework                integrates data lake
   and a data warehouse through a                centralized fire control
   data resource pool in                partitioned manner.
   USE - System for controlling and managing                fire-fighting
   data based on lake-cabinet integrated                architecture.
   ADVANTAGE - The unprocessed original fire-fighting data is              
   directly piled and stored as the digital asset by               
   combining the data lake technology and the data                warehouse
   technology, which can contain mass                structured,
   semi-structured and non- structured                data, and there is no
   capacity upper limit                theoretically.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for method      
   for controlling and managing fire-fighting data                based on
   lake warehouse integrated framework.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of the system for controlling and managing                fire-fighting
   data based on lake-cabinet integrated                architecture
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2024-05-23
UT DIIDW:202448783G
ER

PT P
AU CHENG Y
   SUN Z
   XU K
TI Data governance task running method based on big            data used in
   data processing, involves processing and            obtaining governance
   judgment results of engineering            data based on governance
   metric values of engineering            data and data governance
   comparison information of            affiliated engineering projects
PN CN117971818-A
AE CHINA NAT INST STANDARDIZATION
AB 
   NOVELTY - The method involves receiving (S1) the               
   engineering data by data governance supervision                center.
   The engineering data is converted into a                data set in
   javascript object notation (JSON)                format. The data lake
   receives (S2) the engineering                transformation data set and
   analyzes and processes                to obtain the governance
   measurement values of the                engineering data. Information
   of subordinate                engineering projects are extracted (S3)
   from the                engineering transformation data set, and        
   synchronously matched to obtain data management                control
   information of the subordinate engineering                projects. The
   governance judgment results of the                engineering data is
   processed and obtained (S4)                based on the governance
   metric values of the                engineering data and the data
   governance comparison                information of the affiliated
   engineering projects,                and the response processing
   interval of the                engineering data is comprehensively
   automatically                adjust and controlled.
   USE - Data governance task running method based on                big
   data used in field of data processing.
   ADVANTAGE - The engineering data is efficiently manage and              
   processed. The accuracy and reliability of data is               
   ensured, thus improving the data utilization value                and
   decision-making effect by measuring data                quality. An
   efficient data governance mechanism is                established and
   the timeliness and availability of                data governance is
   ensured by adjusting and                controlling the time of
   engineering data                processing.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the data governance task running method
   based on                big data. (Drawing includes non-English language
   text)S1Step for receiving the engineering data by                data
   governance supervision centerS2Step for receiving the engineering       
   transformation data set by data lakeS3Step for extracting information of
   subordinate engineering projects from the                engineering
   transformation data setS4Step for processing and obtaining              
   governance judgment results of the engineering data                based
   on the governance metric values of the                engineering data
   and the data governance comparison                information of the
   affiliated engineering                projects
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202448524T
ER

PT P
AU HU P
   LI J
   WU W
   HAO F
TI Enterprise informatization management platform            based on large
   data, has intelligent query optimization            module utilizes the
   optimized data lake structure, and            resource allocation system
   module applies graph theory            algorithms
PN CN117973812-A
AE BEIJING ZHUODAO TECHNOLOGY CO LTD
AB 
   NOVELTY - The platform comprises a data lake management               
   module, an intelligent query optimization module, a               
   resource allocation system module, an energy                efficiency
   management module, a customer                relationship analysis
   module, and a DevOps                optimization module. The data lake
   management                module is based on internal and external data 
   sources of the enterprise, uses autoencoders                combined
   with K-means clustering algorithm to                automatically
   classify data, and uses data heat                analysis methods to
   dynamically adjust data storage                and indexing strategies
   to generate an optimized                data lake structure. The
   intelligent query                optimization module utilizes the
   optimized data                lake structure, and uses genetic
   algorithms to                optimize query paths. The resource
   allocation                system module applies graph theory algorithms
   for                network modeling based on the resource usage         
         revealed in the query plan.
   USE - Enterprise informatization management platform               
   based on large data for managing and analyzing                large
   amount of data generated in business                operations.
   ADVANTAGE - The platform improves the DevOps efficiency,               
   optimizes the enterprise operation flow by the                discrete
   event simulation method and the process                mining
   technology, and improves the flow                efficiency. The
   resource distribution efficiency,                strengthening learning
   and application of deep Q                network obviously reduces
   energy consumption,                improving whole energy efficiency,
   isolated forest                abnormal detection algorithm and
   application                optimization software development life period
   of                self-regression integral sliding average              
    model.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an          
   enterprise informatization management method based                on
   large data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a         
   enterprise information management platform based on                big
   data. (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2024-05-23
UT DIIDW:202448758F
ER

PT P
AU GUAN Q
   GONG Z
   DENG L
TI Uniform data management application system based            on
   intelligent computing platform, has service area            performing
   shared exchange and application programming            interface service
   to outside by using capability of            data warehouse to perform
   sharing exchange and API            service
PN CN117909322-A; CN117909322-B
AE GUANGDONG QINZHI TECHNOLOGY RES INST CO
AB 
   NOVELTY - The system has a data lake public area for               
   receiving public data uploaded by a data source. A                data
   lake special area receives special data upload                by the
   data source, where the data lake is provided                with a data
   sharing exchange platform area. A                service area includes a
   data middle station. The                data stored in the data
   intermediate station is                precipitated in a data warehouse.
   The service area                performs shared exchange and application
   programming interface (API)service to an outside by                using
   capability of the data warehouse to perform                sharing
   exchange and API service.
   USE - Uniform data management application system for                use
   in education, medical treatment, society,                government and
   business, and big data.
   ADVANTAGE - The data management application system uses               
   the data lake to provide service for the data                application
   and data visualization. The data lake                is used for sharing
   exchange and application                programming interface (API)
   service to the outside.                The service area includes a data
   middle station, in                which the stored data is deposited in
   data lake,                and the data is shared exchange and API
   service to                outside.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a unified   
   data management application method based on                intelligent
   computing platform.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
   uniform data management application system based on               
   intelligent computing platform (Drawing includes               
   non-English language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202443619J
ER

PT P
AU KANGDUKHO
   KIM E
TI System for automatically generating environmental,            social and
   governance (ESG) and carbon emission            verification report, has
   data collection module that            collects environmental and carbon
   emissions-related            data in real time from various data sources
PN KR2024049785-A
AE OILEX CO LTD
AB 
   NOVELTY - The system has a data processing and analysis               
   module processing data stored in a data lake                structure
   using a big data processing platform,                machine learning
   algorithm and natural language                processing (NLP)
   technology. A data collection                module (100) collects
   environmental and carbon                emission-related data in real
   time from multiple                data sources by using an application
   program                interface (API) gateway, web crawling technology 
   and an internet-of- things (IoT) device data                collection
   system.
   USE - System for automatically generating                environmental,
   social and governance (ESG) and                carbon emission
   verification report.
   ADVANTAGE - The accuracy and efficiency of the carbon               
   emissions measurement and ESG reporting are                increased.
   The safety and reliability of data are                ensured.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   system for automatically generating environmental,                social
   and governance (ESG) and carbon emission                verification
   report. (Drawing includes non-English                language
   text)100Data collection module200Analysis module300Security and data
   integrity module400Report generation module500Continuous
   integration/continuous delivery                and DevOps infrastructure
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2024446361
ER

PT P
AU WU H
   WEN P
   SHANG X
   QIAN X
   YAN Y
   LI X
TI Data cataloguing method based on lake warehouse            integration,
   involves configuring and managing each            directory in
   multi-level cataloging architecture for            specific data field,
   and regularly refreshing latest            data through background
   thread and updating metadata            when external data source is
   updated
PN CN117851396-A
AE KASHGAR PREFECTURE ELECTRONIC INFORMATIO
AB 
   NOVELTY - The method involves constructing a data lake               
   and extracting data from external data sources into                the
   data lake. The raw data is cleaned in the data                lake. The
   metadata information is added to the                cleaned data and
   catalog it using a multi-level                cataloging architecture.
   Each directory in the                multi-level cataloging architecture
   is configured                and managed for a specific data field. The
   latest                data is regularly refreshed through the background
   thread and the metadata is updated when the                external data
   source is updated.
   USE - Data cataloguing method based on lake                warehouse
   integration.
   ADVANTAGE - The method realizes the integration,               
   integration and cataloging of multi-source                heterogeneous
   data to provide more convenient,                reliable and efficient
   concurrent data access and                analysis.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a data      
     cataloguing system based on lake warehouse                integration.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
   data cataloguing system based on lake warehouse               
   integration. (Drawing includes non-English language                text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202439944X
ER

PT P
AU BEI M
   GAO S
   CHEN Y
   CHEN H
   CHEN X
   SUO H
TI Management method for forming knowledge based on            structured
   and unstructured data, involves analyzing            non-informationized
   data in target enterprise by using            knowledge base application
   and establishing knowledge            index of data uploaded by user
PN CN117807126-A
AE CHINA PETROLEUM & CHEM CORP; PETRO-CYBERWORKS INFORMATION TECHNOLOGY
AB 
   NOVELTY - The method involves collecting types of data               
   from each information system of the target                enterprise
   (S101). The collected data is indexed                according to the
   type cataloguing to form                informationized data. Data
   management on the                informationized data is performed
   (S102). Target                knowledge is formed according to a preset
   knowledge                model. The target knowledge is stored (S103) in
   the                data lake according to the pre-configured template.  
   A knowledge base application is used (S104) to                establish
   knowledge index of data lake data for                target knowledge in
   the data lake. The                non-informationized data is analyzed
   (S105) in the                target enterprise by using the knowledge
   base                application. The knowledge index of the data        
   uploaded by the user is established. Knowledge                management
   is performed (S106) based on the                knowledge index of the
   data lake data and the                knowledge index of the data
   uploaded by the                user.
   USE - Management method for forming knowledge based                on
   structured and unstructured data.
   ADVANTAGE - The method enables uniformly managing the               
   knowledge carried by the structured data and the               
   non-structured data, thus improving management                efficiency
   of the knowledge, improving the                efficiency of the
   enterprise and reducing cost of                the enterprise.
   DETAILED DESCRIPTION - The non-informationized data comprises plan,     
   summary, report, analysis document and report form                of the
   worker, and data in the form of file.                INDEPENDENT CLAIMS
   are included for: (1) a                management device for forming
   knowledge based on                structured and unstructured data; (2)
   an electronic                device; (3) a computer readable storage
   medium                comprising a set of instructions for management   
   method for forming knowledge based on structured                and
   unstructured data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a management method for forming knowledge
   based on                structured and unstructured data. (Drawing
   includes                non-English language text).S101Step for
   collecting types of data from                each information system of
   target enterpriseS102Step for performing data management on             
   informationized dataS103Step for storing target knowledge in data       
   lakeS104Step for using knowledge base application                to
   establish knowledge index of data lake data for                target
   knowledge in data lakeS104Step for analyzing non-informationized        
   data in target enterpriseS105Step for performing knowledge              
    management
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202437490J
ER

PT J
AU Cheung, Liege
   Lau, Adela S. M.
   Lam, Kwok Fai
   Ng, Pauline Yeung
TI A Review of Environmental Factors for an Ontology-Based Risk Analysis
   for Pandemic Spread
SO COVID
VL 4
IS 4
BP 466
EP 480
DI 10.3390/covid4040031
DT Review
PD APR 2024
PY 2024
AB Contact tracing is a method used to control the spread of a pandemic.
   The objectives of this research are to conduct an empirical review and
   content analysis to identify the environmental factors causing the
   spread of the pandemic and to propose an ontology-based big data
   architecture to collect these factors for prediction. No research
   studies these factors as a whole in pandemic prediction. The research
   method used was an empirical study and content analysis. The keywords
   contact tracking, pandemic spread, fear, hygiene measures, government
   policy, prevention programs, pandemic programs, information disclosure,
   pandemic economics, and COVID-19 were used to archive studies on the
   pandemic spread from 2019 to 2022 in the EBSCOHost databases (e.g.,
   Medline, ERIC, Library Information Science & Technology, etc.). The
   results showed that only 84 of the 588 archived studies were relevant.
   The risk perception of the pandemic (n = 14), hygiene behavior (n = 7),
   culture (n = 12), and attitudes of government policies on pandemic
   prevention (n = 25), education programs (n = 2), business restrictions
   (n = 2), technology infrastructure, and multimedia usage (n = 24) were
   the major environmental factors influencing public behavior of pandemic
   prevention. An ontology-based big data architecture is proposed to
   collect these factors for building the spread prediction model. The new
   method overcomes the limitation of traditional pandemic prediction model
   such as Susceptible-Exposed-Infected-Recovered (SEIR) that only uses
   time series to predict epidemic trend. The big data architecture allows
   multi-dimension data and modern AI methods to be used to train the
   contagion scenarios for spread prediction. It helps policymakers to plan
   pandemic prevention programs.
RI Lam, Kwok/D-3091-2009; Lau, Adela SM/JQW-7796-2023; Ng, Pauline Yeung/
OI Lam, Kwok/0000-0001-5453-994X; Lau, Adela SM/0000-0001-5918-8309; Ng,
   Pauline Yeung/0000-0001-6671-5963
Z8 0
ZR 0
ZA 0
TC 0
ZS 0
ZB 0
Z9 0
U1 0
U2 3
EI 2673-8112
DA 2024-06-06
UT WOS:001237057100001
ER

PT P
AU YE Q
   LI L
   CHEN W
TI Method for storing data and processing data facing            large
   model scene use in field of artificial            intelligence such as
   data processing, involves taking            feature vector as
   replacement of data to be stored, and            storing to target data
   lake
PN CN117743335-A; US2025013658-A1; CN117743335-B; US12536190-B2
AE BEIJING BAIDU NETCOM SCI & TECHNOLOGY CO
AB 
   NOVELTY - The method involves detecting a data type of               
   data to be stored in response to receiving data               
   to-be-stored in a target data lake. A feature                vector of
   the data is generated. The data type is                determined as an
   unstructured data type. The                feature vector is used as a
   replacement for the                data. A data is stored in the target
   data                warehouse. The target data database is generated    
          according to the data type and the feature                vector.
   USE - Method for storing data and processing data                facing
   large model scene use in a field of                artificial
   intelligence such as data processing,                data storage, cloud
   platform, cloud service, deep                learning, large model,
   large language model.
   ADVANTAGE - The method enables improving data support               
   capability of a data lake and expanding an                application
   scene of the data lake in an efficient                manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   method for processing data facing to large model                scene;
   (2) a device for storing data facing to                large model
   scene; (3) a device for processing data                oriented to large
   model scene; (4) an electronic                device for storing data
   and processing data facing                large model scene use in a
   field of artificial                intelligence; (5) a non-instantaneous
   computer                readable storage medium has set of instructions
   for                storing data and processing data facing large model  
   scene use in a field of artificial intelligence;                (6) a
   computer program product has set of                instructions for
   storing data and processing data                facing large model scene
   use in a field of                artificial intelligence.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for storing data and processing
   data                facing large model scene use in field of artificial 
   intelligence. (Drawing includes non-English                language
   text).
Z9 0
U1 0
U2 0
DA 2024-04-12
UT DIIDW:202433709G
ER

PT P
AU XU J
   LI X
   HE F
   ZHANG Z
TI Method for managing bigdata in various industries,            involves
   invoking engine in data calculation layer of            big data
   platform to transfer service data to            intermediate storage
   layer in big data platform, and            calculating service data in
   intermediate storage layer            to obtain calculation results
PN CN117668124-A
AE IFLYTEK CO LTD
AB 
   NOVELTY - The method involves collecting and caching               
   service data from a source database connected to a                data
   collection layer of a big data platform, where                the source
   database adopts an architecture of a                data lake or a data
   warehouse. An engine in a data                calculation layer of the
   big data platform is                invoked to transfer the service data
   to an                intermediate storage layer in the big data         
   platform, where the intermediate storage layer is a                data
   storage layer of a data lake architecture. The               
   calculation engine is selected and invoked in the                data
   calculation layer. The service data in the                intermediate
   storage layer is calculated to obtain                the calculation
   results.
    USE - Method for managing bigdata in various                industries.
   ADVANTAGE - The method enables reducing the complexity of               
   the big data platform architecture, hardware costs,               
   maintenance and operation costs, and computing and               
   storage resources, and improving data                consistency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a device
   for managing bigdata in various                industries;(2) an
   electronic device comprising a                processor and a memory to
   execute a method for                managing bigdata in various
   industries;(3) a computer-readable storage medium for               
   storing a set of instructions to execute a method                for
   managing bigdata in various industries.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for managing bigdata in various    
   industries. (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202427086X
ER

PT P
AU WANG L
   WANG S
   BAI M
TI System for discovering full-flow safe operation            threat of
   electronic device, has analysis layer used            for deeply
   analyzing external threat based on            information report and
   information product and            generating threat evaluation and
   pre-warning            information of external threat
PN CN117640142-A
AE QI AN XIN TECHNOLOGY GROUP INC
AB 
   NOVELTY - The system has a storage layer used for               
   classifying, storing and managing multi-source               
   information data based on category, level and                sensitivity
   requirement, and constructing a                structured data warehouse
   and an unstructured data                lake. A tool layer is used for
   deeply digging and                fusing the data warehouse and the
   multi-source                information data in the data lake based on a
   tool                set to generate information report and information  
   product. An analysis layer is used for deeply                analyzing
   external threat based on the information                report and the
   information product and generating                threat evaluation and
   pre-warning information of                the external threat.
   USE - System for discovering full-flow safe                operation
   threat of an electronic device                (claimed).
   ADVANTAGE - The system can accurately sense the safety               
   situation and improves validity and high efficiency                of
   detection and threat discovery.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   non-transient computer readable storage                medium storing a
   computer program for discovering                full-flow safe operation
   threat of an electronic                device;(2) a method for
   discovering full-flow safe                operation threat of an
   electronic device;(3) a computer program product comprising a           
   computer program for discovering full-flow safe                operation
   threat of an electronic device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for discovering full-flow safe operation threat of
   an electronic device. (Drawing includes non-English               
   language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202425231F
ER

PT P
AU WANG X
   LUO J
   ZHOU X
TI System for managing multi-source heterogeneous            data by
   computer device, has data management service            for performing
   basic operation of client to standard            structured data and
   storing standard structured data in            data lake table
PN CN117573759-A
AE CHINA ELECTRONICS INVESTMENT HOLDINGS CO
AB 
   NOVELTY - The system has a client for performing basic               
   operation to metadata and standard structured data                in a
   data lake table. A service end is provided                with a
   metadata management service, a data                conversion integrated
   service and a data management                service, where the metadata
   management service for                performing the basic operation of
   a client to the                metadata and obtaining the metadata from
   the data                conversion integrated service and storing the   
   metadata. A data management service for performing                the
   basic operation of the client to the standard                structured
   data and storing the standard structured                data in the data
   lake table.
   USE - System for managing multi-source heterogeneous                data
   by a computer device (claimed).
   ADVANTAGE - The system manages multi-source heterogeneous               
   data and the metadata for performing integration                and
   storage operation to provide uniform data and a                metadata
   management platform for a user.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:A device for managing multi-source                heterogeneous data
   of a service end,A method for managing multi-source               
   heterogeneous data of a service end, andA computer-readable storage
   medium for storing                a set of instructions for managing
   multi-source                heterogeneous data by a computer device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for managing multi-source heterogeneous data by a 
   computer device. (Drawing includes non-English                language
   text).
Z9 0
U1 0
U2 0
DA 2024-03-14
UT DIIDW:202421445L
ER

PT P
AU FU Y
   LIN X
   XU B
   JIANG Y
   WANG L
   ZHANG J
   ZHANG S
   QI Y
   FAN J
   XU J
   WANG Z
   LIU J
TI Data lake metadata modelling system for processing           
   unstructured data in complex data environment, has            storage
   module and metadata extraction module connected            together,
   where storage module performs modeling            process by taking data
   entity of entity types as center            to form lake metadata model
PN CN117520571-A
AE YUNDING TECHNOLOGY CO LTD; SHANDONG ENERGY GROUP CO LTD
AB 
   NOVELTY - The system has a system module assembly               
   provided with a data collecting module, a data                processing
   module and a metadata extracting module.                The data
   collecting module for receiving an                original heterogeneous
   data source. A modeling                module is connected with a
   storage module. The                storage module and the metadata
   extraction module                are connected together, where the
   storage module                determines a data entity, realizes a
   contact                between the data entity and a global metadata    
   stored separately, and performs an integrated                modeling
   process by taking the data entity of                multiple data entity
   types as a center to form a                universal extensible data
   lake metadata                model.
   USE - Data lake metadata modelling system for                processing
   and utilization unstructured data in a                heterogeneous
   complex data environment.
   ADVANTAGE - The system improves processing and utilization              
   operations of the unstructured data by separately               
   modeling and storing the data entity of the data                entity
   types and types of meta-data, thus reducing                complexity of
   meta-data coupling operation and                heterogeneous data
   support difficulty, and hence                improving commonality and
   expandability of the data                lake metadata model.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a data 
   lake metadata modelling system. (Drawing includes               
   non-English language text).
Z9 0
U1 0
U2 0
DA 2024-03-05
UT DIIDW:202417580N
ER

PT P
AU OH J Y
TI Cloud-based smart greenhouse big data analysis            system, has
   preprocessing module that is provided at            front side of
   correlation analysis module and deletes            data if other data
   other than missing measurement value            is not within normal
   range
PN KR2620425-B1
AE GSITM CO LTD
AB 
   NOVELTY - The system has a greenhouse data collection               
   unit (10) for collecting data that is measured or               
   acquired in a smart greenhouse. A data lake (20)               
   processes and stores the collected data. A                visualization
   unit (40) visually displays the data                by using a table, an
   image or video. The data lake                converts the data
   representing same physical                properties into structured
   data in a standardized                format. An abnormal data analysis
   module deletes                the data and extracts only outliers to
   analyze                trends. A growth information analysis module     
   performs data clustering on the data in which                correlation
   between the data is derived from a                correlation analysis
   module. A preprocessing module                is provided at a front
   side of a correlation                analysis module and deletes the
   data including                specific measurement value if other data
   other than                missing measurement value is not within normal
                  range.
   USE - Cloud-based smart greenhouse big data analysis               
   system.
   ADVANTAGE - The system comprises the greenhouse data               
   collection unit for collecting data that is                measured or
   acquired in the smart greenhouse so as                to effectively
   improve possibility of reaching the                desired output
   variable by controlling the control                variable based on
   cases with a particularly high                correlation among
   correlations, and controls                devices such as switches,
   according to size                relationship between sensed data and
   set data so as                to maintain appropriate growth conditions
   in the                smart greenhouse. The visualization unit visually 
   displays the data handled by the greenhouse data               
   collection unit, the data lake, and the data                analysis
   unit, so that people easily recognizes the                data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the
   cloud-based smart greenhouse big data analysis                system
   (Drawing includes non-English language                text).10Greenhouse
   data collection unit20Data lake30Data analysis unit40Visualization unit
Z9 0
U1 0
U2 0
DA 2024-01-14
UT DIIDW:202403038L
ER

PT P
AU WANG Y
   LIU D
TI Industrial chain big data architecture based on            task
   hierarchical planning, has industrial big data            resource
   domain that is provided with big data storage            module and
   information system module, and big data            storage module which
   is provided to store industrial            big data
PN CN117331698-A
AE UNIV GUANGDONG TECHNOLOGY
AB 
   NOVELTY - The architecture has an industrial big data               
   analysis domain which is provided to divide data               
   processing tasks into several different subtasks                based on
   common characteristics. The industrial big                data engine
   domain is provided with a map-reduce                engine equipped with
   a hierarchical assignment                algorithm. The map-reduce
   engine is provided to                assign computing units to subtasks
   in resource                clusters. The industrial big data resource
   domain                is provided with a big data storage module and an 
   information system module. The big data storage                module is
   provided to store industrial big data.                The information
   system module stores resource                clusters. The resource
   clusters are provided with                computing units.
   USE - Industrial chain big data architecture based                on
   task hierarchical planning.
   ADVANTAGE - The industrial chain big data architecture               
   solves the problem in the existing technology that                the
   system performance upper limit is insufficient                and the
   performance cannot be maintained, and has                the
   characteristics of high efficiency and                comprehensiveness.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a task      
   execution method of industrial big data based on                task
   hierarchical planning.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
   industrial chain big data architecture based on                task
   hierarchical planning. (Drawing includes                non-English
   language text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202405096H
ER

PT J
AU Abdellaoui, Kamel
   Taieb, Mohamed Ali Hadj
   Mahjoubi, Rafik
   Aouicha, Mohamed Ben
TI Data-driven journey: a data management paradigm-centric review and data
   mesh capabilities
SO INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT
VL 16
IS 2
DI 10.1504/IJDMMM.2024.138865
DT Article
PD 2024
PY 2024
AB Becoming data driven is one of the top strategic objectives of data-rich
   organisations. Africa must join the wave to capture and unlock the
   highest value from data. Therefore, this survey analyses the drivers,
   challenges, and evolution, of existing data management paradigms
   including data warehouse, data lake and data lakehouse. It reveals the
   limitations of monolithic approaches to address data at scale and how
   they led to a paradigm shift toward a more distributed and decentralised
   data mesh. The paper discusses data mesh capabilities to address the
   challenges of data availability and accessibility at scale in Africa to
   enable leapfrog development in its journey to being data driven.
RI Taieb, Mohamed/AAF-6473-2019
Z8 0
ZA 0
ZS 0
ZB 0
ZR 0
TC 0
Z9 0
U1 0
U2 38
SN 1759-1163
EI 1759-1171
DA 2024-06-12
UT WOS:001237489300001
ER

PT C
AU Abidi, Eya
   Ayoub, Zayneb Trabelsi
   Ouni, Sofiane
GP IEEE
TI Enhancing Speed and Quality of Somatic Variant Calling via Big Data
   Architecture and Deep Learning Models
SO 2024 IEEE/ACS 21ST INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS AND
   APPLICATIONS, AICCSA
SE International Conference on Computer Systems and Applications
DI 10.1109/AICCSA63423.2024.10912614
DT Proceedings Paper
PD 2024
PY 2024
CT 21st International Conference on Computer Systems and
   Applications-AICCSA
CY OCT 22-26, 2024
CL Sousse, TUNISIA
SP Institute of Electrical and Electronics Engineers Inc
RI Ayoub, Zayneb/G-1444-2015; Ouni, Sofiane/GLU-5455-2022
ZS 0
ZA 0
TC 0
ZB 0
Z8 0
ZR 0
Z9 0
U1 0
U2 0
SN 2161-5322
BN 979-8-3315-1825-7; 979-8-3315-1824-0
DA 2025-07-24
UT WOS:001481352700083
ER

PT C
AU Alvarez-Valera, Hernan Humberto
   Maurice, Alexandre
   Ravat, Franck
   Song, Jiefu
   Roose, Philippe
   Valles-Parlangeau, Nathalie
BE Nguyen, NT
   Chbeir, R
   Manolopoulos, Y
   Fujita, H
   Hong, TP
   Nguyen, L
   Wojtkiewicz, K
TI Energy Measurement System for Data Lake: An Initial Approach
SO INTELLIGENT INFORMATION AND DATABASE SYSTEMS, PT I, ACIIDS 2024
SE Lecture Notes in Artificial Intelligence
VL 14795
BP 15
EP 27
DI 10.1007/978-981-97-4982-9_2
DT Proceedings Paper
PD 2024
PY 2024
AB Data Lakes are increasingly deployed as a solution for Big Data
   analytics. Recent improvements in Data Lake technology have focused on
   improving data access, governance, and discoverability. However, the
   energy consumption of data operations, a non-trivial issue for
   eco-conscious organizations, is currently overlooked. Furthermore,
   existing monitoring tools do not adequately address the complexities of
   Data Lake architectures.
   This paper presents the initial phase of developing a system for
   measuring energy in Data Lake pipeline operations. The novelty of our
   solution lies in the fact that we define four measures to assess the
   power usage of crucial hardware components in a Data Lake context: CPU,
   RAM, NIC, and storage devices. To validate our approach, we developed a
   monitoring tool grounded in real-world datasets from a Data Lake
   benchmark.
CT 16th Asian Conference on Intelligent Information and Database Systems
   (ACIIDS)
CY APR 15-18, 2024
CL Amer Univ Ras Al Khaimah, French SIGAPP Chapter, Ras Al Khaimah, U ARAB
   EMIRATES
HO Amer Univ Ras Al Khaimah, French SIGAPP Chapter
SP Wroclaw Univ Sci & Technol; IEEE SMC Tech Comm Computat Collect
   Intelligence; European Res Ctr Informat Syst; Univ Newcastle; Yeungnam
   Univ; Int Univ Vietnam Natl Univ HCMC; Leiden Univ; Univ Teknologi
   Malaysia; Ton Duc Thang Univ; Open Univ Cyprus; BINUS Univ; Vietnam Natl
   Univ; Nguyen Tat Thanh Univ; Quang Binh Univ
RI Roose, Philippe/A-5319-2009; Ravat, Franck/AAG-7714-2019
ZR 0
ZS 0
ZB 0
ZA 0
TC 0
Z8 0
Z9 0
U1 1
U2 1
SN 2945-9133
EI 1611-3349
BN 978-981-97-4981-2; 978-981-97-4982-9
DA 2024-11-16
UT WOS:001314380100002
ER

PT J
AU An, Gi-taek
   Oh, Seyoung
   Kim, Eunhye
   Park, Jung-min
TI Data Lake Conceptualized Web Platform for Food Research Data Collection
SO JOURNAL OF WEB ENGINEERING
VL 23
IS 3
BP 377
EP 392
DI 10.13052/jwe1540-9589.2333
DT Article
PD 2024
PY 2024
AB Food research is uniquely intertwined with everyday life and
   necessitates the utilization of big data. Within this domain, the
   research data consist of various forms and formats, encompassing
   biological experiment results, chemical analysis data, nutritional
   information, microbiological data, sensor data, images, and videos. This
   diversity stems from the integration of data from various subdomains
   within the larger field. With recent advancements in deep learning
   technology, the importance of data has grown significantly, resulting in
   increased reliance on data -driven research. Although specialized
   platforms for sharing and utilizing data have been established at the
   national level, particularly in the bioscience field, food research
   lacks a dedicated infrastructure and specialized data-sharing platforms.
   In this study, we develop a platform that leverages Hadoop-based
   distributed file systems to create a data lake. This platform enables
   data storage and sharing through a web-based interface. The distributed
   file system supports scalability by adding data nodes, making it an
   effective solution for capacity expansion. In addition, the web-based
   platform ensures high accessibility, allowing users access from
   anywhere, at any time, using any device. Finally, we introduce the
   establishment of a 1.8 PB Hadoop-based physical storage system and
   present an approach for building a highly accessible web platform with
   substantial utility.
OI An, Gi-taek/0009-0005-7131-8889
ZS 0
ZR 0
ZB 0
ZA 0
Z8 0
TC 0
Z9 0
U1 2
U2 10
SN 1540-9589
EI 1544-5976
DA 2024-06-22
UT WOS:001246175100003
ER

PT B
AU Ayyalasomayajula, Madan Mohan Tito
Z2  
TI Metadata Enhanced Micro-Partitioned Bitmap Indexes for Managing
   Large-Scale Datasets
DT Dissertation/Thesis
PD Jan 01 2024
PY 2024
Z8 0
TC 0
ZS 0
ZB 0
ZA 0
ZR 0
Z9 0
U1 0
U2 1
BN 9798384474838
UT PQDT:105761810
ER

PT C
AU Barret, Nelly
   Ebel, Simon
   Galizzi, Theo
   Manolescu, Ioana
   Mohanty, Madhulika
BE Sellami, M
   Vidal, ME
   VanDongen, B
   Gaaloul, W
   Panetto, H
TI User-Friendly Exploration of Highly Heterogeneous Data Lakes
SO COOPERATIVE INFORMATION SYSTEMS, COOPIS 2023
SE Lecture Notes in Computer Science
VL 14353
BP 488
EP 496
DI 10.1007/978-3-031-46846-9_30
DT Proceedings Paper
PD 2024
PY 2024
AB The proliferation of digital data sources and formats has led to the
   apparition of data lakes, systems where numerous data sources coexist,
   with less (or no) control and coordination among the sources, than
   previously practised in enterprise databases and data warehouses. While
   most data lakes are designed for very large number of tables,
   ConnectionLens [2,3] is a data lake system for structured,
   semi-structured, and unstructured data, which it integrates into a
   single graph; the graph can be explored via graph queries with keyword
   search [4] and entity path enumeration [5]. In this paper, we describe
   ConnectionStudio, a userfriendly platform leveraging ConnectionLens, and
   integrating feedback from non-expert users, in particular, journalists.
   Our main insights are: (i) improve and entice exploration by giving a
   first global view; (ii) facilitate tabular exports from the integrated
   graph; (iii) provide interactive means to improve the graph
   constructions. The insights can be used to further advance the
   exploration and usage of data lakes for non-IT users.
CT 29th International Conference on Cooperative Information Systems-CoopIS
CY OCT 30-NOV 03, 2023
CL Groningen, NETHERLANDS
RI Mohanty, Madhulika/IUM-9989-2023
ZR 0
ZA 0
ZS 0
TC 0
Z8 0
ZB 0
Z9 0
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-46845-2; 978-3-031-46846-9
DA 2025-09-26
UT WOS:001560834900030
ER

PT B
AU Basso, Denzel
Z2  
TI Enhancing Data Extraction and Transformation for Business Intelligence:
   Integrating Database Sources with Cloud Storage for Streamlined
   Reporting
DT Dissertation/Thesis
PD Jan 01 2024
PY 2024
ZA 0
Z8 0
ZB 0
TC 0
ZR 0
ZS 0
Z9 0
U1 1
U2 1
BN 9798346835899
UT PQDT:120586072
ER

PT C
AU Benhissen, Redha
   Bentayeb, Fadila
   Boussaid, Omar
BE Gusikhin, O
   Hammoudi, S
   Cuzzocrea, A
TI Temporal and Flexible Data Warehouses
SO DATA MANAGEMENT TECHNOLOGIES AND APPLICATIONS, DATA 2023
SE Communications in Computer and Information Science
VL 2105
BP 25
EP 49
DI 10.1007/978-3-031-68919-2_2
DT Proceedings Paper
PD 2024
PY 2024
AB The present trend among companies involves a significant revamp of their
   data architecture, aiming to streamline data processes and phase out
   outdated systems. The advent of big data has profoundly influenced
   businesses, empowering them to adeptly manage and analyze vast datasets.
   In the realm of business intelligence, particularly in decision-making,
   data warehouses play a crucial role, leveraging OLAP technology for the
   efficient analysis of structured data. Constructed by amalgamating data
   from diverse sources, a data warehouse faces the challenge of
   accommodating big data-comprising unstructured, semi-structured, or
   structured data from myriad sources-where alterations in content and
   structure are frequent. To address this, our paper introduces a temporal
   multidimensional model utilizing a graph formalism for multi-version
   data warehouses, adept at assimilating changes from data sources. This
   approach relies on multi-version evolution for schema modifications and
   employs bi-temporal labeling for entities and their relationships to
   capture data evolution. Our proposal enhances data warehouse evolution
   flexibility, broadening analysis possibilities within the decision
   support system, and enabling adaptable temporal queries to yield
   consistent results. Building upon our prior work [6], where we presented
   the GAMM model emphasizing evolutionary data treatment, including
   dimensional changes, this study expands on the temporal labeling
   principle in our approach. It delves into various functions governing
   the evolution of temporal data, offering illustrative examples. We
   validate our approach through a case study demonstrating temporal
   queries and conduct runtime performance tests on graph data warehouses.
CT 12th International Conference on Data Management Technologies and
   Applications (DATA)
CY JUL 11-13, 2023
CL Rome, ITALY
RI BOUSSAID, OMAR/AAF-2540-2020; benhissen, redha/
OI BOUSSAID, OMAR/0000-0001-6388-3152; benhissen, redha/0000-0002-6974-0838
TC 0
ZR 0
ZS 0
ZB 0
Z8 0
ZA 0
Z9 0
U1 0
U2 2
SN 1865-0929
EI 1865-0937
BN 978-3-031-68918-5; 978-3-031-68919-2
DA 2024-11-23
UT WOS:001331179000002
ER

PT B
AU Boga, Cristina Isabel Palma Caeiro
Z2  
TI Sistema Para Auxiliar na Tomada de Decisão de Arquitetura Big DataSystem
   to assist in the decision making of Big Data architecture
DT Dissertation/Thesis
PD Jan 01 2024
PY 2024
Z8 0
ZS 0
ZR 0
TC 0
ZB 0
ZA 0
Z9 0
U1 0
U2 0
BN 9798311963046
UT PQDT:123794626
ER

PT C
AU Botha, L.
   Taylor, E.
BE Rocha, A
   Adeli, H
   Dzemyda, G
   Moreira, F
   Poniszewska-Maranda, A
TI Choosing a Data Model for a Data Warehouse from a Non-experienced
   End-User Perspective
SO GOOD PRACTICES AND NEW PERSPECTIVES IN INFORMATION SYSTEMS AND
   TECHNOLOGIES, VOL 3, WORLDCIST 2024
SE Lecture Notes in Networks and Systems
VL 987
BP 261
EP 270
DI 10.1007/978-3-031-60221-4_26
DT Proceedings Paper
PD 2024
PY 2024
AB A critical decision that will determine if the implementation of a data
   warehouse is a success or a failure is selecting the data modelling
   approach. This research was done in the Design Science Research (DSR)
   Paradigm. As part of the first phase (awareness of the problem),
   questionnaires were completed by 112 respondents at companies in South
   Africa. The results of these showed that data models and the use thereof
   still present problems in many of these companies in South Africa. The
   aim of this paper is to assist an end user or company to choose a
   suitable data model. The DataModelling Selection Framework (DMSF) is
   introduced in this paper. This framework considers: business information
   needs, data parameters, enterprise size, business processes, current
   data architecture and environment, as well as data strategy. The DMSF
   includes 15 guidelines that can be followed by an end-user or a company
   when having to decide which data model will be suitable for a data
   warehouse.
CT 12th World Conference on Information Systems and Technologies
   (WorldCIST)
CY MAR 26-28, 2024
CL Lodz Univ Tech, Lodz, POLAND
HO Lodz Univ Tech
ZA 0
TC 0
ZR 0
ZS 0
Z8 0
ZB 0
Z9 0
U1 0
U2 3
SN 2367-3370
EI 2367-3389
BN 978-3-031-60220-7; 978-3-031-60221-4
DA 2024-08-29
UT WOS:001267237000026
ER

PT J
AU Brett Romeo Anderson; BRUCE D GELB; Matthew  Lewis; RICHMOND, MARC ERIC
TI Pediatric Heart Network New York Consortium
DT Awarded Grant
PD Jan 01 2024
PY 2024
AB PROJECT SUMMARY / ABSTRACTCongenital heart defects are the most common
   and resource intensive birth defects managed in the UnitedStates and
   have high morbidity and mortality. Further, significant disparities are
   known to exist in bothoutcomes and resource requirements. Neighborhood
   economics, education, environment, and interpersonalbias are thought to
   contribute to these inequities, yet mechanisms—and optimal targets for
   intervention—areunknown. Marked heterogeneity in disease subtypes among
   congenital heart patients limits the power ofsingle-center studies and
   complicates conduct and interpretation of clinical research. Multicenter
   data are oftensiloed in diagnostic or procedural registries, in
   in-patient databases, or are the product of individual trials
   andinvestigations. Consequently, research conducted to improve outcomes
   in the CHD population is ofteninsufficiently powered or lacks the degree
   of phenotypic and socioeconomic detail necessary to allow forclinical
   progress for all patients. Enhanced registry-based clinical trials that
   expand the “data lake” by marryinga variety of “big data”
   sources—clinical registry, administrative, individual and neighborhood
   level socialdeterminants, and National Death Index (NDI) data—with
   prospectively collected patient-reported and clinicaloutcome measures
   and deep genomic and proteomic phenotyping can increase effectiveness
   and efficiency ofclinical research. Such collaborative efforts can help
   us understand mechanisms underlying health inequitiessuch that we can
   develop interventions to improve care for all CHD patients. We propose
   bringing togethertwo integrated, pediatric and adult congenital heart
   centers in northern Manhattan with complementary, dataand clinical
   research expertise, to form the Pediatric Heart Network New York
   Consortium (PHN-NYC).Together, these centers serve a large and diverse
   patient population that represents ~60% of patientsundergoing congenital
   heart surgery across all of New York State, is 47% low income and 36%
   NH-Black orHispanic. This consortium would leverage its
   interdisciplinary strengths to accomplish the following aims: 1)Create a
   novel infrastructure, linking existing locally held clinical registry,
   administrative, social determinants ofhealth, and NDI data with
   prospectively collected genomic, proteomic, and clinical trial data to
   supportenhanced registry-based investigations, capable of identifying
   modifiable mediators of health inequities; 2)expand engagement of
   families affected by congenital heart disease traditionally
   underrepresented in clinicalresearch; and 3) leverage existing
   NIH-funded investigator training initiatives to expand and support a
   nextgeneration of diverse congenital heart clinical researchers.
ZB 0
ZS 0
ZA 0
Z8 0
ZR 0
TC 0
Z9 0
U1 0
U2 0
G1 10841299; 1UM1HL172720-01; UM1HL172720
DA 2024-03-10
UT GRANTS:17494852
ER

PT C
AU Bureva, Veselina
   Atanassov, Krassimir
   Genov, Miroslav
   Sotirov, Sotir
BE Kahraman, C
   Onar, SC
   Cebi, S
   Oztaysi, B
   Tolga, AC
   Sari, IU
TI Index Matrix Representation of Data Storage Structures Using
   Intuitionistic Fuzzy Logic
SO INTELLIGENT AND FUZZY SYSTEMS, INFUS 2024 CONFERENCE, VOL 1
SE Lecture Notes in Networks and Systems
VL 1088
BP 459
EP 466
DI 10.1007/978-3-031-70018-7_51
DT Proceedings Paper
PD 2024
PY 2024
AB In the current research work a big data structure representation using
   extended intuitionistic fuzzy index matrix (EIFIM) is presented. The
   investigation is based on the theories of index matrices, intuitionistic
   fuzzy sets and databases. It is known that big data systems use
   different data structures. The data warehouses are implemented to
   integrate structured datasets. A data lake provides storing capabilities
   for unstructured data. Nowadays, the two concepts are integrated in data
   lakehouse platforms that provide facilities for structured,
   semi-structured and unstructured data. The open-source framework for
   managing and processing huge amounts of data Hadoop is observed. More
   precisely, the HDFS (Hadoop Distributed File System) organization is
   discussed. The aim of the investigation is to present big data structure
   using EIFIM. User access through big data system environment to reach
   files and other resources is analyzed. The intuitionistic fuzzy logic is
   applied to evaluate the big data system processes.
CT International Conference on Intelligent and Fuzzy Systems (INFUS)
CY JUL 16-18, 2024
CL Istanbul Tech Univ, Canakkale, TURKEY
HO Istanbul Tech Univ
SP Canakkale Onsekiz Mart Univ
RI Sotirov, Sotir/M-2488-2013; Bureva, Veselina/; Atanassov, Krassimir/S-2877-2016
OI Bureva, Veselina/0000-0003-4344-4392; 
ZR 0
ZA 0
TC 0
ZS 0
ZB 0
Z8 0
Z9 0
U1 0
U2 1
SN 2367-3370
EI 2367-3389
BN 978-3-031-70017-0; 978-3-031-70018-7
DA 2024-11-13
UT WOS:001331332200050
ER

PT J
AU Cha, ByungRae
   Pan, Sungbum
TI Appliance Study in Various Industrial Areas based on Abyss Storage for
   AI Services in the Small and Medium Enterprise
Z1 중소기업의 AI 서비스를 위한 Abyss Storage 기반 다양한 산업 영역의 Appliance 연구
SO Journal of Korean Institute of Information Technology
S1 한국정보기술학회논문지
VL 22
IS 3
BP 135
EP 145
DT research-article
PD 2024
PY 2024
AB The ICT field is an environment where various cutting-edge technologies
   such as IoT, Big Data, Cloud Computing, Digital transformation(DX),
   Robot, and AI are emerging. As a strategy to support these technologies,
   we are researching and developing a prototype of large-capacity Abyss
   Storage Ⅰ based on Connected Data Architecture(CDA) to provide AI
   services to small and medium-sized businesses. In this study,
   scalability of storage and computing resources can be provided by using
   other storages or public cloud by CDA to utilize distributed
   large-capacity storage efficiently, and a technology to support various
   IoT services was proposed. Also, we studied a number of business models
   for AI services in various industries. In detail, it is intended to be
   used in the areas of smart factory/smart farm, industrial sites and
   disaster damage analysis, and search engine.
AK ICT 분야는 IoT, BigData, Cloud Computing, DX(Digital transformation),
   Robot, 그리고 AI 등의 다양한 첨단 기술들이 대두되고 있는 환경이다. 이러한 기술들을 지원하기 위한 하나의 전략으로
   중소기업에 AI 서비스를 제공하기 위한 CDA(Connected Data Architecture) 기반 대용량 Abyss
   Storage Ⅰ의 프로토타입을 연구 및 개발을 진행하고 있다. 본 연구에서는 효율적인 분산 대용량 스토리지 활용을 위해 CDA에
   의한 다른 스토리지 또는 퍼블릭 클라우드를 사용하여 스토리지 및 컴퓨팅 자원의 확장성을 제공할 수 있으며, 다양한 IoT 서비스를
   지원하기 위한 기술을 제안하였다. 또한 다양한 산업 분야에 AI 서비스를 위한 다수의 비즈니스 모델을 연구하였다. 세부적으로
   스마트 팩토리/스마트 팜, 산업현장과 재해 피해 분석, 그리고 검색 엔진 영역에 활용하고자 한다.
ZA 0
Z8 0
ZS 0
ZR 0
ZB 0
TC 0
Z9 0
U1 0
U2 0
SN 1598-8619
DA 2025-01-26
UT KJD:ART003062320
ER

PT C
AU Chabod, Amaury
BE Kolowrocki, K
   Magryta-Mut, B
TI Practical Use Of Data Lake To Improve Fatigue Life Estimation
SO ADVANCES IN RELIABILITY, SAFETY AND SECURITY, ESREL 2024, PT 6
BP 19
EP 26
DT Proceedings Paper
PD 2024
PY 2024
AB The qualification of components in terms of Durability and Reliability
   relies on the analysis on a lot of sensors, CAN, IIOT data, which needs
   a data management infrastructure, to understand the customer usage and
   variability. Such big data infrastructure is often called data lake, and
   may lead to storing huge amount of data. This infrastructure must be
   generic yet test data-oriented to understand the data structure and its
   analysis required, and to be optimized for such application. All those
   data may come from connected equipment, instrumented fleets, test lab or
   proving ground measurements, digital twins and multi-body dynamics
   simulations. Further, the data must be managed in terms of quality and
   traceability. It must be indexed to be able to be retrieved through
   searches (customer, vehicle, measurement site, engine specification,
   road condition, usage conditions).
   Once this step is achieved, the product development team is able to have
   a better understanding of customer usage and inputs variabilities in
   different environments and conditions, which have to be taken into
   account in the product's mission profile or duty cycle. This ad-hoc
   mission profile enables the creation of realistic, meaningful design and
   test specifications.
   Understanding the customer usage and input variabilities enables a
   probabilistic approach to fatigue life prediction. The uncertainties on
   inputs (geometry, material and loading) may be propagated through the
   life process, knowing each input's probability distribution function,
   using a Monte Carlo analysis. The infrastructure enables the life
   analysis to be done through multiple runs on cloud-oriented server,
   which enables automation and streamlining the whole process. A use case
   will be presented to illustrate the approach and its benefits.
CT 34th European Safety and Reliability Conference
CY JUN 23-27, 2024
CL Krakow, POLAND
SP Jagiellonian University
Z8 0
ZR 0
ZB 0
ZA 0
ZS 0
TC 0
Z9 0
U1 0
U2 0
BN 978-83-68136-05-0
DA 2025-09-12
UT WOS:001542512800002
ER

PT C
AU Gill, Asif
   Bandara, Madhushi
BE Liao, HC
   Cid, DD
   Macadar, MA
   Bernardini, F
TI Using Knowledge Graphs for Architecting and Implementing Air Quality
   Data Exchange: Australian Context
SO PROCEEDINGS OF THE 25TH ANNUAL INTERNATIONAL CONFERENCE ON DIGITAL
   GOVERNMENT RESEARCH, DGO 2024
BP 534
EP 541
DI 10.1145/3657054.3657117
DT Proceedings Paper
PD 2024
PY 2024
AB The air quality data ecosystem consists of several interacting actors
   such as government agencies and researchers that collect large volumes
   of data from different air quality monitoring stations via IoT sensors
   and data systems. The challenge is: how to enable data linking and
   sharing in the complex and federated data ecosystem for more
   comprehensive research and reporting for air quality improvement? This
   paper presents a knowledge graph-based approach for architecting and
   implementing the air quality data exchange platform for enabling the
   data linking and sharing in the federated air quality data ecosystem.
   The application of the proposed approach is demonstrated with the help
   of an air quality data case study example in the Australian context.
   This work has been done as a part of the large air quality digital data
   infrastructure project with the state and local government. The
   learnings from this paper can be used by government agencies and
   researchers for architecting and implementing knowledge-graph based data
   exchanges as appropriate to their context.
CT 25th Annual International Conference on Digital Government Research
   (DGO) - Internet of Beings - Transforming Public Governance
CY JUN 11-14, 2024
CL Taipei, TAIWAN
SP Digital Govt Soc; Natl Taiwan Univ, Coll Social Sci; Taipei City Govt,
   Dept Informat Technol; Natl Inst Cyber Secur; Natl Taiwan Univ, Dept
   Polit Sci; Natl Taiwan Univ, Grad Inst Publ Affairs; Natl Taiwan Univ,
   Taiwan Social Resilience Res Ctr; Acad Sinica, Inst Polit Sci; Natl
   Chengchi Univ, Dept Publ Adm; Chung Hua Univ, Dept Publ Adm; Minist
   Digital Affairs; Natl Sci & Technol Council; Natl Taiwan Univ, Off Int
   Affairs; Taiwan Fdn Democracy; Minist Educ, YuShan Fellow Program;
   Elsevier; Emerald Publishing, Journal Transforming Govt People, Process
   & Policy
RI Bandara, Madhushi/; Gill, Asif/K-4375-2017
OI Bandara, Madhushi/0000-0001-6543-3841; Gill, Asif/0000-0001-6239-6280
ZR 0
TC 0
ZA 0
Z8 0
ZS 0
ZB 0
Z9 0
U1 0
U2 3
BN 979-8-4007-0988-3
DA 2024-07-14
UT WOS:001238979900054
ER

PT B
AU Hámori, Péter Sándor
   Scott, Ian James
Z2  
TI [not available]
DT Dissertation/Thesis
PD Oct 16 2024
PY 2024
ZR 0
ZB 0
ZA 0
Z8 0
TC 0
ZS 0
Z9 0
U1 0
U2 0
BN 9798384193005
UT PQDT:100668351
ER

PT C
AU Harby, Ahmed A.
   ElKhodary, Eyad
   Almeida, Ronan
   Sharma, Drishti
   Zulkernine, Farhana
   Alaca, Furkan
   Elganar, Khalid
   Almarzouqi, Amina
   Al-Yateem, Nabeel
   Rahman, Syed Aziz
BE Shahriar, H
   Ohsaki, H
   Sharmin, M
   Towey, D
   Majumder, AKMJA
   Hori, Y
   Yang, JJ
   Takemoto, M
   Sakib, N
   Banno, R
   Ahamed, SI
TI Revolutionizing Healthcare Management: Architecture of a Web-based
   Medical Triage Service
SO 2024 IEEE 48TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE,
   COMPSAC 2024
SE IEEE Annual International Computer Software and Applications Conference
BP 1887
EP 1894
DI 10.1109/COMPSAC61105.2024.00299
DT Proceedings Paper
PD 2024
PY 2024
AB During the COVID-19 pandemic, the traditional emergency healthcare
   systems faced unprecedented strain due to the sharp rise in demands for
   urgent care, scarcity of resources, and increased risks of people
   getting infected while waiting at the emergency care facility. We
   present Triage-Bot, an online medical triage provisioning service, that
   can revolutionize emergency care by decreasing the load on emergency
   departments (ED), reducing healthcare expenses, and improving the
   quality of care. Empowered by artificial intelligence and natural
   language processing, the Triage-Bot service assesses and prioritizes
   patients' needs based on symptoms, medical history, and perceived
   conditions from multimodal video, audio, and text data captured during
   patients' interactions. The captured summarized information with a
   severity ranking is sent to a human expert to suggest the next action on
   the user's part. The diverse data types used by the Triage-Bot in
   communication, authentication, data collection, storage, and analytics
   requires a robust and scalable system architecture for online service
   provisioning. In this paper, we specifically focus on the system design
   and architecture of the Triage-Bot for emergency healthcare settings.
   With integrated electronic medical records (EMR) and online platforms,
   the bot fosters collaboration among healthcare professionals and enables
   swift and informed decision-making even in the face of crises. By
   partially automating and offering a hybrid triage process, the
   Triage-Bot improves resource allocation, reduces healthcare management
   costs for emergency care, minimizes patient waiting times, and improves
   wellbeing. To address the complexities and demands of healthcare data
   management, our proposed system incorporates MongoDB database for
   flexibility, scalability, and versatility in supporting different types
   of data. Additionally, we implement a data linking and analytics
   pipeline utilizing a data Lakehouse system to effectively ingest,
   manage, process, and generate knowledge from heterogeneous data sources.
CT 48th Annual IEEE International Computers, Software, and Applications
   Conference (COMPSAC) - Digital Development for a Better Future
CY JUL 02-04, 2023
CL Osaka Univ, Nakanoshima Ctr, Osaka, JAPAN
HO Osaka Univ, Nakanoshima Ctr
SP IEEE; IEEE Comp Soc
RI Al-Yateem, Nabeel/GQZ-2152-2022; Rahman, Syed/
OI Rahman, Syed/0000-0002-2583-6037
ZR 0
TC 0
ZS 0
Z8 0
ZB 0
ZA 0
Z9 0
U1 0
U2 1
SN 2836-3787
BN 979-8-3503-7697-5; 979-8-3503-7696-8
DA 2024-12-03
UT WOS:001308581200291
ER

PT J
AU Hlavacka, Jakub
   Bobak, Martin
   Hluchy, Ladislav
TI Big Data Deduplication in Data Lake
SO ACTA POLYTECHNICA HUNGARICA
VL 21
IS 11
BP 307
EP 328
DI 10.12700/APH.21.11.2024.11.17
DT Article
PD 2024
PY 2024
AB Data lakes are the next generation of technology to process and store
   big data. As usual, new challenges and problems arise inevitably with
   new technologies. One of these problems is the occurrence of duplicate
   data in the storage. Our paper aims to address this challenge during the
   data ingestion phase that is currently overlooked or addressed
   insufficiently. The first part discusses the design of a suitable
   architecture for the data lake and deduplication workflow for processing
   structured and unstructured data. The proposed solution is evaluated
   through experiments that deal with the flexible deduplication window,
   the scalability of the proposed solution, the suitable hash function,
   and the advantages of an in-memory pointer repository.
RI Bobak, Martin/O-9524-2019; Hluchy, Ladislav/ABD-7620-2020
TC 0
ZA 0
Z8 0
ZR 0
ZS 0
ZB 0
Z9 0
U1 1
U2 1
SN 1785-8860
DA 2025-10-08
UT WOS:001410769800001
ER

PT J
AU Kim, Ka Ram
   Suh,, Jin Hyung
TI Design of Storage and Processing of Aviation Safety Data in a Hybrid
   Data Lake Platform
Z1 항공 안전 데이터의 혼합 데이터레이크 플랫폼에서의 저장과 처리 설계
SO Journal of Creative Information Culture
S1 창의정보문화연구
VL 10
IS 4
BP 251
EP 260
DT research-article
PD 2024
PY 2024
AB In the aviation industry, safety is a key element in protecting lives
   and maintaining business continuity, and for this purpose, it is
   essential to collect, store, and analyze a large amount of safety data.
   Due to the importance of these aviation assets, many aviation-related
   organizations have studied methods for storing and processing aviation
   safety data and providing processed data. However, existing data storage
   and processing methods are diverse and have limitations in quickly
   processing aviation safety data generated in real time. In particular, a
   mixed data processing platform that can accommodate and process both
   structured data such as maintenance records and operation logs and
   unstructured data such as voice and video data is required, and it
   should be structured with a focus on real-time data processing, data
   governance, and data security. In this study, we will examine the
   structure of a mixed data lake platform that can centrally manage
   aviation safety data from various sources by combining a cloud-based
   data lake and an on-premise database.
AK 항공산업에서의 안전은 생명의 보호와 비즈니스 지속성을 유지하는 핵심 요소로 이를 위하여 방대한 양의 안전 데이터를 수집, 저장,
   분석하는 것이 필수적이다. 이러한 항공 자산의 중요성으로 많은 항공 관련 기관에서 항공 안전 데이터의 저장과 처리, 처리된
   데이터의 제공 방법에 대한 연구가 있었으나 기존의 데이터 저장 및 처리 방식으로는 다양하며 실시간으로 발생하는 항공 안전 데이터의
   신속한 처리에 한계가 있다. 특히 생성 데이터가 정비 기록, 운영 로그와 같은 정형 데이터와 음성 및 영상 데이터와 같은 비정형
   데이터를 모두 수용, 처리할 수 있는 혼합 데이터 처리 플랫폼이 필요하며 실시간 데이터 처리, 데이터 거버넌스, 그리고 데이터
   보안성을 중점으로 구성되어야 한다. 본 연구에서는 클라우드 기반의 데이터레이크와 온프레미스 데이터베이스를 결합하여 다양한 소스의
   항공 안전 데이터를 중앙 집중적으로 관리할 수 있는 혼합 데이터레이크 플렛폼 구조를 살펴보고자 한다.
Z8 0
ZS 0
ZR 0
ZB 0
TC 0
ZA 0
Z9 0
U1 0
U2 0
SN 2384-2008
DA 2024-12-27
UT KJD:ART003141570
ER

PT C
AU Kulkarni, Radhika V.
   Jagtap, Vedant
   Naik, Tanaya
   Shaha, Shraddha
   Nikumbh, Khushi
BE So-In, C
   Joshi, A
   Senjyu, T
TI Leveraging Azure Data Factory for COVID-19 Data Ingestion,
   Transformation, and Reporting
SO SMART TRENDS IN COMPUTING AND COMMUNICATIONS, VOL 3, SMARTCOM 2024
SE Lecture Notes in Networks and Systems
VL 947
BP 275
EP 285
DI 10.1007/978-981-97-1326-4_23
DT Proceedings Paper
PD 2024
PY 2024
AB The COVID-19 pandemic has necessitated the collection and analysis of
   large volumes of data from various sources to understand its impact and
   make informed decisions. This research paper explores the utilization of
   Azure Data Factory, a managed cloud solution, for integrating and
   reporting COVID-19 data. The study focuses on two primary objectives.
   Firstly, it establishes a data platform using Azure Data Factory to
   enable data scientists to leverage machine learning models for
   predicting the spread of the virus and gaining insightful findings. This
   work demonstrates Azure Data Lake Gen2 as a reliable storage repository
   for ingesting and storing data. Furthermore, it explores the data
   transformation and analysis capabilities of Azure Data Factory using
   Data Flows, HDInsight, and Azure Databricks. The second objective
   revolves around the creation of a reporting platform for data analysts.
   This study illustrates how Azure Data Factory orchestrates the
   extraction, transformation, and loading of selected COVID-19 data into a
   SQL database. This subset of data is then leveraged to develop a Power
   BI report, enabling data analysts to visualize and report COVID-19
   trends efficiently. Through a comprehensive analysis and practical
   implementation of Azure Data Factory, this research highlights its
   efficacy in handling COVID-19 data integration and reporting tasks. It
   sheds light on the data ingestion, transformation, and storage options
   provided by Azure Data Factory. It also showcases the seamless
   integration with other Azure services, including Data Flows, HDInsight,
   Azure Databricks, SQL databases, and Power BI.
CT 8th Smart Trends in Computing and Communications (SmartCom)
CY JAN 12-13, 2024
CL Pune, INDIA
RI Kulkarni, Radhika V/ABB-1471-2021
OI Kulkarni, Radhika V/0000-0001-6614-6297
ZB 0
ZR 0
ZA 0
Z8 0
TC 0
ZS 0
Z9 0
U1 1
U2 4
SN 2367-3370
EI 2367-3389
BN 978-981-97-1325-7; 978-981-97-1326-4
DA 2024-09-15
UT WOS:001289068000023
ER

PT C
AU Lefebvre, Sylvain
   Lecointre, Michael
   Lardeux, Benoit
   Guyader, Jean-Marie
   Aubrun, Olivier
   Toure, Birane
   Jridi, Maher
BE Chbeir, R
   Benslimane, D
   Zervakis, M
   Manolopoulos, Y
   Ngyuen, NT
   Tekli, J
TI A Digital Ecosystem for Improving Product Design
SO MANAGEMENT OF DIGITAL ECOSYSTEMS, MEDES 2023
SE Communications in Computer and Information Science
VL 2022
BP 251
EP 263
DI 10.1007/978-3-031-51643-6_18
DT Proceedings Paper
PD 2024
PY 2024
AB Digitization is reaching to every corner of the industry. The industry
   4.0 (I4.0) movement initiated a move towards a stronger reliance on data
   in the manufacturing domain in order to improve processes and product
   quality. Multiple works highlight the potential benefits of deploying
   artificial intelligence or big data management platforms for industrial
   companies to improve their processes and provide a better understanding
   of their production tools. Many I4.0 work often assume the existence of
   interconnected machinery, sensors, and Manufacturing Execution System
   (MES) in the company and assume that most data is already available from
   these interconnected systems on the production line. Unfortunately, this
   does not reflect the state of many companies whose production systems
   are not interconnected due to historical reasons or security and
   normative issues. This report describes a big data architecture for the
   collection, storage and analysis of industrial prototype data. We
   provide details on how such an architecture can be structured and how it
   supports the engineering cycle in a partner company through a case
   study.
CT 15th International Conference on Management of Digital EcoSystems
   (MEDES)
CY MAY 05-07, 2023
CL Heraklion, GREECE
OI Lefebvre, Sylvain/0000-0003-4045-3779
Z8 0
ZB 0
ZS 0
TC 0
ZR 0
ZA 0
Z9 0
U1 1
U2 3
SN 1865-0929
EI 1865-0937
BN 978-3-031-51642-9; 978-3-031-51643-6
DA 2024-08-23
UT WOS:001260534100018
ER

PT C
AU Li, Ziyu
   Zhao, Wenjie
   Katsifodimos, Asterios
   Hai, Rihan
GP ACM
TI LLM-PQA: LLM-enhanced Prediction Query Answering
SO PROCEEDINGS OF THE 33RD ACM INTERNATIONAL CONFERENCE ON INFORMATION AND
   KNOWLEDGE MANAGEMENT, CIKM 2024
BP 5239
EP 5243
DI 10.1145/3627673.3679210
DT Proceedings Paper
PD 2024
PY 2024
AB The advent of Large Language Models (LLMs) provides an opportunity to
   change the way queries are processed, moving beyond the constraints of
   conventional SQL-based database systems. However, using an LLM to answer
   a prediction query is still challenging, since an external ML model has
   to be employed and inference has to be performed in order to provide an
   answer. This paper introduces LLM-PQA, a novel tool that addresses
   prediction queries formulated in natural language. LLM-PQA is the first
   to combine the capabilities of LLMs and retrieval-augmented mechanism
   for the needs of prediction queries by integrating data lakes and model
   zoos. This integration provides users with access to a vast spectrum of
   heterogeneous data and diverse ML models, facilitating dynamic
   prediction query answering. In addition, LLM-PQA can dynamically train
   models on demand, based on specific query requirements, ensuring
   reliable and relevant results even when no pre-trained model in a model
   zoo, available for the task.
CT 33rd ACM International Conference on Information and Knowledge
   Management (CIKM)
CY OCT 21-25, 2024
CL Boise, ID
SP Assoc Comp Machinery; ACM SIGIR; ACM SIGWEB
OI Katsifodimos, Asterios/0000-0002-6717-2945; Hai,
   Rihan/0000-0002-3720-6585
ZS 0
TC 0
ZR 0
ZB 0
Z8 0
ZA 0
Z9 0
U1 1
U2 6
BN 979-8-4007-0436-9
DA 2025-03-05
UT WOS:001349579605052
ER

PT C
AU Maso, Joan
   Brobia, Alba
   Zamzov, Malte
   Serall, Ivette
   Hodson, Thomas
   Palma, Raul
   Noardo, Francesca
   Bastin, Lucy
   Lush, Victoria
GP IEEE
TI DIGITAL TWIN READY DATA AVAILABLE IN THE GREEN DEAL DATA SPACE
SO IGARSS 2024-2024 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING
   SYMPOSIUM, IGARSS 2024
SE IEEE International Symposium on Geoscience and Remote Sensing IGARSS
BP 3589
EP 3591
DI 10.1109/IGARSS53475.2024.10641950
DT Proceedings Paper
PD 2024
PY 2024
AB The Destination Earth (DestinE) initiative will develop and deploy a
   service infrastructure of computer processing, data and software. The
   Green Deal Data Space will organize the available data to contribute to
   the data lake of the DestinE infrastructure. This paper focuses on how
   to overcome data challenges in the Green Deal Data Space by producing
   Digital Twin Ready Data under the big data paradigm. In the Green Deal
   Data Space, the traditional organization of data in layers is no longer
   efficient as data is constantly evolving and mixed in new ways. First,
   there is a need for a new organization based on a flexible and
   encompassing Information Model composed by a suite of ontologies reusing
   best practices, and existing standards. Secondly, dynamic
   multidimensional data cubes replace the traditional two-dimensional
   view. Third, the OGC APIs offer a set of building blocks to implement
   data filtering for extracting the data with its provenance metadata.
   The AD4GD project will demonstrate the proposed capabilities of the
   Green Deal Data Space in three pilots about water pollution in Berlin's
   small lakes, biodiversity connectivity in Catalonia and air quality for
   the Copernicus Atmosphere Monitoring Service. While data spaces should
   allow for data exchange in a secure environment that enabling the
   digital economy, this aspects are out of scope of this communication.
CT IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
CY JUL 07-12, 2024
CL Athens, GREECE
SP Inst Elect & Elect Engineers; Inst Elect & Elect Engineers, Geoscience &
   Remote Sensing Soc
RI Bastin, Lucy/ABD-2663-2021; Maso, Joan/C-2361-2012; Noardo, Francesca/ABG-9595-2020
OI Bastin, Lucy/0000-0003-1321-0800; 
ZB 0
ZR 0
TC 0
ZS 0
ZA 0
Z8 0
Z9 0
U1 1
U2 1
SN 2153-6996
BN 979-8-3503-6033-2; 979-8-3503-6032-5
DA 2025-02-28
UT WOS:001316158503218
ER

PT J
AU Oh, Jae-Yong
   Kim, Hye-Jin
TI Research on the Design of Multimodal Maritime Data Management System
Z1 멀티 모달 해사 데이터 관리 체계 설계에 관한 연구
SO Journal of the Korean Society of Marine Environment & Safety
S1 해양환경안전학회지
VL 30
IS 7
BP 836
EP 843
DT research-article
PD 2024
PY 2024
AB Maritime data can be defined as all data generated at maritime domain,
   including information about ships entering and leaving ports, cargo
   transported, and the VTS information that monitors and manages vessels.
   This maritime data is sent and received in a variety of formats and is
   multimodal, with each data type being closely related to the other,
   making it difficult to integrate and manage. Moreover, utilizing
   maritime data for AI systems requires knowledge of the data domain,
   which limits the ability of non-experts to utilize the data. In this
   paper, we proposed a data architecture that can effectively manage
   multimodal maritime data by utilizing the correlation of data. The
   proposed management architecture includes a preprocessing procedure for
   multimodal data, a graph database based on associations, and an object
   storage space for unstructured data, which can automatically extract and
   store associations from the collected data. In addition, we examined the
   feasibility of the data management system through the example of
   building a database of VHF data, and confirmed that it can improve the
   user's understanding of the data and improve data utilization compared
   to the existing data management system.
AK 해사 데이터는 항만을 입출항하는 선박 정보, 해상에서 운송되는 화물 정보, 이를 모니터링하고 관리하는 해상교통관제 정보 등
   해상에서 생성되는 모든 데이터로 정의할 수 있다. 이러한 해사 데이터는 그 종류만큼이나 다양한 형식으로 송수신되고 있으며, 각각의
   데이터가 서로 밀접하게 연관되어 있는 멀티모달의 특징을 가지고 있기 때문에 데이터의 통합 관리가 어려운 실정이다. 더욱이 해사
   데이터를 인공지능 시스템에 활용하기 위해서는 데이터 도메인에 대한 지식이 필요하기 때문에 비전문가의 경우 데이터를 활용하는 데
   제약이 많았다. 이에 본 논문에서는 데이터의 연관 관계를 이용하여 멀티 모달 해사 데이터를 효과적으로 관리할 수 있는 데이터
   체계를 제안하였다. 제안하는 관리 체계는 멀티 모달 데이터의 전처리 작업 절차와 연관 관계 기반의 그래프 데이터베이스, 비정형
   데이터를 위한 객체 저장 공간을 포함하고 있으며, 이를 통해 수집된 데이터로부터 연관 관계를 자동으로 추출하여 저장할 수 있도록
   설계하였다. 또한, VHF 데이터의 데이터베이스 구축 예시를 통해 제안하는 데이터 관리 체계의 활용 가능성을 검토하였으며, 기존의
   데이터 관리 체계에 비해 데이터의 이해도를 높이고, 활용도를 향상시킬 수 있을 것으로 예상된다.
Z8 0
ZS 0
ZA 0
TC 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
SN 1229-3431
DA 2025-01-26
UT KJD:ART003161398
ER

PT R
AU Pastor-Galindo, Javier
TI Categorization of Tor onion addresses
SO Mendeley Data
VL 1
DI https://doi.org/10.17632/9NPMF5V6KR.1
DT Data set
VN 1
PD 2024-04-02
PY 2024
AB This dataset is linked to the article "A Big Data architecture for early
   identification and categorization of dark web sites":- The "onions.csv"
   contains the different onion addresses with their topic, topic label and
   main keywords.- The "mirrors.csv" contains the different onion addresses
   with their mirror group.- The "topics.csv" contains the topic, topic
   label and main keywords.
Z8 0
TC 0
ZA 0
ZR 0
ZB 0
ZS 0
Z9 0
U1 0
U2 0
DA 2024-12-07
UT DRCI:DATA2024077029387705
ER

PT R
AU Pastor-Galindo, Javier
TI Categorization of Tor onion addresses
SO Mendeley Data
DI https://doi.org/10.17632/9NPMF5V6KR
DT Data set
PD 2025-05-05
PY 2024
AB This dataset is linked to the article "A Big Data architecture for early
   identification and categorization of dark web sites":- The "onions.csv"
   contains the different onion addresses with their topic, topic label and
   main keywords.- The "mirrors.csv" contains the different onion addresses
   with their mirror group.- The "topics.csv" contains the topic, topic
   label and main keywords.
ZB 0
TC 0
ZR 0
ZS 0
Z8 0
ZA 0
Z9 0
U1 0
U2 0
DA 2024-12-07
UT DRCI:DATA2024077029387706
ER

PT C
AU Ramos, Isabel
   Santos, Maribel Yasmina
   Joshi, Divya
   Pratik, Sheetal
BE Papadaki, M
   Themistocleous, M
   AlMarri, K
   AlZarouni, M
TI Data Mesh Adoption: A Multi-case and Multi-method Readiness Approach
SO INFORMATION SYSTEMS, PT 2, EMCIS 2023
SE Lecture Notes in Business Information Processing
VL 502
BP 16
EP 29
DI 10.1007/978-3-031-56481-9_2
DT Proceedings Paper
PD 2024
PY 2024
AB Data Warehousing systems have been used to support Business Intelligence
   applications by ingesting operational data and providing analytical
   data. As data volume, variety, and velocity increased in Big Data
   contexts, this data architecture needed to be modernised, and Big Data
   Warehouses emerged as scalable, high-performance, and highly flexible
   processing systems capable of handling ever-increasing volumes of data.
   These monolithic techniques, however, create major challenges to data
   engineering teams in terms of design, development, management, and
   evolution. Data Mesh emerged as a novel and disruptive concept aimed at
   data-driven businesses. The research detailed in this paper seeks to
   characterise Data Mesh readiness by examining the elements that
   influence the adoption choice using the
   technology-organization-environment (TOE) paradigm. A survey and a set
   of interviews were used in a multi-case and multi-method approach.
   Researchers and data triangulation were implemented to ensure rigour and
   arrive at a comprehensive understanding of Data Mesh adoption. The
   obtained results demonstrate the successful adoption of Data Mesh once
   its benefits are well understood, with increased teams' creativity, data
   accuracy, data security, data governance and interoperability.
CT 20th European, Mediterranean, and Middle Eastern Conference (EMCIS)
CY DEC 11-12, 2023
CL British Univ, Dubai, U ARAB EMIRATES
HO British Univ
RI Pratik, Sheetal/; Santos, Maribel Yasmina/M-5214-2013; Ramos, Isabel/D-5862-2012
OI Pratik, Sheetal/0009-0000-4865-1599; Santos, Maribel
   Yasmina/0000-0002-3249-6229; Ramos, Isabel/0000-0001-8035-4703
ZB 0
ZS 0
TC 0
ZA 0
Z8 0
ZR 0
Z9 0
U1 1
U2 5
SN 1865-1348
EI 1865-1356
BN 978-3-031-56480-2; 978-3-031-56481-9
DA 2024-10-12
UT WOS:001308346700002
ER

PT C
AU Salcher, Felix
   Finck, Steffen
   Hellwig, Michael
GP IEEE
TI A Smart Shop Floor Information System Architecture based on the Unified
   Namespace
SO 2024 IEEE INTERNATIONAL CONFERENCE ON ENGINEERING, TECHNOLOGY, AND
   INNOVATION, ICE/ITMC 2024
SE International ICE Conference on Engineering Technology and Innovation
AR 172
DI 10.1109/ICE/ITMC61926.2024.10794387
DT Proceedings Paper
PD 2024
PY 2024
AB Modern research in the field of Smart Manufacturing often focuses on the
   big data aspect, where the goal is to obtain actionable insights from
   the data. In this paper, the focus is shifted back to the Smart Shop
   Floor and how to efficiently derive information with the big data tasks
   that follow as simple as possible. A condensed literature review of the
   existing architectures and frameworks for Smart Manufacturing is
   combined with the experience of practitioners to assess the requirements
   for a Smart Shop Floor Information System Architecture. On this basis,
   an architecture is proposed that consists of eight modular building
   blocks. After a detailed description of the roles and functionalities of
   these building blocks, a reference implementation using readily
   available, open-source tools and technologies is laid out. This
   reference implementation intends to strike the right balance between
   generality and specificity. It provides the reader with a tangible
   starting point for implementing and adapting the proposed architecture
   to their own needs.
CT 30th IEEE International Conference on Engineering, Technology, and
   Innovation
CY JUN 24-28, 2024
CL Funchal, PORTUGAL
SP Institute of Electrical and Electronics Engineers Inc; Universidade do
   Minho; NOVA School of Science and Technology (FCT NOVA); Universidade
   Nova De Lisboa
RI Hellwig, Michael/; Finck, Steffen/V-7919-2019
OI Hellwig, Michael/0000-0002-6731-8166; 
ZB 0
ZR 0
ZA 0
TC 0
Z8 0
ZS 0
Z9 0
U1 2
U2 5
SN 2334-315X
BN 979-8-3503-6244-2; 979-8-3503-6243-5
DA 2025-04-02
UT WOS:001429193000105
ER

PT C
AU Stach, Christoph
   Li, Yunxuan
   Schuiki, Laura
   Mitschang, Bernhard
BE Khalil, I
   Strauss, C
   Amagasa, T
   Manco, G
   Kotsis, G
   Tjoa, AM
TI LALO-A Virtual Data Lake Zone for Composing Tailor-Made Data Products on
   Demand
SO DATABASE AND EXPERT SYSTEMS APPLICATIONS, PT II, DEXA 2024
SE Lecture Notes in Computer Science
VL 14911
BP 288
EP 305
DI 10.1007/978-3-031-68312-1_22
DT Proceedings Paper
PD 2024
PY 2024
AB The emerging paradigm of data products, which has become increasingly
   popular recently due to the rise of data meshes and data marketplaces,
   also poses unprecedented challenges for data management. Current data
   architectures, namely data warehouses and data lakes, are not able to
   meet these challenges adequately. In particular, these architectures are
   not designed for a just-in-time provision of highly customized data
   products tailored perfectly to the needs of customers. In this paper, we
   therefore present a virtual data lake zone for composing tailor-made
   data products on demand, called LALO. LALO uses data streaming
   technologies to enable just-in-time composing of data products without
   allocating storage space in the data architecture permanently. In order
   to enable customers to tailor data products to their needs, LALO uses a
   novel mechanism that enables live adaptation of data streams. Evaluation
   results show that the overhead for such an adaptation is negligible.
   Therefore, LALO represents an efficient solution for the appropriate
   handling of data products, both in terms of storage space and runtime.
CT 35th International Conference on Database and Expert Systems
   Applications (DEXA)
CY AUG 26-28, 2024
CL Naples, ITALY
SP Software Competence Ctr Hagenberg; JKU, Inst Telecooperat; Web Applicat
   Soc; Univ Studi Napoli Federico II, Dipartimento Ingn Elettrica
   Tecnologie Informazione
RI Schuiki, Laura/; Stach, Christoph/AAZ-3044-2020; Li, Yunxuan/HKM-8952-2023
OI Schuiki, Laura/0009-0008-0219-5485; Stach,
   Christoph/0000-0003-3795-7909; Li, Yunxuan/0000-0003-1907-9591
TC 0
ZS 0
Z8 0
ZR 0
ZB 0
ZA 0
Z9 0
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-68311-4; 978-3-031-68312-1
DA 2024-10-18
UT WOS:001315824400022
ER

PT C
AU Sun, Bowen
   Yang, Xiaohan
   Guo, Ying
   Zhao, Zhihao
   Chen, Jing
   Zhang, Hu
   Wang, Jibin
GP IEEE
TI OCF-HP: An Offline Compaction Framework based on Hotspot Prediction for
   Data Lakehouse
SO 2024 9TH INTERNATIONAL CONFERENCE ON ELECTRONIC TECHNOLOGY AND
   INFORMATION SCIENCE, ICETIS 2024
BP 733
EP 741
DI 10.1109/ICETIS61828.2024.10593781
DT Proceedings Paper
PD 2024
PY 2024
AB With the rapid advancement of information technologies, the global data
   volume is increasing at an unprecedented speed. Data Lakehouse has
   become an effective solution for real-time aggregation of multimodal
   heterogeneous data and supporting multiple query modes. It is worth
   noting that due to the column-based storage commonly employed in Data
   Lakehouse and the resulting mechanisms such as Copy-on-Write and
   Merge-On-Read, data query speed, data freshness, and storage costs have
   become huge challenges. In this paper, an Offline Compaction Framework
   based on Hotspot Prediction (OCF-HP) was designed to address the above
   challenges. Specifically, we analyzed the Timeline data in Hudi which is
   the mainstream Data Lakehouse framework, then predicted the hotspot
   periods of data writing based on the improved LSTM model using such
   data, and realized automatic trigger of Hudi offline compaction tasks in
   reasonable non-hot periods. The experiment results show that OCF-HP
   provides Hudi with a more optimized offline compaction solution so that
   data lakehouse managers could achieve better query speed and data
   freshness with reducing storage costs in Hudi's read optimized mode.
CT 9th International Conference on Electronic Technology and Information
   Science (ICETIS)
CY MAY 17-19, 2024
CL Hangzhou, PEOPLES R CHINA
SP IEEE
RI Sun, Bowen/OLQ-8767-2025; Yang, Xiaohan/MIK-2741-2025
ZR 0
TC 0
Z8 0
ZB 0
ZA 0
ZS 0
Z9 0
U1 0
U2 0
BN 979-8-3503-8835-0; 979-8-3503-8834-3
DA 2024-11-01
UT WOS:001298131100136
ER

PT C
AU Sun, Susan
   Ye, Jeff
   Schwarthoff, Hubert
   Rosin, Jon
   Vakkalagadda, Varalakshmi
   Chang, Jimmy
   Ubbara, Sesidhar Reddy
   Chinthakindi, Anil
GP IEEE
TI Cloud Big Data Lake for Advanced Analytics in Semiconductor
   Manufacturing
SO 2024 35TH ANNUAL SEMI ADVANCED SEMICONDUCTOR MANUFACTURING CONFERENCE,
   ASMC
SE Advanced Semiconductor Manufacturing Conference and Workshop-Proceedings
DI 10.1109/ASMC61125.2024.10545365
DT Proceedings Paper
PD 2024
PY 2024
AB Data driven business intelligence is changing how semiconductor
   manufacturing thrives in the long term. A cloud big data lake is
   designed and implemented based on state-of-the-art cloud architecture
   providing complete services for data ingestion, storage, processing,
   advanced analytics, and machine learning with a high level of security.
   Efficient and effective use of this big data lake and data science
   enables problem solving and decision making to improve productivity and
   performance.
CT 35th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)
CY MAY 13-16, 2024
CL Albany, NY
SP IEEE
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
TC 0
Z9 0
U1 1
U2 4
SN 1078-8743
BN 979-8-3503-8456-7; 979-8-3503-8455-0
DA 2024-07-27
UT WOS:001245033700005
ER

PT C
AU Ustunboyacioglu, Ipek
   Kumara, Indika
   Di Nucci, Dario
   Tamburri, Damian Andrew
   van den Heuvel, Willem-Jan
BE Galster, M
   Scandurra, P
   Mikkonen, T
   Antonino, PO
   Nakagawa, EY
   Navarro, E
TI Integrating Data Quality in Industrial Big Data Architectures: An Action
   Design Research Study
SO SOFTWARE ARCHITECTURE, ECSA 2024
SE Lecture Notes in Computer Science
VL 14889
BP 3
EP 19
DI 10.1007/978-3-031-70797-1_1
DT Proceedings Paper
PD 2024
PY 2024
AB In today's data-driven business environments, organizations heavily rely
   on high-quality data to make informed decisions and gain a competitive
   advantage. Organizations typically use big data architectures to store,
   process, and manage their exponentially growing enterprise data.
   However, ensuring data quality in such scenarios remains a significant
   challenge for many organizations. Despite the vast number of data
   quality tools available, integrating such tools into big data
   architectures has not been fully explored. In this study, we aim to
   formulate design principles to support systematically incorporating data
   quality testing into big data architectures. For this purpose, we
   performed an action design research study at a large organization in the
   Netherlands. Finally, we employed the architecture trade-off analysis
   method (ATAM) to evaluate our solution design.
CT 18th European Conference on Software Architecture (ECSA)
CY SEP 03-06, 2024
CL Luxembourg City, LUXEMBOURG
RI Tamburri, Damian/AAJ-2507-2021; Di Nucci, Dario/Y-9679-2019; Weerasingha Dewage, Indika Priyantha Kumara/
OI Weerasingha Dewage, Indika Priyantha Kumara/0000-0003-4355-0494
TC 0
ZS 0
ZR 0
ZB 0
ZA 0
Z8 0
Z9 0
U1 1
U2 5
SN 0302-9743
EI 1611-3349
BN 978-3-031-70796-4; 978-3-031-70797-1
DA 2024-10-09
UT WOS:001307866400001
ER

PT C
AU Veloso, Artur F. da S.
   Costa, Matheus M. do N.
   Junior, Jose V. R.
   Abreu, Pedro F. F.
   Neto, Geraldo S.
   Silva, Thiago A.
   Mendes, Luis H. de O.
GP IEEE
TI Big Data Architecture for Efficient Energy Management in Multi Microgrid
   Scenarios
SO 2024 XIV BRAZILIAN SYMPOSIUM ON COMPUTING SYSTEMS ENGINEERING, SBESC
SE Brazilian Symposium on Computing System Engineering
DI 10.1109/SBESC65055.2024.10771911
DT Proceedings Paper
PD 2024
PY 2024
AB The traditional electric power distribution network is evolving to
   support scalable and connected services in Smart Cities (SC). The
   emergence of Smart Grid (SG) architecture connects Smart Meters (SM) to
   the Electric Power Company (EPC) using various communication
   technologies like Power Line Communication (PLC) and wireless networks
   such as 5G, LoRa, and others. However, the vast amount of data generated
   by this infrastructure can lead to network overload and storage and
   processing challenges. To address this, this paper proposes implementing
   a Big Data Analytics framework under the Hybrid Demand Side Management
   as a Service (HyDSMaaS) architecture in the Multi Microgrid scenario.
   This approach divides the SG into Microgrids, creating smaller groups of
   nodes and transforming the centralized infrastructure into a scalable
   and decentralized one. Additionally, a monitoring system using Grafana
   was implemented to manage server usage flow and monitor access and
   services. The framework achieved high speed compared to traditional
   models and demonstrated accuracy of nearly 99.9% in consumption
   prediction and power quality classification algorithms when operating
   with up to 1000 microgrids in under 1000 seconds.
CT 14th Brazilian Symposium on Computing Systems Engineering
CY NOV 26-29, 2024
CL Recife, BRAZIL
SP Universidade Federal de Pernambuco
RI dos Reis, José Valdemir/E-8297-2015; , Geraldo Abrantes Sarmento Neto/
OI dos Reis, José Valdemir/0000-0002-9556-5471; , Geraldo Abrantes Sarmento
   Neto/0009-0008-6009-3989
ZS 0
Z8 0
TC 0
ZA 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
SN 2324-7886
BN 979-8-3315-3285-7; 979-8-3315-0900-2
DA 2025-06-12
UT WOS:001456780800014
ER

PT C
AU Xu, Yuan
   Su, Yinsheng
   Zhao, Ligang
   Zhou, Baorong
   Tang, Yingqi
   Huang, Guanbiao
   Hu, Jiaming
   Li, Kaihang
GP IEEE
TI Distributed Data Storage Architecture Model and Data Platform Design for
   Power Grid Dispatching
SO 2024 3RD INTERNATIONAL CONFERENCE ON ENERGY AND ELECTRICAL POWER
   SYSTEMS, ICEEPS 2024
BP 859
EP 863
DI 10.1109/ICEEPS62542.2024.10693170
DT Proceedings Paper
PD 2024
PY 2024
AB The construction of a distributed heterogeneous data platform for power
   grid dispatching faces challenges of diversity, large scale, and high
   performance. However, existing data platform design methods in both the
   power and computer science fields struggle to meet practical production
   requirements effectively. This paper constructs a distributed data
   storage architecture model for power grid dispatching, defining the
   elements and their relationships within the architecture. Additionally,
   it proposes methods for managing massive source data and distributed
   heterogeneous database clusters. Based on these findings, a power grid
   dispatching business data platform is designed. Test results indicate
   that the proposed architecture effectively supports the efficient
   execution of power grid dispatching business, providing a specialized
   data platform design paradigm for the power industry.
CT 3rd International Conference on Energy and Electrical Power Systems
   (ICEEPS)
CY JUL 14-16, 2024
CL S China Univ Technol, Guangzhou, PEOPLES R CHINA
HO S China Univ Technol
SP IEEE; Univ Macau; Shenzhen Univ; Guangxi Univ; Sun Yat Sen Univ; IEEE
   PES Tech Comm Smart Bldg, Load, & Customer Syst
RI Tang, Yingqi/; Zhao, Ligang/MHQ-3675-2025
OI Tang, Yingqi/0000-0002-8662-2219; 
TC 0
ZR 0
Z8 0
ZA 0
ZB 0
ZS 0
Z9 0
U1 0
U2 0
BN 979-8-3503-7514-5; 979-8-3503-7513-8
DA 2025-01-11
UT WOS:001339287300159
ER

PT C
AU Zhang, Aaron
   Weber, Gerald
GP ASSOC COMPUTING MACHINERY
TI Yugen SDL: Semantic Data Lake Design for Relational Database from
   Enterprise Data Platforms
SO 6TH WORLD SYMPOSIUM ON SOFTWARE ENGINEERING, WSSE 2024
BP 54
EP 61
DI 10.1145/3698062.3698070
DT Proceedings Paper
PD 2024
PY 2024
AB Although data lake technology has received increasing research attention
   in recent years with the popularity of Big Data and heterogeneous data
   technologies, it still receives little attention in the enterprise space
   and relation to relational databases. Since enterprise data lakes
   include structured data (rows and columns) and semi-structured data like
   CSV from relational databases, etc. [1], relational data requires a
   complete real-time corresponding data management platform to handle this
   data. Yet research challenges for these still exist. The Yugen Semantic
   Data Lake (Yugen SDL) for data enterprise data involved in this research
   is designed to solve this problem, improving the handling of relational
   databases by data lakes in the enterprise domain. In this project, a
   traditional Big Data platform and a complete real-time relational system
   are built, a semantic layer is defined, a knowledge graph is created,
   and data is managed in a unified way for SPARQL queries. A bespoke query
   engine is built, and the structured data generated by the internal IT
   systems in the data lake is queried using the D2RQ tool. In contrast,
   the internal semi-structured data is managed by querying using TARQL.
   Yugen SDL will focus on the data needs of the enterprise and address the
   critical challenges of developing large-scale SDLs for relational data
   and big data platforms.
CT 6th World Symposium on Software Engineering
CY SEP 13-15, 2024
CL Kyoto, JAPAN
SP Wuhan University; University of Houston; Huaqiao University; University
   Of North Dakota
OI Zhang, Aaron/0000-0002-7261-754X
ZA 0
TC 0
ZB 0
ZS 0
ZR 0
Z8 0
Z9 0
U1 0
U2 0
BN 979-8-4007-1708-6
DA 2025-04-04
UT WOS:001440422700008
ER

PT C
AU Zhang, Tao
   Liu, Hao
   Liu, Yinlong
   Chen, Wei
GP IEEE
TI DataLakeIO: A Connector between Apache Beam and Data Lake
SO 2024 9TH INTERNATIONAL CONFERENCE ON ELECTRONIC TECHNOLOGY AND
   INFORMATION SCIENCE, ICETIS 2024
BP 790
EP 793
DI 10.1109/ICETIS61828.2024.10593666
DT Proceedings Paper
PD 2024
PY 2024
AB In the era of big data, Apache Beam is widely used because it provides a
   unified programming model and supports multiple data processing engines.
   At the same time, data lake is becoming more and more widely used.
   However, Apache Beam is not currently interconnected with the data lake.
   In this paper, we proposed DataLakeIO, a connector between Apache Beam
   and data lake. First, a Apache Beam pipeline is initialized. Then,
   Apache Beam ingests the data from the data source, converts it to Row
   type, and put it into pipeline. Then, Apache Beam passes the data and
   parameters to the write method of DataLakeIO. In this method, a Spark
   session is created based on these parameters, the data is converted into
   a Dataset, and finally stored in data lake. The process for Apache Beam
   to ingest data from data lake into its pipeline is similar. This
   connector supports the interconnection of Apache Beam with Delta Lake,
   Apache Hudi, and Apache Iceberg, which shows the extensibility for data
   lakes. The experimental result shows the efficiency of DataLakeIO.
CT 9th International Conference on Electronic Technology and Information
   Science (ICETIS)
CY MAY 17-19, 2024
CL Hangzhou, PEOPLES R CHINA
SP IEEE
RI LIU, Yinlong/JFB-1157-2023
ZB 0
Z8 0
ZS 0
ZA 0
TC 0
ZR 0
Z9 0
U1 1
U2 1
BN 979-8-3503-8835-0; 979-8-3503-8834-3
DA 2024-11-01
UT WOS:001298131100146
ER

PT J
AU 이승신
   오염덕
TI The Design of Big Data Railway Safety Platform Architecture and
   Application Model
Z1 빅데이터 철도 안전 플랫폼 아키텍처 설계 및 활용모델
S1 전기학회논문지
VL 73
IS 3
BP 567
EP 575
DT research-article
PD 2024
PY 2024
AB In this study, we propose a big data railway safety platform
   architecture by applying communication and database technologies and
   platform architectures used in many industries for real-time failure and
   anomaly detection of railway operations. There have been studies on big
   data architecture in data collection, communication, storage, and
   analysis areas. However, previous studies have not addressed the design
   of big data architecture for the safe operation of railways
   specifically. Therefore, in this study, in order to collect, store, and
   analyze data that may occur in railway operations, we designed an
   architecture that can be implemented by using currently available
   technologies from the perspective of the entire data life cycle. In
   particular, a combination of MQTT and Kafka was proposed as a message
   and event broker for the railway safety platform architecture, and
   MongoDB was ultimately proposed as a NoSQL database. In addition, the
   application model of the big data railway safety platform was presented
   using the designed architecture, and YOLOv5, an object detection
   algorithm, was used to conduct an experiment on how image data from
   railroad tracks can be used in anomaly detection of railway operations.
   The neural network trained with YOLOv5 can accurately classify eight
   rail components of the railway and also classify the abnormal states of
   the eight components relatively accurately. In subsequent research, we
   plan to implement this architecture as a real big data platform to
   expand anomaly detection experiments on railroad tracks.
Z8 0
ZA 0
TC 0
ZS 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
SN 1975-8359
DA 2025-01-26
UT KJD:ART003058343
ER

PT J
AU Nevzorova, O. A.
   Khakimullin, R. R.
   Idrisov, I. I.
TI Digital scientific platform "Aggregator of unstructured geological and
   field data": architecture and basic models of data extraction
SO GEORESURSY
VL 25
IS 4
BP 149
EP 156
DI 10.18599/grs.2023.4.13
DT Review
PD DEC 30 2023
PY 2023
AB The article describes the project being developed for the digital
   scientific platform "Aggregator of unstructured geological and field
   data", which could potentially be important for the oil and gas
   industry. The use of new intelligent technologies within the framework
   of this project will significantly improve the efficiency of processing,
   storage and use of geological and field information contained in various
   text sources, mainly in field reports.
   The main goal of developing a digital scientific platform is to
   integrate heterogeneous information about the objects of subsurface
   exploration, which is extracted from reports on deposits of the Republic
   of Tatarstan. This will create a consolidated database that will become
   the basis for making informed decisions in the oil and gas sector. The
   project of the digital scientific platform includes the development of
   architecture, algorithms and software solutions based on modern methods
   of text processing and data mining.
RI Nevzorova, Olga/B-7956-2016
ZS 0
Z8 0
ZA 0
ZB 0
TC 0
ZR 0
Z9 0
U1 0
U2 1
SN 1608-5043
EI 1608-5078
DA 2024-10-30
UT WOS:001316647400009
ER

PT P
AU MITTAL S
   TAMMA B R
   INUKONDA M S
TI Method for performing e.g. privacy-preserving            continuous
   internet forensics at enterprise level for            performing
   cybercrime investigations while providing            internet
   protocol-based services in e.g. laptop,            involves generating
   secured data associated with            internet protocol detail record
   and system logs
PN IN202241035158-A
AE INDIAN INST TECHNOLOGY HYDERABAD
AB 
   NOVELTY - The method (400) involves receiving a first                set
   of data from multiple network servers at a                processor
   (S402). A second set of data is received                (S404) at the
   processor from a network address                translation (NAT)
   server. The first set of data and                the second set of data
   are analyzed (S406) in a                parallel manner by the
   processor. Secured data                associated with an internet
   protocol detail record                (IPDR) and NAT-system logs
   (SYSLOGs) are generated                (S408) by the processor based on
   analysis of the                first set of data and the second set of
   data. The                secured data associated with the IPDR and the  
   NAT-SYSLOGs are stored (S410) in a storage unit by                the
   processor through a spooling mechanism. An                entity to
   access the secured data associated with                the IPDR and the
   NAT-SYSLOGs is authorized (S412)                by the processor based
   on a request received from                the entity to access the
   secured data.
   USE - Method for performing network forensics i.e.               
   privacy-preserving continuous internet forensics                (CIF) at
   internet service provider (ISP) and                enterprise level for
   performing investigation, such                as cybercrime
   investigations, intelligence                gathering for national
   security, and investigation                related to other crimes while
   providing IP-based                services in different devices. Uses
   include but are                not limited to a laptop, a phone, a
   tablet, a                workstation, and a server.
   ADVANTAGE - The method enables reducing storage               
   requirements for performing network forensics by                the ISP
   or an enterprise, and performing analytics                on the stored
   IPDRs and NAT-SYSLOGs for                intelligence gathering, crime
   investigation, and                checking compliance of requirements
   related to                storage of IPDR and the NAT- SYSLOG by the ISP
   or                the enterprise, and utilizing a data lake based       
   security information and event management system               
   (DL-SIEM) with a compact storage format to consumes                less
   storage, thus allowing rapid retrieval for                doing big-data
   analytics, and optimizing storage                resulting in decrease
   of total cost of performing                network forensics by
   utilizing DL-SIEM.
   DETAILED DESCRIPTION - The network servers comprise a remote            
   authentication dial-In user service (RADIUS)                server, a
   dynamic host configuration protocol                (DHCP) server, an
   internet protocol (IP) address                management (IPAM) server,
   and a domain name system                (DNS) server.An INDEPENDENT
   CLAIM is included for a system                for performing network
   forensics.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for performing network forensics.400Performing network
   forensicsS402Receiving first set of data from network               
   servers at processorS404Receiving second set of data at processor       
   from NAT serverS406Analyzing first set of data and second               
   set of data in parallel manner by processorS408Generating secured data
   associated with                IPDR and NAT-SYSLOGs by processor based
   on analysis                of first set of data and second set of
   dataS410Storing secured data associated with IPDR                and
   NAT-SYSLOGs in storage unit by processor                through spooling
   mechanismS412Authorizing entity to access secured data               
   associated with IPDR and NAT-SYSLOGs by processor                based
   on request received from entity to access                secured data
Z9 0
U1 0
U2 0
DA 2024-03-08
UT DIIDW:202421832R
ER

PT P
AU ZHANG X
   ZENG J
   LIU G
   SUN Y
   HONG H
   PANG Z
   LIU M
TI Method for processing power metering data by using            electronic
   device, involves storing target power meter            data in data
   lake, and integrating distributed file            system through data
   warehouse, so that data warehouse            accesses data in lake
PN CN117171276-A
AE SHENZHEN ZHIXIN MICROELECTRONIC CO LTD; CHINA GRIDCOM CO LTD
AB 
   NOVELTY - The method involves obtaining original power               
   metering data from a database through a flow                processing
   technology. The data in the data lake is                stored in a
   distributed file system. The                distribution file system is
   integrated through a                data warehouse such that the data
   warehouse                accesses the data in a data lake, where the
   data is                provided with the target power meter data. The   
   target power meter data is encrypted and decrypted                to
   obtain the data of the data. The original power                metering
   data is unstructured data.
   USE - Method for processing power metering data by                using
   an electronic device (claimed).
   ADVANTAGE - The method enables improving the data               
   management efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:a data processing device; anda computer-readable storage medium
   storing a                set of instructions for processing power
   metering                data by using an electronic device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for processing power metering data by using                an
   electronic device (Drawing includes non-English                language
   text).
Z9 0
U1 0
U2 0
DA 2024-01-07
UT DIIDW:2023C9679B
ER

PT P
AU ZHANG M
TI Method for monitoring data loading in data lake of            big data
   platform by using computer device, involves            obtaining data
   file set transmitted by upstream service            system in monitoring
   period, and determining data date            according to date interval
   between date of data and            receiving date
PN CN117008991-A
AE BANK CHINA LTD
AB 
   NOVELTY - The method involves obtaining a data file set               
   transmitted by an upstream service system in a                monitoring
   period. T+n for each data file is marked                according to the
   data date and receiving date of                each data file in the
   data file set, where T                represents the data date of each
   data file, n                represents the date interval between the
   data date                of each data file and the receiving date of
   each                data file received by the data lake. The average    
   loading time of the data file set is obtained                according
   to the T+n of each data file in the data                file set and the
   loading finishing time of each                data file. Determination
   is made to check whether                the loading of the data file set
   is abnormal                according to the average loading time.
   USE - Method for monitoring loading of data in a                data
   lake of a big data platform for receiving and                storing a
   data file set transmitted by multiple                upstream business
   systems and providing the data                service for the downstream
   of multiple tenants by                using computer device (claimed).
   ADVANTAGE - The method involves obtaining the data file               
   set transmitted by the upstream service system in                the
   monitoring period, and marking the data date                and
   receiving date of each data file, and thus                enables to
   accurately determine whether the loading                is abnormal, and
   also accurately reacts the loading                time length of data
   file and accurately determines                the abnormal loading.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:a device for monitoring data loading;                anda computer
   readable storage medium comprising                set of instructions
   for monitoring data                loading.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for monitoring data loading in data lake                of
   big data platform by using computer device                (Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2023B85153
ER

PT P
AU JIANG M
   GAN Y
   ZHAO Z
TI Method for optimizing heterogeneous data            synchronous task
   parameter based on metadata, involves            combining actual
   environment, task bias and            comprehensive cost value to
   generate suggestion            parameter value list of synchronous task
PN CN116955486-A
AE HANGZHOU DTWAVE TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves collecting a metadata               
   object related to heterogeneous data                synchronization. The
   data synchronization task is                inputted. An object list
   related to the data                synchronization tasks is obtained
   through reading                the heterogeneous task. Cost analysis is
   performed                on the obtained object list. A suggested       
   optimization parameter value list is outputted. An                actual
   environment, a task bias and a comprehensive                cost value
   are combined to generate a suggestion                parameter value
   value list of the data synchronous                task. The generated
   parameter value is used to                supplement or replace the
   original parameter value.                The task is submitted and
   actually operated.
   USE - Method for optimizing heterogeneous data               
   synchronous task parameter based on metadata in                large
   data warehouse and data lake                construction.
   ADVANTAGE - The method enables realizing the task               
   parameter optimization under the heterogeneous data               
   synchronization scene, the influence evaluation                check
   before the task is executed, improving the                efficiency of
   the data synchronization, and                ensuring the stability of
   the whole                environment.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for optimizing heterogeneous data                synchronous task
   parameter based on metadata                (Drawing includes non-English
   language                text).
Z9 0
U1 0
U2 0
DA 2023-11-17
UT DIIDW:2023B4770R
ER

PT P
AU ZHANG J
   KE Z
   HUANG Y
   DONG K
TI Power supply system for supplying power to network            device,
   comprises accommodating space filled with            liquid coolant for
   cooling power supply module, where            wire-diameter current
   density of primary winding is            greater than or equal to
   specific value
PN US2023337405-A1; US12279403-B2
AE DELTA ELECTRONICS (SHANGHAI) CO LTD
AB 
   NOVELTY - Power supply system (1) comprises a power               
   supply module including a primary circuit (10) with                a
   primary switch to receive an alternating current                (AC)
   input voltage and convert the AC input voltage                into a
   voltage. The transformer includes a primary                winding and a
   secondary winding to convert the                voltage into another
   voltage. The accommodating                space is provided between a
   housing and the module,                where the space is filled with a
   liquid coolant for                cooling the module. The wire-diameter
   current                density of the primary winding is greater than or
            equal to 15 Ampere per square millimeter                (A/mm).
   USE - Power supply system for supplying power to a               
   network device i.e. data center, in a distributed               
   computing architecture e.g. artificial intelligence               
   architecture, cloud computing architecture and big                data
   architecture.
   ADVANTAGE - The power density of the power supply module               
   is greater than or equal to 60 W/inch 3, and the               
   stereoscopic accommodating space is filled with the               
   liquid coolant for cooling the module, thus                improving the
   power density and the power level of                the network device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a
   power                supply module in the power supply system.1Power
   supply module10Primary circuit11PFC circuit30Secondary circuit
Z9 0
U1 0
U2 0
DA 2023-11-03
UT DIIDW:2023A8502P
ER

PT P
AU FANG Z
   YU Z
   ZHANG H
   ZHU Y
TI Method for performing data encryption and            decryption based on
   data classification in data            security field, involves
   establishing national security            channel between local security
   password services for            data synchronization after
   ultra-administrators stored            by security administrator are
   authenticated
PN CN116861451-A; CN116861451-B
AE BEIJING COMPUTER TECHNOLOGY & APPL RES; BEIJING COMPUTER TECHNOLOGY &
   APPLICATIO
AB 
   NOVELTY - The method involves transmitting internal               
   security level through HTTP and FTP network                protocol for
   secret data. Determination is made to                check whether there
   will be cross-level,                cross-network, cross-domain
   transmission in a data                transmission under condition of
   ensuring absolute                safety of data through three different
   personnel                system administrators, audit administrators. A 
   national security channel is established between                two
   local security password services for data                synchronization
   after three ultra-administrators                stored by the security
   administrator are                authenticated, where the data in the   
   synchronization process does not fall to a ground.               
   Message abstract of plaintext data is obtained by                MD5,
   SHAor SM3using national secret                algorithm.
   USE - Method for performing data encryption and               
   decryption based on data classification in data                security
   field. Can also be used in cloud, Internet                of things and
   artificial intelligence fields.
   ADVANTAGE - The method enables enhancing the security of               
   data in storage and transmission, the key is                managed by
   the system, so as to avoid the                possibility of human key
   leakage. The method                enables classifying and grading the
   data, and                safely storing and safely transmitting the data
   in                various forms, such as data center, big data and      
            data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of a system for performing data encryption and                decryption
   based on data classification. (Drawing                includes
   non-English language text).
Z9 0
U1 0
U2 0
DA 2023-10-30
UT DIIDW:2023A7624Y
ER

PT P
AU GUO H
TI Bank data processing system for use in field of            big data, has
   bank service data system for invoking            data processing
   operator corresponding to data storage            policy to process data
   by calculation engine and            storing processed data to data lake
   through storage            engine
PN CN116841980-A
AE BANK CHINA LTD
AB 
   NOVELTY - The bank data processing system has a data               
   collection cluster, a data lake, a public function                layer
   and an infrastructure layer. The data lake                provides data
   processing operator and data storage.                The different data
   processing operators are used                for executing different
   data treatment operations.                The infrastructure layer
   provides a storage engine                for realizing storage function,
   a calculation                engine for calculation function, and a
   calculation                resource and task management engine for      
   distributing resource and tasks management. The                data
   collection cluster is connected with the bank                service
   data system. The common functional layer is                configured to
   receive data batch processing                task.
   USE - Bank data processing system for use in the                field of
   big data.
   ADVANTAGE - The system receives the historical data and               
   real-time data in each bank service data system                through
   the data collection cluster, which can                improve the
   sharing degree of the data. The system                improves the
   efficiency of data statistical                analysis.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of bank   
   data processing system for use in the field of big                data.
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2023-10-26
UT DIIDW:2023A6959D
ER

PT P
AU XU B
   GENG Z
   YANG Q
   XU H
   XIAO Z
TI Heterogeneous data integration method based on            virtualization
   technology, involves obtaining optimal            integration solution
   of heterogeneous database to be            integrated or data source
   table
PN CN116701504-A
AE YUNNAN POWER GRID CO LTD INFORMATION CEN
AB 
   NOVELTY - The method involves combining the open source               
   language to construct the data source linker                according to
   the heterogeneous database to be                integrated or/and the
   data source table. The                metadata information of the target
   data source in                the data source linker is extracted. A    
   corresponding packaging table for packaging is                generated,
   and a mapping virtual table is                established. The data
   service issue on the mapping                virtual table is performed,
   and an optimization                operation by combining with the data
   virtualization                engine is performed. The optimal
   integration                solution of the heterogeneous database to be 
        integrated or/and the data source table is                obtained.
   USE - Heterogeneous data integration method based on               
   virtualization technology for use in data                integration
   system (claimed) such as data warehouse                system and data
   lake system.
   ADVANTAGE - Reduces the difficulty of data processing by               
   setting uniform data access and virtual data market                and
   autonomous service analysis, simplifies the                data
   integration mode, data development speed, has                high
   flexibility, can access in fast and controlled                mode, and
   has low use cost.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a heterogeneous data integration system based               
   on virtualization technology;a computer device; anda computer readable
   storage medium storing                heterogeneous data integration
   program based on                virtualization technology.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating
   a                method for integrating heterogeneous data based on     
   virtualization technology and a system. (Drawing                includes
   non-English language text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202396227N
ER

PT P
AU HU J
TI Method for testing data warehouse for big data            application,
   involves obtaining test data according to            production data,
   and utilizing test data for testing            test environment data
   warehouse in test            environment
PN CN116627822-A
AE BANK CHINA LTD
AB 
   NOVELTY - The method involves copying (S11) production               
   data in a production environment by copying the                first
   data of an attached source layer of a data                lake and the
   second data of a collected data layer                of the production
   environment data warehouse. The                production data is
   provided with data of an input                data lake and data
   obtained by the production                environment data warehouse
   operation. The data lake                is arranged in the production
   environmental data                warehouse. Test data is obtained (S12)
   according to                the production data. The test data is
   utilized                (S13) to test the test environment data
   warehouses                in the testing environment.
   USE - Method for testing a data warehouse for big                data
   application and financial application.
   ADVANTAGE - The method can obtain enough test data through              
   the recording of the data, and reduce the                dependence on
   the upstream service. The structure                model of data table
   of upstream service does not                influence the test of test
   environment data                warehouse, thus the test efficiency of
   data                warehouse can be improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:(1) a data warehouse testing device;(2) a storage medium
   storing a program for                testing data warehouse; and(3) an
   electronic device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the method for testing the data warehouse.
   (Drawing                includes non-English language text).S11Step for
   copying production data in a                production
   environmentS12Step for obtaining test data according to               
   the production dataS13Step for utilizing test data to test the          
   test environment data warehouses in the testing               
   environment
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202391626A
ER

PT P
AU RENICK D
TI Method for ingesting data into data lake by using            data
   ingestion pipelines to generate actionable            business insights
   and analytics from big data for use            in organization, involves
   initiating data pipeline by            performing lookup activity
PN US2023259518-A1
AE INSIGHT DIRECT USA INC
AB 
   NOVELTY - The method (100) involves initiating (110) a               
   data pipeline by performing a lookup activity,                which
   includes querying a data structure to                retrieve metadata
   corresponding to data sources, by                a computer device. The
   metadata stored in the data                structure includes
   information that enables                identifying and connecting to
   each of the data                sources. A sequence of activities are
   performed for                each of the data sources by the computer
   device.                Connection is performed (120) to a respective
   data                source by using a portion of the metadata that      
   corresponds to the respective one of the data                sources to
   form a connection to the respective one                of the data
   sources. A data object specified by the                portion of the
   metadata that corresponds to the                respective one of the
   data sources is accessed                (122) from the respective one of
   the data sources                through the connection. The data object
   is stored                (124) in the data lake, which is remote from
   each                of the data sources.
   USE - Method for ingesting data into a data lake by                using
   data ingestion pipelines to generate                actionable business
   insights and analytics from big                data for use in
   organization.
   ADVANTAGE - The method can greatly reduce the time needed               
   from developers to establish and maintain                connections to
   an organization's data sources. The                method provides
   secure access credentials storage,                that allows an
   additional level of security to be                built into the
   pipeline. The method allows the                organization to analyze
   the business data by using                business intelligence (BI)
   technologies to generate                actionable business insights and
   analytics from big                data. The data pipeline integrates
   data source                metadata to ingest large amounts of data from
   multiple data sources into cloud-based data storage                in a
   secure and scalable manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a method
   for forming a data pipeline                template for ingesting data
   into a data lake;(2) a data pipeline system for ingesting data          
        into cloud storage.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of method for 
   ingesting data into a data lake by using data                ingestion
   pipelines to generate actionable business                insights and
   analytics from big data for use in                organization.100Data
   ingesting method110Initiating a data pipeline by performing a           
   lookup activity120Performing connection to a respective data            
   source by using a portion of the metadata122Accessing a data object
   specified by the                portion of the metadata124Storing the
   data object in the data                lake
Z9 0
U1 0
U2 0
DA 2023-08-31
UT DIIDW:202385386Y
ER

PT P
AU WU X
   WU W
   ZHANG G
   ZHAO Y
TI Method for automatically optimizing table data            structure
   layout in big data and data lakes, involves            using k8s target
   service to set target task, operating            optimized task through
   set target task, obtaining            related operation result of data
   lake iceberg            table
PN CN116578570-A; CN116578570-B
AE BEIJING DEEPEXI TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves obtaining (S1) resource               
   information to be optimized. The optimized resource               
   information is used (S2). Resource binding between                k8s
   and the data lake iceberg table is performed.                The
   metadata information of the iceberg table and                the result
   of the isolated resource is obtained.                The metadata
   information of the iceberg table and                the result of the
   isolation resource is used (S3).                An optimization task is
   automatically generated.                The k8s target service to is
   used set the target                task. The optimized task through the
   set target                task is operated. The related operation result
   of                the data lake iceberg table is obtained.
   USE - Method for automatically optimizing table data               
   structure layout in big data and data lakes.
   ADVANTAGE - The application realizes the table               
   optimization through the step of optimizing task,                which
   reduces the small file number of the table,                delete the
   file number, through rewriting the table                structure, which
   can improve the query performance                of table. At the same
   time, when optimizing the                table in the optimized task,
   the resource setting                of table optimization can be
   automatically                generated through the task generation rule
   and the                scoring rule, the user does not need to care the 
   small number of table and delete file number. Thus,                the
   operation and maintenance efficiency of the                system can be
   improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a system for automatically optimizing table               
   data structure layout; anda device for automatically optimizing table   
               data structure layout.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method  
   for automatically optimizing table data structure                layout.
   (Drawing includes non-English language                text)S1Step for
   obtaining resource information to                be optimizedS2Step for
   using optimized resource                informationS3Step for using
   metadata information of                iceberg table and junction of
   isolation                resourceS4Step for setting target task by using
   k8s                target service
Z9 0
U1 0
U2 0
DA 2023-09-02
UT DIIDW:202387351X
ER

PT P
AU LIU X
   ZHANG K
   CHEN M
   ZHAO H
   HUANG C
TI Data lake-based spatial large data processing            system, has
   obtaining module for obtaining multiple            types of space large
   data in different data sources,            and data lake analyzing
   module for storing multiple            kinds of structured data to table
PN CN116501810-A; CN116501810-B
AE BEIJING GISTACK INFORMATION TECHNOLOGY; GEOSCENE INFORMATION TECHNOLOGY
   CO LTD
AB 
   NOVELTY - The system has an obtaining module (1) for               
   obtaining multiple types of space large data in                different
   data sources. A data lake-entering module                (2) stores the
   space big data in the different data                source to data lake.
   A data lake analyzing module                (3) analyzes space-large
   data to obtain structured                data corresponding to each type
   of the space data,                where the structured data is stored in
   a first                Delta table. Different data sources comprise a
   file                type server data source, an APIserver data source,  
    a database type data source and a file type data                source.
   USE - Data lake-based spatial large data processing               
   system for realizing uniform storage of space large                data
   in a file system and a database.
   ADVANTAGE - The system realizes uniform storage of large               
   data in space.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
       data lake-based spatial large data processing                method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a data 
   lake-based spatial large data processing system                (Drawing
   includes non-English language                text).1Obtaining
   module2Data entry module3Data lake analyzing module4Data lake analysis
   module5Data lake management module21File data source access
   sub-module22Api lake-entering sub-module23Database access
   sub-module24Manual file uploading sub-module31Json data analysis
   sub-module32Shp data analysis sub-module33Gdb data analyzing
   sub-module34Image data analyzing sub-module35Picture data analyzing
   sub-module41Data lake calculating sub-module42Data layer data
   distribution                sub-module51Metadata management
   sub-module52Space large data preview sub-module
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2023820675
ER

PT P
AU ZHOU J
   DING B
   SUN X
   WANG Y
   WANG H
TI Data relationship identification method in a data            lake of a
   storage system in business field, involves            determining data
   relation identification result            according to original label
   sequence and predicted            label sequence
PN CN116467500-A; CN116467500-B
AE ALIBABA CHINA CO LTD
AB 
   NOVELTY - The method involves obtaining (302) an                original
   annotation sequence group and a guide                annotated sequence
   group. The original annotation                sequence group is provided
   with original data                sequence and original tag sequence.
   The guide                annotation sequence group is provided with a
   guide                data sequence and a guide tag sequence. The        
   original data sequence and the guide data sequence                are
   respectively provided with data in at least two                data
   sources. The guide label sequence group and                the original
   data sequence are inputted (304) into                a data relation
   identification model to obtain a                prediction label
   sequence corresponding to the                original data sequence. The
   guide label sequence                group is used for guiding the data
   relation                identification model to predict the original
   data                sequence. The data relation identification result   
   is determined (306) according to the original label               
   sequence and the predicted label sequence.
   USE - Method for identifying data relationship in a                data
   lake of a storage system for storing data in                an original
   format for storing, processing and                protecting large
   quantities of structured,                semi-structured and
   unstructured data in business                field.
   ADVANTAGE - The method shortens the task logic link,               
   reduces the task difficulty and improves the data               
   relation identification efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:an automatic question and answering                method;a
   query statement generating method;a computing device; anda
   computer-readable storage medium storing                program for data
   relationship identification,                automatic question and
   answering or query statement                generation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method
   for                identifying a data relationship. (Drawing includes   
   non-English language text)302Step for obtaining an original annotation  
   sequence set and a guide annotation sequence                set304Step
   for inputting the guiding label                sequence group and
   original data sequence into the                data relation
   identification model to obtain the                prediction label
   sequence306Step for determining the data relation               
   identification result according to the original                label
   sequence and the predicted label                sequence
Z9 0
U1 0
U2 0
DA 2023-08-25
UT DIIDW:202380300P
ER

PT P
AU ZHANG M
   ZHANG Y
   HU Z
   TAO D
   ZHOU F
   CHEN K
   ZONG W
   WU Z
   LV F
   HE M
   WANG L
TI Lake-bin integration-based power distribution           
   internet-of-things data real-time processing system,            has
   application data processing unit for performing            service
   classification to data processed by aggregation            association
   processing unit and outputting data to            database
PN CN116431635-A
AE NARI TECHNOLOGY CO LTD
AB 
   NOVELTY - The system has an internet-of-things               
   pre-collecting module for collecting power                distribution
   internet of things data and sending it                to a lake-bin
   integrated core processing module.                The lake bin
   integrated core treatment module is                provided with a data
   processing unit. The data                processing units are provided
   with an original data                table for writing the power
   distribution Internet                of Things data into the data lake
   table to generate                an original table. A dimension model
   processing                unit combines the different data tables of the
   related services in multiple streams into the same                data
   lake tables to generate a dimension table. An                aggregation
   correlation processing unit performs                correlation
   processing on the dimension table and                the fact table. The
   application data treatment unit                performs service
   classification on the data                processed by the aggregation
   association processing                unit and outputs the data to the
   database.
   USE - Lake-bin integration-based power distribution               
   internet-of-things data real-time processing                system.
   ADVANTAGE - The system can perform effective data               
   processing and storage on mass heterogeneous data,                so as
   to improve confusion of data calculation                analysis and
   statistics, reduce repeated                calculation, realize
   multiplexing of calculation                result, improve quick
   retrieval efficiency of data,                at the same time, can
   quickly provide                multi-dimensional data support for the
   application                side, and ensure the response performance of
   the                large-scale complex data calculation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:(1) a lake-bin integration-based power                distribution
   internet-of-things data real-time                processing method;(2)
   an electronic device for realizing                lake-bin
   integration-based power distribution                internet-of-things
   data real-time processing                process; and(3) a computer
   readable storage medium                comprising a set of instructions
   for realizing                lake-bin integration-based power
   distribution                internet-of-things data real-time processing
                  process.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   lake-bin integration-based power distribution               
   internet-of-things data real-time processing system               
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2023-08-16
UT DIIDW:2023774347
ER

PT P
AU MATAWALA H S
   BHAT S G D
   MAKHIJA S
TI Data lake for self-driven system for operating            applications
   such as enterprise resource planning and            supply chain
   management applications, comprises            functional database stores
   a library of functions to            perform functions of the
   applications
PN US2023214448-A1
AE NB VENTURES INC DBA GEP
AB 
   NOVELTY - The data lake (108) comprises relational               
   database (122) and non-relational databases (123)                to
   store multiple structured and unstructured data                received
   from distinct sources in real-time.                Functional database
   (124) stores a library of                functions to perform functions
   of the applications.                Data model database (126) store data
   models. Data                models are re-calibrated on the basis of
   predicted                impact of characteristic of attribute of the
   stored                data on the applications. Processor (114) is      
   communicatively coupled to the data lake to                identify
   multiple data models to generate an impact                data for
   predicting impact of the change on the                applications.
   USE - Data lake for self-driven system for operating               
   applications such as enterprise resource planning                and
   supply chain management applications.
   ADVANTAGE - The self-driven system is capable of producing              
   accurate, faster and efficient data results. The                system
   utilizes sub network of devices and server                for secured
   communication with reduced processing                time due to
   automatic creation of scripts by a bot                based on the data
   models, the change in the at                least one attribute, the
   impact data and artificial                intelligence based processing
   logic for                recommending an action/task to a user.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(a) a system
   with data lake for storing data                from distinct sources in
   real-time;(b) a computer program product for operating               
   multiple application of a computing device with                memory;
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of    
   artificial intelligence based self-driven system                for
   operation of applications.108Data lake114Processor122Relational
   database124Functional database126Data model database
Z9 0
U1 0
U2 0
DA 2023-07-15
UT DIIDW:202367792D
ER

PT P
AU CAO Y
   NIU N
   ZHAO Z
   WANG C
   WANG X
   WANG D
   BIAN Q
   CHEN Z
TI Method for performing multi-source heterogeneous            data fusion
   based on coal mine information physical            system in production
   and environmental protection            fields, involves converting data
   in M-CPS data lake            into M-CPS multi-source data fact table,
   and realizing            abstract fusion of data semantic level
PN CN116340885-A; CN116340885-B
AE UNIV TAIYUAN TECHNOLOGY
AB 
   NOVELTY - The method involves establishing an M-CPS data               
   logic model through a global data modeling                technology to
   realize a uniform modeling of each                inter-subsystem data
   of the coal mine information                physical system M-CPS. A
   large data technology is                used to collect data from M-CPS
   different service                subsystem. An M-CPS data lake is used
   as a data                cache layer of the M-CPS warehouse laking      
   integrated data warehouse to enable it to contain                and
   process a multi-source heterogeneous data. A                data is
   converted in the M-CPS data lake into M-CPS                multi-source
   data fact table to realize abstract                fusion of data
   semantic level.
   USE - Method for performing multi-source                heterogeneous
   data fusion based on coal mine                information physical
   system in production,                equipment, security and
   environmental protection                fields.
   ADVANTAGE - The method enables realizing the coal flow,               
   wind flow, flow, current, logistics, vehicle flow                and
   other heterogeneous data flow cross-system               
   interconnection, integrated and shared closed-loop               
   automatic flow. The data in the full M-CPS is                uniformly
   modelled through global data modeling                technology to
   realize uniform modeling of each                inter-subsystem data of
   coal mine information                physical system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for performing multi-source                heterogeneous data
   fusion (Drawing includes                non-English language text).
Z9 0
U1 0
U2 0
DA 2023-07-15
UT DIIDW:2023694628
ER

PT P
AU ZHANG T
   CHEN Z
   LIU Z
   YU C
   WANG P
   WANG Q
   CHEN W
   LIU Y
   LIU H
TI Method for realizing delta lake data lake index            based on
   Elasticsearch, involves locating storage            position of content
   corresponding to keyword, and            entering positioning position
   according to selection of            user
PN CN116340317-A
AE NANHU LAB
AB 
   NOVELTY - The method involves extracting the source                data,
   and forming a structured Dataset data set                based on Spark.
   The Schema analysis on the                extracted data set is
   performed, and the data                storage address information is
   increased to form an                index structure. The content of the
   index structure                is converted into the support format of
   the search                server and storing in the index database. The 
   keyword input by the user is received by the search               
   server. The search is started based on the index               
   database, and the storage position of the content                is
   located corresponding to the keyword. The                positioning
   position is entered according to the                selection of the
   user.
   USE - Method for realizing delta lake data lake                index
   based on Elasticsearch (RTM: open-source                computer
   programming language developed and                marketed by
   Microsoft(RTM: Company Name)).
   ADVANTAGE - The method enables realizing automatic               
   generating data index scheme on the Delta Lake,                storing
   or searching according to the index scheme,                providing
   efficient index organization and fast                query to support
   high performance of large data                analysis, and realizing
   full fuzzy matching, data                isolation and data fusion
   freely interactive                matching and using full text search
   theory of                Elasticsearch, the multi-source heterogeneous
   data                to reach the purpose of full-text search, and       
       automatically optimizing cache data in the data                lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a  
   method for realizing Deltalake data lake index                based on
   Elasticsearch. (Drawing includes                non-English language
   text)
Z9 0
U1 0
U2 0
DA 2023-07-15
UT DIIDW:202369461J
ER

PT P
AU ZHAO Y
   FAN D
   HE H
TI Method for organizing fishery resource environment            data based
   on English extract-transform-load structure,            involves
   classifying data of multi-source heterogeneous            data set, and
   uploading multi-level ocean data sets to            internet database
PN CN116340435-A
AE ZHUHAI MANMAN TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves obtaining (S1) marine               
   environmental monitoring data from multiple marine               
   environment monitoring platforms. A multi-source               
   heterogeneous data set is obtained (S3), where the                data
   in the multi- source heterogeneously data set                is provided
   asmarine environmental data. The data                of multi-source
   heterogeneous data set is                classified (S2) according to
   data use. The initial                standardized classification
   heterogeneous sets are                stored (S4) in a temporary
   database. The secondary                data standardization processing
   is performed (S5)                on data in the initial standardized
   classifications                in the temporary database to obtain a set
   of                standardized fishery resource environmental data      
   sets to obtain the data of the standard fishery                resource
   environmental data set. The multiple                multi-level ocean
   data set are obtained (S6). The                multilevel ocean data
   sets are uploaded (S7) to the                internet database.
   USE - Method for organizing multi-source                heterogeneous
   fishery resource environment data                based on ETL structure
   in ocean field.
   ADVANTAGE - The method enables converging and uniformly               
   processing data in the ocean environment monitoring               
   platform, so that the ocean data standard is                unified, and
   the data is polynary and full-sided by                data collection.
   The method enables satisfying                timeliness and correctness
   requirement of marine                environment element data
   architecture, data                integrated layer and data subsequent
   processing and                application and providing consistent data
   space                resolution.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a fishery   
   resource environment data organization system based                on
   English extract-transform-load (ETL)                structure.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the Method for organizing multi-source      
   heterogeneous fishery resource environment data                based on
   ETL structure in ocean field (Drawing                includes
   non-English language text).S1Step for obtaining marine environmental    
   monitoring data from multiple marine environment               
   monitoring platformsS2Step for classifying data of multi-source         
   heterogeneous data set according to data useS3Step for obtaining
   multi-source                heterogeneous data setS4Step for storing
   initial standardized                classification heterogeneous sets in
   temporary                databaseS5Step for performing secondary data   
   standardization processing on data in initial               
   standardized classifications in temporary database                to
   obtain set of standardized fishery resource                environmental
   data sets to obtain data of standard                fishery resource
   environmental data setS6Step for obtaining multiple multi-level         
   ocean data setS7Step for uploading multilevel ocean data               
   sets to internet database
Z9 0
U1 0
U2 0
DA 2023-08-07
UT DIIDW:202378568R
ER

PT P
AU SHI Y
   LI H
   XU H
   XIAO Z
   CHEN B
TI Method for performing data management based on            data weaving
   framework, involves constructing data            virtualization engine,
   and constructing dataOps data            development and management
   system for realizing agile            and high quality of data delivery
PN CN116303336-A
AE YUNNAN POWER GRID CO LTD INFORMATION CEN
AB 
   NOVELTY - The method involves constructing an active               
   metadata management tool. A metadata knowledge                graph is
   formed. A panoramic data image is                generated. The metadata
   is deeply excavated based                on a service experience and a
   machine learning                model. An intelligent recommendation
   engine is                formed based on a deep excavated metadata. The 
   active metadata is taken as a core. An artificial               
   intelligence and the machine learning model are                used to
   realize the data automatic cataloe. A data                virtualization
   engine is constructed. A DataOps                data development and
   management system is                constructed for realizing agile and
   high quality                data delivery.
   USE - Method for performing data management based on                data
   weaving framework of enterprise data volume                and data
   demand in data technology. Uses include                but are not
   limited to data warehouse, data lake,                NoSQLdatabase, OLAP
   database and real-time data                source.
   ADVANTAGE - The method enables directly using the service               
   user by the data analysis result and form                prediction
   capability, without repeating complex                data scientific
   work, realizing the data delivery                of extremely agile, at
   the same time through                active, intelligent, continuous
   data management                makes the data architecture continuous
   health,                providing more value than the traditional data   
   management. By optimizing the discovery and access                of
   cross-source heterogeneous data, the trusted                data is
   delivered to all related data consumers in                flexible and
   business understanding manner in a                manner from all data
   sources, the data consumer                self-service and efficient
   cooperation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                of a method for performing data management
   based on                data weaving framework (Drawing includes        
          non-English language text).
Z9 0
U1 0
U2 0
DA 2023-07-21
UT DIIDW:202371476N
ER

PT P
AU YANG H
   JIANG X
   FAN W
TI Method for utilizing construction and data of coal            mine
   safety large data lake, involves classifying coal            mine data
   with different data quantity, data frequency            and priority
   level, and storing and sharing each            data
PN CN116304256-A
AE BEIJING FANMI INTERNET THINGS TECHNOLOGY CO              LTD
AB 
   NOVELTY - The method involves using (S1) a graph               
   structure for coal mine integral data modeling. A                sensing
   number and a measuring point code are                applied (S2) to
   mark a device parameter. A                multi-programming language is
   realized (S3) through                an intelligent strategy task, a
   script and a big                data task for intelligent strategy
   arrangement                scheduling. Coal mine full information data
   is                entered (S4) into a lake based on the data modeling   
   and the intelligent strategy. The coal mine data is               
   classified (S5) with different data quantity, data               
   frequency and priority level. Each data is stored                and
   shared.
   USE - Method for utilizing construction and data of                coal
   mine safety large data lake.
   ADVANTAGE - The method enables using a graph structure for              
   coal mine integral data modeling, and applying the               
   sensing number and the measuring point code to mark                the
   parameter of the device, thus improving mass                data storage
   computing capability in the                construction process,
   strengthening the data asset                line and normalizing on the
   management,                automatically facilitating collaborative
   production                of the coal enterprise, performing safety     
   monitoring of intelligent linkage, providing                emergency
   rescue scientific and precise service                scene to provide
   omni-directional application of                data driving as core and
   ensure continuity of the                data, thus forming a higher
   value of data                asset.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a coal mine safety large data lake
   construction and                data utilization method (Drawing
   includes                non-English language text).S1Step for using
   graph structure for coal                mine integral data
   modelingS2Step for applying sensing number and                measuring
   point code to mark device                parameterS3Step for realizing
   multi-programming                language through intelligent strategy
   task, script                and big data task for intelligent strategy  
   arrangement schedulingS4Step for entering coal mine full               
   information data into lake based on data modeling                and
   intelligent strategyS5Step for classifying coal mine data with          
   different data quantity, data frequency and                priority
   level
Z9 0
U1 0
U2 0
DA 2023-07-27
UT DIIDW:2023758507
ER

PT P
AU ZHANG K
   LI Y
   DENG L
   LI B
   AI C
   YIN C
TI Data intelligent interaction method based on cloud            native and
   big data architecture used in internet data            identification
   technology field involves constructing            demand-side data
   sharing and knowledge-driven smart            government services, and
   acquiring spatial data based            on user behavior
PN CN116307757-A; CN116307757-B
AE LIAONING BRINGSPRING SMART WE CLOUD TECH
AB 
   NOVELTY - The method involves constructing (S1) a               
   multi-level and multi-granularity holographic image                of
   the city information unit, based on the                information
   channel coupling of the knowledge map.                The implicit
   association between the multi-level                information units of
   the city is obtained.                Multi-modal government affairs data
   and social                sensing data are integrated (S2). Group
   behavior                evolution process is reconstructed (S3) by
   fusing                multi-modal spatio-temporal data. Demand-side data
   sharing and knowledge-driven smart government                services
   are conducted (S4). Spatial data is                acquired (S5) based
   on user behavior to actively                serve targets.
   USE - Data intelligent interaction method based on                cloud
   native and big data architecture used in                internet data
   identification technology                field.
   ADVANTAGE - The method breaks through technologies such as              
   transparent management of massive data, urban                management
   collaboration technology, integrated                service platform for
   urban management, intelligent                urban management services,
   and develops a                visualized, iterative, and evaluable
   transparent                management and intelligent service platform
   for big                data in government affairs to realize            
   cross-departmental, multi-scale, and                multi-dimensional
   transparent management of big                data in government affairs
   and the supply of                efficient, fast and non-perceptual
   intelligent                services.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:1. a data intelligent interaction system based               
   on cloud native and big data architecture                implementing
   the data intelligent interaction                method based on cloud
   native and big data                architecture; and2. a computer device
   for data intelligent                interaction.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the data intelligent interaction method based on                cloud
   native and big data architecture used in                internet data
   identification technology field.                (Drawing includes
   non-English language text)S1Step for constructing the multi-level and   
   multi-granularity holographic image of the city               
   information unitS2Step for integrating multi-modal government           
   affairs data and social sensing dataS3Step for reconstructing group
   behavior                evolution processS4Step for conducting
   demand-side data                sharing and knowledge-driven smart
   government                servicesS5Step for acquiring spatial data
Z9 0
U1 0
U2 0
DA 2023-07-23
UT DIIDW:2023702124
ER

PT P
AU ZHAO J
   SHEN J
   LIU Y
   YANG L
   TANG L
TI Operating system based on industrial internet            platform of
   computer device, has application support            micro-service set
   layer that is used for realizing            agile development facing to
   manufacturing full life            cycle industrial application
PN CN116301825-A
AE BEIJING AEROSPACE SMART MFG TECHNOLOGY
AB 
   NOVELTY - The operating system has a data collecting               
   micro-service set layer that is used for connecting                mass
   industrial device, industrial system,                industrial product,
   and for realizing the template                access of industrial
   resource. A heterogeneous                large data lake layer is
   operated on the basic                operating environment for
   collecting and                integrating the multi-source heterogeneous
   large                data, standardized management and comprehensive    
   analysis. A core industrial component layer                provides
   industrial resource data and the relation                of the
   structured, normalized expression,                standardized package
   for supporting the                micro-service set API interface. An
   application                support micro-service set layer is used for  
   manufacturing support, device asset management type                data
   and query analysis, and for realizing agile                development
   facing to manufacturing full life cycle                industrial
   application.
   USE - Operating system based on industrial internet               
   platform of computer device (claimed).
   ADVANTAGE - The operation system based on industrial               
   internet platform of the invention is suitable for               
   transparent monitoring and optimization management                facing
   to manufacturing full life cycle. The                operation system
   provides data sharing of each                business system of
   manufacturing enterprise, and                realizes mass multi-source
   heterogeneous data of                centralized storage, standardized
   management and                comprehensive analysis. The operation
   system solves                the low code multiplexing rate, application
   development period is long, improves the                development
   efficiency of customized industrial                application, and
   realizes customized application                flexible development
   facing to manufacturing full                life cycle.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
   operating system based on industrial internet                platform.
   (Drawing includes non-English language                text)
Z9 0
U1 0
U2 0
DA 2023-07-21
UT DIIDW:202371336C
ER

PT P
AU KIM T H
   LIM J T
   HAM K S
   JI H H
TI Method for collecting high resolution data for            analyzing wind
   farm operation information, involves            collecting data
   generated from wind farm by collection            unit
PN KR2023077842-A
AE KOREA ELECTRONICS TECHNOLOGY INST
AB 
   NOVELTY - The method involves collecting data generated               
   from a wind farm by a collection unit (110), and               
   classifying the collected data according to data                types.
   The collected data is pre-processed by a                pre-processing
   unit (120). A database (DB) is                utilized for storing the
   preprocessed data. The                data includes state data, control
   data,                meteorological data, weather data and operation    
   data of the wind farm. A data lake (112)                temporarily
   stores real-time data of a time series.                A non-time-series
   analysis data is temporarily                stored by another data lake.
   A connection module                (114) restores missing data from the
   real time                data.
   USE - Method for collecting high resolution data for               
   analyzing wind farm operation information and                status of
   wind farm.
   ADVANTAGE - The method enables collecting and storing               
   high-resolution data based on operation information                and
   condition of the wind farm, so that precise                condition and
   operation analysis of wind farm can                be performed in an
   effective manner.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a big 
   data system.110Data collection unit112Data lake114Connection
   module120Data pre-processing unit
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202358639H
ER

PT P
AU CHANG Q
   QING C
   SUN H
TI System for supporting expandable data acquisition            and
   analyzing in large enterprise tax system, has            multiple
   service systems located for reading data from            core system and
   performing corresponding service            processing based on read
   data
PN CN116150240-A
AE HANGTIAN INFORMATION SOFTWARE TECHNOLOGY
AB 
   NOVELTY - The system has a large data platform located               
   for storing enterprise original data and provided                with a
   data center and an enterprise data lake. A                core system is
   located for utilizing different                authentication modes,
   extracting enterprise                original data from the large data
   platform and                classifying and processing the enterprise
   original                data into different data structures to store    
   according to a data source. Multiple service                systems are
   located for reading data from the core                system and
   performing corresponding service                processing based on the
   read data. The data center                and the enterprise data lake
   are located for                storing the enterprise original data. A
   data                processing application server is located to adopt a 
   lightweight directory access protocol(LDAP)               
   authentication mode or a Kerberos(RTM: Computer                network
   security protocol) authentication                mode.
   USE - System for supporting expandable data                acquisition
   and analyzing in a large enterprise tax                system. Uses
   include but are not limited to invoice                data, voucher data
   and financial accounting data                for corporate financial
   management.
   ADVANTAGE - The system supports different authentication               
   modes and different service systems to butt joint,                avoids
   requirement for number and mode of the                service system,
   realizes collection of large data                quantity data and big
   data platform of different                authentication modes, ensures
   compatibility for                supporting data analyzing, processing
   and storing                and convenience for data management, and
   satisfies                a call of different service systems.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a method
   for supporting expandable data                acquisition and analyzing
   in a large enterprise tax                system;(2) a computer readable
   storage medium                including instructions for supporting
   expandable                data acquisition and analyzing in a large     
   enterprise tax system; and(3) an electronic equipment comprises a       
   processor and a memory for supporting expandable                data
   acquisition and analyzing in a large                enterprise tax
   system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for supporting expandable data acquisition and    
   analyzing in a large enterprise tax system.                (Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2023-06-23
UT DIIDW:202357984A
ER

PT P
AU LIU Z
   YU C
   ZHANG T
   CHEN W
   CHEN Z
   LIU Y
   WANG Q
   ZHANG L
   WANG P
   LIU H
TI Method for storing digital object based on data            lake,
   involves outputting digital object architecture            (DOA)
   architecture based on data lake, encapsulating            digital
   objects, and managing create, read, update and            delete (CRUD),
   and metadata within data lake
PN CN116150410-A
AE BEIJING BIG DATA ADVANCED INST
AB 
   NOVELTY - The method involves organizing a digital                object
   warehouse (DOA) framework. A data lake is                introduced. The
   data lake is taken as a digital                object warehouse of the
   DOA architecture. The DOA                architecture is outputted based
   on the data lake.                The digital objects are abstract
   encapsulated. The                create, read, update and delete (CRUD)
   and metadata                is managed within the data lake. The digital
   object                DO is connected to the core member by a protocol. 
   The core components are provided with an                identification
   resolution system (IRS), a digital                object Repository
   system Registry, and a digital                object Registry system.
   USE - Method for storing digital object based on                data
   lake.
   ADVANTAGE - The data lake as digital object warehouse in               
   the framework provides a possibility for data                application
   across a set of systems, thus improving                the ability of
   large data management. The method                supports fast
   processing and analyzing the digital                object data, and
   provides ACID transaction                capability, compatible Spark
   and upper layer                service, and can be further commercial
   report                analysis by various analysis engine, flow         
   calculation and artificial intelligence (AI)                analysis.
   The method provides good big data                processing effect, and
   compatibility with various                data systems.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the
                  architecture of the DOA.
Z9 0
U1 0
U2 0
DA 2023-06-23
UT DIIDW:202358053E
ER

PT J
AU Coburn, Gina
TI SBIR Phase I:  A platform to connect underserved and underrepresented
   communities to science, technology, engineering and mathemetics (STEM)
   careers
DT Awarded Grant
PD Apr 15 2023
PY 2023
AB The broader/commercial impact of this Small Business Innovation Research
   (SBIR) Phase I project is in providing more access to educational
   pathways, and equitable opportunities for learning, professional
   development, and career growth to marginalized student communities. The
   project proposes the development and implementation of a digital human
   element to meet the needs of these impacted individuals. The project is
   not such organizations, but also to create pathways to equitable
   opportunities for those coming from marginalized communities to
   participate in some of the most innovative learning modalities, while
   studying for some of the most promising STEM careers. <br/><br/>The
   intellectual merit of this project lies in the use of big data and
   machine learning in personality and skills measurement. The artificial
   intelligence algorithms will be juxtaposed onto game mechanics to
   facilitate ease of use due to the familiarity with other existing user
   interfaces.  The interface has a complex and dynamic personality
   profiling engine. The research will deliver standards and methodologies,
   evaluate existing exchange formats, improve accuracy metrics for neural
   networks, and deliver an initial digital human prototype. The technology
   will create a Data Lake containing professions, skills, certificate
   requirements, social media profiles, resumes, recorded interviews, and
   other online activities that are shared by the users for establishing
   personalized, artificial intelligence (AI)-supported career growth
   profiles. Information in the Data Lake will be curated to facilitate the
   development of personalized career development strategies. A Delta Lake
   model will be used to continuously stream data with improved data
   quality to drive the personalization requirements of both the digital
   human and the user.<br/><br/>This award reflects NSF's statutory mission
   and has been deemed worthy of support through evaluation using the
   Foundation's intellectual merit and broader impacts review criteria.
ZS 0
ZR 0
Z8 0
ZA 0
ZB 0
TC 0
Z9 0
U1 0
U2 0
G1 2232689
DA 2024-04-26
UT GRANTS:17668587
ER

PT P
AU LIU Y
   CAO L
   WANG S
   ZHENG L
   TANG L
   SHEN J
   ZHAO J
TI Industrial internet big data lake based discrete           
   manufacturing enterprise production data cockpit            system, has
   heterogeneous data storage unit comprising            semi-structured
   data module, graphic storage module,            distributed file system
   module, time sequence database            module and service interface
   module
PN CN115936296-A
AE BEIJING AEROSPACE SMART MFG TECHNOLOGY
AB 
   NOVELTY - The system has a data integrated management               
   module comprising a data convergence sub-module, a                data
   processing sub-module, a data file/interface                sub-module
   and an online filling sub-module for                transmission of
   cross-department data. A data                security module is used for
   uniformly and safely                managing data resources stored in
   different types                and shielding details of different
   storage                technologies. A data operation monitoring module 
   comprises a platform monitoring sub-module, a data               
   processing process monitoring sub-module and a                service
   resource monitoring sub-module. A                heterogeneous data
   storage unit comprises a                relational database module, a
   semi-structured data                module, a graphic storage module, a
   distributed                file system module, a time sequence database
   module                and a service interface module.
   USE - Industrial internet big data lake based                discrete
   manufacturing enterprise production data                cockpit system.
   Uses include but are not limited to                industrial internet
   of large data lake, data                acquisition, data storage, data
   management, data                development, data service to data
   operation                monitoring analysis, large data lake
   omni-bearing                assistance workshop production monitoring   
   visualization, production line production                monitoring
   visualization, workshop level of device                monitoring
   management, and visual display of device                level production
   process key index.
   ADVANTAGE - The data security module is used for uniformly              
   and safely managing the data resources stored in               
   different types, shielding details of different                storage
   technologies, and establishing security                control mechanism
   and flow of the data. The system                realizes data
   management, data development, data                service to data
   operation monitoring analysis,                large data lake
   omnibearing assistance workshop                production monitoring
   visualization, production                line production monitoring
   visualization, workshop                level of device monitoring
   management, production                line level, visual display of
   device level                production process key index data for
   assisting                manufacturing enterprise user for quick,       
   intuitionistic understanding enterprise production               
   manufacturing plan, progress, quality, process,                device
   subject domain information.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an     
   Industrial internet big data lake based discrete               
   manufacturing enterprise production data cockpit                system.
   (Drawing includes non-English language               
   text).11Application unit12Data lake unit13Heterogeneous data storage
   unit14External service system
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202339528L
ER

PT P
AU WU L
   WANG P
   LIN Z
   LI L
   LIU J
   SUN F
TI Semi-supervised flow-shaped collaborative learning            (SMOL)
   based multi-channel invoice data aggregation            processing
   method, involves accessing floor application            invoice data
   into uniform data Application Programming            Interface (API)
PN CN115908031-A
AE INNER MONGOLIA AISINO CO LTD
AB 
   NOVELTY - The method involves outputting standard               
   structured invoice data by using big data                aggregation
   processing based on invoice data from                different channel
   sources.Uniform abstract invoice                data is output by using
   uniform adaptation                processing based on the standard
   structured invoice                data. Different conversion rules are
   configured to                output standardized invoice data based on
   the                uniform abstract invoice data. Checked qualified     
   invoice data is output by using invoice checking                process
   based on the standardized invoice data. The                checked
   qualified invoice data is input into a data                lake to form
   floor application invoice data.
   USE - SMOL based multi-channel invoice data                aggregation
   processing method.
   ADVANTAGE - The method enables realizing accurate               
   regression function training and prediction model                to
   realize precise locating, and combining                Bluetoothand
   Wireless Fidelity (Wi-Fi) data for                training to improve
   training and prediction                precision of regression function,
   and improving                location precision effect of pure Wi-Fi
   data.
   DETAILED DESCRIPTION - The floor application invoice data is accessed   
   into a uniform data Application Programming                Interface
   (API), where original invoice data access                channel
   comprises interface entry, Excel(RTM:                Spreadsheet
   developed by Microsoft for Windows,                macOS, Android and
   iOS) batch introduction, third                party interface
   introduction, image recognition OCR                import, invoice
   two-dimensional (2D) code                recognition introduction and
   rapid expansion                access, and original invoice data
   comprises pure                text format, XMLformat, JSON(RTM: Open
   standard                file format and data interchange format) format
   and                Excelformat. The invoice data is stored in a         
   Hadoop(RTM: Open-source software framework) big                data
   processing middleware by using Hudi(RTM:                transactional
   data lake platform) based on the                checked qualified
   invoice data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of an SMOL 
   based multi-channel invoice data aggregation                processing
   method. (Drawing includes non-English                language
   text).S101Multi-channel original invoice data               
   accessS102Original invoice data adaptationS103Invoice data
   standardizationS104Invoice data checking floorS105Invoice data docking
   application
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202346039B
ER

PT P
AU OU Z
   DAI S
   MAI X
   CHEN M
   YANG J
   XIE C
   LI B
TI Method for treating AI fusion based on data lake            for
   enterprise application large data, involves            automatically
   identifying image by data of data lake            access by AI
   technology, word recognition and voice            recognition to collect
   AI data
PN CN115809235-A; CN115809235-B
AE GUANGZHOU HUITONE TECHNOLOGY CO LTD
AB 
   NOVELTY - The AI fusion treatment method involves               
   accessing data lake data. An image is automatically               
   identified by data of the data lake access by AI               
   technology, word recognition and voice recognition                to
   collect artificial intelligence (AI) data to                structured
   data and unstructured data. The                collected structured data
   is compared with                unstructured data model of an
   enterprise. The                structured data meeting the enterprise is
   screened.                The use frequency of a data standard is        
   automatically recognized by an AI learning                algorithm,
   heat and input data through an                enterprise service. The
   data quality evaluation is                participated in the data
   quality evaluation. The                data standard evaluation of a
   level is improved.                The data capability is optimized.
   USE - Method for treating AI fusion based on data                lake
   for enterprise application large data.
   ADVANTAGE - The self-adaptive learning production of the               
   structured model is realized. The data standard               
   evaluation of the level is improved and the data               
   capability is optimized.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of method  
   for treating AI fusion based on data lake for                enterprise
   application large data. (Drawing                includes non-English
   language text).
Z9 0
U1 0
U2 0
DA 2023-04-07
UT DIIDW:2023307394
ER

PT P
AU TONG C
   LU C
   CHEN J
   HUANG X
   ZHANG Y
TI Warehouse laking based enterprise integrated data            middle
   platform framework, has data collecting layer            formed to
   collect application data outside data table,            and data lake
   storage layer formed to provide data            storage and monitor data
   storage
PN CN115794962-A
AE CHINA COMMUNICATIONS SERVICES PUBLIC INF
AB 
   NOVELTY - The framework has a data collecting layer               
   formed to collect application data outside a data                table
   and provided with JDBC(RTM: Application                programming
   interface for Java programming                language) collection,
   HTTPcollection, file transfer                protocol (FTP) collection,
   message queue (MQ)                collection and web service collection.
   A data lake                storage layer is formed to provide data
   storage and                monitor the data storage. A data service
   layer is                located to provide uniform external service     
   according to data provided by the data lake storage                layer
   and provided with SQLor not only SQL(NoSQL)                query and
   data analyzing services.
   USE - Warehouse laking based enterprise integrated                data
   middle platform framework.
   ADVANTAGE - The framework is convenient to manage and               
   excavate data, and combines technical service and                service
   service, utilizes container technology to                provide elastic
   extension, deploys the service                service in a container
   cluster, provides high                available service to outside,
   supports structured,                semi-structured and unstructured
   data types,                realizes real time batch, full amount,
   increment                and collecting modes according to service      
   requirement, confirms correct data set before                starting
   data search, ensures capability component                collaborative
   work in a data lake building, and                improves data analyzing
   and processing                ability.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   warehouse laking based enterprise integrated data                middle
   platform framework. (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2023-04-01
UT DIIDW:202330391K
ER

PT P
AU MENG Z
   YAN Y
   BIAN Q
   HOU Y
TI Method for realizing database entering lake            configuration
   automation of electronic device, involves            creating downstream
   statement based on data source            mapping statement and
   full-quantity query insertion            statement, and controlling
   databases sequentially in            data to finish operation of
   entering lake
PN CN115774750-A
AE JINGYINGSHUZHI TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves using an identifier of a               
   data source to generate created catalog sentence. A                data
   task is used to generate created downstream                table
   sentence. A data source connection item and                mapping
   relation table are used to generate a data                source mapping
   statement. The mapping relation                table is used to generate
   full-quantity query                inserted sentence. Creation of
   catalog sentence is                executed according to configuration
   item.                Downstream statement is created based on the data  
   source mapping statement and full-quantity query               
   insertion statement. Multiple databases are                sequentially
   controlled in data to finish operation                of entering lake.
   USE - Method for realizing database entering lake               
   configuration automation of an electronic device               
   (claimed).
   ADVANTAGE - The method enables realizing easy passing a               
   user through an external data source and local data                lake
   data, and improving high efficiency of data                into the
   lake, and achieving database entering the                lake
   configuration automation, and supporting                automatic
   metadata and flexible supporting flow and                reducing cost
   of user access multi-source                heterogeneous data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:a system for realizing database entering lake               
   configuration automation of electronic device;                anda
   computer readable storage medium has set of                instructions
   for storing Method for realizing                database entering lake
   configuration automation of                electronic device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the Method for realizing database entering
   lake                configuration automation of electronic device       
           (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2023-04-07
UT DIIDW:202328434G
ER

PT P
AU ZHOU M
   ZHANG X
   YANG B
   MA R
TI Method for identification and matching of            certificate
   information based on big data, involves            obtaining test
   pictures of certificate samples, input            them into text
   recognition model and face recognition            model for recognition
PN CN115775317-A
AE TIANYI ELECTRONIC BUSINESS CO LTD
AB 
   NOVELTY - The method involves collecting a picture               
   containing certificate content by using an image               
   collecting device. The picture is pre-processed. A               
   semantic segmentation algorithm is used to extract                an
   effective area comprising certificate                information. A
   character recognition model is                constructed. A sample
   image data set is inputted to                the model to identify. An
   identified result is                output. A face recognition model and
   a face model                are constructed. The sample image is input
   into the                model for identification. A large data matching 
   process is performed by using a Faiss similarity                search
   tool. Related information of the certificate                is
   outputted.
   USE - Method for identification and matching of               
   certificate information based on big data, for use                in
   field of image identification.
   ADVANTAGE - The HDFS distributed file system is used to               
   store massive user data of operators. The HDFS can                be
   expanded horizontally, and the stored files can                support
   data storage of PB level or higher. The                problem of
   excessive pressure on the Name-node                single point caused
   by the large amount of metadata                data caused by the
   massive number of files is                relieved, by improving the
   structure of HDFS.
   DETAILED DESCRIPTION - The method involves building (S110) a text       
   recognition model. The sample image data set is                input
   into model for recognition.INDEPENDENT CLAIMS are included for:(1) an
   identification matching system based on                large data
   certificate information; and(2) a computer readable storage medium
   storing                program for identification matching.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the large data certificate information                identification
   matching method. (Drawing includes                non-English language
   text).S100Step for using the image collecting                device,
   shooting and collecting the picture                containing the
   certificate content,S110Step for establishing character               
   recognition model, inputting the sample image data                set to
   the model to identify, outputting the                identified result,
   and converting the result into                the character.S120Step for
   building face recognition                modelS130Step for using
   improved Hadoop                distributed file system of the big data  
   architecture to store the image information in the               
   operator's databaseS140Step for using Faiss similarity search           
   tool to match text content and face image feature               
   vectorsS150Step for obtaining the certificate sample                test
   picture, inputting to the character                recognition model
Z9 0
U1 0
U2 0
DA 2023-03-22
UT DIIDW:2023278549
ER

PT P
AU WANG F
TI Data lake-based medical data integrated method for           
   intelligent development, involves determining whether            table
   structure change information conforms to preset            data
   structure agreement, and updating target medical            data table
   corresponding to target medical system in            data lake
PN CN115730000-A
AE LIANREN HEALTH & MEDICAL BIG DATA TECHNO
AB 
   NOVELTY - The method involves obtaining (S101) the               
   medical change data generated in the target medical               
   system. The data change information corresponding                to the
   medical change data is determined.                Determination is made
   on whether (S102) the table                structure change information
   conforms to a preset                data structure agreement when the
   data change                information includes table structure change  
   information corresponding to medical change data.                The
   target medical data table corresponding to the                target
   medical system in the data lake is updated                (S103) based
   on the medical change data when so.                The medical change
   data is stored in the abnormal                table medical data.
   USE - Data lake-based medical data integrated method                for
   intelligent development and digital application                of
   medical health, for efficient integration of                multi-source
   heterogeneous data.
   ADVANTAGE - The target medical data system corresponding               
   to the target in the data lake is updated based on                the
   medical change data, so that the medical data                integrated
   cost is reduced, and the integrated                efficiency medical
   data is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   data lake-based medical data integrated device; (2)                an
   electronic equipment; (3) a computer readable                storage
   medium storing program data lake-based                medical data
   integrated method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a medical 
   data integration method based on a data lake.                (Drawing
   includes non-English language                text).S101Step for
   obtaining the medical change                data generated in the target
   medical systemS102Step for determining whether the table               
   structure change information meets the preset data               
   structure protocol.S103Step for updating the target medical             
   system corresponding to the target medical data the                data
   lake
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202325487K
ER

PT P
AU SANDHYAVERMA
   PARVEZ A
   HATI M
   SHEDTHI A
   SINGH R
   BARDE S
   ANAND A
   KHARAT V J
TI Method for performing mix of content analysis, and            empirical
   study to explain current lack of adoption of            DL in business
   analytics, involves employing mix of            content analysis, and
   empirical study to explain            current lack of adoption of DL in
   business analytics            functions
PN IN202321008801-A
AE SANDHYAVERMA; PARVEZ A; HATI M; SHEDTHI A; SINGH R; BARDE S; ANAND A;
   KHARAT V J
AB 
   NOVELTY - The method involves employing a mix of content               
   analysis, and empirical study to explain the                current lack
   of adoption of deep learning (DL) in                business analytics
   functions. The progress, and                breakthroughs achieved by DL
   are undeniable as                witnessed by a vast array of new
   real-world                applications all around us. The adoption rate,
   and                diffusion across business analytics function lacks   
   behind. The content analysis suggested that the                lack of
   an adoption across business functions is                based on the
   four bottlenecks computational                complexity, no existing
   big-data architecture, lack                of transparency nature of DL,
   and skill                shortage.
   USE - Method for performing a mix of content                analysis,
   and empirical study to explain the                current lack of
   adoption of deep learning (DL) in                business analytics.
   ADVANTAGE - The method enables the deep learning to               
   improve data analysis in an ML ecosystem. The                method
   allows the practitioners to focus on                DL-enabled use cases
   instead of driving model                replacement, which speeds up
   adoption of DL within                business analytics.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a     
   method for performing a mix of content analysis,                and
   empirical study to explain the current lack of                adoption
   of DL in business analytics.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202324412F
ER

PT P
AU DEGIRMENCI A
   SONMEZ B
   TURK T
   KAVAK C
   YILMAZ E
   AKIN M
   SAGIROGLU S
   SOENMEZ B
   TUERK T
TI System for determining state of traffic density            for e.g.
   passenger vehicle, has intelligent zone unit            for filtering
   constraints with processed data over            warehouse, where routing
   service is provided for users            with live traffic information
PN WO2023014311-A1; EP4162464-A1; EP4162464-A4; TR2021012500-A2
AE BASARSOFT BILGI TEKNOLOJILERI AS
AB 
   NOVELTY - The system has a traffic service unit (41)               
   being operated on a server (40). The traffic                service unit
   calculates historical traffic data by                performing basic
   addition and averaging operations,                which are applied over
   a data set obtained by                applying time-filtered geographic
   intersection to                raw global positioning system (GPS) data
   in a data                warehouse (10). A routing unit (42) enables a  
   routing service to be provided for users with live               
   traffic information, predictive traffic information                and
   intelligent zones. An intelligent zone unit                (43) filters
   constraints with the processed data                over the data
   warehouse.
   USE - System for determining a state of traffic                density
   for a vehicle e.g. passenger vehicle,                medium vehicle and
   heavy vehicle, by processing and                analyzing instant GPS
   information in big data                architecture.
   ADVANTAGE - The system effectively determines the state of              
   traffic density by processing and analyzing instant                GPS
   information in big data architecture, thus                accurately
   finding a traffic congestion area.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
          method for determining a state of traffic                density.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a  
   system for determining a state of traffic density                for a
   vehicle.10Data warehouse40Server41Traffic service unit42Routing
   unit43Intelligent zone unit
Z9 0
U1 0
U2 0
DA 2023-02-25
UT DIIDW:202317592F
ER

PT P
AU LI H
   CHIARAVALLOTI M
   MEREDITH P
   CHIPMAN R
TI Distributed database system for identifying and            resolving
   structured queries on dynamic schema and/or            unstructured
   datasets in computer system, has query            engine for identifying
   query language elements in user            defined queries and mapping
   language semantics for            execution on unstructured data
PN US2023039860-A1; US12265539-B2
AE MONGODB INC
AB 
   NOVELTY - The system has a processor (210) operatively               
   connected to a memory (220). A distributed database               
   including data stored under a dynamic schema                architecture
   or an unstructured architecture. A                query engine is
   executed by the processor to accept                user defined queries
   and execute user defined                queries against a distributed
   database. The query                engine identifies structured query
   language                elements in the user defined queries, maps      
   structured query language semantics for execution                on
   unstructured data in the distributed database,                and
   outputs result of the structured query language                semantics
   for communication to a user for further                processing by
   another query stage.
   USE - Distributed database system for identifying                and
   resolving structured queries on dynamic schema                and/or
   unstructured datasets in a computer                system.
   ADVANTAGE - The system identifies and resolves structured               
   queries to execute consistently and accurately                against
   any data architecture in dynamic or                unstructured database
   stores, allows a system to                enable operation to perform
   static type-checking                and result-set-metadata computation
   without source                schema information to execute a query,
   thus                improving known approaches that often cannot        
   identify errors until execution and resulting in                massive
   computational waste, providing simpler                construction, and
   avoiding the need to distinguish                table sub-queries and
   row subqueries.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
              for managing a distributed database system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   computer system.200Computer system210Processor220Memory230Non-volatile
   storage device
Z9 0
U1 0
U2 0
DA 2023-02-25
UT DIIDW:202317780Y
ER

PT P
AU ERDMANN C A
   BRUSH R A
   SUTARIYA B B
TI Non-transitory computer-storage media for            providing
   closed-loop intelligence in medical data            computing
   environment e.g. hospital, has set of            instructions for
   pushing insights gained from trained            machine learning model
   and external data from other            healthcare organizations into
   clinical workflow
PN US11561938-B1
AE CERNER INNOVATION INC
AB 
   NOVELTY - The medium has a set of instructions for               
   receiving selection of data at a cloud service from                a
   database comprising data from multiple sources in                a Fast
   Healthcare Interoperability Resources (FHIR)                format to
   build a data model. A feature vector                corresponding to the
   data model is extracted.                Selection of an algorithm for a
   machine learning                model is received to apply to the data
   model. The                machine learning model is applied to the
   training                data. The trained machine learning model and
   data                comprising internal data of a particular healthcare 
   organization and external data from other                healthcare
   organizations are accessed and utilized                by other
   healthcare organizations. Insights gained                from the
   trained machine learning model based on                data comprising
   internal data of the particular                healthcare organization
   and external data from                other healthcare organizations are
   pushed into                clinical workflow.
   USE - Non-transitory computer-storage media for                providing
   closed-loop intelligence in medical data                computing
   environment such as hospital, laboratory                and research or
   other facilities.
   ADVANTAGE - The method enables providing efficient and               
   convenient system to enable structured views of                data
   to-be aggregated into a big data architecture                to train
   and test predictive tools on an entire                population of
   data, so as to push insights gained                from the predictive
   tools into a clinical workflow                and to provide access and
   use to predictive tools                by other organizations.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a system
   for establishing a healthcare                computing environment;(2) a
   computerized method for establishing a                healthcare
   computing environment.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an     
   exemplary computing environment.100Computing environment102Control
   server104Data store106Computer network108Remote computers
Z9 0
U1 0
U2 0
DA 2023-02-04
UT DIIDW:2023110353
ER

PT P
AU CUI L
   XUE X
   FENG G
   GE J
   XU T
   GE T
   WANG J
   DU H
   YANG L
   PENG L
TI Data lake system based on stream data and batch            data
   cooperative scheduling processing, has calculation            engine
   module that pushes or stores processed data            based on
   requirement of data processing task in            centralized storage
   module
PN CN115599524-A; CN115599524-B
AE ORDNANCE IND COMPUTER APPL TECHNOLOGY
AB 
   NOVELTY - The system has a data management module which               
   is provided with a task flow management unit. The                task
   flow management unit is provided to arrange                data
   processing tasks based on application                requirements,
   construct a DAG directed acyclic                graph. A corresponding
   computing engine in a                computing engine module is provided
   to schedule and                process the data processing tasks of each
   node in                the directed acyclic graph based on a preset data
   processing task collaborative scheduling method. A               
   calculation engine module is provided to process                the data
   in a centralized storage module through                different
   calculation engines based on the data                processing task
   requirements. The processed data is                pushed or stored in
   the centralized storage module                based on the requirements
   of the data processing                task.
   USE - Data lake system based on stream data and                batch
   data cooperative scheduling processing for                storing
   various large scale original data sets in                native format,
   which allows all structured and                unstructured data to be
   stored in any scale.
   ADVANTAGE - The data lake system supports real time,               
   offline structured, non-structured cleaning of                all-form
   data, conversion, loading, realizes                complex data exchange
   and cross-system between                multiple service systems,
   inter-departmental,                cross-organization, cross center data
   fusion                sharing, and solves the problem of data lake      
   systems usability and efficiency. The data                structure
   adaptation and other technical problems,                improving data
   access and output efficiency are                improved when arranging
   the task, without data                exchange between the tasks.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the
   data                lake system based on stream data and batch data     
   cooperative scheduling processing. (Drawing                includes
   non-English language text)
Z9 0
U1 0
U2 0
DA 2023-02-04
UT DIIDW:202310217F
ER

PT P
AU CHEN T
   CAO C
   WANG L
   ZHANG Q
   ZHU K
   YANG F
   ZHOU H
   ZHANG T
   SHI X
   TAN R
   WU Y
TI Method for enhancing object information value            based on
   multi-source data useful in large data            analysis comprises
   aiming service unique code,            obtaining unique complete trusted
   data object            associated with service unique code according to 
   supplementary data object and matching data object            processing
PN CN115563196-A; CN115563196-B
AE SHANGHAI BIG DATA CO LTD
AB 
   NOVELTY - The method comprises obtaining multi-source               
   data from each government affair system to form                data lake
   according to the large data analysis                requirement of at
   least one business scene,                extracting satisfy requirement
   field deletion                standard from the data lake for each of
   the service                scene according to the multiple preset
   requirement                fields associated with the service scene,
   aiming at                each of the standard object set, screening each
   of                the standard data object to obtain the               
   corresponding target data object according to the                preset
   at least one service condition associated                with the
   service scene, aiming at each of the                trusted object set,
   aiming at each of the service                unique code, obtaining the
   unique complete trusted                data object associated with the
   service unique code                according to each of the
   supplementary data object                and each of the matching data
   object processing, so                as to enhance the object
   information value of the                multisource data.
   USE - Method for enhancing object information value                based
   on multi-source data useful in the large data                analysis
   technology field in various social fields                e.g. government
   department efficiency, electronic                government system with
   big data technology as                core.
   ADVANTAGE - The method can effectively improve the               
   accuracy and integrity of the data, so that the                data has
   high information value, so as to                effectively improve the
   data island                phenomenon.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   system for enhancing object information value based                on
   multi-source data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the method for enhancing object information
   value                based on multi-source data useful in large data    
      analysis (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2023-02-25
UT DIIDW:2023060722
ER

PT C
AU Ahmad, Bashar
BE Pesquita, C
   Skaf-Molli, H
   Efthymiou, V
   Kirrane, S
   Ngonga, A
   Collarana, D
   Cerqueira, R
   Alam, M
   Trojahn, C
   Hertling, S
TI A Distributed and Parallel Processing Framework for Knowledge Graph OLAP
SO SEMANTIC WEB: ESWC 2023 SATELLITE EVENTS
SE Lecture Notes in Computer Science
VL 13998
BP 288
EP 297
DI 10.1007/978-3-031-43458-7_47
DT Proceedings Paper
PD 2023
PY 2023
AB Business intelligence and analytics refers to the ensemble of tools and
   techniques that allow organizations to obtain insights from big data for
   better decision making. Knowledge graphs are increasingly being
   established as a central data hub and prime source for BI and analytics.
   In the context of BI and analytics, KGs may be used for various
   analytical tasks; the integration of data and metadata in a KG
   potentially facilitates interpretation of analysis results. Knowledge
   Graph OLAP (KG-OLAP) adapts the concept of online analytical processing
   (OLAP) from multidimensional data analysis for the processing of KGs for
   analytical purposes. The current KG-OLAP implementation is a monolithic
   system, which greatly inhibits scalability. We propose a research plan
   for the development of a framework for distributed and parallel data
   processing for KG-OLAP over big data. In particular, we propose a
   framework for KG-OLAP over big data based on the data lakehouse
   architecture, which leverages existing frameworks for parallel and
   distributed data processing. We are currently at an early stage of our
   research.
CT 20th European Semantic Web Conference-ESWC-Annual
CY MAY 28-JUN 01, 2023
CL Khersonisos, GREECE
SP Springer Nature; IOS Press
Z8 0
ZS 0
ZB 0
ZA 0
ZR 0
TC 0
Z9 0
U1 1
U2 1
SN 0302-9743
EI 1611-3349
BN 978-3-031-43457-0; 978-3-031-43458-7
DA 2023-01-01
UT WOS:001560618200047
ER

PT C
AU Bianchini, Devis
   Garda, Massimiliano
BE Zhang, F
   Wang, H
   Barhamgi, M
   Chen, L
   Zhou, R
TI A Methodological Approach for Data-Intensive Web Application Design on
   Top of Data Lakes
SO WEB INFORMATION SYSTEMS ENGINEERING - WISE 2023
SE Lecture Notes in Computer Science
VL 14306
BP 349
EP 359
DI 10.1007/978-981-99-7254-8_27
DT Proceedings Paper
PD 2023
PY 2023
AB Data exploration and decision making may benefit from the availability
   of data-intensive web applications, that enable domain experts to
   navigate across massive, dynamic and heterogeneous data sources, stored
   in the so-called Data Lakes. However, traditional design strategies for
   this kind of applications require in the background well-defined and
   cleaned data structures. Conceptual modelling may be fruitfully employed
   to provide web developers with a comprehensive vision over Data Lake
   sources, on which web applications are designed. Nevertheless, the
   cumbersome nature of Data Lakes turns the conceptual model into a
   dynamic entity, which must be properly managed. In this paper, we
   propose a methodological approach to design data-intensive web
   applications on top of a Data Lake. A conceptual data model, weaved over
   Data Lake sources, is leveraged to identify the relevant information to
   be included in the web application. The methodology makes the model
   evolve both with new data sources content emerging from the Data Lake,
   through a zone-based operations pipeline that prepares a curated version
   of the raw data (bottom-up), and with additional domain knowledge
   provided by web developers derived from the data-intensive web
   application design (top-down). The approach, independent from any
   specific implementation technology, is declined in the context of a real
   case study regarding an ongoing research project in the cultural
   heritage domain.
CT 24th International Conference on Web Information Systems
   Engineering-WISE-Annual
CY OCT 25-27, 2023
CL Melbourne, AUSTRALIA
OI Garda, Massimiliano/0009-0006-5823-6595
ZS 0
ZB 0
ZR 0
ZA 0
TC 0
Z8 0
Z9 0
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-981-99-7253-1; 978-981-99-7254-8
DA 2023-01-01
UT WOS:001547081100026
ER

PT C
AU Bonilla, Lander
   Lopez-De-Armentia, Juan
   Zarate, Gorka
   Torre-Bastida, Ana I.
   Vigo, Luis
   Bidaguren, Peru
   Alonso, Olga
   Capelastegui, Abel
GP IEEE
TI CCIR: An architecture for collecting and storing Connnected Corridor
   Infrastructure and Mobility Data
SO 2023 IEEE 26TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION
   SYSTEMS, ITSC
SE IEEE International Conference on Intelligent Transportation Systems-ITSC
BP 5344
EP 5350
DI 10.1109/ITSC57777.2023.10422103
DT Proceedings Paper
PD 2023
PY 2023
AB Connected corridors are programs and initiatives that emerge under the
   umbrella of major paradigms such as SmartCities, Big Data, Artificial
   Intelligence or Internet of Things (IoT). Their main objective is to
   provide environments for testing, validating and demonstrating all kinds
   of technologies related to Cooperative, Connected and Autonomous
   Mobility, and Intelligent and Digital Infrastructures, within a real
   scenario. These corridors need to equip themselves with new intelligent
   and extremely powerful systems, based on technologies such as
   intelligent traffic systems. They aim to enhance and evolve these
   systems by leveraging the possibilities and solutions offered by this
   new wave of disruptive technologies allows. In this article, we describe
   the needs of a crucial component for connected corridors, such as the
   information repository, and we propose its possible technological
   implementation using the Data Lake paradigm, with a focus on data
   interoperability as a primary requirement. Finally, we validate the
   significant usefulness of the proposed architecture for the connected
   corridor information repository, called CCIR, through the concrete
   implementation of a specific collaborative corridor, such as the Bizkaia
   Connected Corridor - BCC is. To demonstrate it, we present different use
   cases that exploit the data generated and collected in the BCC
   environment relative to the infrastructure and mobility domain.
CT IEEE 26th International Conference on Intelligent Transportation Systems
   (ITSC)
CY SEP 24-28, 2023
CL Bilbao, SPAIN
SP IEEE
OI López-de-Armentia, Juan/0000-0002-7911-3141
TC 0
ZA 0
ZR 0
ZS 0
ZB 0
Z8 0
Z9 0
U1 0
U2 0
SN 2153-0009
BN 979-8-3503-9946-2
DA 2024-06-12
UT WOS:001178996705058
ER

PT B
AU Bouziane, Anas
Z2  
TI On-Demand Health Data Provisioning With Custom Temporary Data Views for
   Big Data Platforms
DT Dissertation/Thesis
PD Jan 01 2023
PY 2023
TC 0
ZA 0
ZB 0
Z8 0
ZR 0
ZS 0
Z9 0
U1 0
U2 0
BN 9798302848277
UT PQDT:121301049
ER

PT B
AU da Costa Chiolas Macedo, Ricardo Pinto
Z2  
TI Implementation of a Data Lake in a Microservices Architecture
DT Dissertation/Thesis
PD Jan 01 2023
PY 2023
ZR 0
Z8 0
ZS 0
ZA 0
ZB 0
TC 0
Z9 0
U1 1
U2 1
BN 9798346711360
UT PQDT:119381893
ER

PT B
AU de Oliveira Margarido, Inês
   S. Mamede, José Henrique Pereira
   dos Santos, Vitor Manuel Duarte
Z2  
TI [not available]
DT Dissertation/Thesis
PD Aug 09 2024
PY 2024
ZS 0
ZA 0
ZR 0
Z8 0
TC 0
ZB 0
Z9 0
U1 1
U2 1
BN 9798382707754
UT PQDT:89141420
ER

PT C
AU Diamantini, Claudia
   Mele, Alessandro
   Potena, Domenico
   Storti, Emanuele
BE Abello, A
   Vassiliadis, P
   Romero, O
   Wrembel, R
TI Assessment of Data Quality Through Multi-granularity Data Profiling
SO ADVANCES IN DATABASES AND INFORMATION SYSTEMS, ADBIS 2023
SE Lecture Notes in Computer Science
VL 13985
BP 195
EP 209
DI 10.1007/978-3-031-42914-9_14
DT Proceedings Paper
PD 2023
PY 2023
AB The management of modern solutions for Big Data management and
   analytics, most notably Data Lakes and Data Lakehouses, is faced with
   new challenges stemming from the versatility offered by such
   technologies, as well as the continuously evolving variety and volume of
   data sources, necessitating the tracking of data quality concerns. In
   this scenario, this paper proposes a metadata management framework for
   summary data sources with the capability to generate data profiles at
   various levels of detail. The approach leverages a Knowledge Graph,
   which defines dimensions and measures according to the multidimensional
   model. Profiles are then exploited to efficiently assess a set of
   quality properties of sources in a Big Data framework, including
   completeness, coverage and consistency that are formally defined and
   evaluated.
CT 27th European Conference on Advances in Databases and Information
   Systems (ADBIS)
CY SEP 04-07, 2023
CL Barcelona, SPAIN
SP Fac dInformatica Barcelona; Univ Polytech Catalunya; Springer
RI Potena, Domenico/J-8653-2013; , Alessano Mele/; Storti, Emanuele/; DIAMANTINI, Claudia/
OI Potena, Domenico/0000-0002-7067-5463; , Alessano
   Mele/0009-0009-7852-0528; Storti, Emanuele/0000-0001-5966-6921;
   DIAMANTINI, Claudia/0000-0001-8143-7615
TC 0
ZB 0
Z8 0
ZS 0
ZA 0
ZR 0
Z9 0
U1 2
U2 3
SN 0302-9743
EI 1611-3349
BN 978-3-031-42913-2; 978-3-031-42914-9
DA 2024-12-05
UT WOS:001352193200014
ER

PT J
AU Eloy De Lera Acedo
TI The UK Square Kilometre Array Regional Centre 2023-2025
DT Awarded Grant
PD Jan 01 2023
PY 2023
AB The Square Kilometre Array Observatory (SKAO) is a large,
   next-generation radio telescope that is planned to be many times more
   sensitive than the current most sensitive telescopes in the world and
   will transform our view of the Universe. It is a global mega-science
   project involving scientists and engineers from institutes and industry
   partners in 16 member countries.   The Observatory comprises two
   telescope facilities; one located in South Africa, SKA-MID, to observe
   radio frequencies between 350MHz and 15.4GHz, and one located in
   Australia, SKA-LOW, to observe lower band frequencies between 50MHz and
   350MHz. The SKAO's Global Headquarters (HQ), from where the SKA Design
   Authority directs and manages the whole SKA Programme, is adjacent to
   the University of Manchester's (UoM's) Jodrell Bank Observatory (JBO),
   home to the iconic Lovell telescope and operational HQ of the e-MERLIN
   interferometer, an SKA Pathfinder instrument. The SKA is one of a small
   number of flagship astronomical instruments that will span the entire
   electromagnetic spectrum from radio to gamma rays, and beyond the
   electromagnetic spectrum to gravitational waves, cosmic rays and
   neutrinos, and whose collective aim is chart the full history of the
   universe from its beginnings in the Big Bang to the present day.   The
   operation of the SKA Observatory assumes the existence of SKA Regional
   Centres (SRCs) to deliver a range of support to the science community.
   The SRCs are required in order to provide the main portal for scientists
   to access the SKA including provision of computing resources and support
   to enable the science user community to analyse and extract science from
   data products produced by the SKA. An SRC Network (SRCNet) will be made
   up of SRCs distributed around the world in SKA Member countries. Each
   SRC will be required to conform to agreed standards in protocols, data
   architecture and information management policies to ensure that they
   appear as a single federated entity to SKA users. The SRCNet will
   provide a collection of both services and infrastructure that will
   comprise a global capability to distribute, process and curate the data
   from the SKA telescopes. The SRCNet will provide the basic governance
   and operational model and structures, and the baseline functionality of
   the SRC network. SKAO member states and SRC stakeholders are already
   engaging in the design of the SRC Network.  This joint proposal aims to
   deliver a working prototype of an SKA Regional Centre (SRC) node that
   will have 20% of the capacity and 80% of the functionality required by a
   SRC node when SKA becomes fully operational. The UK's contribution to
   this work will bring international expertise in the areas of wide field
   radio interferometry, all sky pulsar and transient detection, the
   characterisation of radio flux data, e-infrastructure, advanced cloud
   technologies, cloud and hardware platforms, remote job provision, high
   throughput network and data movement, big data performant storage
   systems and hierarchies, identity management, scientific computing, high
   performance and GPU computing, the FAIRirification of digital artifacts,
   the automated archiving, curation, collection and co-ordination of data
   artifacts. All these are needed for a programme of work from 2023-2025
   to ensure workflows with large datasets (>10-100TB) are tractable on the
   proposed UKSRC proto-node by 2024 to prepare the UK SKA Science
   Community for SKA science and to allow the UK to participate in SKA
   engineering commissioning work from 2024 onwards.
TC 0
ZA 0
ZS 0
Z8 0
ZR 0
ZB 0
Z9 0
U1 0
U2 1
G1 ST/Y000447/1
DA 2024-04-24
UT GRANTS:17618401
ER

PT C
AU Guyot, Alexis
   Leclercq, Eric
   Gillet, Annabelle
   Cullot, Nadine
BE Wrembel, R
   Gamper, J
   Kotsis, G
   Tjoa, AM
   Khalil, I
TI Preventing Technical Errors in Data Lake Analyses with Type Theory
SO BIG DATA ANALYTICS AND KNOWLEDGE DISCOVERY, DAWAK 2023
SE Lecture Notes in Computer Science
VL 14148
BP 18
EP 24
DI 10.1007/978-3-031-39831-5_2
DT Proceedings Paper
PD 2023
PY 2023
AB Data analysts compose various operators provided by data lakes to
   conduct their analyses on big data through complex analytical workflows.
   In this article, we present a formal framework based on type theory to
   prevent technical errors in such compositions of operators. This
   framework uses restrictions on type definitions to transform technical
   errors into type errors. We show how to use this framework to prevent
   errors related to schema or model transformations in analytical
   workflows. We provide an open-source implementation in Scala which can
   be used to detect errors at compile time.
CT 25th International Conference on Big Data Analytics and Knowledge
   Discovery (DaWaK)
CY AUG 28-30, 2023
CL Penang, MALAYSIA
SP Soft Comput Center Hagenburg; Inst telecooperat; Web applicat Soc
RI CULLOT, nadine/; LECLERCQ, Eric/AAY-9094-2020; Gillet, Annabelle/
OI CULLOT, nadine/0000-0003-1307-3287; LECLERCQ, Eric/0000-0001-6382-2288;
   Gillet, Annabelle/0000-0002-4204-9262
Z8 0
ZB 0
ZS 0
ZA 0
ZR 0
TC 0
Z9 0
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-39830-8; 978-3-031-39831-5
DA 2024-11-13
UT WOS:001330381900002
ER

PT J
AU Jeremy Yates; Louise Chisholm
TI The UK Square Kilometre Array Regional Centre 2023-2025
DT Awarded Grant
PD Jan 01 2023
PY 2023
AB The Square Kilometre Array Observatory (SKAO) is a large,
   next-generation radio telescope that is planned to be many times more
   sensitive than the current most sensitive telescopes in the world and
   transform our view of the Universe. It is a global mega-science project
   involving scientists and engineers from institutes and industry partners
   in 16 member countries.  The Observatory comprises two telescope
   facilities; one located in South Africa, SKA-MID, to observe radio
   frequencies between 350MHz and 15.4GHz, and one located in Australia,
   SKA-LOW, to observe lower band frequencies between 50MHz and 350MHz. The
   SKAO's Global Headquarters (HQ), from where the SKA Design Authority
   directs and manages the whole SKA Programme, is adjacent to the
   University of Manchester's (UoM's) Jodrell Bank Observatory (JBO), home
   to the iconic Lovell telescope and operational HQ of the e-MERLIN
   interferometer, an SKA Pathfinder instrument. The SKA is one of a small
   number of flagship astronomical instruments that will span the entire
   electromagnetic spectrum from radio to gamma rays, and beyond the
   electromagnetic spectrum to gravitational waves, cosmic rays and
   neutrinos, and whose collective aim is chart the full history of the
   universe from its beginnings in the Big Bang to the present day.  The
   operation of the SKA Observatory assumes the existence of SKA Regional
   Centres (SRCs) to deliver a range of support to the science community.
   The SRCs are required in order to provide the main portal for scientists
   to access the SKA including provision of computing resources and support
   to enable the science user community to analyse and extract science from
   data products produced by the SKA. An SRC Network (SRCNet) will be made
   up of SRCs distributed around the world in SKA Member countries. Each
   SRC will be required to conform to agreed standards in protocols, data
   architecture and information management policies to ensure that they
   appear as a single federated entity to SKA users. The SRCNet will
   provide a collection of both services and infrastructure that will
   comprise a global capability to distribute, process and curate the data
   from the SKA telescopes. The SRCNet will provide the basic governance
   and operational model and structures, and the baseline functionality of
   the SRC network. SKAO member states and SRC stakeholders are already
   engaging in the design of the SRC Network.  This joint proposal aims to
   deliver a working prototype of an SKA Regional Centre (SRC) node that
   will have 20% of the capacity and 80% of the functionality required by a
   SRC node when SKA becomes fully operational. The UK's contribution to
   this work will bring international expertise in the areas of wide field
   radio interferometry, all sky pulsar and transient detection, the
   characterisation of radio flux data, e-infrastructure, advanced cloud
   technologies, cloud and hardware platforms, remote job provision, high
   throughput network and data movement, big data performant storage
   systems and hierarchies, identity management, scientific computing, high
   performance and GPU computing, the FAIRirification of digital artifacts,
   the automated archiving, curation, collection and co-ordination of data
   artifacts. All these are needed for a programme of work from 2023-2025
   to ensure workflows with large datasets (>10-100TB) are tractable on the
   proposed UKSRC proto-node by 2024 to prepare the UK SKA Science
   Community for SKA science and to allow the UK to participate in SKA
   engineering commissioning work from 2024 onwards.
ZS 0
ZB 0
TC 0
Z8 0
ZR 0
ZA 0
Z9 0
U1 0
U2 0
G1 ST/X00256X/1
DA 2024-04-24
UT GRANTS:17622682
ER

PT J
AU Keith Grainge
TI The UK Square Kilometre Array Regional Centre 2023-2025
DT Awarded Grant
PD Jan 01 2023
PY 2023
AB The Square Kilometre Array Observatory (SKAO) is a large,
   next-generation radio telescope that is planned to be many times more
   sensitive than the current most sensitive telescopes in the world and
   transform our view of the Universe. It is a global mega-science project
   involving scientists and engineers from institutes and industry partners
   in 16 member countries.    The Observatory comprises two telescope
   facilities; one located in South Africa, SKA-MID, to observe radio
   frequencies between 350MHz and 15.4GHz, and one located in Australia,
   SKA-LOW, to observe lower band frequencies between 50MHz and 350MHz. The
   SKAO's Global Headquarters (HQ), from where the SKA Design Authority
   directs and manages the whole SKA Programme, is adjacent to the
   University of Manchester's (UoM's) Jodrell Bank Observatory (JBO), home
   to the iconic Lovell telescope and operational HQ of the e-MERLIN
   interferometer, an SKA Pathfinder instrument. The SKA is one of a small
   number of flagship astronomical instruments that will span the entire
   electromagnetic spectrum from radio to gamma rays, and beyond the
   electromagnetic spectrum to gravitational waves, cosmic rays and
   neutrinos, and whose collective aim is chart the full history of the
   universe from its beginnings in the Big Bang to the present day.    The
   operation of the SKA Observatory assumes the existence of SKA Regional
   Centres (SRCs) to deliver a range of support to the science community.
   The SRCs are required in order to provide the main portal for scientists
   to access the SKA including provision of computing resources and support
   to enable the science user community to analyse and extract science from
   data products produced by the SKA. An SRC Network (SRCNet) will be made
   up of SRCs distributed around the world in SKA Member countries. Each
   SRC will be required to conform to agreed standards in protocols, data
   architecture and information management policies to ensure that they
   appear as a single federated entity to SKA users. The SRCNet will
   provide a collection of both services and infrastructure that will
   comprise a global capability to distribute, process and curate the data
   from the SKA telescopes. The SRCNet will provide the basic governance
   and operational model and structures, and the baseline functionality of
   the SRC network. SKAO member states and SRC stakeholders are already
   engaging in the design of the SRC Network.   This joint proposal aims to
   deliver a working prototype of an SKA Regional Centre (SRC) node that
   will have 20% of the capacity and 80% of the functionality required by a
   SRC node when SKA becomes fully operational. The UK's contribution to
   this work will bring international expertise in the areas of wide field
   radio interferometry, all sky pulsar and transient detection, the
   characterisation of radio flux data, e-infrastructure, advanced cloud
   technologies, cloud and hardware platforms, remote job provision, high
   throughput network and data movement, big data performant storage
   systems and hierarchies, identity management, scientific computing, high
   performance and GPU computing, the FAIRirification of digital artifacts,
   the automated archiving, curation, collection and co-ordination of data
   artifacts. All these are needed for a programme of work from 2023-2025
   to ensure workflows with large datasets (>10-100TB) are tractable on the
   proposed UKSRC proto-node by 2024 to prepare the UK SKA Science
   Community for SKA science and to allow the UK to participate in SKA
   engineering commissioning work from 2024 onwards.
ZB 0
TC 0
Z8 0
ZS 0
ZA 0
ZR 0
Z9 0
U1 0
U2 0
G1 ST/X002578/1
DA 2024-04-24
UT GRANTS:17621883
ER

PT C
AU Krtalic, A.
   Divjak, A. Kuvezdic
   Miletic, A.
BE El-Sheimy, N
   Abdelbary, AA
   El-Bendary, N
   Mohasseb, Y
TI TOWARD DATA LAKES FOR CRISIS MANAGEMENT
SO GEOSPATIAL WEEK 2023, VOL. 48-1
SE International Archives of the Photogrammetry, Remote Sensing and Spatial
   Information Sciences
BP 539
EP 546
DI 10.5194/isprs-archives-XLVIII-1-W2-2023-539-2023
DT Proceedings Paper
PD 2023
PY 2023
AB The content of the data lake comes (is filled) from different sources,
   and different users (experts in various fields) of the same data can
   download and analyse the same data for their (different) needs and
   analysis. Big Data about the human environment and the effect of natural
   and human-caused disasters (in this case: heat islands, earthquakes and
   lava flows, and landmine contamination) on that environment have been
   available to many people for years and are the subject of discussions,
   but there are still numerous research challenges in the form of
   structuring and storing data and analysis results. This implies certain
   requirements for efficient integration, access and querying of the
   various data in the data repository for the described purpose. Data
   lakes and data warehouses are offered as solutions to this problem.
   Well-designed data lakes can be a basic building block for different
   solutions in the analysis of the effects of disasters on the
   environment, and high-quality data warehouses for modelling future
   potential disasters in the same area. This paper presents certain
   personal observations and certain proposals for the creation of
   efficient data lakes and data warehouses (based on many years of work on
   problems in areas: humanitarian demining, heat islands and volcanic
   activity) for the needs of decision-making in crisis based on examples
   from practice. The goal is to influence the development of a unique
   framework for the creation and maintenance of a data lake, in terms of
   its better utilization so that it does not become a data swamp.
CT 5th International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS)
   Geospatial Week (GSW)
CY SEP 02-07, 2023
CL Cairo, EGYPT
SP Int Soc Photogrammetry & Remote Sensing
RI Kuveždić Divjak, Ana/AAB-8293-2019; Krtalić, Andrija/AAG-4439-2019; Miletić, Anea/KXR-9745-2024
OI Kuveždić Divjak, Ana/0000-0003-1059-8395; Miletić,
   Anea/0000-0002-3324-201X
TC 0
ZR 0
ZA 0
Z8 0
ZB 0
ZS 0
Z9 0
U1 0
U2 0
SN 1682-1750
EI 2194-9034
BN *****************
DA 2024-04-18
UT WOS:001185682000074
ER

PT J
AU Leah Morabito
TI The UK Square Kilometre Array Regional Centre 2023-2025
DT Awarded Grant
PD Jan 01 2023
PY 2023
AB The Square Kilometre Array Observatory (SKAO) is a large,
   next-generation radio telescope that is planned to be many times more
   sensitive than the current most sensitive telescopes in the world and
   transform our view of the Universe. It is a global mega-science project
   involving scientists and engineers from institutes and industry partners
   in 16 member countries.  The Observatory comprises two telescope
   facilities; one located in South Africa, SKA-MID, to observe radio
   frequencies between 350MHz and 15.4GHz, and one located in Australia,
   SKA-LOW, to observe lower band frequencies between 50MHz and 350MHz. The
   SKAO's Global Headquarters (HQ), from where the SKA Design Authority
   directs and manages the whole SKA Programme, is adjacent to the
   University of Manchester's (UoM's) Jodrell Bank Observatory (JBO), home
   to the iconic Lovell telescope and operational HQ of the e-MERLIN
   interferometer, an SKA Pathfinder instrument. The SKA is one of a small
   number of flagship astronomical instruments that will span the entire
   electromagnetic spectrum from radio to gamma rays, and beyond the
   electromagnetic spectrum to gravitational waves, cosmic rays and
   neutrinos, and whose collective aim is chart the full history of the
   universe from its beginnings in the Big Bang to the present day.  The
   operation of the SKA Observatory assumes the existence of SKA Regional
   Centres (SRCs) to deliver a range of support to the science community.
   The SRCs are required in order to provide the main portal for scientists
   to access the SKA including provision of computing resources and support
   to enable the science user community to analyse and extract science from
   data products produced by the SKA. An SRC Network (SRCNet) will be made
   up of SRCs distributed around the world in SKA Member countries. Each
   SRC will be required to conform to agreed standards in protocols, data
   architecture and information management policies to ensure that they
   appear as a single federated entity to SKA users. The SRCNet will
   provide a collection of both services and infrastructure that will
   comprise a global capability to distribute, process and curate the data
   from the SKA telescopes. The SRCNet will provide the basic governance
   and operational model and structures, and the baseline functionality of
   the SRC network. SKAO member states and SRC stakeholders are already
   engaging in the design of the SRC Network.  This joint proposal aims to
   deliver a working prototype of an SKA Regional Centre (SRC) node that
   will have 20% of the capacity and 80% of the functionality required by a
   SRC node when SKA becomes fully operational. The UK's contribution to
   this work will bring international expertise in the areas of wide field
   radio interferometry, all sky pulsar and transient detection, the
   characterisation of radio flux data, e-infrastructure, advanced cloud
   technologies, cloud and hardware platforms, remote job provision, high
   throughput network and data movement, big data performant storage
   systems and hierarchies, identity management, scientific computing, high
   performance and GPU computing, the FAIRirification of digital artifacts,
   the automated archiving, curation, collection and co-ordination of data
   artifacts. All these are needed for a programme of work from 2023-2025
   to ensure workflows with large datasets (>10-100TB) are tractable on the
   proposed UKSRC proto-node by 2024 to prepare the UK SKA Science
   Community for SKA science and to allow the UK to participate in SKA
   engineering commissioning work from 2024 onwards.
ZR 0
ZA 0
Z8 0
TC 0
ZS 0
ZB 0
Z9 0
U1 0
U2 1
G1 ST/X002586/1
DA 2024-04-24
UT GRANTS:17627162
ER

PT B
AU Machado, Ricardo Jesus
Z2  
TI The Growing Importance of Data Semantics in Large Organizations: A Case
   Study of Nokia Network and Solutions
DT Dissertation/Thesis
PD Jan 01 2023
PY 2023
Z8 0
ZR 0
ZB 0
ZS 0
ZA 0
TC 0
Z9 0
U1 0
U2 0
BN 9798383899830
UT PQDT:118785324
ER

PT J
AU Martin Hardcastle
TI The UK Square Kilometre Array Regional Centre 2023-2025
DT Awarded Grant
PD Jan 01 2023
PY 2023
AB The Square Kilometre Array Observatory (SKAO) is a large,
   next-generation radio telescope that is planned to be many times more
   sensitive than the current most sensitive telescopes in the world and
   transform our view of the Universe. It is a global mega-science project
   involving scientists and engineers from institutes and industry partners
   in 16 member countries.  The Observatory comprises two telescope
   facilities; one located in South Africa, SKA-MID, to observe radio
   frequencies between 350MHz and 15.4GHz, and one located in Australia,
   SKA-LOW, to observe lower band frequencies between 50MHz and 350MHz. The
   SKAO's Global Headquarters (HQ), from where the SKA Design Authority
   directs and manages the whole SKA Programme, is adjacent to the
   University of Manchester's (UoM's) Jodrell Bank Observatory (JBO), home
   to the iconic Lovell telescope and operational HQ of the e-MERLIN
   interferometer, an SKA Pathfinder instrument. The SKA is one of a small
   number of flagship astronomical instruments that will span the entire
   electromagnetic spectrum from radio to gamma rays, and beyond the
   electromagnetic spectrum to gravitational waves, cosmic rays and
   neutrinos, and whose collective aim is chart the full history of the
   universe from its beginnings in the Big Bang to the present day.  The
   operation of the SKA Observatory assumes the existence of SKA Regional
   Centres (SRCs) to deliver a range of support to the science community.
   The SRCs are required in order to provide the main portal for scientists
   to access the SKA including provision of computing resources and support
   to enable the science user community to analyse and extract science from
   data products produced by the SKA. An SRC Network (SRCNet) will be made
   up of SRCs distributed around the world in SKA Member countries. Each
   SRC will be required to conform to agreed standards in protocols, data
   architecture and information management policies to ensure that they
   appear as a single federated entity to SKA users. The SRCNet will
   provide a collection of both services and infrastructure that will
   comprise a global capability to distribute, process and curate the data
   from the SKA telescopes. The SRCNet will provide the basic governance
   and operational model and structures, and the baseline functionality of
   the SRC network. SKAO member states and SRC stakeholders are already
   engaging in the design of the SRC Network.  This joint proposal aims to
   deliver a working prototype of an SKA Regional Centre (SRC) node that
   will have 20% of the capacity and 80% of the functionality required by a
   SRC node when SKA becomes fully operational. The UK's contribution to
   this work will bring international expertise in the areas of wide field
   radio interferometry, all sky pulsar and transient detection, the
   characterisation of radio flux data, e-infrastructure, advanced cloud
   technologies, cloud and hardware platforms, remote job provision, high
   throughput network and data movement, big data performant storage
   systems and hierarchies, identity management, scientific computing, high
   performance and GPU computing, the FAIRirification of digital artifacts,
   the automated archiving, curation, collection and co-ordination of data
   artifacts. All these are needed for a programme of work from 2023-2025
   to ensure workflows with large datasets (>10-100TB) are tractable on the
   proposed UKSRC proto-node by 2024 to prepare the UK SKA Science
   Community for SKA science and to allow the UK to participate in SKA
   engineering commissioning work from 2024 onwards.
RI Hardcastle, Martin/AAM-1395-2020
Z8 0
ZS 0
TC 0
ZA 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
G1 ST/X002543/1
DA 2024-04-24
UT GRANTS:17609362
ER

PT J
AU Philip Best
TI The UK Square Kilometre Array Regional Centre 2023-2025 (Edinburgh
   contribution)
DT Awarded Grant
PD Jan 01 2023
PY 2023
AB The Square Kilometre Array Observatory (SKAO) is a large,
   next-generation radio telescope that is planned to be many times more
   sensitive than the current most sensitive telescopes in the world and
   transform our view of the Universe. It is a global mega-science project
   involving scientists and engineers from institutes and industry partners
   in 16 member countries.   The Observatory comprises two telescope
   facilities; one located in South Africa, SKA-MID, to observe radio
   frequencies between 350MHz and 15.4GHz, and one located in Australia,
   SKA-LOW, to observe lower band frequencies between 50MHz and 350MHz. The
   SKAO's Global Headquarters (HQ), from where the SKA Design Authority
   directs and manages the whole SKA Programme, is adjacent to the
   University of Manchester's (UoM's) Jodrell Bank Observatory (JBO), home
   to the iconic Lovell telescope and operational HQ of the e-MERLIN
   interferometer, an SKA Pathfinder instrument. The SKA is one of a small
   number of flagship astronomical instruments that will span the entire
   electromagnetic spectrum from radio to gamma rays, and beyond the
   electromagnetic spectrum to gravitational waves, cosmic rays and
   neutrinos, and whose collective aim is chart the full history of the
   universe from its beginnings in the Big Bang to the present day.   The
   operation of the SKA Observatory assumes the existence of SKA Regional
   Centres (SRCs) to deliver a range of support to the science community.
   The SRCs are required in order to provide the main portal for scientists
   to access the SKA including provision of computing resources and support
   to enable the science user community to analyse and extract science from
   data products produced by the SKA. An SRC Network (SRCNet) will be made
   up of SRCs distributed around the world in SKA Member countries. Each
   SRC will be required to conform to agreed standards in protocols, data
   architecture and information management policies to ensure that they
   appear as a single federated entity to SKA users. The SRCNet will
   provide a collection of both services and infrastructure that will
   comprise a global capability to distribute, process and curate the data
   from the SKA telescopes. The SRCNet will provide the basic governance
   and operational model and structures, and the baseline functionality of
   the SRC network. SKAO member states and SRC stakeholders are already
   engaging in the design of the SRC Network.  This joint proposal aims to
   deliver a working prototype of an SKA Regional Centre (SRC) node that
   will have 20% of the capacity and 80% of the functionality required by a
   SRC node when SKA becomes fully operational. The UK's contribution to
   this work will bring international expertise in the areas of wide field
   radio interferometry, all sky pulsar and transient detection, the
   characterisation of radio flux data, e-infrastructure, advanced cloud
   technologies, cloud and hardware platforms, remote job provision, high
   throughput network and data movement, big data performant storage
   systems and hierarchies, identity management, scientific computing, high
   performance and GPU computing, the FAIRirification of digital artifacts,
   the automated archiving, curation, collection and co-ordination of data
   artifacts. All these are needed for a programme of work from 2023-2025
   to ensure workflows with large datasets (>10-100TB) are tractable on the
   proposed UKSRC proto-node by 2024 to prepare the UK SKA Science
   Community for SKA science and to allow the UK to participate in SKA
   engineering commissioning work from 2024 onwards.
Z8 0
ZB 0
ZA 0
ZR 0
TC 0
ZS 0
Z9 0
U1 0
U2 0
G1 ST/X002527/1
DA 2024-04-24
UT GRANTS:17630095
ER

PT B
AU Severiano, Renato Alexandre Pires
   Neto, Miguel Castro
   Jardim, Bruno
Z2  
TI [not available]
DT Dissertation/Thesis
PD Oct 13 2024
PY 2024
ZB 0
ZA 0
TC 0
ZS 0
ZR 0
Z8 0
Z9 0
U1 0
U2 2
BN 9798384207801
UT PQDT:100694316
ER

PT J
AU Shabani, B.
   Ali-Lavroff, J.
   Holloway, D. S.
   Penev, S.
   Dessi, D.
   Thomas, G.
TI INTELLIGENT MONITORING OF A LARGE CATAMARAN FERRY
SO INTERNATIONAL JOURNAL OF MARITIME ENGINEERING
VL 165
BP A11
EP A22
DI 10.5750/ijme.v165iA1.791
PN A1
DT Article
PD JAN-MAR 2023
PY 2023
AB Wave load cycles, wet-deck slamming events, accelerations and motion
   comfort are important considerations for high-speed catamarans operating
   in moderate to large waves. Although developing a hull monitoring system
   according to classification guidelines for such vessels is broadly
   acceptable, the data processing requirements for outputs such as
   rainflow counting, filtering, probability distribution, fatigue damage
   estimation and warning due to slamming can be as sophisticated to
   implement as the system components themselves. Advanced analytics such
   as machine learning and deep learning data pipelines will also create
   more complexities for such systems, if included. This paper provides an
   overview of data analytics methods and cloud computing resources
   employed for remotely monitoring motions and structural responses of a
   111 m high-speed catamaran. To satisfy the data processing requirements,
   MATLAB Reference Architectures on Amazon Web Services (AWS) were used.
   Such combination enabled fast parallel computing and advanced feature
   engineering in a time-efficient manner. A MATLAB Production Server on
   AWS has been set up for near real-time analytics and execution of
   functions developed according to the class guidelines. A case study
   using Long ShortTerm Memory (LSTM) networks for ship speed and Motion
   Sickness Incidence (MSI) is provided and discussed. Such data
   architecture provides a flexible and scalable solution, leading to
   deeper insights through big data processing and machine learning, which
   supports hull monitoring functions as a service.
RI Thomas, Giles/; SHABANI, BABAK/ITW-2577-2023; Penev, Spiridon/L-6262-2016; Holloway, Damien/J-7865-2014
OI Thomas, Giles/0000-0002-6122-4329; SHABANI, BABAK/0000-0002-2910-962X;
   Penev, Spiridon/0000-0002-3669-0010; Holloway,
   Damien/0000-0001-9537-2744
ZS 0
ZR 0
ZA 0
Z8 0
ZB 0
TC 0
Z9 0
U1 2
U2 10
SN 1479-8751
EI 1740-0716
DA 2023-08-18
UT WOS:001039463100003
ER

PT C
AU Sore, Safiatou
   Traore, Yaya
   Bikienga, Moustapha
   Ouedraogo, Frederic T.
BE Saeed, RA
   Bakari, AD
   Sheikh, YH
TI An Architecture of a Data Lake for the Sharing, Agricultural Knowledge
   in Burkina Faso
SO TOWARDS NEW E-INFRASTRUCTURE AND E-SERVICES FOR DEVELOPING COUNTRIES,
   AFRICOMM 2022
SE Lecture Notes of the Institute for Computer Sciences Social Informatics
   and Telecommunications Engineering
VL 499
BP 209
EP 218
DI 10.1007/978-3-031-34896-9_13
DT Proceedings Paper
PD 2023
PY 2023
AB With the advent of big data today, agricultural research institutes have
   stored enough data in databases at every level. This data contains a lot
   of knowledge that is often not well known by researchers and on the
   other hand, it may contain hidden semantic relationships. Therefore, it
   is important to find an adequate storage system for these data in order
   to discover new useful information that can be shared between the
   different actors in the field to improve agricultural productivity in
   Burkina. The concept that best responds to this storage and analysis
   problem is the data lake. Indeed, data lakes offer the possibility of
   storing a large volume of data in any format and data structure. They
   also provide services for data access and analysis. Our work revolves
   around the integration and storage of agricultural data (structured,
   semi-structured and unstructured). With the plurality of agricultural
   storage systems, the heterogeneity of the actors in the field and the
   disparity of the tools leads us to think that the data lake is a
   solution.
   The purpose of this essay is to investigate data lakes and their
   requirements while also putting out a broad framework for an
   agricultural data lake in Burkina Faso.
CT 14th International Conference on e Infrastructure and e Services for
   Developing Countries
CY DEC 05-07, 2022
CL Zanzibar, TANZANIA
OI BIKIENGA, Moustapha/0009-0006-2505-7937; Sanou,
   Safiatou/0009-0009-8327-2899
Z8 0
ZA 0
ZR 0
TC 0
ZB 0
ZS 0
Z9 0
U1 0
U2 0
SN 1867-8211
EI 1867-822X
BN 978-3-031-34895-2; 978-3-031-34896-9
DA 2023-01-01
UT WOS:001447391100013
ER

PT J
AU Zemnickis, Janis
TI Data Warehouse Data Model Improvements from Customer Feedback
SO BALTIC JOURNAL OF MODERN COMPUTING
VL 11
IS 3
BP 475
EP 499
DI 10.22364/bjmc.2023.11.3.08
DT Article
PD 2023
PY 2023
AB The amount of unstructured data in organisations is growing each year.
   Unstructured data could hide huge amounts of information, which could
   improve an organisation's performance and daily processes. In comparison
   to structured data, unstructured data is more complex to analyse and get
   useful information from due to a lack of competence and experience in
   organisations. Big data technologies open new opportunities for
   unstructured data processing and for later analysis. Organisations build
   data warehouses for the purpose of data analysis. A data lake, which is
   implemented using big data technologies, is used as one of the data
   sources for data warehouses. Meanwhile the data warehouse and its model
   creation could be a challenging task in order to meet all stakeholders'
   expectations and support all business use cases. There are known
   approaches on how to develop data warehouse data models by relating the
   organisation's key performance indicators (KPI) to information
   requirements, but in this article a new method is introduced which
   allows one to extend the data warehouse data model using unstructured
   data in a semi-automated way. This method is designed for the purpose of
   processing client feedback in the field in which the organisation
   operates in order to acquire relevant aspects about the organisation's
   operations, in other words quantitative KPIs. From the acquired KPI
   information, the requirements of the data warehouse are obtained, which
   are transformed into the attributes of the data model of the data
   warehouse. Newly acquired attributes are integrated into the existing
   data model of the organisation's data warehouse. The method can be
   divided into four phases: 1) data gathering - streaming and storing data
   on server 2) calculations - natural language processing (NLP) and
   special calculations to prepare data for KPI generation 3) KPI
   generation - phase where new KPIs and KPI values are defined using
   special algorithms 4) data model improvements - phase where new data
   model elements are discovered and integrated into the existing warehouse
   data model.
ZA 0
Z8 0
ZR 0
ZB 0
TC 0
ZS 0
Z9 0
U1 2
U2 21
SN 2255-8942
EI 2255-8950
DA 2023-11-09
UT WOS:001089057000008
ER

PT P
AU YUAN H
   HUANG Y
   CHEN Y
   XU G
   DENG S
TI Method for writing original unstructured data file            into
   distributed storage system of data lake, involves            obtaining
   data processing request, setting effective            data validity of
   data to-be-processed, and deleting            target object whose
   processing type is deleted
PN CN115543198-A; CN115543198-B
AE CMG FINTECH CO LTD
AB 
   NOVELTY - The method involves obtaining a data                processing
   request. The data processing requests                are analyzed to
   obtain data to be processed,                to-be-processed metadata and
   processing types. An                expansion metadata of the data to-to
   be processed                is generated. The expansion metadata
   includes a                data key value, a data version and a data
   validity.                An existing target object in a preset storage  
   system is located by using the data key and the                data
   version when the processing type is changed or                deleted. A
   data validity corresponding to the                target object is set
   as invalid. The processing                type to be stored data is
   stored into the data                storage space in the preset storage.
   The                corresponding data and the expansion metadata are    
   stored in the metadata-set storage system of the                storage
   system when the data verification is                passed.
   USE - Method for writing an original unstructured                data
   file into a distributed storage system of a                data lake.
   ADVANTAGE - The method enables improving unstructured data              
   management efficiency. The data validity of the                data to
   be processed is set to be effective, so                that the
   real-time property of the preset storage                system can be
   ensured.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) A device
   for writing an original                unstructured data file into a
   distributed storage                system of a data lake;(2) Electronic
   equipment;(3) A computer readable storage medium               
   comprising a set of instructions for writing an                original
   unstructured data file into a distributed                storage system
   of a data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating 
   of a method for writing an original unstructured                data
   file into a distributed storage system of a                data lake
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2023-02-04
UT DIIDW:202304730P
ER

PT P
AU LI J
   HAN T
   ZHOU W
   NI X
   XU Y
   LIU Y
   HE W
   LIU X
TI Generating meteorological data file catalog            service based on
   virtual data lake involves            distributing node position of
   virtual data lake, and            directly using authorized catalog in
   virtual file            system of virtual data lake server as local
   storage for            reading and writing
PN CN115484276-A
AE NAT METEOROLOGICAL INFORMATION CENT
AB 
   NOVELTY - The method involves distributing a node               
   position of a virtual data lake, configuring                related
   information of a heterogeneous file system                by a manager
   through a background management                service system on a
   virtual data lake server end,                creating an external
   virtual service catalog                through the background management
   service system                interface on the virtual data lake server
   end,                aggregating data in multiple sets of heterogeneous  
   file systems to form a unified service virtual                catalog
   view to realize data virtualization into                the lake,
   creating users and user groups in the                background
   management service system on the virtual                data lake
   service, obtaining the read and write                permissions of
   group to the catalog, hanging the                virtual data lake file
   catalog to access the                client, and directly using the
   authorized catalog                in the virtual file system of virtual
   data lake                server as local storage for reading and        
          writing.
   USE - The method is useful for generating                meteorological
   data file catalog service based on                virtual data lake.
   ADVANTAGE - The method: integrates multiple heterogeneous               
   storage systems e.g. network attached storage (NAS)                and
   object storage through the virtual data lake                for unified
   configuration; provides stable and                logically unified
   unstructured data access catalog,                so that is no longer
   requires each application to                configure the underlying
   storage system, thus                simplifying the configuration
   management of the                program; unifies the namespace of data
   management                through the virtual catalog constructed by the
   virtual data lake; supports common storage                interfaces;
   promotes and supports the separation of                computing and
   storage in the user's overall                architecture design, which
   provides great                flexibility for modern data processing;
   and solves                the technical problem that one storage device 
            cannot meet the capacity requirement at                present.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   meteorological data file catalog service generating               
   system based on virtual data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a structure block diagram 
   of the meteorological data file catalog service               
   generating system based on virtual data lake                (Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2023-01-15
UT DIIDW:2022F8667K
ER

PT P
AU DRIESEN V
   EBERLEIN P
TI Method for provisioning virtual private data lakes            within
   data exploration system and correlating data            from disparate
   data sources for e.g. visualizations, by            using e.g. desktop
   computer, involves reading portions            of enterprise-provided
   data, external data and            correlation data by exploration tool
PN US2022398258-A1
AE SAP SE
AB 
   NOVELTY - The method (400) involves providing a virtual               
   private data lake (VPDL) within a data exploration                system
   (402). Enterprise-provided data is stored                (404) in the
   VPDL. External data is imported (408)                into the data
   exploration system from an external                data source.
   Associations between a sub-set of the                enterprise-provided
   data and a sub-set of the                external data is automatically
   identified to store                correlation data in the VPDL in
   response to                association. A portion of the
   enterprise-provided                data, a portion of the external data,
   and a portion                of the correlation data are read by a data 
   exploration tool, where the data exploration tool               
   generates a set of visualizations and analytics by               
   processing the portion of the enterprise-provided                data,
   the portion of the external data, and the                portion of the
   correlation data.
   USE - Method for provisioning VPDLs within a data               
   exploration system and correlating data from                disparate
   data sources for ad-hoc data analytics                i.e. slicing and
   dicing data to derive aggregated                results, by using a
   computing device. Uses include                but are not limited to
   ad-hoc data analytics such                as dashboards, visualizations,
   big data processing,                real-time analytics and machine
   learning (ML), and                computing device a desktop computer, a
   laptop                computer, a handheld computer, a tablet computer,
   a                personal digital assistant (PDA), a cellular           
   telephone, a network appliance, a camera, a                smartphone,
   an enhanced general packet radio                service (EGPRS) mobile
   phone and a media                player.
   ADVANTAGE - The method enables realizing user-specific               
   ad-hoc data analysis on data combined from multiple                and
   disparate data sources by the data exploration                system,
   realizing provision of VPDLs for ad hoc                data analytics,
   combining and correlating VPDL                enables enterprise data
   with external data, so that                the data onboarded to the VDL
   can be defined by a                user. The method enables allowing the
   user to                explore the combined data with the restricted    
   capabilities of a desktop tool without the advanced               
   features that enterprise-level tools would provide,                so
   that the process can time-consuming, resource                intensive
   and cumbersome.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   non-transitory computer-readable storage                medium for
   storing a set of instructions for                provisioning VPDLs
   within a data exploration system                and correlating data
   from disparate data sources                for ad-hoc data analytics by
   using a computing                device;(2) a system for provisioning
   VPDLs within a                data exploration system and correlating
   data from                disparate data sources for ad-hoc data
   analytics by                using a computing device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for provisioning VPDLs within a
   data                exploration system and correlating data from        
   disparate data sources for ad-hoc data analytics by                using
   a computing device.400Provisioning VPDLs within data exploration        
   system and correlating data from disparate data                sources
   for ad-hoc data analytics by using                computing
   device402Providing VPDL within data exploration               
   system404Storing enterprise-provided data in               
   VPDL406Projecting sub-set of EDL to VPDL408Importing external data into
   data                exploration system from external data
   source410Providing correlation data412Executing exploration,
   visualization, and                analytics414Exporting results to
   enterprise                system416Deleting VPDL
Z9 0
U1 0
U2 0
DA 2022-12-29
UT DIIDW:2022F54883
ER

PT P
AU LI Y
   ZHANG J
   MENG F
   LIU Y
TI Parking operation management system with layered            cloud
   architecture for real-time parking state of car            park, has
   parking operation management system that is            configured to
   monitor real-time parking status and            income of multiple
   parking lots, and parking space is            predicted, through big
   data
PN CN115472034-A
AE CHINA COMMUNICATIONS INFORMATION TECHNOL
AB 
   NOVELTY - The system has a parking data communication               
   module that is adapted to the protocol of                Modbus(RTM:
   Data communications protocol), socket,                hypertext transfer
   protocol (HTTP), serial port, MQ                telemetry transport
   (MQTT) parking equipment. The                cloud architecture of the
   parking operation                management system is used for
   transmission                integration. The management of different
   databases                stored in the data lake of the parking
   operation                management system, according to the type of
   data.                The data lake of the parking operation management  
   system communicates with the cloud server of the                cloud
   architecture of the parking operation                management system.
   The integrated data of the cloud                server is used as a data
   base. The parking-related                data is managed to ensure data
   operation. The                parking operation management system is
   used to                monitor the real-time parking status and income
   of                multiple parking lots, and the parking space is       
           predicted, through the big data.
   USE - Parking operation management system with                layered
   cloud architecture for real-time parking                state of car
   park.
   ADVANTAGE - The system realizes the communication between               
   Modbus, Socket, HTTP, serial port (485/232), MQTT               
   protocol and cloud server database during parking,                and
   effectively guarantees the rapid communication                between
   each data and cloud server. The system                provides the
   required analysis and decision-making                for the overall
   operation of the parking lot, with                the cloud server as
   the service framework, based on                data communication,
   combined with the cloud module                to effectively transmit
   and integrate data. The                system provides effective basis
   for improving                occupancy of parking space, improving
   comprehensive                treatment level of urban comprehensive
   treatment,                and a parking space scheduling management
   module                communicates with each supporting reservation of  
            the car park, when there is spare parking                space.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of
   parking                operation management system with layered cloud   
   architecture. (Drawing includes non-English                language
   text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022F6232N
ER

PT P
AU SUN G
   LIANG R
   CHEN Z
   YANG L
TI Modeling method for data lake metadata, involves            classifying
   metadata in data lake according to action            range of metadata,
   using constant star table for data            storage, recording storage
   path of data entity in data            lake
PN CN115422155-A
AE UNIV ZHEJIANG TECHNOLOGY
AB 
   NOVELTY - The modeling method involves classifying               
   metadata in a data lake according to an action                range of
   the metadata. A data entity characteristic                associated
   with the metadata is referred to. An                optional attribute
   of the semantic planetary table                is updated time. The
   metadata in the data lake is                classified according to an
   action range. The data                entity type corresponding to a
   star table is used                for data storage. An entity grouping
   is to record                the metadata relationship between the data
   entities                with the same feature label. A constant star
   table                is used for data storage. The storage path of data 
                 entity is recorded in the data lake.
                       USE - Modeling method for data lake metadata.
   ADVANTAGE - The data entity of different data entity type,              
   and the corresponding three types of metadata are               
   separately modeled, and stored, which effectively                solves
   the complexity problem of the metadata                coupling, and the
   problem of heterogeneous data                support.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation
   of a modeling method for data lake metadata.                (Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2022-12-21
UT DIIDW:2022F2570W
ER

PT P
AU HARUSH O B
   EZRIELEV O
   SAVIR A
TI Method for optimizing placement of data assets in            data
   retrieval system, involves moving data assets from            one
   location to another location in system using            scoring and
   simulation to minimize latency
PN US2022374329-A1
AE DELL PROD LP
AB 
   NOVELTY - The method involves generating (1502) search               
   results and data asset recommendations in response                to a
   target user query. The recommendations and                past data
   asset access information are provided for                data assets as
   features to a time-series model for                predicting future
   data access patterns by the                target user. An expected
   latency and load risk                created by a weighted mean of these
   expected                latency and load risk values is determined and  
   scored. A placement optimization is simulated using                an
   optimization method. The data assets are moved                from one
   location to another location in the system                using the
   scoring and simulation so as to minimize                latency. The
   optimization method consists of a                genetic algorithm (GA).
   USE - Method for optimizing placement of data assets                in a
   data retrieval system storing data assets for                users in an
   enterprise.
   ADVANTAGE - The method enables utilizing a federation               
   business data lake platform (FBDL) to provide a                central
   repository for all enterprise data in large                organizations
   using big data (Big Data) processes                and data sets, thus
   ensuring effective utilization                of data resources. The
   method allows a search                engine system to adequately
   utilize or leverage                useful relationships between users
   querying the                system and between certain users and the
   various                different data assets, so that information can be
   exploited to more efficiently respond to queries by               
   returning responses that are more relevant than                those
   based on simple keyword matches.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:1) a method
   for processing queries input to a                data retrieval system
   storing data assets for users                in an enterprise; and2) a
   system for optimizing placement of data                assets in a data
   retrieval system storing data                assets for users in an
   enterprise.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method 
   for optimizing placement of data assets in a data               
   retrieval system storing data assets for users in                an
   enterprise.1502Generating search results and               
   recommendations for known and possible users1504Inputting
   recommendations and past data                assets accesses to
   time-series model to predict                future data access
   patterns1506Datermining and scoring expected latency                and
   load risk and simulating optional                placement1508Moving
   data asset locations through a                placement engine to
   inimize latency based on                scores
Z9 0
U1 0
U2 0
DA 2022-12-05
UT DIIDW:2022E67789
ER

PT P
AU LI X
   WANG J
   GUO W
   YIN W
   ZHANG J
   GUO K
   QI Y
   ZHANG Y
   ZHANG K
   LI Z
   CHEN W
   GUO F
   ZHANG Z
   ZHAN J
   LIN G
   GAO H
   YIN H
TI Post-based online training method for electric            power Internet
   majors used in digital cross-over            development of state grid
   company, involves            constructing post capability learning
   resource system            and data statistics module is used to
   coordinate and            analyze data
PN CN115357561-A
AE STATE GRID CORP CHINA; SHANDONG ELECTRIC POWER COLLEGE; STATE GRID CORP
   CHINA TECH COLLEGE BRANC
AB 
   NOVELTY - The method involves constructing a post               
   capability learning resource system. The general               
   functional modules are designed to enable Internet                majors
   to learn corresponding learning resources                independently.
   The job-based Internet professional                modules are designed
   and include digital                technology, digital infrastructure,
   and data                according to different departments of the
   Internet.                The data statistics module is used to
   coordinate                and analyze data including resource usage,
   post                personnel, training implementation, and talent      
   evaluation. The smart big screen is provided to                build an
   independent operation decision-making                platform based on
   the big data architecture.
   USE - Post-based online training method for electric               
   power Internet majors used in digital cross-over               
   development of state grid company.
   ADVANTAGE - The master-slave relationship is clear               
   integrated post capability training resource                system, the
   satisfy of the student, training has                pertinence,
   improving the training effect,                realizing maximum
   utilization of training resource.                The learning resource
   system can realize the                autonomous learning or training
   based on the                working post, it can provide the decision
   service                for the course training, teaching resource       
           management and so on.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an          
   electric power Internet professional online                training
   system based on a job position.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the        
   post-based online training method for electric                power
   Internet majors. (Drawing includes                non-English language
   text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022E67059
ER

PT P
AU LIU Q
   YANG B
   LIU R
   SUO T
   LEI T
   QIU X
   LIANG Z
   HE Z
   LIU L
   DU W
   WANG J
TI Method for processing data based on data lake,            involves
   performing multi-level hierarchical processing            on external
   data imported into data lake, and storing            corresponding
   processing results hierarchically
PN CN115357654-A
AE DS INFORMATION TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves importing (S100) external               
   data into the data lake in real time through Kafka               
   message queue technology. The external data is                provided
   with structured data, unstructured data                and
   semi-structured data. Multi-level hierarchical                processing
   is performed (S200) on the external data                imported into
   the data lake. The corresponding                processing results are
   stored hierarchically. The                data lake is provided with a
   paste source layer, a                standard resource layer, a theme
   summary layer and                an application layer.
   USE - Method for processing data based on data lake                in
   public safety management department.
   ADVANTAGE - The method realizes data centralized               
   management by using data lake, easily realizes                collection
   of text, audio, video and image data,                uses the
   distributed file system storage data, thus                reducing
   storage cost. The method allows a Flink                calculation
   engine to uniformly use a set of                processing logic and
   calculating resource, thus                avoiding resource waste, and
   improving data                real-time processing performance of the
   data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   data processing device based on data lake; and (2)                a
   computer readable storage medium storing program                for
   processing data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for processing data based on data lake.               
   (Drawing includes non-English language text)S100Step for importing
   external data into the                data lake in real time through
   Kafka message queue                technologyS200Step for performing the
   multi-level                hierarchical processing on the external data 
                 imported into the data lake
Z9 0
U1 0
U2 0
DA 2022-12-10
UT DIIDW:2022E6539U
ER

PT P
AU HARUSH O B
   EZRIELEV O
   SAVIR A
TI Computer-implemented method for optimizing            placement of data
   assets in data retrieval system,            involves moving data assets
   from one location to            another location in system using scoring
   and simulation            to minimize maximal load
PN US2022365995-A1
AE DELL PROD LP
AB 
   NOVELTY - The method involves generating (1502) search               
   results and data asset recommendations in response                to a
   target user query. The recommendations and                past data
   asset access information for data assets                as features to a
   time-series model for predicting                future data access
   patterns by the target user is                provided. An expected
   latency and load risk created                by a weighted mean of these
   expected latency and                load risk values is determined and
   scored (1506). A                placement optimization using an
   optimization method                is simulated. The data assets from
   one location to                another location in the system using the
   scoring                and simulation so as to minimize maximal load is 
                 moved (1508).
   USE - Computer-based method for optimizing placement                of
   data assets in data retrieval system storing                data assets
   for users in an enterprise. Uses                include but are not
   limited to sources, parsers,                dashboards, databases,
   stacks of databases, file                systems, enterprise services,
   and enterprise                services.
   ADVANTAGE - The method enables utilizing a federation               
   business data lake platform (FBDL) to provide a                central
   repository for all enterprise data in large                organizations
   using big data (Big Data) processes                and data sets, thus
   ensuring effective utilization                of data resources. The
   method allows a search                engine system to adequately
   utilize or leverage                useful relationships between users
   querying the                system and between certain users and the
   various                different data assets, so that information can be
   exploited to more efficiently respond to queries by               
   returning responses that are more relevant than                those
   based on simple keyword matches.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   method for processing queries input to data                retrieval
   system storing data assets for users in                enterprise; (2) a
   system for optimizing placement                of data assets in data
   retrieval system storing                data assets for users in
   enterprise.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   mehod for optimizing placement of data assets in                data
   retrieval system storing data assets.1502Step for generating search
   results and                data asset recommendations in response to a
   target                user query1504Step for providing the
   recommendations                and past data asset access information
   for data                assets as features to a time-series model for   
   predicting future data access patterns by the                target
   user1506Step for determining and scoring an                expected
   latency and load risk created by a                weighted mean of these
   expected latency and load                risk values1508Step for moving
   the data assets from one                location to another location in
   the system using                the scoring and simulation so as to
   minimize                maximal load
Z9 0
U1 0
U2 0
DA 2022-12-05
UT DIIDW:2022E27075
ER

PT P
AU WANG J
   YU Y
   WANG Y
   ZHANG G
TI Semantic knowledge base construction system, has            semantic
   knowledge base constructing module for            combining structured
   data and distribution relationship            established by each
   divided network and corresponding            processor
PN CN115329768-A
AE CHINESE ACAD SCI AUTOMATION INST
AB 
   NOVELTY - The system has a data acquisition module, a               
   first rule generation module, a second rule                generation
   module, a cost calculation module, a                rule optimization
   module, a rule subnet generation                and pre-allocation
   module, Rule sub-network                division module, semantic
   feature data extraction                module, semantic knowledge base
   construction                module.The semantic feature data extraction
   module                is configured to perform feature extraction on    
   structured data, semi-structured data, and                unstructured
   data extracted from the data lake,                remove redundant data,
   and store the extracted                feature data in a database to
   form a structure                data. The semantic knowledge base
   building module                is configured to build a semantic
   knowledge base                based on the first semantic rule and the
   second                semantic rule, combined with structured data and  
   the allocation relationship established between                each
   segmentation network and the corresponding                processor.
   USE - System for constructing cross-platform                semantic
   knowledge base.
   ADVANTAGE - The system combines expert experience and               
   artificial intelligence algorithm, extracts the                rule with
   more connotation, and models the semantic                rule of the
   user through the structured rule                description language, so
   as to realize the                cross-platform application of the
   semantic                knowledge base.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a constructing method of semantic knowledge               
   base;the device; anda computer readable storage medium storing          
        program of a semantic knowledge base                construction.
   DESCRIPTION Of DRAWING(S) - The drawing shows a frame diagram of the    
   semantic knowledge base construction system.                (Drawing
   includes non-English language text)
Z9 0
U1 0
U2 0
DA 2022-12-09
UT DIIDW:2022E36714
ER

PT P
AU ZHAO T
   LIU J
   BAO Z
TI Method for generating associated data map in real            time based
   on service object intelligent dynamic state,            involves forming
   association relationship diagram            through forward and fast
   query of data association            relationship, and forming
   association relationship            diagram through reverse fast query
PN CN115309789-A; CN115309789-B
AE WHALE CLOUD TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves collecting (S1) real-time               
   or batch data into the lake. The structured,               
   semi-structured and unstructured data are stored in                the
   lake in real time, and stored through the                storage
   component. A trigger for data entering the                lake is
   created (S2). An incremental data is read                (S3) from the
   mq component in real time, and the                incremental data is
   preprocessed to extract key                information. Key information
   is extracted (S4) to                form a data body, and a data
   correlation is                analyzed based on the data body. A
   corresponding                metadata and attribute-related data are
   found (S5)                according to the association relationship of
   the                data. A real-time technology is used (S6) to obtain  
   changing data in a timely manner. An association               
   relationship diagram is formed (S8) through forward                and
   fast query of data association relationship. An               
   association relationship diagram is formed (S9)                through
   reverse fast query of the data association                relationship.
   USE - Method for generating associated data map in                real
   time based on service object intelligent                dynamic state.
   ADVANTAGE - The knowledge graph associated with the data               
   can be drawn quickly according to the associated                relation
   of the data by a query data. The method is                convenient and
   fast to provide useful data for                business. The data of the
   fragmented data in the                original data lake can be made to
   construct the                useful data association graph, when the
   data is in                the lake. The structured data for creating an 
   associated relationship by a trigger can be                provided. The
   intelligent association relation can                easily realize the
   ability of data searching                person.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for generating an associated data map in                real time
   based on an intelligent dynamic state of                a service
   object. (Drawing includes non-English                language
   text)S1Step for collecting real-time or batch data                into
   the lakeS2Step for creating a trigger for data                entering
   the lakeS3Step for reading an incremental data from                the
   mq component in real timeS4Step for extracting key information to form  
   a data bodyS5Step for finding a corresponding metadata               
   and attribute-related data according to the                association
   relationship of the dataS6Step for using a real-time technology to      
   obtain changing data in a timely mannerS8Step for forming an association
   relationship diagram through forward and fast query                of
   data association relationshipS9Step for forming an association          
   relationship diagram through reverse fast query of                the
   data association relationship
Z9 0
U1 0
U2 0
DA 2022-12-10
UT DIIDW:2022E24595
ER

PT P
AU DARSHAN S H
   PRABHU
   PATIL A
   DORAI R D
   BANDARU R
   PATIL V M
   KUMAR S J
   RASHMI M
   SIDDAMMA C
   DIGGIKAR S
TI Method for integrating internet-of-things and big            data to
   build food safety tracking system to protect            consumer health,
   involves storing data in data lake            that is distributed across
   internet worldwide for            enabling necessary data processing
PN IN202241058643-A
AE DARSHAN S H; PRABHU; PATIL A; DORAI R D; BANDARU R; PATIL V M; KUMAR S
   J; RASHMI M; SIDDAMMA C; DIGGIKAR S
AB 
   NOVELTY - The method involves integrating an               
   internet-of-things, artificial intelligence, point                of
   sale and business intelligence, while analyzing                solutions
   and potential proposed in patents.                Development gaps in
   blockchain traceability are                prepared. Suggestions for
   possible solutions are                provided. A food safety tracking
   system is built to                protect consumer health by using an
   artificial                intelligence system to review research and    
   technologies related to food traceability and to                identify
   main needs for food traceability. The                solutions and the
   potential proposed are analyzed.                The data is stored in
   data lake that is distributed                across internet worldwide
   for enabling necessary                data processing.
   USE - Method for integrating internet-of-things and                big
   data to build a food safety tracking system to                protect
   consumer health by using an artificial                intelligence
   system to review research and                technologies related to
   food traceability and to                identify main needs for food
   traceability.
   ADVANTAGE - The method enables utilizing a digital text               
   data analytics for acquiring insights of optimizing                food
   operations and minimizing food lost in supply                chain and
   internet of things. The method allows a                cloud computing
   environment to offer                infrastructure, platforms and
   software as services                for which a cloud consumer does not
   need to                maintain resources on a local computing device,  
   thus enabling convenient on-demand network access                to a
   shared pool of configurable computing                resources that can
   be rapidly provisioned and                released with minimal
   management effort or                interaction with a provider of the
   service without                requiring human interaction with the
   service                provider.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a     
   internet of things framework within an agri-food                industry
   context.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022D46025
ER

PT P
AU ZHU C
TI Method for destroying full life cycle data based            on data
   lake, involves deleting and cleaning data to be            deleted
   according to deleting mark in data file, and            recording
   information executed by deleting task and            generating deleting
   log
PN CN115203183-A
AE CHONGQING CHANGAN AUTOMOBILE CO LTD
AB 
   NOVELTY - The method involves sending a data command to                a
   user on a terminal. A data deletion task is                started. The
   data to be deleted in the data lake is                queried. The
   metadata of the to-be-deleted data in                the query data
   file. The data to be deleted is                deleted and cleaned
   according to the deletion mark                in the data file. The
   information executed by the                deletion task and the
   generating deletion log. The                deletion mark is added to
   the memory or the newly                added data file when the data is
   stored in the                warehouse.
   USE - Method for destroying full life cycle data                based on
   data lake.
   ADVANTAGE - The method enables deleting the multi-table               
   multi-object deletion scene of the personal                information
   e.g. processing the correlation, and                deleting the full
   life cycle data. The turn on                deleting task is dependent
   on specific user                cancellation or other destroying
   command, so it                does not need background protect
   monitoring data                state, thus reducing the whole system
   resource                consumption. The process of the data deleted by
   the                method is preferentially operated in the memory,     
   the efficiency is greatly improved by delaying the               
   operation of the IO. The method fully uses the               
   characteristics lake high speed upsert, which can               
   normally realize the specified structured,               
   semi-structured, unstructured data deleting                operation.
   The actual measurement performance is                improved by about
   30 percent.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for destroying full life cycle data                based on
   data lake (Drawing includes non-English                language text).
Z9 0
U1 0
U2 0
DA 2022-11-17
UT DIIDW:2022D3105C
ER

PT P
AU ZOU C
   YU X
   WANG Q
   ZHANG Y
TI Method for lifting energy optimization of            multi-source
   heterogeneous data, involves associating            file corresponding
   to description with main body to            realize hierarchy, writing
   in graph database, and using            interrogator for querying index
PN CN115129722-A
AE GREEN ENERGY CHINA TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves receiving and constructing               
   a data lake library. Multi-source heterogeneous                data is
   received. The multi-source isomorphic data                is mapped into
   isomorphic data. An index is                established through a
   mapper. A main body is                constructed. An attribute and
   parameter of the main                body are determined. A data lake
   base server is                stored in the data lake library. The
   semantic                meaning of the content of the imported data file
   is                extracted, and Resource description framework (RDF)   
   description is established. The established RDF               
   description is stored in the document database of                the
   database server of the data lake database. The                file
   corresponding to the RDF description is                associated with
   the main body to realize semantic                hierarchy according to
   the RDF description and                referring to the related main
   body. The                interrogator is used for query the index.
   USE - Energy optimization lifting method of                multi-source
   heterogeneous data.
   ADVANTAGE - The energy optimization lifting method of               
   multi-source heterogeneous data optimizes the                problem of
   complexity and convenience of data                recording and query,
   semantic retrieval, realizing                automatic establishment,
   convenient retrieval,                capable of receiving data of
   different sources and                different structures, and the same
   structure, which                greatly improves the semantic and user
   experience                and efficiency.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022C9027U
ER

PT P
AU OU X
   PHELAN T A
   LEE D
TI Method for performing caching and data access            improvements in
   e.g. environments employing collection            of open-source
   software utilities, involves responding            to read request again
   based on data block returned by            remote data lake, and caching
   data block returned by            the remote data lake within cache
PN US2022300422-A1; US11797447-B2
AE HEWLETT PACKARD ENTERPRISE DEV LP
AB 
   NOVELTY - The method involves receiving a read request               
   from a compute task running on a worker node by a                data
   agent (410). A particular worker node of a                compute
   cluster to which a data block containing                data specified
   by the read request is mapped based                on a mapping function
   is identified (420). Judgment                is made (430) to check
   whether the worker node is                identified. Data is fetched
   (435) from the remote                data agent of the identified worker
   node. The read                request is responded (440) with the data
   returned                by the remote data agent. Determination is made 
   (450) whether a local cache exists within the                worker node
   that contains the data block. The data                block is fetched
   (460) from the remote data lake in                which the file is
   stored. The read request is                responded again (470) based
   on the data block                returned by the remote data lake. The
   data block                returned by the remote data lake is cached
   (480)                within the local cache.
   USE - Method for performing caching and data access               
   improvements in a separate compute and storage                deployment
   large scale data processing environment                e.g. environments
   employing Apache Hadoop (RTM:                Collection of open-source
   software utilities),                Apache Spark (RTM: Multi-language
   engine) and big                data frameworks.
   ADVANTAGE - The method enables providing a cloud computing              
   as a model of service delivery for enabling                convenient
   on-demand network access to a shared                pool of configurable
   computing resources that can                be rapidly provisioned and
   released with minimal                management effort or interaction
   with a provider of                the service. The method enables
   allowing a cloud                consumer to unilaterally provision
   computing                capabilities such as server time and network   
   storage, as needed automatically without requiring                human
   interaction with the service's                provider.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a system
   for performing caching and data                access improvements in a
   separate compute and                storage deployment large scale data
   processing                environment;(2) a non-transitory machine
   readable medium                comprises a set of instructions for
   performing                caching and data access improvements in a
   separate                compute and storage deployment large scale data 
                 processing environment.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                read request processing operation.410Step
   for receiving read request from                compute task running on
   worker node by data                agent420Step for identifying
   particular worker                node of a compute cluster to which a
   data block                containing data specified by the read request
   is                mapped based on a mapping function430Step for checking
   whether worker node is                identified435Step for fetching
   data from remote data                agent of identified worker
   node440Step for responding to read request with                data
   returned by remote data agent450Step for determining whether local cache
   exists within worker node that contains data                block460Step
   for fetching data block from remote                data lake in which
   file is stored470Step for responding to read request again              
   based on data block returned by remote data                lake480Step
   for caching data block returned by                remote data lake
   within local cache
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022C1561R
ER

PT J
AU JANOWCZYK, ANDREW ROBERT
TI Histotools: scaling digital pathology curation tools for quality
   control, annotation, labeling, and dataset identification
DT Awarded Grant
PD Sep 21 2022
PY 2023
AB ABSTRACT: With recent approval of whole slide scanners for primary
   diagnosis, wherein routine glasshistopathology slides are digitized and
   presented to clinical pathologists for diagnosis on computer monitors,
   awealth of new untapped data is being created in routine clinical
   practice and placed in growing data lakes. Indigital format, these whole
   slide images (WSIs) can be subjected to digital pathomics, i.e., the
   process ofextracting quantitative image features associated with
   morphology, attributes, and relationships of histologicobjects in WSIs.
   These features can subsequently be employed for discovery in many
   domains such ashistogenomics, which sees associating phenotypical
   presentations with biological pathways and geneontologies. Additionally,
   low-cost non-tissue destructive image-based companion diagnostic assays
   (CDx)can be developed for predicting prognosis and treatment response of
   patients. Unfortunately, unprocessed largedata lakes (e.g., TCGA) are
   not alone sufficient for pathomics, and often require an intractable
   amount of humancuration effort in (i) performing meticulous quality
   control of WSI (i.e., avoid “garbage-in, garbage-out”) andsubsequently
   (ii) precisely annotating (e.g., cell boundary) and labeling (e.g., cell
   type) histologic objects. Toaddress these major limiting factors in
   curating data lakes, we propose developing our small-scale
   HistoToolsprototypes to employ computing clusters and thus enable their
   function at the scale of large digital sliderepositories (DSR): (i)
   HistoQC for robust, reproducible quality control of WSI by identifying
   artifacts (blurriness)and outliers (poorly stained slides) for avoidance
   in downstream analyses, (ii) CohortFinder for identificationand
   compensation of batch affects, (iii) Quick Annotator for rapid computer
   aided annotation generation via acombination of active and machine
   learning, (iv) PatchSorter for improving sub-typing of histologic
   objects withmachine learning. We will evaluate HistoTools for
   improvement of quality control and the efficiency of bothsegmenting and
   labeling histologic objects of interest via (a) onsite curation and
   release of the 14k WSI usedduring our internal validation and (b)
   supported external curation of at least 100k WSI via 24-clinical
   affiliatesfrom every continent, except Antarctica, whom together have
   access to over 20 million WSI during this proposal.Our validation use
   cases are designed to expedite existing onsite projects in the CDx
   space, consisting of 4organs (breast, lung, heart, kidney), 3 diseases
   (cancer, kidney disease, and organ rejection) and WSIs collectedfrom >70
   sites. These cohort characteristics will help ensure the
   generalizability of our tools for curated data lakecreation, with
   open-source and usability study approaches employed to obtain feedback
   from collaborators andthe larger research community. Dissemination
   through consortia (ITCR, NEPTUNE) and websites (Github, TCIA)will
   improve visibility and adoption. The tools and well-curated data sets we
   release are anticipated to bootstrapresearcher-initiated CDx discovery
   projects, along with the creation of their own onsite manicured data
   lakes.Together, this proposal will engender digital pathology based
   precision medicine research.
ZS 0
ZR 0
TC 0
ZB 0
ZA 0
Z8 0
Z9 0
U1 0
U2 0
G1 10708011; 5R01LM013864-02; R01LM013864
DA 2024-07-25
UT GRANTS:17756061
ER

PT J
AU JANOWCZYK, ANDREW ROBERT
TI Histotools: scaling digital pathology curation tools for quality
   control, annotation, labeling, and dataset identification
DT Awarded Grant
PD Sep 21 2022
PY 2022
AB ABSTRACT: With recent approval of whole slide scanners for primary
   diagnosis, wherein routine glasshistopathology slides are digitized and
   presented to clinical pathologists for diagnosis on computer monitors,
   awealth of new untapped data is being created in routine clinical
   practice and placed in growing data lakes. Indigital format, these whole
   slide images (WSIs) can be subjected to digital pathomics, i.e., the
   process ofextracting quantitative image features associated with
   morphology, attributes, and relationships of histologicobjects in WSIs.
   These features can subsequently be employed for discovery in many
   domains such ashistogenomics, which sees associating phenotypical
   presentations with biological pathways and geneontologies. Additionally,
   low-cost non-tissue destructive image-based companion diagnostic assays
   (CDx)can be developed for predicting prognosis and treatment response of
   patients. Unfortunately, unprocessed largedata lakes (e.g., TCGA) are
   not alone sufficient for pathomics, and often require an intractable
   amount of humancuration effort in (i) performing meticulous quality
   control of WSI (i.e., avoid “garbage-in, garbage-out”) andsubsequently
   (ii) precisely annotating (e.g., cell boundary) and labeling (e.g., cell
   type) histologic objects. Toaddress these major limiting factors in
   curating data lakes, we propose developing our small-scale
   HistoToolsprototypes to employ computing clusters and thus enable their
   function at the scale of large digital sliderepositories (DSR): (i)
   HistoQC for robust, reproducible quality control of WSI by identifying
   artifacts (blurriness)and outliers (poorly stained slides) for avoidance
   in downstream analyses, (ii) CohortFinder for identificationand
   compensation of batch affects, (iii) Quick Annotator for rapid computer
   aided annotation generation via acombination of active and machine
   learning, (iv) PatchSorter for improving sub-typing of histologic
   objects withmachine learning. We will evaluate HistoTools for
   improvement of quality control and the efficiency of bothsegmenting and
   labeling histologic objects of interest via (a) onsite curation and
   release of the 14k WSI usedduring our internal validation and (b)
   supported external curation of at least 100k WSI via 24-clinical
   affiliatesfrom every continent, except Antarctica, whom together have
   access to over 20 million WSI during this proposal.Our validation use
   cases are designed to expedite existing onsite projects in the CDx
   space, consisting of 4organs (breast, lung, heart, kidney), 3 diseases
   (cancer, kidney disease, and organ rejection) and WSIs collectedfrom >70
   sites. These cohort characteristics will help ensure the
   generalizability of our tools for curated data lakecreation, with
   open-source and usability study approaches employed to obtain feedback
   from collaborators andthe larger research community. Dissemination
   through consortia (ITCR, NEPTUNE) and websites (Github, TCIA)will
   improve visibility and adoption. The tools and well-curated data sets we
   release are anticipated to bootstrapresearcher-initiated CDx discovery
   projects, along with the creation of their own onsite manicured data
   lakes.Together, this proposal will engender digital pathology based
   precision medicine research.
TC 0
ZS 0
ZA 0
ZB 0
Z8 0
ZR 0
Z9 0
U1 0
U2 0
G1 10520124; 1R01LM013864-01A1; R01LM013864
DA 2023-12-14
UT GRANTS:16276817
ER

PT P
AU CHEN F
   WANG C
   MA W
   YE W
   YU J
   QIAN H
   LENG S
TI Knowledge map construction method based on product           
   manufacturing process multi-field information, involves           
   integrating sub-spectrum of each service domain to            obtain
   knowledge map of multi-service domain facing to            product
   quality
PN CN115062164-A
AE UNIV NANJING AERONAUTICS & ASTRONAUTICS
AB 
   NOVELTY - The method involves obtaining the related data               
   in manufacturing enterprise Data lake according to                the
   mechanical product manufacturing production of                each stage
   of characteristics and product structure                list kBOM
   feature information. The main body model                of each service
   domain facing the product quality                is established. The
   data related to product quality                is extracted in the
   structured and unstructured                data in the original data
   database. The knowledge                map data layer is constructed
   corresponding to each                business domain of the product. The
   relation                between the specific entity is determined. A    
   ternary group consists of entity and relation is               
   obtained. The sub-map is obtained corresponding to                the
   service domain of mechanical product.
   USE - Knowledge map construction method based on                product
   manufacturing process multi-domain                information.
   ADVANTAGE - The method enables extracting the data of each              
   service domain heterogeneous data according to the               
   combined mechanical product manufacturing                production of
   each stage of characteristics and                product structure list
   kBOM to construct the model,                thus effectively screening
   and extracting data in                the data, and fully excavating the
   value of the                data. The method allows the enterprise to
   finish                the multi-field traceability of product quality   
   problem, thus improving product fault location                efficiency
   and fault zeroing ability.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a       
   knowledge map construction method. (Drawing                includes
   non-English language text).
Z9 0
U1 0
U2 0
DA 2022-10-09
UT DIIDW:2022C2168W
ER

PT P
AU UNANNOUNCED I
   REQUEST N N B P
TI Data migration method for data migration system,            involves
   performing data screening on data to be            migrated by master
   node to obtain target migration data            and sending data to
   second slave node for storage, in            response to user data query
   request
PN CN115048463-A; CN115048463-B
AE BEIJING REALAI TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves sending (201) a first               
   connection request by a master node to the first                slave
   node, and sending a second connection request                to the
   second slave node. The cluster communication                connection
   of a preset cluster is established (202)                by master, first
   slave and second slave node, after                the master node
   receives the first response                returned by the first slave
   node based on first                connection request and receives the
   second response                returned by the second slave node based
   on second                connection request. A user data query request
   is                obtained (203) by master node and the user data       
   query request is sent to the first slave node. The                data
   to be migrated is obtained (204) by first                slave node
   according to the user data query                request, and data is
   sent to the master node. The                data screening is performed
   (205) on the data to be                migrated by master node to obtain
   target migration                data and the data is sent to the second
   slave node                for storage, in response to the user data
   query                request.
   USE - Data migration method for data migration                system
   (claimed) in source database to target                database used in
   big data.
   ADVANTAGE - The problem that the first slave node and the               
   second slave node cannot directly migrate data due                to
   different data architecture mode is avoided. The                target
   migration data is obtained by screening the                data based on
   the user data query the granularity                in the main node.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:1. a data migration system;2. a data migration device; and3. a
   computer readable storage medium storing                program for data
   migration method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of data      
   migration method. (Drawing includes non-English                language
   text)201Step for sending a first connection                request by a
   master node to the first slave node,                and sending a second
   connection request to the                second slave node202Step for
   establishing cluster                communication connection of a preset
   cluster by                master node, the first slave node and the
   second                slave node, after the master node receives the    
   first response returned by the first slave node                based on
   the first connection request and receives                the second
   response returned by the second slave                node based on the
   second connection request203Step for obtaining user data query request  
   by the master node and the user data query request                is
   sent to the first slave node204Step for obtaining data to be migrated by
   first slave node according to the user data query               
   request, and sending data to the master node205Step for performing data
   screening on the                data to be migrated by master node to
   obtain target                migration data and the data is sent to the
   second                slave node for storage, in response to the user   
               data query request
Z9 0
U1 0
U2 0
DA 2022-10-06
UT DIIDW:2022C0219E
ER

PT P
AU ZHAO Z
TI Distributed big data platform for real-time            analysis of
   application data, has data show used for            displaying result
   needed by operator through            application system page and
   realizing data            visualization, and access system accessed by
   user            through browser
PN CN114896230-A
AE CHONGQING CARGO TECHNOLOGY CO LTD
AB 
   NOVELTY - Platform comprises a data source provided with               
   a data collecting module and a data integrated                module. A
   data storage layer is provided with a                storage module and
   a data lake. The storage module                is used for storing
   obtained system log data and                service data. A data
   analysis layer is provided                with a mathematical function
   and modelling. A data                application layer is provided with
   a data sharing,                a data display and a data access. The
   data sharing                is used for providing a data sharing service
   between a data warehouse and an application system.                A
   data show is used for displaying a result needed                by an
   operator through an application system page                and realizing
   a data visualization. An access                system is accessed by a
   user through a                browser.
   USE - Distributed big data platform for real-time               
   analysis of application data.
   ADVANTAGE - The distributed large data calculation can               
   change the problem that the original needed                application
   system database can be solved by the                large hash rate
   platform database, so as to reduce                the calculation
   pressure of the application system.                Thus, the operation
   speed and system reaction                efficiency of application
   system can be                improved.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the       
   distributed big data platform for real-time                analysis of
   application data (Drawing includes                non-English language
   text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022A7095K
ER

PT P
AU MA Y
   HUANG K
   PENG W
   DUAN Z
   WEN J
   REN Q
TI Regional medical prescription supervision method            based on
   block chain, involves sending message prompt            that data is
   normal to supervisor when identification            result is not
   illegal
PN CN114866351-A; CN114866351-B
AE HUNAN TRASEN TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves acquiring (S10) a customer               
   account and acquiring customer personal information                and
   customer operation information according to the                customer
   account. The hospitalizing information of                the client is
   acquired (S20) from the academies                according to the
   personal information of the                client. The customer account
   number, the customer                personal information, the customer
   hospitalizing                information and the customer operation
   information                are inputted (S30) into a big data center to
   form a                data lake. The risk identification on each data in
   the data lake is carried out (S40). A judgement is                made
   on whether an identification result is                illegal. The
   recognition result is sent (S50) to a                supervisor when the
   recognition result is illegal.                The message prompt that
   the data is normal is sent                (S60) to a supervisor when the
   identification                result is not illegal.
   USE - Regional medical prescription supervision                method
   based on block chain.
   ADVANTAGE - The data are integrated through the big data               
   center, violation data are identified and the               
   identification result is sent to the supervisor for                the
   supervisor to detect and confirm so that the                work
   difficulty of the supervisor is greatly                reduced and the
   work efficiency of the supervisor                is effectively
   improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   regional medical prescription supervision platform                based
   on block chain.
   DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart of a
   regional                medical prescription supervision method based on
   block chain. (Drawing includes non-English language               
   text)S10Step for acquiring a customer                accountS20Step for
   acquiring the hospitalizing                information of the client
   from the academies                according to the personal information
   of the                clientS30Step for inputting the customer account  
   number, the customer personal information, the                customer
   hospitalizing information and the customer                operation
   information into a big data center to                form a data
   lakeS40Step for carrying out risk identification                on each
   data in the data lakeS50Step for sending recognition result to a        
   supervisor when the recognition result is                illegalS60Step
   for sending a message prompt that the                data is normal to a
   supervisor when the                identification result is not illegal
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022A4661P
ER

PT P
AU JENNINGS S D
   BHAKTA V U
   FALLON K E
   IGOE T D
   RAMSEY M M
   SCHONEMAN M O
   WILCYNSKI J W
TI System for managing data for diverse legal            projects, has
   relational table for legal project based            on legal discipline
   and project type, where            unstructured data is in form of
   documents associated            with legal project
PN CA3146413-A1; US2022237550-A1
AE UNITEDLEX CORP
AB 
   NOVELTY - The system has a project creation user               
   interface configured to receive data defining a                legal
   project by a legal discipline and a project                type. A data
   lake is configured to store                unstructured data and
   structured data which in the                form of relational tables
   specifically configured                for a respective one of multiple
   legal disciplines                and project types. A new relational
   table can be                instantiated upon creation of the legal
   project in                the project creation user interface. The
   relational                table is configured specifically for the legal
   project based on the legal discipline and the                project
   type. The unstructured data is in the form                of documents
   associated with the legal                project.
   USE - System for managing data for diverse legal               
   projects.
   ADVANTAGE - The system allows functions or departments of               
   the organization to be exposed to one or more                projects or
   processes of the other functions or                departments of the
   organization. The platform                improves visibility into
   real-time metrics for                legal services and operations by
   providing                reliable, actionable data to individuals, such
   as                legal professionals, managers, and leaders. The       
   platform coalesces around enterprise technology                strategy
   by reducing reliance on internal custom                software
   initiatives and supports in providing                consistent,
   reliable customer reporting and                technology credibility
   and prowess across strategic                business units of an
   organization.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for creating diverse legal projects in an                enterprise
   legal platform.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   system for managing data for diverse legal               
   projects.100System102Enterprise legal platform104Primary portal106Legal
   flow system108Insights system
Z9 0
U1 0
U2 0
DA 2022-09-16
UT DIIDW:202294028T
ER

PT P
AU SATYANARAYANA P
   DEVIPRIYA K
   VIJAYAN B V
   THIAGARAJAN R
   DEGADWALA S
TI Deep convolution neural network for use in e.g.            clinical
   treatment classification, has data lakes            intended to be
   extremely adaptable, adjustable            solutions for resolving
   difficult queries by using all            accessible data sources
PN IN202241038488-A
AE SATYANARAYANA P; DEVIPRIYA K; VIJAYAN B V; THIAGARAJAN R; DEGADWALA S
AB 
   NOVELTY - The deep convolution neural network has data               
   lakes that are intended to be extremely adaptable,               
   adjustable solutions for resolving difficult                queries by
   using all accessible data sources. The                labeling measures
   demonstrate a higher degree of                relevance with the
   addition of values. A hybrid                ensembled Convolution Neural
   Net is provided for                image classification. A masked
   region-convolutional                neural net not only distinguishes
   each significant                field in an image, but it can also
   forecast each                meaningful insight with high accuracy. The
   data                lakes are used to store health data images in       
   clinical treatment classification in big data               
   applications.
   USE - Deep convolutional neural network for health                data
   images in clinical treatment classification in                big data
   applications. Can also be used in medical                and genetics
   applications.
   ADVANTAGE - The method effectively predicts and classifies              
   the health images using convolution neural nets                with big
   data, thus achieving high accuracy deep                convolutional
   neural network (CNN) for health data                images. The method
   provides a data lake                architecture for domain-based health
   care image                storage. The data lake provides a rich set of 
              labelled data used by the health care                experts.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022938806
ER

PT P
AU MANIKANDAN R
   PRIYA C B
   SRIVASTAVA G
   SIVAGAMI L
   RAMKUMAR R
   THIAGARAJAN R
   OMANA J
   ANANDARAM H
TI Block chain and internet of things (IoT) enabled            access
   control based patent data lake, has analytical            tools that
   uncovers commercially or financially            attractive patents and
   examines their relevance,            importance, transactional value,
   and other key            parameters
PN IN202241035681-A
AE MANIKANDAN R; PRIYA C B; SRIVASTAVA G; SIVAGAMI L; RAMKUMAR R;
   THIAGARAJAN R; OMANA J; ANANDARAM H
AB 
   NOVELTY - The lake has analytical tools that uncovers               
   commercially or financially attractive patents and               
   examines their relevance, importance, transactional               
   value, and other key parameters. The intellectual               
   property risk is evaluated and managed by                proactively
   addressing sources of risk and adopting                mitigation
   techniques aimed at managing the risk                more
   cost-effectively. The cost and complexity of                all types of
   patent operations, including as                licensing, acquisitions,
   protection, sales, raising                capital, and patent pooling is
   reduced. The                internet of things (IoT) based data access
   is                provided for patent access and control.
   USE - Block chain and internet of things (IoT)                enabled
   access control based patent data lake used                in world wide
   web, policy, financial service,                shipping, logistics, and
   education fields, and                urban governance industrialization
   and slightly                elevated industrial progress.
   ADVANTAGE - The system operators and data analysts are               
   prevented from obtaining data that is illegally                obtained,
   cloud provider from embezzling and                interfering with user
   program codes, private data                backup, and backup data in
   cloud hosts,                cloud-based, and other facilities, and
   information                leakage from peripheral devices. The safety
   of                patent data access is enhanced by using cloud         
   technology and big data processing methods by                merging
   existing patent data using fragmentation                and encryption
   technology.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   block chain and IOT enabled access control based                patent
   data lake.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202289340V
ER

PT P
AU SONG S
   DONG X
TI Method for performing intelligent construction for            enterprise
   data, involves supportting switching of            computing engines in
   different scenarios, and            performing real-time query and
   offline query through            Hadoop ecological technology ecology on
   basis of            storage integration
PN CN114691762-A
AE SUZHOU YINGTIANDI INFORMATION TECHNOLOGY
AB 
   NOVELTY - The method involves performing operations at a               
   level of computing and data architecture on a basis                of
   storage integration through a cooperation of a                data
   integration module, a data processing module,                a data
   synchronization module, and a data asset                center. A
   unified management of various data                sources is performed
   in a one-key access mode, and                a database or schema
   required to be accessed is                specified. Uploading is
   performed through                unstructured and semi-structured data,
   a unified                access interface channel is opened for a
   downstream                data processing flow, data expansion and      
   maintenance are performed through a file transfer               
   protocol (FTP) data source acquisition, and a                switching
   of calculation engines of different                scenes is supported
   through Hadoop (RTM: collection                of open-source software
   developed by Apache                Software Foundation) ecological
   technology ecology,                and real-time query and off-line
   query are                performed on the basis of storage              
    integration.
   USE - Method for performing intelligent construction                for
   enterprise data.
   ADVANTAGE - The method enables realizing smooth transition              
   of offline data and real-time data, and ensuring                data
   reliability and abnormal complement. The                method allows a
   data integration module to support                the heterogeneous data
   source of direct number and                off-line synchronization to
   an integrated                collection service of a data lake, thus
   realizing                flexible coding and configuration of different
   data                number requirements, supporting arrangement         
   scheduling of different development languages, and               
   supporting the access of the data to a business                angle of
   understanding for modeling and finishing                into a
   retrievable data development                environment.
Z9 0
U1 0
U2 0
DA 2022-08-09
UT DIIDW:202289641K
ER

PT P
AU JI W
TI Method for constructing water conservancy and hydropower big data
   architecture, involves maintaining water conservancy facilities with
   abnormal data, updating and recovering data on maintenance equipment
   after maintenance is finished
PN CN114661663-A
AE JI W
AB 
   NOVELTY - The method involves monitoring (S101) data information of
   river basin hydrology and water conservancy facilities through a sky and
   ground water integrated monitoring network and transmitting data
   information to a cloud server through a network. The big data
   architecture construction model is preset (S102) through a cloud server.
   The storage management is carried out (S104) on the large data of the
   flow domain space vector. The water conservancy facilities are
   maintained (S104) by using a big data technology. The water conservancy
   facilities with abnormal data are maintained. The data on the
   maintenance equipment is updated and recovered after the maintenance is
   finished. The maintenance content and time on the maintenance equipment
   are recorded. The maintenance records of maintenance personnel is
   uploaded to a cloud database for reference
   USE - Method for constructing water conservancy and hydropower big data
   architecture.
   ADVANTAGE - The retrieval efficiency of the watershed space vector data
   is improved. The storage and retrieval efficiency of the watershed space
   data, the time and the labor are saved and the maintenance efficiency is
   high.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the
   following:1. a computer device; and2. a computer-readable storage medium
   storing computer program for constructing water conservancy and
   hydropower big data architecture.
   DESCRIPTION Of DRAWING(S) - The drawing shows the flow chart of a method
   for constructing water conservancy and hydropower big data architecture.
   (Drawing includes non-English language text)Step for monitoring data
   information of river basin hydrology and water conservancy facilities
   through a sky and ground water integrated monitoring network (S101)Step
   for presetting a big data architecture construction model through a
   cloud server (S102)Step for carrying out storage management on the large
   data of the flow domain space vector; (S103)Step for maintaining the
   water conservancy facilities by using a big data technology (S104)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202287059C
ER

PT J
AU Tshingomba, Urcel Kalenga
   Djibo, Bassira
   Sautot, Lucile
   Teisseire, Maguelonne
   Jouven, Magali
TI A spatialised information system to support decisions regarding grazing
   management in mountainous and Mediterranean rangelands
SO COMPUTERS AND ELECTRONICS IN AGRICULTURE
VL 198
AR 107100
DI 10.1016/j.compag.2022.107100
EA JUN 2022
DT Article
PD JUL 2022
PY 2022
AB Agro-sylvo-pastoral systems are common around the Mediterranean Basin,
   where they provide a variety of goods and services to the local
   populations. Their sustainability relies on efficient grazing
   management, especially in Mediterranean rangelands. The diversity of
   pastoral resources, combined with the variety of grazing management
   techniques and farming objectives, has slowed down the development of
   digital tools to assist grazing management in these conditions. However,
   digital technologies could serve agro-sylvo-pastoral farms by improving
   the efficiency of grazing management and reducing the difficulty of the
   associated work. In this objective, and to take into account the variety
   of situations, we suggest developing an information system (IS) based on
   a variety of (contextualised) complementary data. For southern France,
   the following data can be combined: Sentinel 2 images, Registre
   Parcellaire Graphique (RPG), OSO land use and cover, RGE Alti altimetry
   data, pastoral technical references, herd GPS location and feedback from
   farmers. However, designing and implementing such an IS requires
   overcoming methodological and technical limitations concerning the
   integration of heterogeneous structured and unstructured data and the
   definition of meaningful ways to combine them to produce relevant
   insights for decision-making. In this article, we describe an approach
   to produce a spatialised IS aimed at providing farmers with relevant
   information to support decisions regarding grazing management in
   mountainous and Mediterranean rangelands. The IS was codesigned with
   stakeholders, including farmers, to best match their needs and to
   facilitate its integration into farm management. The various
   stakeholders were involved in choosing the types of data to be
   associated and, defining the functions and the conceptual model of the
   IS. We propose to structure the IS as a spatial data lake, designed to
   integrate and associate the identified heterogeneous data, and to
   produce decision-making insights for grazing management in mountainous
   and Mediterranean rangelands.
RI Teisseire, Maguelonne/A-6576-2011; Sautot, Lucile/
OI Sautot, Lucile/0000-0002-4204-7427
ZS 0
ZA 0
ZB 0
Z8 0
ZR 0
TC 0
Z9 0
U1 2
U2 16
SN 0168-1699
EI 1872-7107
DA 2022-06-26
UT WOS:000809781200005
ER

PT P
AU LIU Z
   LIU Y
   YU C
   WANG P
   CHEN W
   CHEN Z
   ZHANG T
   ZHANG L
   TAN Y
   WANG Q
   LIU H
TI Data bus based on data classification, has data            bus divided
   into multiple data spaces according to data            structure type,
   and each type of data structure            corresponds to data space,
   where each data space            corresponds to interface base class and
   multiple            interface sub-classes
PN CN114510522-A
AE BEIJING BIG DATA ADVANCED INST; NANHU LAB
AB 
   NOVELTY - The data bus has a data bus divided into               
   multiple data spaces according to a data structure                type.
   Each type of data structure corresponds to                the data
   space. Each data space corresponds to an                interface base
   class and multiple interface                sub-classes. Each interface
   sub class carries out a                butt joint between the data bus
   and a sub-type data                structure of the corresponding data
   structure. The                data structure types are determined by
   inheriting                the interface base classes in corresponding
   data                space. The data space comprises a structured data   
   space, a semi-structured data space, a unstructured                data
   space and a binary data space;
   USE - Data bus based on data classification for data               
   interaction management between data lake and data                source,
   used in data interface field of data                interface of data
   lake. Can also be used in                visualization field, analysis
   field and machine                learning field.
   ADVANTAGE - The bus divides the data space according to               
   the data structure type in the data bus, so that                each
   data space corresponding to a set of sub-type                data
   structure is divided when the user operates                the data,
   thus distinguishing the data belonging to                which large
   class without subdividing to the                specific data type, and
   hence improving convenience                for the user to expand new
   data type and avoiding                the need to rewrite.
   DESCRIPTION Of DRAWING(S) - The drawing shows a spatial layout diagram
   of                a data bus based on data classification. (Drawing     
             includes non-English language text).
Z9 0
U1 0
U2 0
DA 2022-07-07
UT DIIDW:202272272M
ER

PT P
AU LI Z
TI Data lake`s knowledge map generating method,            involves
   obtaining multi-source heterogeneous data in            data lake, and
   storing data as main data to database of            corresponding
   structure
PN CN114462603-A
AE BANK CHINA LTD
AB 
   NOVELTY - The method involves obtaining multi-source               
   heterogeneous data in data lake. The multi- source               
   heterogenous data is stored as the main data to the               
   database corresponding to the corresponding                structure,
   where the master data includes                structured main data,
   semi-structured main data and                unstructured master data.
   The structured master                data of the data table structure is
   queried and                obtained. The metadata of the structured boss
   data                is obtained according to the structured data        
   structure. The semistructured boss data is                analyzed. The
   meta-data of the semi-segmented boss                data is extracted
   from the analysis result for                calling the operator
   corresponding to each                unstructural boss data to extract
   the metadata, and                using the knowledge graph to fuse and
   associate                each metadata to obtain a knowledge graph
   network                corresponding to a data lake.
   USE - Method for generating knowledge map of data                lake
   used in technical field of large data. Uses                include but
   are not limited to text, image, voice,                video and other
   non-structure and                multi-heterogeneous data.
   ADVANTAGE - The method enables generating corresponding               
   knowledge map network for data lake so as to                realize
   effective management of data in the data                lake, so that
   the data in data lake is effectively                associated, thus
   avoiding data swamp.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   knowledge map generating device of data lake; (2) a               
   computer device comprising a memory and a processor                for
   generating knowledge map of data lake; (3) a                computer
   readable storage medium for storing a set                of instruction
   for generating knowledge map of data                lake; (4) a computer
   program product for comprising                a set of instruction for
   generating knowledge map                of data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a data
   lake                knowledge map generating method. (Drawing includes  
                non-English language text).
Z9 0
U1 0
U2 0
DA 2022-07-02
UT DIIDW:202268383N
ER

PT P
AU GODDARD D J
   HAZBOUN P E
   KONDURU S
   PATIL V
   PANJWANI N
   MATETI S
TI Framework for viewing data within amalgamated data            lake, has
   processing module located at edge node and            operable to copy
   multiple data files and generating            first copy of multiple
   files and second copy of            files
PN US2022138167-A1; US11561941-B2
AE BANK OF AMERICA CORP
AB 
   NOVELTY - The framework has a receiver module located at               
   an edge node to receive a set of data files, where                the
   data files comprise unstructured data. A                processing
   module is located at the edge node to                copy the set of
   data files and generate a first                copy of the set of files
   and a second copy of the                set of files. A tabulator module
   is located at the                edge node to tabulate the first copy of
   the set of                data files. A transmitter module transmits    
   tabulated copy of the set of data files from the                edge
   node to a stage table. A processing module is                located at
   the stage table to identify unstructured                data in the
   second copy of the set of files within                the stage table
   and tabulates the unstructured data                in the second copy of
   the set of data files within                the stage table. A hardware
   memory stores the edge                node, the stage table and a
   permanent table. A                viewing module provides automatic
   logic and                executable generation to enable a user to
   generate                multiple views of the permanent table.
   USE - Framework for viewing data within an                amalgamated
   data lake for processing large data in                a data lake
   storing information in native format,                for organizations
   e.g. grocery stores, universities                and financial
   institutions.
   ADVANTAGE - The framework receives data files at a               
   location within a data repository, so that the data                files
   are provided with unstructured data and a                copy of the
   data is tabulated into structured                tables, and thus
   creating the frameworks that offer                certain data
   manipulation capabilities unavailable                within the data
   lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block digaram of a      
            framework.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202259909E
ER

PT P
AU REN W
   WU L
   LIU C
   LIU Y
   GONG A
   LV X
   LI F
TI Data resource management method for geological            survey
   intelligent space, involves establishing service            self-defined
   extension tag or classification, and            forming necessary
   information for supporting safety            management and management
PN CN114398374-A; CN114398374-B
AE DEV RES CENT CHINA GEOLOGICAL SURVEY; CHINA GEOLOGICAL SURVEY DEV RES
   CENT NAT              GEOLOGICAL ARCHIVES
AB 
   NOVELTY - The method involves constructing (S1) the core               
   object model of geological survey data resources,                and
   obtain the core objects of geological survey                data
   resources based on the idea of object-oriented               
   programming. The core object of geological survey                data
   resource is instantiated (S2) to generate the                core object
   instance of geological survey data                resource, and build a
   data lake storage. The                business-defined extended labels
   or classifications                are established (S3) to form necessary
   information                to support security management and
   governance, so                as to realize governance of geological
   survey data                according to the core object instance of
   geological                survey data resources in the data lake. A     
   centralized security management model is                established (S4)
   to manage and control the use                rights of core object
   instances of geological                survey data resources.
   USE - Data resource management method for geological               
   survey intelligent space used in field of big data                and
   digital geological survey such as geological                route survey
   data, geological map data,                unstructured geological report
   data and some                structured data.
   ADVANTAGE - The geological survey data resource core               
   object model is instantiated and instantiated, thus               
   solving the problem of data storage and sharing in                the
   geological survey intelligent space, and                reaching the
   data asset value utilization of                geological survey space.
   The method enables forming                necessary information for
   supporting the safety                management and management, so as to
   realize the                management of the geological data, thus
   optimizing                the data resource management method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the data resource management method for
   geological                survey intelligent space. (Drawing includes   
   non-English language text)Step for constructing core object model of    
   geological survey data resources, and obtain core                objects
   of geological survey data resources                (S1)Step for
   instantiating core objects of                geological survey data
   resources to generate core                object instances of geological
   survey data                resources, and build data lake storage
   (S2)Step for establishing business-defined                extended
   labels or classifications to form                necessary information
   to support security                management and governance (S3)Step
   for establishing centralized security                management model to
   manage and control use rights                of core object instances of
   geological survey data                resources (S4)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202263327L
ER

PT P
AU DAMO M A
   LIN W
   SCHMARZO W
TI Method for facilitating feature selection to            facilitate model
   generation, involves generating time            series features from
   structured data and unstructured            data managed in a data lake,
   and executing a feature            selection process on the time series
PN WO2022081143-A1; CN116348870-A; EP4229565-A1; JP2023544145-W;
   US2023334362-A1; US2024256981-A1; EP4229565-A4; JP7549140-B2
AE HITACHI VANTARA LLC; HITACHI DATA SYSTEMS CORP
AB 
   NOVELTY - The method involves generating time series               
   features from structured data and unstructured data               
   managed in a data lake. A feature selection process                is
   executed on the time series features and                supervised
   training on the selected time series                features are
   conducted across the multiple                different types of models
   iteratively to generate                the multiple models. A best model
   is selected (618)                from the multiple models for deployment
   and the                best model exceeds a predetermined criteria is   
   continuously iterated. The time series features are               
   generated from the structured data and the                unstructured
   data managed in a data lake.
   USE - Method for determining propensity for action                of
   entity such as customer, user, employee and                bank.
   ADVANTAGE - The selection data quality is improved by               
   removing outliers after normalization computation,               
   filtering out data entry problems, and treating the               
   missing values using interpolation techniques. The                data
   could improve the model and its performance                provided that
   the data scientist retrains the model                with the new data,
   when a system receives more                additional data.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   non-transitory computer readable medium storing a                program
   for machine learning frameworks for feature                selection to
   facilitate model generation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for determining propensity for action of               
   entity.611Step for extracting datasets from the                input
   data615Step for defining the parameter                ranges617Step for
   computing the performance                metric618Step for selecting a
   best model from the                models for deployment621Step for
   outputting the results
Z9 0
U1 0
U2 0
DA 2022-05-04
UT DIIDW:202253380W
ER

PT P
AU CAO X
   CHEN J
   YUE C
   YAN P
   WANG F
TI Satellite ephemeris cloud processing analysis            platform, has
   monitoring layer providing all-weather            hosting for determined
   time key target, and space            transmission layer providing
   global mesh network of            data and communication all day
PN CN114357039-A; CN114357039-B
AE HARBIN INST TECHNOLOGY
AB 
   NOVELTY - The platform has an application layer that is               
   used to select the execution task. The core layer                is used
   to store and process the data collected by                the hardware
   layer. The hardware layer is used to                perform data
   collection and computing power                provision. The support
   layer is ground command and                control settings and user
   terminals, as well as                rapid response launch services. The
   navigation                layer is used to provide backup positioning,  
   navigation and timing services in Beidou and global               
   positioning system(GPS)-denied environments. The               
   perception layer is used to provide space                situational
   awareness, detection and tracking of                space objects to aid
   in satellite collisions. The                tracking layer is used to
   provide tracking,                targeting and advanced warning of
   missile threats.                The monitoring layer is used to provide
   24/7                hosting for all identified time-critical targets.   
   The space transport layer is used to provide a                global
   mesh network of data and communications                24/7.
   USE - Cloud processing and analysis platform for               
   satellite constellation based on big data.
   ADVANTAGE - The software framework improves the               
   utilization rate of the satellite cluster, improves                the
   reliability and disaster tolerance of the large                data
   cloud platform between the satellite, reduces                the system
   deployment cost and maintenance cost of                the personnel.
   The platform through the data                processing integration
   collected by each sensor,                constructing database and data
   lake of the unified                architecture, and can flexibly deploy
   the machine                learning model for different task
   requirements, so                as to improve the data analysis
   capability, finally                independently or under the support of
   the ground                base station to monitor the ground.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   of using the satellite constellation cloud                processing and
   analysis platform based on big                data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic structural    
   diagram of the cloud processing and analysis                platform for
   satellite constellation based on big                data. (Drawing
   includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2022-06-04
UT DIIDW:202259427U
ER

PT P
AU MEI L
   WANG Y
   HUANG J
   CHENG L
   LI Z
   TANG Y
TI General data model for big data marketing, has            industry model
   package that is general model obtained            by abstracting domain
   knowledge applied to big data            marketing in certain field, and
   industry model packages            includes data entity classes and
   composite type            fields
PN CN114330998-A
AE SHANGHAI XINZHAOYANG INFORMATION TECHNOL
AB 
   NOVELTY - The model has an industry model package that                is
   a general model obtained by abstracting the                domain
   knowledge applied to big data marketing in a                certain
   field. The data between the industry model                packages are
   independent of each other. Each of                industry model
   packages includes data entity                classes and composite type
   fields.
   USE - General data model for big data marketing such                as
   internet advertisement industry, and used in                large data
   ecological system represented by data                lake, current
   mainstream mixed practice mode, real                time data stream
   processing architecture, frame                integrated batch
   processing and stream processing,                data pipeline execution
   engine and machine learning                platform.
   ADVANTAGE - The GDM provides layered expandable service               
   modelling mode from the service layer viewing angle                of
   the large data marketing field to make up the                service
   uniform language lack between the service                party and the
   data engineer, can normalize the data                source data type,
   and keep the type consistency.                The logic layer of the
   universal data model                modelling system is convenient for
   the service                parties to quickly define the service, easy
   to                simulate service innovation. The model layer is       
   directly dropped to the physical layer by the data                table
   editor. The normalized data is convenient to                integrate
   the service value with the third-party                system or tool.
   The data table tool generates                database table, and helps
   data engineer to                understand the service and guide data
   development                and use through the service description on
   the                GDM.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:1. a construction method of the general data               
   model; and2. a data specification method for big data               
   marketing.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
   general data model for big data marketing. (Drawing               
   includes non-English language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022565183
ER

PT P
AU BI D
TI Industrial data sharing system, has            cross-department data
   sharing calling module for data            sharing call, where data
   sharing information to each of            data providing party to
   broadcast through block            chain
PN CN114281891-A; CN114281891-B
AE UNIV TSINGHUA
AB 
   NOVELTY - The industrial data sharing system comprises               
   multiple industrial data sources for generating               
   multi-source heterogeneous data. A data lake is                used for
   accessing the industrial data source. A                cross-department
   data sharing calling module is                used to call the data from
   the data lake when the                data provider calls the data of
   the multi- source                through the data request party. The
   data sharing                information is broadcasted to each data
   providing                party through a block chain.
                       USE - Industrial data sharing system
   ADVANTAGE - The system can realize the data fully sharing               
   between each department in the enterprise,                circulation
   and reference, at the same time ensure                the data of the
   home ownership, application process                safety controllable,
   so as to improve the resource                coordination utilization
   rate of the enterprise.                The data sharing system can
   support the large data                module, cross-department data
   sharing calling                module of convenient access, the
   multi-source                heterogeneous automation device elastic
   access to                perform industrial large data collection,
   realizing                the bottom data and the upper application of
   the                industrial data safety.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an     
   industrial data sharing system. (Drawing includes               
   non-English language text).
Z9 0
U1 0
U2 0
DA 2022-05-19
UT DIIDW:202252519T
ER

PT J
AU Keith Grainge
TI UK SRC Bridging Work
DT Awarded Grant
PD Apr 01 2022
PY 2022
AB The operation of the SKA Observatory assumes the existence of SKA
   Regional Centres (SRCs) to deliver a range of support to the science
   community. The SRCs are required in order to provide the main portal for
   scientists to access the SKA including provision of computing resources
   and support to enable the science user community to analyse and extract
   science from data products produced by the SKA. An SRC Network (SRCNet)
   will be made up of SRCs distributed around the world in SKA Member
   countries. Each SRC will be required to conform to agreed standards in
   protocols, data architecture and information management policies to
   ensure that they appear as a single federated entity to SKA users. The
   SRCNet will provide a collection of both services and infrastructure
   that will comprise a global capability to distribute, process and curate
   the data from the SKA telescopes. The SRCNet will provide the basic
   governance and operational model and structures, and the baseline
   functionality of the SRC network. SKAO member states and SRC
   stakeholders are already engaging in the design of the SRC Network. In
   order to enable the greatest possible return to the UK from its
   investment in the SKA, both scientific and within the data intensive
   supercomputing / big data / data science technical work, it is essential
   for the UK to fully participate and contribute to the SRCNet Project.
   The UK SKA Rregional Centre (UKSRC) Forum has helped generate the
   SRCNet's basic requirements and high level architecture. The next Phase,
   which begins in April 2022, is to refine the architecture and produce a
   verified system architecture based on advanced requirements. The four
   main Work Packages (WPs) needed to deliver this are:- WP1 Refined
   high-level architecture, which includes the requirements that describe
   the specifics of implementation WP2 Detailed component architecture
   (including interfaces and protocols). WP3 Verified technologies to be
   used in the implementation. WP4 Limited functional, but demonstrable
   prototypes that are tested and verified against our basic requirements.
   Resources are sought to allow the UK to participate fully in these 4 WPs
   to allow the UK to play a leading role in the design of the SRCNet. This
   will be particularly accomplished by participation in WP4, the
   prototyping programme. There are 5 prototyping projects; these will
   enable (i) Data products replication, distribution, and synchronization
   across multiple locations; (ii) the creation of a Federated
   Authentication and Authorization API and proto-Identity Management
   service; (iii) Data Processing Notebooks; (iv) Visualization of SKA data
   with high volume of users and high amount of data; and (v) a service for
   the distribution of software, tools and services. To help deliver this
   the UKSRC Forum requests the following for the preiod April-September
   2022 inclusive; 6 FTE of research software engineering, data steward and
   research devops effort; two FTE of Project Manager support to make sure
   the UK delivers its outputs; a 0.5FTE SRC Project Coordinator and a
   0.5FTE Project Scientist to implement the strategy and project plan, and
   the day to day running of the Project - they will also oversee 2 UKSRC
   work packages, WP0: Set up UKSRC Governance and Management structure and
   WP5: Direction and Management of the Project, and the generation of
   reports. In addition &pound;10k is requested for T&S to allow attendance
   at meetings and conferences to help progress WPs 1-4. These resources
   will in effect create the first elements of the proto-UKSRC node. This
   will create a basic infrastructure that allows the UK to take a leading
   role the development of the SRCNet, and also allows the UK SKA Science
   Community access to UKSRC services that will allow them to create,
   develop and refine software technologies and workflows that will allow
   the UK to maximise its exploitation of SKA data, both during the
   commissioning phase (2024-27) and the production phase (2028-).
ZB 0
ZR 0
TC 0
Z8 0
ZS 0
ZA 0
Z9 0
U1 0
U2 0
G1 ST/X000524/1
DA 2024-04-24
UT GRANTS:17629283
ER

PT J
AU Eloy De Lera Acedo; Paul Alexander
TI Proposal for a limited Early Release of Funds to the UK SKA Regional
   Centre Project
DT Awarded Grant
PD Mar 31 2022
PY 2022
AB The operation of the SKA Observatory assumes the existence of SKA
   Regional Centres (SRCs) to deliver a range of support to the science
   community. The SRCs are required in order to provide the main portal for
   scientists to access the SKA including provision of computing resources
   and support to enable the science user community to analyse and extract
   science from data products produced by the SKA. An SRC Network (SRCNet)
   will be made up of SRCs distributed around the world in SKA Member
   countries. Each SRC will be required to conform to agreed standards in
   protocols, data architecture and information management policies to
   ensure that they appear as a single federated entity to SKA users.  The
   SRCNet will provide a collection of both services and infrastructure
   that will comprise a global capability to distribute, process and curate
   the data from the SKA telescopes. The SRCNet will provide the basic
   governance and operational model and structures, and the baseline
   functionality of the SRC network. SKAO member states and SRC
   stakeholders are already engaging in the design of the SRC Network.  In
   order to enable the greatest possible return to the UK from its
   investment in the SKA, both scientific and within the data intensive
   supercomputing / big data / data science technical work, it is essential
   for the UK to fully participate and contribute to the SRCNet Project.
   The UK SKA Rregional Centre (UKSRC) Forum has helped generate the
   SRCNet's basic requirements and high level architecture. The next Phase,
   which begins in April 2022, is to refine the architecture and produce a
   verified system architecture based on advanced requirements. The four
   main Work Packages (WPs) needed to deliver this are:-  WP1 Refinement of
   the high-level architecture (which includes the requirements that
   describe the specifics of implementation)  WP2 Detailed component
   architecture (including interfaces and protocols) WP3 Verified
   technologies to be used in the implementation WP4 Limited functional,
   but demonstrable prototypes that are tested and verified against our
   basic requirements.  There are 5 prototyping projects proposed that
   together enable (i) Data products replication, distribution, and
   synchronization across multiple locations; (ii) the creation of a
   Federated Authentication and Authorization API and proto-Identity
   Management service; (iii) Data Processing Notebooks; (iv) Visualization
   of SKA data with high volume of users and high amount of data; and (v) a
   service for the distribution of software, tools and services.   The
   early release of funds under this grant enables the UK to contribute to
   the early prototyping efforts in the period April-September 2022
   inclusive. Overall the request seeks 6 FTE of research software
   engineering, data steward and research devops effort; 2 FTE of Project
   Manager support to make sure the UK delivers its outputs; a 0.5FTE SRC
   Project Coordinator and a 0.5FTE Project Scientist to implement the
   strategy and project plan, and the day to day running of the Project
   (they will also oversee 2 UKSRC work packages, WP0: Set up UKSRC
   Governance and Management structure and WP5: Direction and Management of
   the Project)   These resources will in effect create the first elements
   of the proto-UKSRC node. This will create a basic infrastructure that
   allows the UK to take a leading role the development of the SRCNet, and
   also allows the UK SKA Science Community access to UKSRC services that
   will allow them to create, develop and refine software technologies and
   workflows that will help the UK to maximise its exploitation of SKA
   data, both during the commissioning phase (2024-27) and the production
   phase.
ZA 0
ZB 0
Z8 0
TC 0
ZS 0
ZR 0
Z9 0
U1 0
U2 0
G1 ST/X00046X/1
DA 2023-12-08
UT GRANTS:15561597
ER

PT J
AU Jeremy Yates
TI UK SRC Bridging Work
DT Awarded Grant
PD Mar 31 2022
PY 2022
AB The operation of the SKA Observatory assumes the existence of SKA
   Regional Centres (SRCs) to deliver a range of support to the science
   community. The SRCs are required in order to provide the main portal for
   scientists to access the SKA including provision of computing resources
   and support to enable the science user community to analyse and extract
   science from data products produced by the SKA. An SRC Network (SRCNet)
   will be made up of SRCs distributed around the world in SKA Member
   countries. Each SRC will be required to conform to agreed standards in
   protocols, data architecture and information management policies to
   ensure that they appear as a single federated entity to SKA users. The
   SRCNet will provide a collection of both services and infrastructure
   that will comprise a global capability to distribute, process and curate
   the data from the SKA telescopes. The SRCNet will provide the basic
   governance and operational model and structures, and the baseline
   functionality of the SRC network. SKAO member states and SRC
   stakeholders are already engaging in the design of the SRC Network.   In
   order to enable the greatest possible return to the UK from its
   investment in the SKA, both scientific and within the data intensive
   supercomputing / big data / data science technical work, it is essential
   for the UK to fully participate and contribute to the SRCNet Project.
   The UK SKA Rregional Centre (UKSRC) Forum has helped generate the
   SRCNet's basic requirements and high level architecture. The next Phase,
   which begins in April 2022, is to refine the architecture and produce a
   verified system architecture based on advanced requirements. The four
   main Work Packages (WPs) needed to deliver this are:- WP1 Refined
   high-level architecture, which includes the requirements that describe
   the specifics of implementation WP2 Detailed component architecture
   (including interfaces and protocols). WP3 Verified technologies to be
   used in the implementation. WP4 Limited functional, but demonstrable
   prototypes that are tested and verified against our basic requirements. 
   Resources are sought to allow the UK to participate fully in these 4 WPs
   to allow the UK to play a leading role in the design of the SRCNet. This
   will be particularly accomplished by participation in WP4, the
   prototyping programme. There are 5 prototyping projects; these will
   enable (i) Data products replication, distribution, and synchronization
   across  multiple locations; (ii) the creation of a Federated
   Authentication and Authorization API and proto-Identity Management
   service; (iii) Data Processing Notebooks; (iv) Visualization of SKA data
   with high volume of users and high amount of data; and (v) a service for
   the distribution of software, tools and services. To help deliver this
   the UKSRC Forum requests the following for the preiod April-September
   2022 inclusive; 6 FTE of research software engineering, data steward and
   research devops effort; two FTE of Project Manager support to make sure
   the UK delivers its outputs; a 0.5FTE SRC Project Coordinator and a
   0.5FTE Project Scientist to implement the strategy and project plan, and
   the day to day running of the Project - they will also oversee 2 UKSRC
   work packages, WP0: Set up UKSRC Governance and Management structure and
   WP5: Direction and Management of the Project, and the generation of
   reports. In addition &pound;10k is requested for T&S to allow attendance
   at meetings and conferences to help progress WPs 1-4.  These resources
   will in effect create the first elements of the proto-UKSRC node. This
   will create a basic infrastructure that allows the UK to take a leading
   role the development of the SRCNet, and also allows the UK SKA Science
   Community access to UKSRC services that will allow them to create,
   develop and refine software technologies and workflows that will allow
   the UK to maximise its exploitation of SKA data, both during the
   commissioning phase (2024-27) and the production phase (2028-).
ZS 0
ZB 0
Z8 0
ZA 0
TC 0
ZR 0
Z9 0
U1 0
U2 0
G1 ST/X000486/1
DA 2024-04-24
UT GRANTS:17630077
ER

PT J
AU Martin Hardcastle
TI UK SRC Bridging Work (Hertfordshire)
DT Awarded Grant
PD Mar 31 2022
PY 2022
AB The operation of the SKA Observatory assumes the existence of SKA
   Regional Centres (SRCs) to deliver a range of support to the science
   community. The SRCs are required in order to provide the main portal for
   scientists to access the SKA including provision of computing resources
   and support to enable the science user community to analyse and extract
   science from data products produced by the SKA. An SRC Network (SRCNet)
   will be made up of SRCs distributed around the world in SKA Member
   countries. Each SRC will be required to conform to agreed standards in
   protocols, data architecture and information management policies to
   ensure that they appear as a single federated entity to SKA users.  The
   SRCNet will provide a collection of both services and infrastructure
   that will comprise a global capability to distribute, process and curate
   the data from the SKA telescopes. The SRCNet will provide the basic
   governance and operational model and structures, and the baseline
   functionality of the SRC network. SKAO member states and SRC
   stakeholders are already engaging in the design of the SRC Network.  In
   order to enable the greatest possible return to the UK from its
   investment in the SKA, both scientific and within the data intensive
   supercomputing / big data / data science technical work, it is essential
   for the UK to fully participate and contribute to the SRCNet Project.
   The UK SKA Rregional Centre (UKSRC) Forum has helped generate the
   SRCNet's basic requirements and high level architecture. The next Phase,
   which begins in April 2022, is to refine the architecture and produce a
   verified system architecture based on advanced requirements. The four
   main Work Packages (WPs) needed to deliver this are:-  WP1 Refined
   high-level architecture, which includes the requirements that describe
   the specifics of implementation WP2 Detailed component architecture
   (including interfaces and protocols). WP3 Verified technologies to be
   used in the implementation. WP4 Limited functional, but demonstrable
   prototypes that are tested and verified against our basic requirements. 
   Resources are sought to allow the UK to participate fully in these 4 WPs
   to allow the UK to play a leading role in the design of the SRCNet. This
   will be particularly accomplished by participation in WP4, the
   prototyping programme. There are 5 prototyping projects; these will
   enable (i) Data products replication, distribution, and synchronization
   across multiple locations; (ii) the creation of a Federated
   Authentication and Authorization API and proto-Identity Management
   service; (iii) Data Processing Notebooks; (iv) Visualization of SKA data
   with high volume of users and high amount of data; and (v) a service for
   the distribution of software, tools and services. To help deliver this
   the UKSRC Forum requests the following for the period April-September
   2022 inclusive; 6 FTE of research software engineering, data steward and
   research devops effort; two FTE of Project Manager support to make sure
   the UK delivers its outputs; a 0.5FTE SRC Project Coordinator and a
   0.5FTE Project Scientist to implement the strategy and project plan, and
   the day to day running of the Project - they will also oversee 2 UKSRC
   work packages, WP0: Set up UKSRC Governance and Management structure and
   WP5: Direction and Management of the Project, and the generation of
   reports. In addition &pound;10k is requested for T&S to allow attendance
   at meetings and conferences to help progress WPs 1-4.  These resources
   will in effect create the first elements of the proto-UKSRC node. This
   will create a basic infrastructure that allows the UK to take a leading
   role the development of the SRCNet, and also allows the UK SKA Science
   Community access to UKSRC services that will allow them to create,
   develop and refine software technologies and workflows that will allow
   the UK to maximise its exploitation of SKA data, both during the
   commissioning phase (2024-27) and the production phase (2028-).
Z8 0
ZS 0
ZB 0
TC 0
ZR 0
ZA 0
Z9 0
U1 0
U2 0
G1 ST/X000478/1
DA 2024-04-24
UT GRANTS:17620399
ER

PT P
AU TANG J
   CUI Z
   HUANG Z
   WU R
   ZHANG W
   OUYANG J
   ZHANG Y
TI Method for performing dynamic collection of            interactive
   transformation and distribution daily            statistical report of
   e.g. power plant, involves            performing new file replacement on
   file of data lake            corresponding to dynamic number if file is
   searched in            data lake
PN CN114238219-A
AE ELECTRIC POWER RES INST GUANGDONG POWER
AB 
   NOVELTY - The method involves receiving (A1) multiple               
   transmission and distribution daily statistical                reports
   in parallel. A dynamic serial number is                obtained (A2) by
   dynamic serial number marking                rule. A dynamic sending
   number of a transformation                distribution daily statistical
   report is sent (A3)                to a data sending table to obtain a
   sending                sequence number. The daily statistic report of   
   transmission and distribution is received (A4),                where
   report file type of the daily statistic                report is
   analyzed to obtain multiple files with                the dynamic serial
   number. Metadata information is                automatically generated
   (A5) by an obtained file.                Update request is received
   (B1). Judging process is                performed (B2) to check whether
   the update request                is updated. The information not be
   updated is fed                back (B3). New file replacement on the
   file of data                lake is performed (B5) corresponding to the
   dynamic                number if the file is searched in the data       
           lake.
   USE - Method for performing dynamic collection of an               
   interactive transformation and distribution daily               
   statistical report of a power plant and a                substation.
   ADVANTAGE - The method enables improving submitting speed               
   by software, avoiding need to increase or replace               
   hardware facilities and require of high cost,                obtaining
   multiple files with dynamic numbers and                report file types
   by receiving the daily statistic                report and analyzing the
   type of the report of the                transmutation distribution,
   automatically                generating the metadata information of the
   obtained                file, loading the metadata information to the
   data                lake, improving compatibility of the data lake with 
   performance of heterogeneous data after                continuously
   optimizing and collecting by data lake                with high
   efficiency, and achieving that                classification summary of
   a transmitting and                changing data structure of the daily
   statistical                report is reasonable such that consistency of
   data,                accuracy and timeliness can be ensured.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system    
   for performing dynamic collection of an interactive               
   transformation and distribution daily statistical                report.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for performing dynamic collection
   of an                interactive transformation and distribution daily  
   statistical report. (Drawing includes non-English               
   language text).Receiving multiple transmission and               
   distribution daily statistical reports in parallel               
   (A1)Obtaining dynamic serial number by dynamic                serial
   number marking rule (A2)Sending dynamic sending number of               
   transformation distribution daily statistical                report to
   data sending table to obtain sending                sequence number
   (A3)Receiving daily statistic report of                transmission and
   distribution (A4)Automatically generating metadata information          
   by obtained file (A5)Receiving update request (B1)Performing judging
   process to check whether                update request is updated
   (B2)Feeding back information not be updated               
   (B3)Performing new file replacement on file of                data lake
   (B5)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022494315
ER

PT P
AU QIU J
TI North direction collecting and analyzing and            summarizing
   method based on big data architecture by            computer device,
   comprises analyzing data and            warehousing by multiple servers
   at same time, and            executing data summary according to network
   element            management system type multi-processing data         
     summary
PN CN114218213-A
AE INSPUR COMMUNICATION INFORMATION SYSTEMS
AB 
   NOVELTY - Method comprises using a network element               
   management system (EMS) as a unit multi-process to               
   collect a north data. Collected data is stored in a               
   directory sharing server. The data is analyzed and               
   warehoused by multiple servers at a same time. A                state of
   a task total table is modified to an                EXEC-END. A data
   summary is executed according to a                network element
   management system type                multi-processing data summary.
   USE - North direction collecting and analyzing and               
   summarizing method based on big data architecture                by a
   computer device (claimed).
   ADVANTAGE - The method is simple and efficient, and solves              
   the problem that the collecting and analyzing                solution of
   the string is too slow. It is capable                of quickly finding
   out the problem in the mass                data, ensuring the stability
   of the communication                operation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:north direction collecting and analyzing and               
   summarizing system based on big data architecture;               
   andcomputer readable storage medium comprising a                set of
   instructions for north direction collecting                and analyzing
   and summarizing method based on big                data architecture.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the north direction collecting and analyzing and               
   summarizing method based on big data architecture                by
   computer device (Drawing includes non-English                language
   text).
Z9 0
U1 0
U2 0
DA 2022-05-02
UT DIIDW:202246372M
ER

PT P
AU BAI Z
TI Algorithm that supports centralized control in distributed big data
   architecture network involves calculating data transmission path so that
   sum of overhead associated with link and associated overhead of false
   node reaches minimum
PN CN114185285-A
AE HUNAN PENGCHENG INFORMATION TECHNOLOGY
AB 
   NOVELTY - The algorithm involves considering each node in the network as
   source point in turn, when a centrally controlled distributed network
   receives a network request and linear programming is used to find the
   minimum cost flow method to find the relationship between each source
   point and the sink point. The path with the least link cost is recorded.
   The data traffic passing through each link in the network is considered
   as an unknown number. The number of false nodes added at this point is
   represented by the unknown for each node with an out-degree greater than
   or equal to 2, according to whether the flow from this point flows to
   the chain from this point to the sink Next-hop on the path with the
   smallest path cost. The method of finding the minimum cost flow by
   linear programming is combined with the number of false nodes expressed
   by the unknown, and a data transmission path is obtained, so that the
   sum of the link-related overhead and the false node-related overhead can
   be minimized.
   USE - Algorithm that supports centralized control in distributed big
   data architecture network.
   ADVANTAGE - The algorithm supports centralized control in distributed
   large data architecture network, reduces the total overhead of using the
   central control in the distributed network from the quality, and the
   algorithm can be remotely controlled, and using the recording learning
   algorithm, continuously correcting the optimized path calculation value,
   intelligent starting the preset electric appliance, there is no need to
   send instruction manually.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202243949A
ER

PT J
AU Barquero-Álvarez, David Eladio
   Rojas-Morales, Rodrigo
TI Consumo eléctrico: una propuesta de gestión desarrollada en R a partir
   de medidores inteligentes en plantas de generación
X1 Electricity consumption: a management proposal developed in R based on
   smart meters for power plants
SO Revista Tecnología en Marcha
VL 35
IS 1
BP 162
EP 173
DI 10.18845/tm.v35i1.5367
DT research-article
PD 2022-03
PY 2022
AB Abstract Costa Rican electricity matrix is already considered renewable
   due to the high running on clean sources and the near-zero use of
   fossil-fuels; however, it is possible to complement this sustainability
   by improving the management of electricity consumption in power plants.
   On the other hand, the change to smart meters recording data every 15
   minutes is a fact in the country and it implies an opportunity to
   propose new ways of managing the energy consumption information to take
   advantage of its complete potential. In this paper an interactive
   digital dashboard developed in R is proposed, from where it is possible
   to manage the electricity consumption for both a hydroelectric and a
   thermal power plant, according to the previously identified requirements
   from the users and the existing data architecture, including smart
   meters. The dashboard is located at a web address, from where users
   access and navigate to visualize the defined analyses easily and
   friendly-user. The main contribution in this paper is to provide a
   creative an ingenious alternative for big data from smart meters through
   a tool that improves its management and increase the awareness of
   electricity usage. Since it is working for two different power plants
   technologies, it could be considered useful for other facilities.
X4 Resumen La matriz eléctrica en Costa Rica es considerada renovable,
   debido al alto porcentaje de participación de fuentes limpias y la casi
   nula utilización de combustibles fósiles; sin embargo, es posible
   complementar esa sostenibilidad, mediante una mejora en la gestión del
   consumo eléctrico en las plantas de generación. Por otra parte, el
   cambio a medidores inteligentes que registran datos cada 15 minutos es
   una realidad en el país, lo que implica la oportunidad de proponer
   nuevas formas de gestión de la información de consumo, que logren
   utilizar todo su potencial. En relación con lo indicado, este artículo
   expone el desarrollo de un panel digital interactivo desarrollado en R,
   desde donde es posible gestionar el consumo eléctrico de dos plantas,
   una hidroeléctrica y otra térmica, según las necesidades de información
   requeridas por parte de los usuarios y a partir de la arquitectura de
   datos existente, incluyendo medidores inteligentes. El panel se ubica en
   una dirección URL, desde donde los usuarios pueden acceder y navegar
   para visualizar los análisis definidos, de manera amigable y práctica.
   La principal contribución de este estudio es ofrecer una alternativa
   innovadora y creativa sobre el uso de los datos masivos provenientes de
   los medidores inteligentes, aplicando una herramienta que mejora su
   gestión e incrementa la conciencia en el uso del recurso. Al ser
   funcional en dos plantas de tecnologías diferentes, podría considerarse
   como alternativa para otras instalaciones.
ZA 0
Z8 0
ZS 0
TC 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
SN 0379-3982
DA 2022-03-01
UT SCIELO:S0379-39822022000100162
ER

PT J
AU Stephens, Donpaul
AA Yu, Xiangyao
TI STTR Phase I:  Real-Time Smart Data Lake
DT Awarded Grant
PD Feb 15 2022
PY 2022
AB The broader impact of this Small Business Technology Transfer (STTR)
   Phase I project will be to fundamentally accelerate the pace of data
   science. New solutions that alleviate or bypass the bottlenecks inherent
   in existing data analytics technologies are required to unlock the value
   contained in ���big data��� and bring greater benefits to research,
   business, and society at large.  The potential benefits include improved
   quality control for manufactured goods, reduced fraud in financial
   transactions, and enhanced customization of consumer services.  Many
   common software applications, such as search engines, online shopping,
   e-commerce, medical applications, and social networks, are backed by
   data analytical processing services. This project will enable massive
   data sets to be pre-processed directly by a shared storage service, then
   allow this capability to be efficiently utilized by client analytic
   applications. This project will accelerate transformation of analytical
   data to useful insights while reducing network congestion, simplifying
   complex analytics systems, and lowering information technology
   costs.<br/> <br/>This Small Business Technology Transfer (STTR) Phase I
   project examines the challenge of how to dramatically accelerate
   data-intensive computing problems by enabling large data objects to be
   processed directly in the storage layer then efficiently utilized by
   client applications. The technology will be built on a software defined
   data lake used for big data applications. Key technology will be added
   to enable JSON, one of the most widely used data interchange formats, to
   be processed in a distributed manner in-place within this storage
   solution with the objective of enabling client analytic applications to
   retrieve not the complete object but only the desired subset of content
   they require. The storage solution will be augmented to transform the
   data into a format that can be directly consumed by the clients. This
   will substantially increase efficiency within the storage itself and
   between analytics clients and the storage solution ��� while
   acceelrating data processing by bypassing bottlenecks. The result will
   be a smart data lake that can reduce network traffic, improve data
   freshness, and enable real-time operation ��� while accelerating big
   data analytics by an order of magnitude or more.<br/><br/>This award
   reflects NSF's statutory mission and has been deemed worthy of support
   through evaluation using the Foundation's intellectual merit and broader
   impacts review criteria.
TC 0
ZR 0
ZA 0
ZB 0
Z8 0
ZS 0
Z9 0
U1 0
U2 0
G1 2135007
DA 2023-12-08
UT GRANTS:15516357
ER

PT J
AU MURUGIAH, KARTHIK 
TI Automated ascertainment of bleeding and target lesion revascularization
   after percutaneous coronary intervention (PCI) using electronic health
   record (EHR) data
DT Awarded Grant
PD Jan 25 2022
PY 2023
AB PROJECT SUMMARYPercutaneous coronary intervention (PCI) is the most
   common cardiac procedure with over 650,000 PCIperformed annually in the
   U.S. Post-PCI complications which occur in a significant proportion of
   patients areassociated with an increased risk of morbidity and
   mortality. Reliable ascertainment of post-PCI events isimportant for
   performance measurement, submission to disease registries, clinical
   trials, and for cardiaccatheterization laboratory (CCL) safety
   monitoring. Claims based detection of PCI complications is
   inadequate.Assessing post-PCI events reliably requires an in-depth
   manual chart review, which incurs a significantprovider and
   administrative burden. However, with advances in health information
   technology and nationwideadoption of electronic health record (EHR)
   systems, it possible to utilize EHR for the automatic derivation
   ofclinical events. Dr. Murugiah proposes to create and validate
   automated algorithms which can be applied toEHR data to detect two
   important post-PCI events which are a common focus of clinical trials
   and qualityimprovement efforts – in-hospital bleeding and 1-year target
   lesion revascularization (TLR). Using EHR data ata large health system,
   Dr. Murugiah will develop a hybrid algorithm to detect major bleeding
   post-PCI byleveraging structured data fields such as laboratory values,
   as well as unstructured data such as imagingreports, cardiac
   catheterization reports, and progress notes incorporating Natural
   Language Processing (NLP)techniques (Aim 1). Similarly, using cardiac
   catheterization reports for patients undergoing repeatrevascularization
   within 1 year, an algorithm will be developed to detect TLR (Aim 2).
   Both algorithms will beexternally validated using EHR data from another
   large institution. The final algorithm will be implemented intoa tool
   generating scheduled reports of bleeding and TLR, to be fed back to the
   quality assurance team for theCCL and to individual operators.
   Individual operators will be surveyed to obtain feedback about the
   algorithm,reporting process, and their perceived benefit. The final
   tools will be made open source (Aim 3). An automatedalgorithm for the
   detection of post-PCI events within EHR can reduce administrative
   burden, enable thegeneration of new knowledge from EHR based
   observational studies, and enable pragmatic clinical trials.Further,
   this project can serve as a proof of concept of the utility of hybrid
   tools leveraging both structured dataand clinical text for surveillance
   and quality measurement. Dr. Murugiah has a career interest in studying
   andimproving the treatment for ischemic heart disease using
   multidimensional datasets and EHR data to developreal time risk
   prediction models and decision support tools, and conduct EHR based
   comparative effectivenessstudies and clinical trials. During the award
   period he will leverage the experience of his mentorship teamwhich
   includes national experts in cardiovascular outcomes research, clinical
   informatics, and computationallinguistics. He will also acquire formal
   training in clinical informatics by completing a Master of Health
   Sciencedegree which will provide him the necessary platform to make the
   transition into an independent investigator.
ZA 0
ZR 0
TC 0
ZB 0
ZS 0
Z8 0
Z9 0
U1 0
U2 0
G1 10555326; 5K08HL157727-02; K08HL157727
DA 2024-07-23
UT GRANTS:17710121
ER

PT J
AU MURUGIAH, KARTHIK 
TI Automated ascertainment of bleeding and target lesion revascularization
   after percutaneous coronary intervention (PCI) using electronic health
   record (EHR) data
DT Awarded Grant
PD Jan 25 2022
PY 2022
AB PROJECT SUMMARYPercutaneous coronary intervention (PCI) is the most
   common cardiac procedure with over 650,000 PCIperformed annually in the
   U.S. Post-PCI complications which occur in a significant proportion of
   patients areassociated with an increased risk of morbidity and
   mortality. Reliable ascertainment of post-PCI events isimportant for
   performance measurement, submission to disease registries, clinical
   trials, and for cardiaccatheterization laboratory (CCL) safety
   monitoring. Claims based detection of PCI complications is
   inadequate.Assessing post-PCI events reliably requires an in-depth
   manual chart review, which incurs a significantprovider and
   administrative burden. However, with advances in health information
   technology and nationwideadoption of electronic health record (EHR)
   systems, it possible to utilize EHR for the automatic derivation
   ofclinical events. Dr. Murugiah proposes to create and validate
   automated algorithms which can be applied toEHR data to detect two
   important post-PCI events which are a common focus of clinical trials
   and qualityimprovement efforts – in-hospital bleeding and 1-year target
   lesion revascularization (TLR). Using EHR data ata large health system,
   Dr. Murugiah will develop a hybrid algorithm to detect major bleeding
   post-PCI byleveraging structured data fields such as laboratory values,
   as well as unstructured data such as imagingreports, cardiac
   catheterization reports, and progress notes incorporating Natural
   Language Processing (NLP)techniques (Aim 1). Similarly, using cardiac
   catheterization reports for patients undergoing repeatrevascularization
   within 1 year, an algorithm will be developed to detect TLR (Aim 2).
   Both algorithms will beexternally validated using EHR data from another
   large institution. The final algorithm will be implemented intoa tool
   generating scheduled reports of bleeding and TLR, to be fed back to the
   quality assurance team for theCCL and to individual operators.
   Individual operators will be surveyed to obtain feedback about the
   algorithm,reporting process, and their perceived benefit. The final
   tools will be made open source (Aim 3). An automatedalgorithm for the
   detection of post-PCI events within EHR can reduce administrative
   burden, enable thegeneration of new knowledge from EHR based
   observational studies, and enable pragmatic clinical trials.Further,
   this project can serve as a proof of concept of the utility of hybrid
   tools leveraging both structured dataand clinical text for surveillance
   and quality measurement. Dr. Murugiah has a career interest in studying
   andimproving the treatment for ischemic heart disease using
   multidimensional datasets and EHR data to developreal time risk
   prediction models and decision support tools, and conduct EHR based
   comparative effectivenessstudies and clinical trials. During the award
   period he will leverage the experience of his mentorship teamwhich
   includes national experts in cardiovascular outcomes research, clinical
   informatics, and computationallinguistics. He will also acquire formal
   training in clinical informatics by completing a Master of Health
   Sciencedegree which will provide him the necessary platform to make the
   transition into an independent investigator.
ZR 0
ZB 0
Z8 0
ZS 0
ZA 0
TC 0
Z9 0
U1 0
U2 0
G1 10371710; 1K08HL157727-01A1; K08HL157727
DA 2023-12-14
UT GRANTS:15572751
ER

PT P
AU OEZER R A
TI Location and motion information prediction system            for
   providing estimation of instant location and            movement
   information of people receiving service from            network, has
   server that is configured to determine            movement information
   of people
PN TR2021020884-A2
AE TURKCELL TEKNOLOJI ARASTIRMA & GELISTIRM
AB 
   NOVELTY - The system (1) has a database (2) that is               
   configured to record masked CDR data generated in a               
   mobile communication network, to communicate with a               
   network server (S) over any remote communication                protocol
   in the state of the art, to receive CDR                data over this
   communication and to record it in                the database, to manage
   the database and to purify                and mask the personal
   characteristics by processing                the CDR data registered in
   the database. The                combining masked CDR data is suitable
   for big data                architecture, processes and analyzes big
   data in                real time and accesses instant location          
   information. A server (3) is configured to                determine the
   movement information of people by                estimating with route
   estimation algorithms based                on the analyzed data. The
   database is in                communication with the server and is
   configured to                be managed by the server. The database is  
   configured to record the CDR big data flowing in a                mobile
   communication network and consisting of                millions of
   lines.
   USE - Location and motion information prediction                system
   for providing estimation of instant location                and movement
   information of people receiving                service from network.
   ADVANTAGE - The system provides real-time estimation of               
   the location and movement information of the people               
   receiving service from the network by processing                the
   masked CDR data generated in the mobile                communication
   network in real time, enables the                reporting of human
   density and instantaneous                reaction to be taken by
   processing the CSV                formatted flowing data, which is not
   processed in                mobile communication networks and whose
   potential                is idle with big data technologies in
   approximately                real time, enables instant information
   about the                number of people in the coverage areas, online
   and                numerical display, detection and action of           
   gatherings and clusters in the base station areas                in case
   of disasters such as epidemics,                earthquakes, floods, and
   landslides and enables                sudden population movements in
   disaster situations                such as epidemics, earthquakes,
   floods, landslides,                determination of instant route
   information and                estimation by using route-path
   algorithms, mapping,                machine learning technologies.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
   location and motion information prediction               
   system.1Location and motion information prediction               
   system2Database3ServerSNetwork server
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202371785P
ER

PT C
AU Abdelhedi, Fatma
   Jemmali, Rym
   Zurfluh, Gilles
BE Filipe, J
   Smialek, M
   Brodsky, A
   Hammoudi, S
TI Data Ingestion from a Data Lake: The Case of Document-oriented NoSQL
   Databases
SO ICEIS: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON ENTERPRISE
   INFORMATION SYSTEMS - VOL 1
BP 226
EP 233
DI 10.5220/0011068300003179
DT Proceedings Paper
PD 2022
PY 2022
AB Nowadays, there is a growing need to collect and analyze data from
   different databases. Our work is part of a medical application that must
   allow health professionals to analyze complex data for decision making.
   We propose mechanisms to extract data from a data lake and store them in
   a NoSQL data warehouse. This will allow us to perform, in a second time,
   decisional analysis facilitated by the features offered by NoSQL systems
   (richness of data structures, query language, access performances). In
   this paper, we present a process to ingest data from a Data Lake into a
   warehouse. The ingestion consists in (1) transferring NoSQL DBs
   extracted from the Data Lake into a single NoSQL DB (the warehouse), (2)
   merging so-called "similar" classes, and (3) converting the links into
   references between objects. An experiment has been performed for a
   medical application.
CT 24th International Conference on Enterprise Information Systems (ICEIS)
CY APR 25-27, 2022
CL ELECTR NETWORK
SP INSTICC
RI , Fatma/; Jemmali, Rym/; Abdelhedi, Fatma/AAY-3404-2020
OI , Fatma/0000-0003-1658-8067; Jemmali, Rym/0000-0003-1276-7658; 
ZR 0
TC 0
ZA 0
Z8 0
ZS 0
ZB 0
Z9 0
U1 1
U2 6
BN 978-989-758-569-2
DA 2022-07-03
UT WOS:000814767200024
ER

PT C
AU Abdellatif, Takoua
   Testouri, Houcem Eddine
   Yahyaoui, Aymen
BE Badica, C
   Treur, J
   Benslimane, D
   Hnatkowska, B
   Krotkiewicz, M
TI MAFC: Multimedia Application Flow Controller for Big Data Systems
SO ADVANCES IN COMPUTATIONAL COLLECTIVE INTELLIGENCE, ICCCI 2022
SE Communications in Computer and Information Science
VL 1653
BP 485
EP 497
DI 10.1007/978-3-031-16210-7_40
DT Proceedings Paper
PD 2022
PY 2022
AB A lot of research has been conducted on different areas of Big Data
   technology in the multimedia domain, such as multimedia Big Data
   capture, storage, indexing and retrieval. Nevertheless, ensuring
   real-time processing for multimedia Big Data is still a big challenge.
   Indeed, when the streaming speed is higher than the processing time,
   data can be lost or the processing no longer respects real-time
   constraints even though efficient streaming technology is used. In this
   context, we propose MAFC, a Multimedia Application Flow Controller that
   filters incoming multimedia data so that only significant data is sent
   for processing. Data extraction is executed following application-based
   policies. MAFC is a building block that can be embedded in the
   management layer of Big Data systems. The main contribution of our work
   is a Big Data architecture that couples multimedia data filtering with
   the Big Data streaming technology in order to reduce the stress on the
   processing layer. As a use-case, the integration of MAFC in a Big
   Data-based surveillance system for video frame extraction shows a
   significant performance improvement compared to the initial system.
CT 14th International Conference on Computational Collective Intelligence
   (ICCCI)
CY SEP 28-30, 2022
CL ACM Special Interest Grp Appl Comp, French SIGAPP Chapter, Hammamet,
   TUNISIA
HO ACM Special Interest Grp Appl Comp, French SIGAPP Chapter
SP Wroclaw Univ Sci & Technol; IEEE SMC Tech Comm Computat Collect
   Intelligence; European Res Ctr Informat Syst; Univ Pau Pays Adour; Univ
   Jendouba; Int Univ VNU HCM
RI Yahyaoui, Aymen/ABE-9297-2020
ZR 0
ZA 0
TC 0
Z8 0
ZS 0
ZB 0
Z9 0
U1 0
U2 0
SN 1865-0929
EI 1865-0937
BN 978-3-031-16210-7; 978-3-031-16209-1
DA 2022-11-05
UT WOS:000871953900040
ER

PT J
AU Aleisa, Monirah Ali
Z2  
TI [not available]
DT Dissertation/Thesis
PD Mar 06 2023
PY 2023
ZR 0
ZS 0
TC 0
ZB 0
Z8 0
ZA 0
Z9 0
U1 0
U2 2
UT PQDT:68432830
ER

PT C
AU Argento, Luciano
   De Francesco, Erika
   Lambardi, Pasquale
   Piantedosi, Paolo
   Romeo, Carlo
BE Ceci, M
   Flesca, S
   Masciari, E
   Manco, G
   Ras, ZW
TI TrueDetective 4.0: A Big Data Architecture for Real Time Anomaly
   Detection
SO FOUNDATIONS OF INTELLIGENT SYSTEMS (ISMIS 2022)
SE Lecture Notes in Artificial Intelligence
VL 13515
BP 449
EP 458
DI 10.1007/978-3-031-16564-1_43
DT Proceedings Paper
PD 2022
PY 2022
AB Industry suffers from many machine-related problems, such as breakdown,
   failures, personnel safety, and management cost. Predictive maintenance
   is an industrial and research area that is permeating goods and services
   production systems, aimed at preventing critical issues in machinery and
   workplaces, and reducing the costs in terms of resources, time and money
   caused by incoming risk events that can slow or even stop the
   production. This paper presents TD4 a Big Data IoT architecture able to:
   (i) acquire huge amounts of data from real-time sensor streams, (ii)
   analyze and prepare the data, scaling over a network of distributed
   working nodes, (iii) perform real-time fault prediction. Experiments on
   well-known benchmarks show the applicability of the proposed
   architecture on different real scenarios.
CT 26th International Symposium on Methodologies for Intelligent Systems
   (ISMIS)
CY OCT 03-05, 2022
CL Cosenza, ITALY
SP ICAR CNR; Univ Calabria, Dept Comp Engn Modeling Elect & Syst Engn; Univ
   Bari Aldo Moro, Comp Sci Dept; Univ Naples Federico II, DIETI Dept
ZB 0
ZR 0
Z8 0
ZS 0
TC 0
ZA 0
Z9 0
U1 0
U2 1
SN 0302-9743
EI 1611-3349
BN 978-3-031-16564-1; 978-3-031-16563-4
DA 2022-12-07
UT WOS:000886990100043
ER

PT C
AU Autarrom, Suphatchaya
   Chantaranimi, Kittayaporn
   Chanton, Chanwit
   Chompupoung, Anchan
   Jinapook, Pichan
   Mahanan, Waranya
   Lumpoon, Pathathai Na
   Natwichai, Juggapong
   Nuntachit, Nontakan
   Prapaitrakul, Nitchanan
   Sukhahuta, Rattasit
   Sugunsil, Prompong
   Sangamuang, Sumalee
   Sukhvibul, Titipat
   Thiengburanathum, Pree
BE Barolli, L
   Miwa, H
   Enokido, T
TI Data Architecture for Data-Driven Service Platform: Royal Project
   Foundation Case Study
SO ADVANCES IN NETWORK-BASED INFORMATION SYSTEMS, NBIS-2022
SE Lecture Notes in Networks and Systems
VL 526
BP 131
EP 141
DI 10.1007/978-3-031-14314-4_13
DT Proceedings Paper
PD 2022
PY 2022
AB In the data era, many organizations aim to gather and maintain data to
   drive their organization, Royal Project Foundation is one of them. The
   foundation has been working on social development, which includes
   population structure, drug problems, educational development, and
   community organization. The data of the foundation works have been
   collected from various sources and forms. Thus, to evaluate the Royal
   Project Foundation data, we proposed a hybrid big data architecture
   containing data storage and data processing pipelines to operate data
   services. Furthermore, the data model and data report system of social
   and community data are presented along with the business intelligence
   (BI) dashboard.
CT 25th International Conference on Network-Based Information Systems
   (NBiS)
CY SEP 07-09, 2022
CL Kwansei Gakuin Univ, Nishinomiya, JAPAN
HO Kwansei Gakuin Univ
RI Natwichai, Juggapong/HTO-0073-2023; Thiengburanathum, Pree/; Sangamuang, Sumalee/JFS-0145-2023
OI Thiengburanathum, Pree/0000-0001-6983-8336; Sangamuang,
   Sumalee/0000-0003-0757-0607
ZA 0
TC 0
ZB 0
Z8 0
ZR 0
ZS 0
Z9 0
U1 0
U2 1
SN 2367-3370
EI 2367-3389
BN 978-3-031-14314-4; 978-3-031-14313-7
DA 2022-11-02
UT WOS:000870661300013
ER

PT C
AU Ben Rhaiem, Mohamed Amine
   Selmi, Mouna
   Farah, Imed Riadh
   Bouzeghoub, Amel
GP IEEE
TI A big spatiotemporal streaming data architecture for smart city crisis
   monitoring using VGI
SO 2022 2ND INTERNATIONAL CONFERENCE OF SMART SYSTEMS AND EMERGING
   TECHNOLOGIES (SMARTTECH 2022)
BP 107
EP 111
DI 10.1109/SMARTTECH54121.2022.00035
DT Proceedings Paper
PD 2022
PY 2022
AB The exponential growth of human activities and the climate change put
   cities around the world in face of multiple risks and threats that led
   eventually to the emergence of a new urban model, which is the smart
   city resilience. Although being equipped with a myriad of connected
   smart devices and sensors, the smart city is still physically made up of
   buildings, roads, parks, industrial sites, shopping centers, etc.
   Therefore, location-based crisis management endorses a geospatial
   modeling strategy approach for major hazard data management in a smart
   city. Hence, spatial data remains always at the center of risk
   management processes. However, smart and resilient cities still strive
   to solve the imparity between the huge amounts of geospatial data
   generated mostly in real time in particular geographic user content
   contributions also known as Volunteered Geographic Information (VGI) and
   the delayed decision-making. In this paper, we reviewed major studies
   using VGI in big spatiotemporal data analytics in supporting smart city
   resilience. Then, we propose a vision of big spatiotemporal data
   architecture perquisites leveraging big data technologies, VGI and deep
   learning techniques for smart hazard management.
CT 2nd IEEE International Conference on Smart Systems and Emerging
   Technologies (SMARTTECH)
CY MAY 09-11, 2022
CL Prince Sultan Univ, Riyadh, SAUDI ARABIA
HO Prince Sultan Univ
SP IEEE Comp Soc; IEEE; Res & Initiat Ctr; Robot & Internet Things; Prince
   Sultan Univ, IEEE Student Branch; Prince Sultan Univ, Coll Comp &
   Informat Sci; IEEE Saudi Arabia Sect; IEEE Reg 8
RI farah, imed/AAL-7809-2021
ZS 0
ZB 0
ZR 0
TC 0
Z8 0
ZA 0
Z9 0
U1 1
U2 13
BN 978-1-6654-0973-5
DA 2022-09-28
UT WOS:000855229000020
ER

PT C
AU Bernardo, Ernesto
   Bonfa, Stefano
   Anderson, Jesse
BE BorgognoMondino, E
   Zamperlin, P
TI Implementation of the Digital Inland Water Smart Strategy Using
   Geomatics Instruments and the Big Data SmartRiver Platform
SO GEOMATICS AND GEOSPATIAL TECHNOLOGIES, ASITA 2021
SE Communications in Computer and Information Science
VL 1507
BP 83
EP 94
DI 10.1007/978-3-030-94426-1_7
DT Proceedings Paper
PD 2022
PY 2022
AB This research concerns an innovative method for land monitoring using
   data provided by geomatics instruments (images from ESA's earth
   observation satellites; images from UAVs; climatic data provided by
   Copernicus satellites), integrated with socio-economic data and
   geophysical data, in combination with the use of Artificial Intelligence
   and Analytics and integrating the data obtained within a cloud
   containing a Data Cube that acts as an Analytic-Ready-Data (ARD)
   data-lake storage; a GIS web application that allows the visualization
   of data and facilitates the involvement of stakeholders relevant to the
   river/water basin. Specifically, thanks to the use of the SmartRiver
   Platform implemented by us, using large amounts of data (Big Data)
   appropriately selected and processed, through Artificial and Analytical
   Intelligence we are able to provide knowledge and assistance in the
   development and forecasting the risks of climatic impacts on the
   territory (useful for the management/planning of projects by the bodies
   in charge). In the specific case, these forecasts will be useful both in
   the safety sector (allowing to foresee possible floods, and consequently
   allowing to plan the possible construction of dams, canals, embankments,
   etc.), and in the sector of management of water resources. It is
   possible with a view to an adaptation strategy in the management of
   resources according to climate change, allowing for drought periods to
   be foreseen, the bodies in charge will be able to evaluate in advance
   the best strategies to be implemented to contain any risks due to the
   lack of water in the agricultural and energy production sectors.
CT 24th Italian Conference of the
   Italian-Federation-of-Scientific-Associations-for-Territorial-and-Enviro
   nmental-Information (ASITA)
CY JUL 01-23, 2021
CL Genoa, ITALY
SP Italian Federat Sci Assoc Territorial & Environm Informat
RI Bernardo, Ernesto/AAD-9489-2022
OI Bernardo, Ernesto/0000-0003-0484-2822
ZR 0
ZB 0
ZA 0
Z8 0
TC 0
ZS 0
Z9 0
U1 0
U2 8
SN 1865-0929
EI 1865-0937
BN 978-3-030-94426-1; 978-3-030-94425-4
DA 2022-03-23
UT WOS:000759617900007
ER

PT C
AU Cakir, Altan
   Ozkaya, Emre
   Akkus, Fatih
   Kucukbas, Ezgi
   Yilmaz, Okan
BE Kahraman, C
   Tolga, AC
   Onar, SC
   Cebi, S
   Oztaysi, B
   Sari, IU
TI Real Time Big Data Analytics for Tool Wear Protection with Deep Learning
   in Manufacturing Industry
SO INTELLIGENT AND FUZZY SYSTEMS: DIGITAL ACCELERATION AND THE NEW NORMAL,
   INFUS 2022, VOL 2
SE Lecture Notes in Networks and Systems
VL 505
BP 148
EP 155
DI 10.1007/978-3-031-09176-6_18
DT Proceedings Paper
PD 2022
PY 2022
AB Industry 4.0 is a motivation that represents the transformation by
   data-driven industrial operations and decision making by digitization of
   manufacturing processes to gain operational advantages in the market.
   Considering how the manufacturing sector is adopting data-driven
   operations is challenging, given that there is not a straightforward
   definition of machine traceability, receiving and storing raw data from
   manufacturing lines, gives an opportunity to analyse the processes in
   real time nature. Thanks to big data management platforms and artificial
   intelligence decision support algorithms, it gives the ability to deeply
   understand the complexity of the processes and, accordingly, to
   eliminate or minimise false methods and reduce the costs that are
   insufficient for production. In addition, one of the biggest preventable
   costs for metal machining processes is the tool breakage and tool
   wearing problems. The motivation of this paper is to discuss data-driven
   decision making possibilities of the tool wearing and optimise breakage
   costs with using artificial intelligence. Furthermore, the analysis
   provides a proof-of-concept that the existence of a digital
   infrastructure combined with the analytical capabilities, such as
   real-time data management and monitoring, and having a highly accurate
   LSTM based time-series integrated artificial intelligent predictive
   model, to deal with inefficiencies in production processes. To this end,
   in this context, by developing the latest advancements in big data
   analytics, we propose a scalable predictive and preventive maintenance
   architecture for metal machining processes domain. We also show the
   opportunities and challenges of utilizing the big data architecture in
   the manufacturing domain.
CT 4th International Conference on Intelligent and Fuzzy Systems (INFUS)
CY JUL 19-21, 2022
CL Bornova, TURKEY
RI Akkus, Mehmet Fatih/; Cakir, Altan/ABD-4450-2020
OI Akkus, Mehmet Fatih/0000-0002-3628-5656; 
TC 0
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
Z9 0
U1 1
U2 7
SN 2367-3370
EI 2367-3389
BN 978-3-031-09176-6; 978-3-031-09175-9
DA 2022-12-10
UT WOS:000889132600018
ER

PT B
AU Correia, Ricardo André Araújo
Z2  
TI Iiot Data Ness: From Streaming to Added Value
DT Dissertation/Thesis
PD Jan 01 2022
PY 2022
ZA 0
ZB 0
Z8 0
ZR 0
TC 0
ZS 0
Z9 0
U1 1
U2 1
BN 9798381548549
UT PQDT:87643133
ER

PT B
AU da Costa, Daniel Vilar
   Vilaça, Ricardo Manuel Pereira
   Pereira, José Orlando Roque Nascimento
Z2  
TI [not available]
DT Dissertation/Thesis
PD Aug 09 2024
PY 2024
ZR 0
TC 0
Z8 0
ZA 0
ZB 0
ZS 0
Z9 0
U1 0
U2 0
BN 9798380747059
UT PQDT:86285441
ER

PT B
AU da Silveira, Duarte Miguel
   Sanders, Michael
   Lourenço, João
Z2  
TI [not available]
DT Dissertation/Thesis
PD Aug 09 2024
PY 2024
ZR 0
ZB 0
ZA 0
TC 0
Z8 0
ZS 0
Z9 0
U1 0
U2 0
BN 9798382567198
UT PQDT:89156675
ER

PT R
AU Drakopulos, Lauren Alexandra
   Havice, Elizabeth
   Crisp, Katie
   Zurita Posas, Ana
   Campbell, Lisa M
TI Catalog of Ocean Data Science Initiatives
SO Qualitative Data Repository
DI https://doi.org/10.5064/F6ZQWQJS
DT Data set
PD 2025-07-28
PY 2022
AB h3>Project Overview /h3>   This dataset is a catalog of oceans data
   science initiatives (ODSIs). We define an ODSI as an initiative that
   mobilizes (often geospatial and temporal) big data and/or novel data
   sources about the oceans with an express goal of informing or improving
   conditions in the oceans. ODSI identification began in Jan 2020.
   Additional ODSIs will continue to be added. We identified more than 150
   ODSIs and populated the catalog with data gathered from ODSI websites
   describing key features of their work including 1) the data
   infrastructure 2) their organizational structure, 3) the ocean worlds,
   or ontologies, they create, and 4) the (explicit or implicit) policy and
   governance ���solutions��� and relations they promote. The ODSIs in the
   catalog are global and regional in scope and aim to enhance
   understanding around three topical concerns: fisheries extraction,
   biodiversity conservation, and enhancing basic scientific knowledge. /  
   h3>Data overview /h3>   For 100 ODSIs, we created metadata about the
   data architecture, organizational governance, and world-making practices
   such as their stated purpose, theory of change, and problem/solution
   framing. For a subset of 30 ODSIs, we created metadata about their
   policy and governance stances and practices. All metadata was created
   based on a textual analysis of their websites and public communications.
   /   h3>Data collection overview /h3>   Sampling strategy: We began with
   a purposive sample of ODSIs based on the research team���s prior
   knowledge of and participation in global and regional ODSIs. This sample
   allowed us to pilot and refine our metadata catalog approach. We then
   used a combination of keyword searches on Google using search terms such
   as ���ocean data��� ���marine data��� and ���fisheries data���. Adopting
   a snowball sampling method, we reviewed the websites of ODSIs that came
   up in our initial search to find references to additional ODSIs. /    To
   determine if an entity was an ODSI, we reviewed web pages for
   information on purpose, goals, objectives, mission, values (usually in
   tabs labeled ���About��� ���Goals��� or ���Objectives���) and we looked
   for links to ���data��� or ���data products.��� Entities were selected
   for our catalog based on two criteria: 1) their stated purpose, goals,
   objectives, mission, values indicated a commitment to advancing ocean
   science and data and 2) if they focused on regional or global scales. We
   selected and categorized ODSIs according to three broad focal areas in
   global and regional oceans governance: fisheries extraction,
   biodiversity conservation, and basic ocean science development. /  
   h3>Shared data organization /h3>   This catalog is comprised of three
   files. 'Havice_ODSIC.pdf' provides a list of each ODSI included in the
   catalog, and a permalink to the webpage used to populate catalog
   metadata categories. 'Havice_ODSIC-CodingScheme.pdf' provides a list of
   code description for the catalog metadata. 'Havice_ODSIC-Metadata.xlsx'
   is the full catalog with populated metadata. /
ZR 0
ZA 0
Z8 0
ZS 0
TC 0
ZB 0
Z9 0
U1 0
U2 0
DA 2024-12-07
UT DRCI:DATA2024201031151842
ER

PT J
AU Eladio Barquero-Alvarez, David
   Rojas-Morales, Rodrigo
TI Electricity consumption: a management proposal developed in R based on
   smart meters for power plants
SO TECNOLOGIA EN MARCHA
VL 35
IS 1
BP 162
EP 173
DI 10.18845/tm.v35i1.5367
DT Article
PD JAN-MAR 2022
PY 2022
AB Costa Rican electricity matrix is already considered renewable due to
   the high running on clean sources and the near-zero use of fossil-fuels;
   however, it is possible to complement this sustainability by improving
   the management of electricity consumption in power plants. On the other
   hand, the change to smart meters recording data every 15 minutes is a
   fact in the country and it implies an opportunity to propose new ways of
   managing the energy consumption information to take advantage of its
   complete potential. In this paper an interactive digital dashboard
   developed in R is proposed, from where it is possible to manage the
   electricity consumption for both a hydroelectric and a thermal power
   plant, according to the previously identified requirements from the
   users and the existing data architecture, including smart meters. The
   dashboard is located at a web address, from where users access and
   navigate to visualize the defined analyses easily and friendly-user. The
   main contribution in this paper is to provide a creative an ingenious
   alternative for big data from smart meters through a tool that improves
   its management and increase the awareness of electricity usage. Since it
   is working for two different power plants technologies, it could be
   considered useful for other facilities.
OI Rojas Morales, José Roigo/0000-0002-3881-9076
ZS 0
ZA 0
ZB 0
Z8 0
TC 0
ZR 0
Z9 0
U1 0
U2 1
SN 0379-3982
EI 2215-3241
DA 2022-03-29
UT WOS:000768686900014
ER

PT C
AU Iniguez, Luis
   Galar, Mikel
BE Gonzalez, HS
   Lopez, IP
   Bringas, PG
   Quintian, H
   Corchado, E
TI A Scalable and Flexible Open Source Big Data Architecture for Small and
   Medium-Sized Enterprises
SO 16TH INTERNATIONAL CONFERENCE ON SOFT COMPUTING MODELS IN INDUSTRIAL AND
   ENVIRONMENTAL APPLICATIONS (SOCO 2021)
SE Advances in Intelligent Systems and Computing
VL 1401
BP 273
EP 282
DI 10.1007/978-3-030-87869-6_26
DT Proceedings Paper
PD 2022
PY 2022
AB The advancements of Big Data, Internet of Things and Artificial
   Intelligence are causing the industrial revolution known as Industry
   4.0. For automated factories, adopting the necessary technologies for
   its implementation involves a series of challenges such as the lack of a
   proper infrastructure, financial limitations, coordination problems or a
   low understanding of Industry 4.0 implications. Additionally, many
   implementations focus on solving specific problems without taking other
   future or parallel projects into account, leading to continuous
   restructuring and increased complexity, that is, increasing costs. A
   lack of a global view when implementing Industry 4.0 solutions can cause
   difficulties in its adoption, leading to future problems that may be
   unaffordable for Small and Medium-sized Enterprises (SMEs). Traditional
   Big Data architectures offer remarkable solutions to complex data
   issues, but do not cover the complete flow of information that is
   required in Industry 4.0 applications. Therefore, there is a need to
   create solutions for the difficulties that this new digital
   transformation brings to avoid future problems, making it affordable
   also for SMEs. In this work we propose a flexible and scalable Big Data
   architecture that is well-suited for SMEs with automated factories,
   taking the aforementioned difficulties into account.
CT 16th International Conference on Soft Computing Models in Industrial and
   Environmental Applications (SOCO)
CY SEP, 2021
CL Bilbao, SPAIN
SP Startup Ole; Basque Govt, Dept Educ & Univ; Logistar Project DeustoTech;
   Univ Deusto
RI Galar, Mikel/H-4846-2011
ZS 0
Z8 0
TC 0
ZA 0
ZB 0
ZR 0
Z9 0
U1 0
U2 12
SN 2194-5357
EI 2194-5365
BN 978-3-030-87869-6; 978-3-030-87868-9
DA 2021-11-30
UT WOS:000719656700026
ER

PT C
AU Kachaoui, Jabrane
   Belangour, Abdessamad
BE BenAhmed, M
   Boudhir, AA
   Karas, IR
   Jain, V
   Mellouli, S
TI Improving a New Data Lake Architecture Design Based on Data Ponds and
   Multi-Agent Paradigms
SO 6TH INTERNATIONAL CONFERENCE ON SMART CITY APPLICATIONS
SE Lecture Notes in Networks and Systems
VL 393
BP 815
EP 821
DI 10.1007/978-3-030-94191-8_65
DT Proceedings Paper
PD 2022
PY 2022
AB For several years, Big Data has been considered the next evolution for
   processing various data in Data Lake. However, one of the major
   difficulties for Big Data development is its need for structured
   knowledge. This is why, a large part of research focuses on ontologies
   formalization that has, among other things, given rise to ad hoc
   languages as Web Ontology Language (OWL). In this paper, ontologies were
   embedded into Data Ponds to operate and cooperate with agents. Indeed, a
   new architecture has been built based on traditional multi-agent system
   in a Big Data context. This architecture enables end users to access to
   information from Data Lake in real time with the slightest effort. This
   issue has been the subject of research for nearly ten years, but it
   remains one of the major obstacles to Big Data development.
CT 6th International Conference on Smart City Applications
CY OCT 27-29, 2021
CL Safranbolu, TURKEY
RI Belangour, Abdessamad/KAL-6712-2024
Z8 0
ZS 0
ZA 0
TC 0
ZB 0
ZR 0
Z9 0
U1 0
U2 2
SN 2367-3370
EI 2367-3389
BN 978-3-030-94191-8; 978-3-030-94190-1
DA 2023-03-09
UT WOS:000928840400065
ER

PT B
AU Kaynar Terzioglu, Emine Ugur
   Krieger, Orran
   Mancuso, Renato
   Desnoyers, Peter
   Rudolph, Larry
Z2  
TI [not available]
DT Dissertation/Thesis
PD Mar 06 2023
PY 2023
TC 0
ZB 0
ZA 0
ZR 0
ZS 0
Z8 0
Z9 0
U1 0
U2 0
BN 9798352961674
UT PQDT:68581771
ER

PT B
AU Luna, Daniel
Z2  
TI Data and Model Integration, Evaluation, Application, and Provenance With
   CyberWater and Its Toolkits
DT Dissertation/Thesis
PD Jan 01 2022
PY 2022
TC 0
ZB 0
ZR 0
ZA 0
Z8 0
ZS 0
Z9 0
U1 0
U2 0
BN 9798311923903
UT PQDT:123172656
ER

PT B
AU Luo, Yu
   Plale, Beth A.
   Leake, David B.
   Wild, David
   Liu, Xiaozhong
Z2  
TI [not available]
DT Dissertation/Thesis
PD Mar 06 2023
PY 2023
ZR 0
ZS 0
ZB 0
Z8 0
TC 0
ZA 0
Z9 0
U1 0
U2 1
BN 9798368418988
UT PQDT:68362392
ER

PT C
AU Maamouri, Amine
   Sfaxi, Lilia
   Robbana, Riadh
BE Themistocleous, M
   Papadaki, M
TI Phi: A Generic Microservices-Based Big Data Architecture
SO INFORMATION SYSTEMS (EMCIS 2021)
SE Lecture Notes in Business Information Processing
VL 437
BP 3
EP 16
DI 10.1007/978-3-030-95947-0_1
DT Proceedings Paper
PD 2022
PY 2022
AB We present in this paper Phi, a generic microservices-based Big Data
   architecture dedicated to complex multi-layered systems, that rallies
   multiple machine learning jobs, stream and batch processing. We show how
   to apply our architecture to an adaptive e-learning application that
   adjusts its recommendation to the emotions of the learner on the spot.
   We deploy our application on the cloud using AWS services, and perform
   some performance tests to show its feasibility in a realistic
   environment.
CT 18th European, Mediterranean, and Middle Eastern Conference on
   Information Systems (EMCIS)
CY DEC 08-09, 2021
CL ELECTR NETWORK
SP Unov Nicosia; British Univ Dubai; Dubai Block Chain Ctr
RI Maamouri, Amine/; Sfaxi, Lilia/AAO-6154-2021; Robbana, Riadh/
OI Maamouri, Amine/0000-0002-0507-4434; Robbana, Riadh/0000-0001-5736-4137
ZA 0
TC 0
ZB 0
ZR 0
ZS 0
Z8 0
Z9 0
U1 0
U2 3
SN 1865-1348
EI 1865-1356
BN 978-3-030-95947-0; 978-3-030-95946-3
DA 2022-04-02
UT WOS:000771721100001
ER

PT B
AU Pimenta, Cláudia Maria Silva
   da Maia Malta, Pedro Manuel Carqueijeiro Espiga
Z2  
TI [not available]
DT Dissertation/Thesis
PD Aug 09 2024
PY 2024
ZS 0
ZR 0
Z8 0
ZB 0
ZA 0
TC 0
Z9 0
U1 0
U2 0
BN 9798382663692
UT PQDT:89140937
ER

PT C
AU Sousa, Regina
   Oliveira, Daniela
   Carneiro, Ana
   Pinto, Luis
   Pereira, Ana
   Peixoto, Ana
   Peixoto, Hugo
   Machado, Jose
BE Yin, H
   Camacho, D
   Tino, P
TI The Covid-19 Influence on the Desire to Stay at Home: A Big Data
   Architecture
SO INTELLIGENT DATA ENGINEERING AND AUTOMATED LEARNING - IDEAL 2022
SE Lecture Notes in Computer Science
VL 13756
BP 199
EP 210
DI 10.1007/978-3-031-21753-1_20
DT Proceedings Paper
PD 2022
PY 2022
AB The COVID-19 pandemic has had an impact on many aspects of society in
   recent years. The ever-increasing number of daily cases and deaths makes
   people apprehensive about leaving their homes without a mask or going to
   crowded places for fear of becoming infected, especially when
   vaccination was not available. People were expected to respect
   confinement rules and have their public events cancelled as more
   restrictions were imposed. As a result of the pandemic's insecurity and
   instability, people became more at ease at home, increasing their desire
   to stay at home. The present research focuses on studying the impact of
   the COVID-19 pandemic on the desire to stay at home and which metrics
   have a greater influence on this topic, using Big Data tools. It was
   possible to understand how the number of new cases and deaths influenced
   the desire to stay at home, as well as how the increase in vaccinations
   influenced it. Moreover, investigated how gatherings and confinement
   restrictions affected people's desire to stay at home.
CT 23rd International Conference on Intelligent Data Engineering and
   Automated Learning (IDEAL)
CY NOV 24-26, 2022
CL Manchester, ENGLAND
SP IEEE UK & Ireland CIS Chapter
RI Sousa, Regina/IWE-4124-2023; Ferreira Machado, José Manuel/B-5887-2009; Peixoto, Hugo/E-3195-2010; Peixoto, Ana/KPA-1199-2024; Oliveira, Daniela Sofia/GON-5760-2022
OI Sousa, Regina/0000-0002-2988-196X; Ferreira Machado, José
   Manuel/0000-0003-4121-6169; Peixoto, Hugo/0000-0003-3957-2121; 
Z8 0
ZR 0
TC 0
ZA 0
ZS 0
ZB 0
Z9 0
U1 0
U2 1
SN 0302-9743
EI 1611-3349
BN 978-3-031-21752-4; 978-3-031-21753-1
DA 2023-02-09
UT WOS:000904430900020
ER

PT C
AU Yang, Mingbao
   Zhou, Peng
   Li, Shaobo
   Pu, Ruiqiang
GP IEEE
TI Hierarchical Scalable Data Lake Design for Game Engine
SO 2022 8TH INTERNATIONAL CONFERENCE ON BIG DATA AND INFORMATION ANALYTICS,
   BIGDIA
SE International Conference on Big Data and Information Analytics
BP 360
EP 365
DI 10.1109/BIGDIA56350.2022.9874158
DT Proceedings Paper
PD 2022
PY 2022
AB With the rapid development of information technology, people are more
   and more inseparable from smart devices, and people and devices produce
   a large amount of data all the time. How to store and analyze massive
   data has become a research hotspot of the times. However, researchers
   mainly study the high reliability, high availability, high fault
   tolerance, high throughput, and high concurrent processing of big data
   systems, but there are few studies on designing hierarchical and
   scalable data lakes. In this study, a layered and extensible design of
   the data lake is carried out, which reduces the complexity of
   implementing business indicators during data analysis, and at the same
   time improves code reuse, scalability, development efficiency and code
   security. In addition, in order to avoid the problem of data
   accumulation or even job execution failure due to the skew of ip
   dimension table data, this paper designs the ip dimension table rowkey.
   This paper designs a set of real-time and offline big data analysis data
   lake architecture based on game engine and applies it to the actual
   production environment.
CT 8th International Conference on Big Data and Information
   Analytics-BigDIA
CY AUG 24-25, 2022
CL Guiyang, PEOPLES R CHINA
SP Institute of Electrical and Electronics Engineers Inc; Guizhou
   University; IEEE Systems Man and Cybernetics Society
ZS 0
TC 0
ZR 0
ZA 0
ZB 0
Z8 0
Z9 0
U1 0
U2 0
SN 2771-6910
BN 978-1-6654-8796-2
DA 2022-01-01
UT WOS:001448335400057
ER

PT P
AU PAN L
   KE J
   ZHAO X
   ZHANG M
TI Construction method of parallel distributed large            data
   architecture, involves distributing data to data            storage unit
   through transmission unit according to            grid data correlation
PN CN113873031-A; CN113873031-B
AE NANJING YIMIAO INFORMATION TECHNOLOGY CO LTD; NANJING YIMIAO INFORMATION
   TECHNOLOGY CO
AB 
   NOVELTY - The method involves preparing a mixture of               
   multiple raw materials. A grid is established                through a
   grid unit, and data is sequentially                stored into the grid
   according to time stamps. A                grid data correlation and a
   data node sampling time                interval with a calculation unit
   are calculated.                The data is assigned to the data storage
   unit by                the transmission unit according to the mesh data 
   correlation. The common nasteriskm data is defined,                where
   n is data layer number and m is the node                number of each
   layer of data, and the average                density of each layer of
   data node is calculated.                The grid of each layer of data
   area is divided,                according to the average density, and
   whether the                density of the data node in the grid is close
   to                the average density is judged, and the grid is        
          divided according to the data node area.
   USE - Method for constructing parallel distributed                big
   data architecture for use in field of parallel                data
   processing.
   ADVANTAGE - The method enables combining data correlation,              
   rationally planning transmission path and                distributing
   data storage space, thus ensuring the                load balance of the
   data node, and hence improving                the data query capability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system    
   for constructing parallel distributed big data               
   architecture.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for constructing parallel distributed big                data
   architecture. (Drawing includes non-English                language
   text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202214686C
ER

PT P
AU KOCHAR P
   LILES R
   ALKHATIB A
TI Distributed database system for managing database            data within
   database service environment, has data            processing entity for
   receiving query of single logical            database that is stored
   across online database and            offline database
PN US2021382874-A1
AE MONGODB INC
AB 
   NOVELTY - The system (101) has a data processing entity               
   for receiving a query of a single logical database,                where
   single logical database is stored across an                online
   database (109) and an offline database and                the query is a
   single unified query of the data                being stored across the
   online database and the                offline database. The online
   database comprises a                data lake architecture (106) to
   store a set of                unstructured data entities and is stored
   within a                cluster of nodes. A processing entity creates   
   read-only views of storage relating to the online               
   database and an archive database.
   USE - Distributed database system for managing                database
   data within a database service                environment.
   ADVANTAGE - The system maintains the online database and               
   the offline database, and provides a single access                point
   for performing data operations on elements of                the online
   and offline databases, thus enabling to                manage and store
   data in more efficient                manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   method for managing database data within database                service
   environment.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   distributed database system.Distributed database system (101)Users
   (102)Data lake architecture (106)Data archive (107)Online database (109)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2021E13715
ER

PT J
AU Tang, Yongsheng
TI Risk Chain and Key Hazard Management for Urban Rail Transit System
   Operation Based on Big Data Mining
SO DISCRETE DYNAMICS IN NATURE AND SOCIETY
VL 2021
AR 3692151
DI 10.1155/2021/3692151
DT Article
PD DEC 6 2021
PY 2021
AB With the promotion of the national transportation power strategy, super
   large operation networks have become an inevitable trend, and
   operational safety and risk management and control have become
   unavoidable problems. Existing safety management methods lack support
   from actual operational and production data, resulting in a lack of
   guidance of fault cause modes and risk chains. Large space is available
   to improve the breadth, depth, and accuracy of hazard source control. By
   mining multisource heterogeneous operation big data generated from
   subway operation, this study researches operation risk chain and refined
   management and control of key hidden dangers. First, it builds a data
   pool based on the operation status of several cities and then links them
   into a data lake to form an integrated data warehouse to find coupled
   and interactive rail transit operation risk chains. Second, it reveals
   and analyzes the risk correlation mechanisms behind the data and refines
   the key hazards in the risk chain. Finally, under the guidance of the
   risk chain, it deeply studies the technologies for refined control and
   governance of key hidden dangers. The results can truly transform rail
   transit operation safety from passive response to active defense,
   improving the special emergency rail transit operation plans, improving
   the current situation of low utilization of rail transit operation data,
   but high operation failure rate, and providing a basis for
   evidence-based formulation and revision of relevant industry standards
   and specifications.
ZB 0
ZA 0
ZR 0
TC 0
ZS 0
Z8 0
Z9 0
U1 4
U2 72
SN 1026-0226
EI 1607-887X
DA 2022-03-29
UT WOS:000770315700003
ER

PT P
AU PHAM A T
TI Improving hair and scalp of vehicle seat occupant            using
   vehicle cameras by comparing each imaged hair and            scalp of
   occupant with stored images having earlier            timestamp and
   determining treatment recommendation            based on detected
   changes
PN US2021334561-A1; US11373420-B2
AE TOYOTA MOTOR ENG & MFG NORTH AMERICA INC
AB 
   NOVELTY - Improving hair and scalp of a vehicle seat               
   occupant using vehicle cameras involves imaging                hair and
   scalp of occupant by vehicle cameras each                time the
   occupant sits in a seat of the vehicle;                storing images
   with timestamps of images; comparing                each current image
   with stored images having an                earlier timestamp; detecting
   changes between                current image and stored images;
   determining hair                and scalp parameters related to the
   changes;                accessing hair and scalp data related to the    
   parameters; analyzing the hair and scalp data;                accessing
   hair and scalp treatment options based on                the analysis;
   accessing hair care and scalp care                product information
   based on the analysis;                determining hair and scalp
   treatment recommendation                including the changes in the
   images, the                parameters, the treatment options, and the
   hair and                scalp product information; updating vehicle
   memory;                notifying occupant of the recommendation; and    
   transmitting the hair and scalp images and hair and                scalp
   parameters to a data lake.
   USE - The method is useful for improving hair and                scalp
   of a vehicle seat occupant using vehicle                cameras.
   ADVANTAGE - When riding in any one of a set of connected               
   vehicles having the hair and scalp analysis                software and
   connected to the hair and scalp                improvement application,
   the vehicle performs                imaging of the hair and scalp and
   transmits the                images to the hair and scalp improvement   
   application for further processing. The hair and                scalp
   images, changes in hair and scalp parameters,                and hair
   and scalp treatment recommendations of a                set of vehicle
   occupants are stored in the data                lake.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) system
   for improving the hair and scalp of                a vehicle seat
   occupant by using vehicle cameras,                comprising vehicles
   including cameras located above                the hair and scalp of the
   occupant for imaging the                hair and scalp of the occupant,
   sensors for                recording sensor signature of the occupant,
   CPU                operatively connected with cameras and sensors,      
   vehicle memory including user profile for each                occupant
   and a data record of hair and scalp images                of each
   occupant, image processor configured to                receive the hair
   and scalp images, timestamp the                hair and scalp images,
   and store the hair and scalp                images and timestamps of the
   hair and scalp images                in the vehicle memory, identity
   module configured                to identify the occupant by matching
   the sensor                signature of each occupant to a sensor
   signature                stored in the user profile, hair and scalp
   image                comparison module configured to compare current    
   hair and scalp images of each occupant with the                data
   record of hair and scalp images of the                occupant and to
   detect changes in the hair and                scalp images, hair and
   scalp analysis module                configured to determine hair and
   scalp parameters                from the changes in the hair and scalp
   images, user                interface configured to communicate with the
   occupant, onboard communications module configured                for
   transmitting data packet including the hair and                scalp
   images, hair and scalp parameters, identity                and user
   profile of each occupant and further                configured for
   receiving treatment recommendations                and updates, hair and
   scalp improvement application                including transceiver
   configured to receive the                data packet, application
   processor configured to                request a search for information
   related to the                changes in the hair and scalp images and
   the hair                and scalp parameters, hair and scalp analysis   
   processor, database including subscriber data,               
   application memory configured to store the identity                and
   user profile, registration module, and                treatment
   recommendation module, controller, hair                and scalp data
   artificial intelligence (AI)                analytics program configured
   to receive search                requests from the application processor
   and to form                search queries to retrieve information
   relating to                the changes in the hair and scalp images and
   the                parameters, hair and scalp treatment options and     
   hair care and scalp care products, and data lake               
   configured to receive the search queries, search               
   unstructured data and structured databases for the               
   information and to transmit the information to the                hair
   and scalp data AI analytics program; and(2) non-transitory computer
   readable medium                having instructions stored that, when
   executed by                one or more processor, cause the processors
   to                perform a method for improving the hair and scalp     
             of the occupant using vehicle cameras.
Z9 0
U1 0
U2 0
DA 2022-03-03
UT DIIDW:2021C22481
ER

PT P
AU XI J
   YANG X
   SHEN G
   FU H
   LIANG Z
TI Cloud platform based on custom device protocol            library and
   big data architecture, has cloud platform            management program
   to define device protocol library,            and data display program
   to obtain data from            distributed cache Redis to display in
   real time
PN CN113542363-A; CN113542363-B
AE ANHUI TRANSPORT CONSULTING & DESIGN INST; RES & DEV CENT HIGHWAY
   TRANSPORT TECHNOL
AB 
   NOVELTY - The device has a cloud platform management               
   program to define a device protocol library. An               
   industrial computer initialization program can               
   automatically initialize industrial control                computer
   operating environment, and download the                industrial
   control computer collecting program                image from Docker
   (computer operating system) hub                image warehouse. A
   collecting program is used to                send collected data to
   cloud platform distributed                message queue kafka (network
   communications                software) cluster through network in a
   self-defined                data format. A stream-big data processing
   program                based on stream is used to obtain data from the  
   distributed message queue kafka (network                communications
   software) cluster, process the data,                and store the
   processed data in a distributed cache                Redis (database
   management system). A data display                program is used to
   obtain the data from the                distributed cache Redis
   (database management                system) to display in real time.
   USE - Cloud platform based on custom device protocol               
   library and big data architecture.
   ADVANTAGE - The cloud platform has the characteristics of               
   strong versatility, high degree of automation, good               
   scalability, simple and convenient deployment, and                high
   real-time data, and has the ability to                automatically
   deploy and run the collection program                on the industrial
   computer by configuring the                sensor, acquisition
   instrument, and industrial                computer information in the
   cloud platform                management program.
Z9 0
U1 0
U2 0
DA 2021-12-02
UT DIIDW:2021C3818T
ER

PT P
AU HAO Y
TI Data lake management platform device, has screw rod whose bottom end is
   fixedly connected with driven gear, screw cylinder is set at bottom end
   of two sides of machine body, and screw rod passes through screw
   cylinder
PN CN214275161-U
AE BEIJING SHUJU ZHILIAN TECHNOLOGY CO LTD
AB 
   NOVELTY - The utility model claims a data lake management platform
   device based on big data, comprising a machine body, a display screen
   and a pull rod; the top end of the surface of the machine body is
   provided with a display screen; two sides of the display screen is
   provided with a pull rod; the bottom end of the machine body is provided
   with a waterproof structure; the waterproof structure comprises a screw
   cylinder, a screw rod, a rotating handle; a substrate, a limiting
   spring, a gear box; a driving gear and a driven gear; the base plate is
   set at the bottom end of the machine body; the utility model is provided
   with a base plate at the bottom of the machine body; the base plate is
   connected with the screw cylinder on the machine body by the screw rod
   at the top of two sides; the whole machine body is lifted by the screw
   rod; it is far away from the ground; it avoids the moist caused by the
   ground contact the inner part of the shell is provided with two groups
   of radiating fans through the supporting rod; the heat in the machine
   body is quickly flowing out; making it fast radiating; finishing the
   radiating operation; between the display screen and the machine body is
   provided with a slide rail capable of adjusting the height of the
   display screen; the different people can better experience the display
   screen; and the working efficiency is improved.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2021B4430H
ER

PT P
AU PRADO A B
   SENRA R D A
   BREITMAN K
TI Method for performing person-centric multi-channel            opinion
   mining in single data repository, involves            obtaining
   multi-channel heterogeneous data from set of            channels to
   aggregate into data lake, and determining            sentiment
   classification of opinion information
PN US11113306-B1
AE EMC IP HOLDING CO LLC
AB 
   NOVELTY - The method involves obtaining multi-channel               
   heterogeneous data from a set of channels and                aggregating
   the data into a data lake. A set of                target entities that
   are targets of opinion                information across the channels is
   identified, and                user identities are extracted from the
   channels.                Sentiment classification of the opinion
   information                is determined by identifying subjective
   information                and applying a trained classifier to the
   subjective                information. A communication is initiated to
   one of                the linked common user identities in a given node 
   cluster based on an evaluation of the frequency of                the
   information and the classification of                information.
   USE - Method for performing person-centric                multi-channel
   opinion mining in a single data                repository such as a data
   lake.
   ADVANTAGE - The method enables providing significant               
   improvements relative to conventional opinion                mining
   systems by performing person-centric opinion                mining on
   the data in the data lake.
   DETAILED DESCRIPTION - Independent claims are included for:1. A computer
   program product comprising a                tangible machine-readable
   storage medium having                encoded executable code of software
   programs for                implementing the method for performing      
   person-centric multi-channel opinion mining.2. A system for performing
   person-centric                multi-channel opinion mining in a single
   data                repository such as a data lake.
Z9 0
U1 0
U2 0
DA 2022-01-08
UT DIIDW:2021A23677
ER

PT P
AU SUN W
   ZHUANG Q
   WANG D
   LI Z
TI Method for loading operation of service node in            server for
   data lake development management system,            involves determining
   whether to load recently added            operation into data lake based
   on relationship between            total and preset loading time
PN CN113347249-A; CN113347249-B
AE IND & COMML BANK CHINA LTD
AB 
   NOVELTY - The method involves obtaining operation               
   information of a service node, where the operation               
   information comprises an existing work. A                to-be-allocated
   operation is performed based on a                preset node
   distribution strategy. The                to-be-allocated operation is
   distributed to                corresponding service node. A preset node 
   distribution strategy is determined based on the               
   operation information of the service node. Total               
   operation loading time of the service node is                calculated
   according to the operation information                of the service
   node and recently added operation.                Determination is made
   whether to load the recently                added operation into a data
   lake based on relation                of the total operation loading
   time and preset                loading time of each service node.
   USE - Method for loading operation of a service node                in a
   server based on big data for a data lake                development
   management system.
   ADVANTAGE - The method enables satisfying downstream               
   operation loading time requirements and fully using               
   resources of each server in production mode.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the     
   following:an operation loading device;a device comprising a processor
   for loading                operation; anda computer readable storage
   medium comprising                a set of instructions for loading
   operation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for loading operation of a service
   node in                a server based on big data. (Drawing includes    
              non-English language text).
Z9 0
U1 0
U2 0
DA 2022-01-08
UT DIIDW:2021A5486S
ER

PT J
AU Huang, Tingting
TI SBIR Phase II: Information fusion-driven adaptive corridor-wide traffic
   signal re-timing
DT Awarded Grant
PD Sep 01 2021
PY 2021
AB The broader impact of this Small Business Innovation Research (SBIR)
   Phase II project focuses on an adaptive traffic signal timing solution.
   Cities and municipalities worldwide spend over $4 billion annually to
   retime traffic signals and yet often fail to adequately reduce
   congestion on roadways. The consequences of mistimed traffic signal
   timing are: a) increasing productivity losses due to congestion with the
   average American spending 97 hours stuck in traffic every year, b)
   increasing accidents due to traffic, with one fatality every 15 minutes
   on US roads, and c) increasing greenhouse emissions with a third of all
   emissions caused by vehicles on the roads. This project will support the
   development and commercialization of a web-based technology to support
   traffic managers in cities and municipalities to better manage traffic
   using artificial intelligence (AI) and big data analytics. In addition
   to improving traffic flow and reducing congestion, the system will also
   significantly reduce harmful emissions, leading to more environmentally
   friendly city streets. The serviceable markets for this technology in
   the US and Europe, which together constitute 60% of the global
   signal-timing market, represent a $2.4 billion opportunity. 
   <br/><br/>This Small Business Innovation Research Phase II project seeks
   to develop a proof-of-concept for a fully adaptive traffic signal
   retiming solution that can robustly handle multiple signal corridors for
   commercialization. The key intellectual merit of this effort will be
   developing deep learning models that can run at scale and handle sensor
   noise robustly. The reinforcement learning process will help the system
   to adapt to changing traffic scenarios at different scales without the
   need for manual interventions. Research objectives that must be overcome
   in Phase II are focused on: 1) scaling the solution; 2) making the
   solution robust; and 3) ensuring that the system is user ready.
   Achieving these objectives may help ensure the product can successfully
   run on big-data architecture economically deployed on the cloud.  The
   solution will also provide a deeper understanding of human-machine
   interaction. Overall, the proposed system may reduce implementation time
   as well as capital and maintenance expenditures for signal timing
   systems. These advantages will encourage cities around the US and
   internationally to adopt such signal timing strategies.<br/><br/>This
   award reflects NSF's statutory mission and has been deemed worthy of
   support through evaluation using the Foundation's intellectual merit and
   broader impacts review criteria.
RI huang, ting/OCK-5261-2025
ZA 0
ZR 0
Z8 0
ZB 0
TC 0
ZS 0
Z9 0
U1 0
U2 0
G1 2052257
DA 2023-12-08
UT GRANTS:15497252
ER

PT J
AU Vasquez-Morales, Felipe
   Cravero-Leal, Ania
TI Big Data Architecture for Forest Fire Management Support in the Region
   of Araucania
SO REVISTA CIENTIFICA
VL 42
IS 3
BP 304
EP 314
DI 10.14483/23448350.18349
DT Article
PD SEP-DEC 2021
PY 2021
AB Wildfires have been a growing problem in the last decades. In recent
   years, Big Data technology has been used to process large volumes of
   data from sensors, photos, satellite and images, as well as valuable
   data from field experience. In Chile, there are no Big Data systems to
   support forest fire management. This work aims to propose a Big Data
   architecture for managing the volume of data provided by satellite
   images and supporting fire management in Chile. This architecture was
   tested through a prototype implemented with Cloud Computing tools, which
   processes satellite images and is focused on the analysis of controlled
   burns in the region of La Araucania. The results show that the resulting
   images are valuable for decision-making in the management of burns
   within the region. Although there is much to improve, the results are
   encouraging in terms of the value generated by the resulting images and
   the improvement of this prototype and the architecture itself.
RI Cravero, Ania/MTD-8173-2025
OI Cravero, Ania/0000-0003-0883-7254
TC 0
ZR 0
ZA 0
ZS 0
ZB 0
Z8 0
Z9 0
U1 1
U2 12
SN 0124-2253
EI 2344-8350
DA 2021-10-21
UT WOS:000704485100005
ER

PT P
AU CHEN L
   ZHU R
   YUAN C
   YANG D
   CHEN Q
TI Transformer life cycle detection device based on big data architecture,
   has upper end supporting column that is provided with anti-cover, second
   movable buckle whose lower end is provided with second connecting rod,
   and lifting mechanism that is provided with upper connecting seat
PN CN113325205-A; CN215866780-U
AE JIANGSU SUPEZET INTELLIGENT HEAVY IND CO; SHANGHAI SUPEZET ENG
   TECHNOLOGY CO LTD
AB 
   NOVELTY - The device has a machine main portion (1) whose bottom portion
   is provided with a lifting mechanism (2). The lifting mechanism is
   provided with an upper connecting seat (21). A lower end of the upper
   connecting seat is provided with a first movable buckle (22). A bottom
   portion of the first movable buckle is provided with a first connecting
   rod (23). A lower end of the first connecting rod is provided with a
   second movable buckle (24). A lower end of the second movable buckle is
   provided with a second connecting rod (25). An upper end supporting
   column is provided with an anti-cover (46).
   USE - Transformer life cycle detection device based on big data
   architecture.
   ADVANTAGE - The device is convenient to adjust height, has better
   water-proof and dust-proof effect, and utilizes a lifting locating
   mechanism that is fixed at a bottom portion of a device main portion so
   as to realize stable lifting adjusting position, thus preventing short
   connection.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a
   transformer life cycle detection device.Machine main portion (1)Lifting
   mechanism (2)Upper connecting seat (21)Movable buckle (22, 24)Connecting
   rod (23, 25)Anti-cover (46)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2021A4060A
ER

PT P
AU SANDVIG C
TI Method for optimizing generalized transfers            between storage
   systems, involves transferring subset            of source data from
   first storage system to second            storage system in dependence
   upon difference            information
PN US2021263651-A1; US11768623-B2
AE PURE STORAGE INC
AB 
   NOVELTY - The method involves identifying a request to               
   transfer source data from a storage system to                another
   storage system. The storage systems                implement respective
   storage architectures. The                Difference information is
   identified between the                source data that is stored on the
   former storage                system using one of the storage
   architectures and                existing data is stored in the latter
   system using                the other storage architecture. A subset of
   source                data is transferred from the former system to the 
   latter storage system in dependence upon the                difference
   information. The storage architectures                are structured
   data architecture and unstructured                data architecture and
   are implemented on physical                block storage resources.
   USE - Method for optimizing generalized transfers                between
   storage systems.
   ADVANTAGE - The method enables utilizing a cloud computing              
   as a model of service delivery for enabling                convenient,
   on-demand network access to a shared                pool of configurable
   computing resources that can                be rapidly provisioned and
   released with minimal                management effort or interaction
   with a provider of                the service. The method allows cloud
   computing                environment to offer infrastructure, platforms 
   and/or software as services for which a cloud                consumer
   does not need to maintain resources on a                local computing
   device.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:1. an apparatus comprising a computer               
   processor, a computer memory operatively coupled to                the
   computer processor2. a computer program product disposed upon a         
         computer readable storage medium
Z9 0
U1 0
U2 0
DA 2022-01-08
UT DIIDW:202198764R
ER

PT P
AU QI W
TI Method for sharing big data of smart city,            involves receiving
   business driving request of            government cloud, retrieving
   semi-structured            intelligent urban instance library associated
   with            business drive, and sharing final retrieval result to   
           administrative cloud
PN CN113282692-A
AE QI W
AB 
   NOVELTY - The method involves performing (S102) the               
   resource description framework (RDF) and related                ontology
   descriptions on the spatiotemporal big                data. The
   associated spatio-temporal big data is                labelled (S103) as
   a data source, and the marked                spatio-temporal big data is
   stored in a data lake,                where the data lake includes a
   business layer and a                source data layer. The data lake is
   split (S104)                into multiple sub-data lakes, where each
   sub-data                lake corresponds to each smart city instance    
   one-to-one based on different instance requirements                of
   the smart city. The semi-structured processing                is
   performed (S105) on each of the sub-data lakes,                each
   semi-structured smart city instance library is                generated,
   and stored in the business layer of the                data lake. The
   business drive request of the                government cloud is
   received (S106), the                semi-structured smart city instance
   database                associated with the business drive is searched,
   and                the final search result is shared in the government  
                cloud.
            USE - Method for sharing big data of smart                city.
   ADVANTAGE - The method enables realizing smart city big               
   data sharing process in an effective manner. The               
   redundancy of the data transmission is reduced, the                data
   transmission efficiency is improved, and the                network
   resource is saved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device    
              for sharing big data of smart city.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the method for sharing big data of smart
   city.                (Drawing includes non-English language text)Step
   for performing the resource description                framework and
   related ontology descriptions on the                spatiotemporal big
   data (S102)Step for labelling the associated               
   spatio-temporal big data as a data source                (S103)Step for
   splitting the data lake into                multiple sub-data lakes
   (S104)Step for performing the semi-structured                processing
   on each of the sub-data lakes                (S105)Step for receiving
   the business drive request                of the government cloud (S106)
Z9 0
U1 0
U2 0
DA 2022-01-08
UT DIIDW:202199554J
ER

PT P
AU GODDARD D J
   HAZBOUN P E
   KONDURU S
   PATIL V
   PANJWANI N
   MATETI S
TI Method for migrating data from disparate source            systems to an
   amalgamated data lake, involves receiving            the data files of
   unstructured data at a first location            within a data
   repository that tabukating the copy of            the data files into
   structured tables within the data            repository
PN US2021232549-A1; US11269826-B2
AE BANK OF AMERICA CORP
AB 
   NOVELTY - The method involves receiving the data files               
   (108) of unstructured data at a first location                within a
   data repository (102). A copy of the data                files is
   tabulated into structured tables within                the data
   repository. The structured tables are                transferred from a
   first location within the data                repository to a database
   within the data                repository. The data files are
   transferred with the                unstructured data, from the first
   location within                the data repository, to an edge node
   (110), within                a data lake. The structured tables are
   transferred                from the database within the data repository
   to a                stage table (112) within the data lake. The data    
   files are tabulated for an unstructured data, into               
   structured tables at the edge node within the data                lake
   (104).
   USE - Method for migrating data from disparate                source
   systems to an amalgamated data lake.
   ADVANTAGE - Method improves the performance of jobs and               
   end user queries. The data lake may be lacking the               
   capabilities, or be optimized in easily consumable                views
   that enable a user to appropriately identify,                select and
   manipulate the data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the     
   following:(a) a framework for viewing data within an               
   amalgamated data lake;(b) a system for migrating data from disparate    
              source systems to an amalgamated data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method  
   for migrating data from disparate source systems to                an
   amalgamated data lake.Data repository (102)Data lake (104)Data files
   (108)Edge node (110)Stage table (112)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202187167Y
ER

PT P
AU GODDARD D J
   HAZBOUN P E
   MATETI S
   PATIL V
   PANJWANI N
   KONDURU S
TI Method for generating structured views of raw data            stored in
   data lake, involves generating tabulated            views of records
   referenced in standard view table and            records referenced in
   history table using viewing tool            running in data lake
PN US2021232538-A1; US11354269-B2
AE BANK OF AMERICA CORP
AB 
   NOVELTY - The method involves receiving tabulated data                at
   an edge node (316) within a data lake (304). The               
   tabulated data is deconstructed into a main flat                table.
   The main flat table is stored in the data                lake. A view
   creation framework configured to                create a standard view
   table consisting of only                unique records on demand is
   executed based on the                main flat table. A snapshot load
   framework is                executed that loads records referenced in
   the                standard view table and associated with a time       
   stamp before to a start of the quantum of time into                a
   history table when a quantum of time elapses                after
   creation of the standard view table. The                history table is
   stored in the data lake. The                tabulated views of records
   referenced in the                standard view table and records
   referenced in the                history table are generated using a
   viewing tool                running in the data lake based on records
   included                in the main flat table.
   USE - Method for generating structured views of raw                data
   stored in data lake for big data processing in                data lake
   storing information in native                format.
   ADVANTAGE - The advantages can include improving               
   performance of jobs and end user queries.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(a) a system
   of frameworks operating in a data                lake for generating
   structured views of information                stored within the data
   lake;(b) a system of frameworks operating in a data                lake
   for generating tabulated views of information                stored in
   native format within the data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows the schematic view of a   
   system for generating structured views of raw data                stored
   in data lake.302Source system316Edge node304Data lake308Condensor
   process312Teradata database
Z9 0
U1 0
U2 0
DA 2022-01-08
UT DIIDW:202187145Y
ER

PT P
AU ZHANG S
   SUN H
   QIAN G
   LI F
   ZHAO Y
TI Method for managing data lake of intelligent bus,            involves
   obtaining data packet uploaded by bus system,            establishing
   data set in data pool according to data in            data packet, and
   sending matched data to user
PN CN113157742-A
AE HUALU ZHIDA TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves obtaining a data packet               
   uploaded by a bus system. The data packet is                classified
   into different data types. The data                packet is generated
   by bus system management                process. The data packet is
   provided with                structured data, semi-structured data and  
   unstructured data. The data packet is stored in a                data
   pool according to the data types. A data set                is
   established in the data pool according to the                data
   packet. The data set is analyzed according to                a query
   request to obtain a query condition. The                matched data is
   sent to a user when the data set is                matched with the
   query condition.
   USE - Method for managing data lake of an                intelligent
   bus.
   ADVANTAGE - The method enables improving utilization rate               
   of the bus management data in an effective                manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
        system for managing data lake of an intelligent                bus.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for managing data lake of an
   intelligent                bus. (Drawing includes non-English language  
                text).
Z9 0
U1 0
U2 0
DA 2021-12-28
UT DIIDW:202191743K
ER

PT P
AU RAE C B
   SEO J
   PARKJINYOUNG
TI Artificial intelligence (AI) framework-enabled distributed edge clusters
   for intelligent weather data processing, which defines and maps
   functions for data lake-centric AI service support by utilizing machine
   learning architecture
PN KR2021070152-A; KR2345786-B1
AE GENO TECH CO LTD
AB 
   NOVELTY - The artificial intelligence (AI) framework-enabled distributed
   edge clusters defines and maps functions for data lake-centric AI
   service support by utilizing machine learning architecture of the
   giuseppe in the data lake framework. The data based on the Abyss
   software defined storage (SDS) infrastructure with integrated computing
   by solid state drive (SSD) and graphics processing unit (GPU).
   USE - Artificial intelligence (AI) framework-enabled distributed edge
   clusters for intelligent weather data processing of remote sensing.
   ADVANTAGE - The distributed edge cluster supporting the AI framework for
   remote sensing intelligent weather data processing can effectively
   support the connected data architecture (CDA) -based AI framework based
   on a large number of accurate data, thus reducing the risk and
   responding to disaster situations. The distributed edge cluster
   supporting AI framework for intelligent weather data processing of
   remote sensing strengthens computing by SSD and GPU to Abyss SDS
   infrastructure to process various big data, and relates to a system that
   can integrate and support a connected data architecture (CDA) based AI
   framework. The messaging layer separates the connection between the data
   collection layer and the work device to prevent unnecessary data from
   flowing in, and the data received from the data collection layer and
   converted into a message to ensure the delivery. The lambda architecture
   facilitates the merging of batch data processing and real-time data
   processing, and can provide consistent data through semi-real-time
   processing of large-scale data sets with high-performance distributed
   computing and scalability. The metadata is structured data about data,
   and data given to content according to a certain rule so as to
   efficiently find and use the information that is being sought from among
   a large amount of information. The security incidents are prevented in
   advance as the number of users who make decisions using large-scale data
   analysis increases.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for
   detecting a weather change using the AI framework-supported distributed
   edge cluster.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a
   Abyss SDS, which is a configuration of an AI framework-supported
   distributed edge cluster for remote sensing intelligent weather data
   processing.Central processing unit (CPU)Graphics processing unit
   (GPU)Solid state drive (SSD)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2021653205
ER

PT P
AU WU C
   HE F
TI System for analyzing and processing chronic            disease quality
   control data based on big data, has            display query layer
   connected with storage layer to            built chronic disease quality
   control data warehouse            and select corresponding database
PN CN112732673-A
AE SHANGHAI BAOSHAN DISEASE PREVENTION
AB 
   NOVELTY - The system has a data source layer for               
   constructing chronic disease sanitary big data                according
   to a service database to form a                three-layer medical
   health data architecture. A                calculating layer is
   connected with the data source                layer for performing
   deterministic data analysis,                exploration data analysis,
   predictive data                analysis, data processing and conversion.
   A storage                layer is connected with the calculating layer
   for                performing online storage of the big data display    
   query layer. A display query layer is connected                with a
   storage layer to built a chronic disease                quality control
   data warehouse and select the                corresponding database by
   creating a collection                task to create a data source.
   USE - System for analyzing and processing chronic                disease
   quality control data based on big                data.
   ADVANTAGE - The system establishes chronic disease related              
   data collecting system and multi-dimensional                explanation
   of associated relationship of chronic                disease and medical
   sanitation big data for                changing data quality control
   index                deficiency.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   system                for analyzing and processing chronic disease      
   quality control data based on big data. (Drawing                includes
   non-English language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2021554552
ER

PT P
AU GE X
   CUI Y
   MAO R
   ZHAN Z
   JIN L
   GAO C
TI Big data architecture based urban rail transit            monitoring
   system has data sharing module that builds            data model for
   stored data, data visualization module            reads/writes/displays
   data and data analysis module            processes/analyzes stored data
PN CN112693502-A
AE SHANGHAI BAOSIGHT SOFTWARE CO LTD
AB 
   NOVELTY - The system has a data storage module that               
   builds a data storage platform. A data access                module
   accesses the data of the monitored system                and store data
   in the built data storage platform.                A data sharing module
   builds a data model for the                stored data. A data
   visualization module reads,                writes and displays data
   according to the data                model to realize data
   visualization. A data                analysis module processes and
   analyzes the stored                data. The data access module provides
   complete data                access for various existing data of various
   rail                transit systems. The various existing data of the   
   various rail transit systems include digital data                and
   analog data in environmental and equipment                monitoring
   system, automatic fire alarm system,                automatic fare
   collection system.
   USE - Big data architecture based urban rail transit               
   monitoring system.
   ADVANTAGE - The functions of the rail transit monitoring               
   system are realized. The data access is                standardized and
   the data storage is unified. The                storage resources are
   set as required. The dynamic                adjustment configuration is
   supported. The bearing                capacity of the system is
   improved. The expansion                can be carried out aiming at
   different scales of                the monitoring system and the
   distributed                calculation is realized. The resource
   utilization                rate is improved and the implementation is   
               convenient.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an urban    
   rail transit monitoring method based on big data               
   architecture.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an
   urban                rail transit monitoring system based on a big data 
   architecture. (Drawing includes non-English                language
   text)
Z9 0
U1 0
U2 0
DA 2021-05-27
UT DIIDW:2021447449
ER

PT P
AU WANG L
   WANG P
   PENG Q
   TIAN J
   XIANG X
   DING Y
   LI F
TI Data lake architecture has data collection layer            that
   collects data which includes unstructured data or           
   multi-structured data, data storage layer that stores            data,
   and data processing layer that processes            data
PN CN112597218-A
AE EVERBRIGHT TECHNOLOGY CO LTD
AB 
   NOVELTY - The data lake architecture has a data               
   collection layer (22) that collects data which                includes
   unstructured data or multi-structured                data. A data
   storage layer (24) stores the data. A                data processing
   layer (26) processes the data. The                data collection layer
   collects the data through                batch processing if the
   throughput of the data is a                key factor in business
   decision-making, and                collects the data in real-time
   collection if the                delay of data is a key factor in
   business                decision-making.
                       USE - Data lake architecture.
   ADVANTAGE - The data is stored and processed through the               
   data lake built on the distributed architecture,                which
   can solve that the data warehouse is only                suitable for
   structured data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a data processing method;a data processing device;a
   computer-readable storage medium storing                program for data
   processing; andan electronic device.
   DESCRIPTION Of DRAWING(S) - The drawing shows the structural block      
   diagram of the data lake architecture. (Drawing                includes
   non-English language text)22Data collection layer24Data storage
   layer26Data processing layer
Z9 0
U1 0
U2 0
DA 2021-05-01
UT DIIDW:2021361104
ER

PT P
AU GUI H
   FENG K
   WANG Y
   WANG H
TI Petri network based multi-source heterogeneous            data quality
   detection method, involves forming data            quality analysis
   report in form of table and chart            based on feedback of
   library for realizing monitoring            data quality detection
   process
PN CN112540975-A; CN112540975-B
AE CAS BIG DATA ACAD COMPUTING TECHNOLOGY; BIG DATA ACAD ZHONGKE
AB 
   NOVELTY - The method involves configuring multiple               
   heterogeneous data sources connected in data lake                managed
   by a main system through a data source for                configuring.
   Multiple heterogeneous data sources                are connected with
   the data lake. Multiple                heterogeneous data sources are
   connected with a                local multi-source heterogeneous data
   processing                server. Metadata is obtained by performing
   metadata                collection task to multi-source heterogeneous   
   database, where the metadata comprises metadata                table
   information, field information, index                information and
   constraint information. External                table connection is
   established through autonomous                expansion of
   PostgreSQL(RTM: free and open-source                relational database
   management system) database                according to data source
   information and metadata                information. Data quality rule
   is established                according to quality task constructing
   adding                information feedback of a petri network           
       model.
   USE - Petri network based multi-source heterogeneous                data
   quality detection method.
   ADVANTAGE - The method enables generating data quality               
   analysis result for the user to check, establishing                data
   quality knowledge base to improve quality                problem solving
   ability for providing effective                support for enhancement
   of system data quality so                as to increase data value.
   DETAILED DESCRIPTION - Data quality analysis report is formed in form   
   of table and chart to help a user for analyzing               
   integrity, consistency, accuracy, timeliness and                validity
   of data in a management data source                according to
   real-time feedback of message library                for realizing
   real-time monitoring data quality                detection process.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a petri network based multi-source
   heterogeneous                data quality detection method. (Drawing
   includes                non-English language text).
Z9 0
U1 0
U2 0
DA 2021-04-27
UT DIIDW:2021314369
ER

PT P
AU QIN L
   LIANG S
   CHEN S
   LI S
   ZHOU Y
   OUYANG J
   LI X
TI Hadoop-based distributed power system abnormal            data
   identification method, involves performing data           
   identification and repair for basic unit of time period            data
   after normalization processing is performed
PN CN112364098-A
AE ELECTRIC POWER RES INST GUANGDONG POWER
AB 
   NOVELTY - The method involves obtaining power load curve               
   data of a power system. Normalization processing is               
   performed to data. A characteristic of a data                sample is
   obtained for a normalized standard data                group by using
   improved iterative k-mean clustering                mode. An adaptive
   boosting (adaboost) classifier is                trained according to a
   data sample. Cloud treatment                is performed to the trained
   adaboost classifier.                Data identification and repair are
   performed for a                basic unit of time period data after
   normalization                processing is performed.
   USE - Hadoop-based distributed power system abnormal                data
   identification method.
   ADVANTAGE - The method enables realizing data               
   identification and repair of power big data on                basis of
   Hadoop data architecture, and reducing                cost and improving
   application capability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   Hadoop-based distributed power system abnormal data               
   identification system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the Hadoop-based distributed power system
   abnormal                data identification method. (Drawing includes   
               non-English language text).
Z9 0
U1 0
U2 0
DA 2021-03-08
UT DIIDW:202118632J
ER

PT P
AU CHEN Z
   XIN S
TI City-level data center has big data supporting            sub-platform
   that provides support platform components            for data management
   sub-platform and data sharing            exchange sub-platform to
   support entire city-level data            center
PN CN112287275-A
AE INESA INTELLIGENT CO LTD
AB 
   NOVELTY - The data center has a district-level data               
   resource center (1) that collects district-level                big data
   and stores the district-level big data to                form a
   district-level data lake. The data sharing                exchange
   sub-platform (2) connects the                district-level data
   resource center and the data                management sub-platform. The
   data sharing exchange                sub-platform is configured for
   realizing data                exchange between the district-level data
   resource                center and the data management sub-platform and 
   realizing data sharing of the district-level data               
   resource center. The artificial modeling                sub-platform is
   connected to the data management                sub-platform and used to
   build an artificial                intelligence modeling scheduling
   platform,                providing mainstream machine learning. The big
   data                supporting sub-platform provides support platform   
   components for the data management sub-platform and                the
   data sharing exchange sub-platform to support                the entire
   city-level data center.
                       USE - City-level data center.
   ADVANTAGE - The data sharing and exchange sub-platform is               
   established to realize the sharing and exchange of               
   district-level big data in the district-level                resource
   center.
   DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of a
   city-level data center. (Drawing includes                non-English
   language text)1District-level data resource center2Data sharing exchange
   sub-platform3Data governance sub-platform12Basic library13Theme library
Z9 0
U1 0
U2 0
DA 2021-02-22
UT DIIDW:2021134920
ER

PT P
AU SENRA R D A
   BREITMAN K
   PRADO A B
   BURSZTYN V
TI Method for semantic multi-database data lake used            to
   integrate heterogeneous data in unified data            repository,
   involves obtaining queries from user            specified in query
   language of given database of            databases are subsequent to
   translating
PN US10901973-B1
AE EMC IP HOLDING CO LLC
AB 
   NOVELTY - The method involves replicating data obtained               
   from the user for storage in the databases of each                of the
   different database types by translating the                obtained
   user-provided ontology definition language                database
   commands into multiple database                type-specific ontology
   definition language database                commands supported by each
   of the different                database types. The database
   type-specific ontology                definition language database
   commands for a first                one of the databases of a first
   database type                comprise database type-specific ontology
   definition                language database commands supported by the
   first                database type. The queries (210) obtained are      
   delegated from the user to the given database of                the
   multiple databases having different database                types
   without translating the query language of the                queries.
   The data responsive to the queries is                retrieved from the
   given database. The cluster                gateways (230) are provided
   to manage multiple                clusters of the database instances of
   the database                type.
   USE - Method for semantic multi-database data lake                used
   to integrate heterogeneous data in unified                data
   repository.
   ADVANTAGE - The object-oriented database serves to persist              
   structured information or in-memory data structures                from
   programming languages, while the data                hierarchical
   relationships are preserved. The                cognitive overhead is
   reduced and training,                development and maintenance are
   simplified.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a computer program product comprises a               
   non-transitory machine-readable storage medium                having
   encoded therein executable code of software                programs when
   executed by one processing device;                anda system for
   semantic multi-database data                lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of       
   semantic multi-database data lake                architecture.200Data
   lake205Database operations210Queries220Mediator gateway230Cluster
   gateways
Z9 0
U1 0
U2 0
DA 2021-02-09
UT DIIDW:202110060P
ER

PT P
AU CHANG E
   GHILDYAL A
   GREEN S
TI System for migrating data and software application           
   functionality from legacy data system to block chain            data
   lake system, has transaction controller that            generatenew
   hash, and blockchain controller that adds            new hash to block
   of block chain
PN WO2021003532-A1; AU2020311300-A1; GB2600315-A; US2022253413-A1
AE NEWSOUTH INNOVATIONS PTY LTD; COMMONWEALTH AUSTRALIA DEPT DEFENCE
AB 
   NOVELTY - The system has a data lake that comprises the               
   data files including in semi-structured or                unstructured
   data format. A software application                interface for the
   data files. A hashing controller                (109) generates the
   hashes. A verification                controller (114) verifies the data
   files. A                transaction controller (108) monitors the       
   transactions performed on the data files by                functions of
   the software applications in which the                transaction
   involves a data file before executing                the transaction.
   The verification controller is                controlled by the
   transaction controller to                generate a hash using the data
   file and the hashing                controller and verify the data file
   by searching                for a matching hash stored in a blockchain
   (113).                The transaction is executed and data within the   
   data file is added or updated if the data file is               
   verified. The transaction controller uses the                hashing
   controller to generate a new hash using the                data file. A
   blockchain controller (111) adds the                new hash to a block
   of the blockchain.
   USE - System for migrating data and software                application
   functionality from legacy data system                to block chain data
   lake system.
   ADVANTAGE - The verification controller uses the hashing               
   controller to hash the data file to verify the               
   authenticity and accuracy.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   blockchain data lake system.108Transaction controller109Hashing
   controller111Blockchain controller113Blockchain114Verification
   controller
Z9 0
U1 0
U2 0
DA 2021-01-25
UT DIIDW:202105126X
ER

PT B
AU Barros, GuilhermePeres
   Naranjo-Zolotov, Mijail
Z2  
TI [not available]
DT Dissertation/Thesis
PD Jul 10 2023
PY 2023
ZB 0
ZS 0
Z8 0
TC 0
ZA 0
ZR 0
Z9 0
U1 0
U2 0
BN 979-8-209-83532-5
UT PQDT:49452035
ER

PT R
AU Caceres, Laura
   Merino, Jose Ignacio
   Diaz-Diaz, Norberto
TI A computational intelligence approach to predict energy demand using
   Random Forest in a Cloudera cluster
SO Zenodo
DI http://dx.doi.org/10.5281/ZENODO.5483898
DT Data set
PD 2021-10-07
PY 2021
AB Society's energy consumption has shot up in recent years, making the
   prediction ofits demand a current challenge to ensure an efficient and
   responsible use. Artificial intelligencetechniques have proven to be
   potential tools in handling tedious tasks and making sense oflarge-scale
   data to make better business decisions in different areas of knowledge.
   In this article,the use of random forests algorithms in a Big Data
   environment is proposed for households energydemand forecasting. The
   predictions are based on the use of information from different
   sources,confirming a fundamental role of socioeconomic data in
   consumer's behaviours. On the otherhand, the use of Big Data
   architectures is proposed to perform horizontal and vertical scaling
   ofthe solution to be used in real environments. Finally, a tool for
   high-resolution predictions withgreat efficiency is introduced, which
   enables energy management in a very accurate way. Raw data is incuded in
   data.csv. This file contains half hourly home electricity consumption
   registers for 4404households with fix tariffs (not subject to dynamic
   time of use) for a period between November 2011 and February 2014.
   Original information was acquired from the Low Carbon London project led
   by UK Power Networks
   (https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london
   -households) RFResults.zip contains the energy predictions for each
   ACORN group using the generated Random Forest algorithm. For this
   purpose, the first 613 days of a total of 818 observations of each group
   were considered for training and the last 205 days for testing.
   Meteorological data was adquired from the darksky app
   (https://darksky.net).These data are included in the
   weather_hourly_darksky.csv uk_bank_holidays. xlsx contains the dated of
   UK bank holidays for the studied period, used as additional variable
   related to occupancy Funding: This research was partially supported by
   the Ministry of Economy and Competitiveness, project
   TIN2015-64776-C3-2-R, and by the Junta de Andalucia, under the
   Andalusian Plan for Research, Development and Innovation, TIC-239.
   Copyright: Creative Commons Attribution 4.0 International Open Access
Z8 0
ZR 0
TC 0
ZB 0
ZA 0
ZS 0
Z9 0
U1 0
U2 0
DA 2024-11-21
UT DRCI:DATA2021221022888175
ER

PT R
AU Caceres, Laura
   Merino, Jose Ignacio
   Diaz-Diaz, Norberto
TI A computational intelligence approach to predict energy demand using
   Random Forest in a Cloudera cluster
SO Zenodo
DI http://dx.doi.org/10.5281/ZENODO.5483899
DT Data set
PD 2021-10-07
PY 2021
AB Society's energy consumption has shot up in recent years, making the
   prediction ofits demand a current challenge to ensure an efficient and
   responsible use. Artificial intelligencetechniques have proven to be
   potential tools in handling tedious tasks and making sense oflarge-scale
   data to make better business decisions in different areas of knowledge.
   In this article,the use of random forests algorithms in a Big Data
   environment is proposed for households energydemand forecasting. The
   predictions are based on the use of information from different
   sources,confirming a fundamental role of socioeconomic data in
   consumer's behaviours. On the otherhand, the use of Big Data
   architectures is proposed to perform horizontal and vertical scaling
   ofthe solution to be used in real environments. Finally, a tool for
   high-resolution predictions withgreat efficiency is introduced, which
   enables energy management in a very accurate way. Raw data is incuded in
   data.csv. This file contains half hourly home electricity consumption
   registers for 4404households with fix tariffs (not subject to dynamic
   time of use) for a period between November 2011 and February 2014.
   Original information was acquired from the Low Carbon London project led
   by UK Power Networks
   (https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london
   -households) RFResults.zip contains the energy predictions for each
   ACORN group using the generated Random Forest algorithm. For this
   purpose, the first 613 days of a total of 818 observations of each group
   were considered for training and the last 205 days for testing.
   Meteorological data was adquired from the darksky app
   (https://darksky.net).These data are included in the
   weather_hourly_darksky.csv uk_bank_holidays. xlsx contains the dated of
   UK bank holidays for the studied period, used as additional variable
   related to occupancy Funding: This research was partially supported by
   the Ministry of Economy and Competitiveness, project
   TIN2015-64776-C3-2-R, and by the Junta de Andalucia, under the
   Andalusian Plan for Research, Development and Innovation, TIC-239.
   Copyright: Creative Commons Attribution 4.0 International Open Access
ZA 0
ZS 0
Z8 0
TC 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
DA 2024-11-21
UT DRCI:DATA2021221022888176
ER

PT J
AU Chang,, Jae-Young
   Kim, Jihoon
   Jee, Seowoo
TI Development of Data Profiling Software Supporting a Microservice
   Architecture
Z1 마이크로 서비스 아키텍처를 지원하는 데이터 프로파일링 소프트웨어의 개발
SO The Journal of The Institute of Internet, Broadcasting and Communication
S1 한국인터넷방송통신학회 논문지
VL 21
IS 5
BP 127
EP 134
DI 10.7236/JIIBC.2021.21.5.127
DT research-article
PD 2021
PY 2021
AB Recently, acquisition of high quality data has become an important issue
   as the expansion of the big data industry. In order to acquiring high
   quality data, accurate evaluation of data quality should be preceded
   first. The quality of data can be evaluated through meta-information
   such as statistics on data, and the task to extract such
   meta-information is called data profiling. Until now, data profiling
   software has typically been provided as a component or an additional
   service of traditional data quality or visualization tools. Hence, it
   was not suitable for utilizing directly in various environments. To
   address this problem, this paper presents the development result of data
   profiling software based on a microservice architecture that can be
   serviced in various environments. The presented data profiler provides
   an easy-to-use interface that requests of meta-information can be
   serviced through the restful API. Also, a proposed data profiler is
   independent of a specific environment, thus can be integrated
   efficiently with the various big data platforms or data analysis tools.
AK 최근 빅데이터 산업의 확대로 고품질의 데이터를 확보하는 것이 중요한 이슈로 떠오르고 있다. 고품질의 데이터를 확보하기 위해서는
   데이터에 품질에 대한 정확한 평가가 선행되어야 한다. 데이터의 품질은 데이터에 대한 통계와같은 메타정보를 통해 평가할 수 있는데
   이러한 메타정보를 자동으로 추출하는 기능을 데이터 프로파일링이라고 하다.지금까지 데이터 프로파일링 소프트웨어는 기존의 데이터 품질
   또는 시각화 관련 소프트웨어의 부품이나 추가적인 서비스로 제공되는 것이 일반적이었다. 따라서 프로파일링이 요구되는 다양한 환경에서
   직접적으로 사용하기에는 적합하지않았다. 본 논문에서는 이를 해결하기 위해 마이크로 서비스 아키텍처를 적용하여 다양한 환경에서
   서비스가 가능한 데이터 프로파일링 소프트웨어의 개발 결과를 제시한다. 개발된 데이터 프로파일러는 restful API를 통해
   데이터의 메타정보에 대한 요청과 응답을 제공하여 사용하기 쉬운 서비스를 제공한다. 또한, 특정 환경에 종속되지 않고 다양한
   빅데이터 플랫폼이나 데이터 분석 도구들과 원활한 연계가 가능하다는 장점이 있다.
Z8 0
ZR 0
ZA 0
TC 0
ZB 0
ZS 0
Z9 0
U1 0
U2 0
SN 2289-0238
DA 2021-11-25
UT KJD:ART002768989
ER

PT R
AU Damiani, A
   Masciocchi, C
   Lenkowicz, J
   Capocchiano, ND
   Boldrini, L
   Tagliaferri, L
   Cesario, A
   Sergi, P
   Marchetti, A
   Luraschi, A
   Patarnello, S
   Valentini, V
TI Table1_Building an Artificial Intelligence Laboratory Based on Real
   World Data: The Experience of Gemelli Generator.DOCX
SO Figshare
DI https://doi.org/10.3389/fcomp.2021.768266.s001
DT Data set
PD 2021-12-30
PY 2021
AB The problem of transforming Real World Data into Real World Evidence is
   becoming increasingly important in the frameworks of Digital Health and
   Personalized Medicine, especially with the availability of modern
   algorithms of Artificial Intelligence high computing power, and large
   storage facilities.Even where Real World Data are well maintained in a
   hospital data warehouse and are made available for research purposes,
   many aspects need to be addressed to build an effective architecture
   enabling researchers to extract knowledge from data.We describe the
   first year of activity at Gemelli Generator RWD, the challenges we faced
   and the solutions we put in place to build a Real World Data laboratory
   at the service of patients and health researchers. Three classes of
   services are available today: retrospective analysis of existing patient
   data for descriptive and clustering purposes; automation of knowledge
   extraction, ranging from text mining, patient selection for trials, to
   generation of new research hypotheses; and finally the creation of
   Decision Support Systems, with the integration of data from the hospital
   data warehouse, apps, and Internet of Things. Copyright: CC BY 4.0
ZB 0
ZS 0
TC 0
Z8 0
ZR 0
ZA 0
Z9 0
U1 0
U2 0
DA 2024-11-28
UT DRCI:DATA2022008023271401
ER

PT C
AU Lansing, Carina
   Levin, Maxwell
   Sivaraman, Chitra
   Fao, Rebecca
   Driscoll, Frederick
GP IEEE
TI Tsdat: An Open-Source Data Standardization Framework for Marine Energy
   and Beyond
SO OCEANS 2021: SAN DIEGO - PORTO
DT Proceedings Paper
PD 2021
PY 2021
AB Many organizations are tasked with the collection and processing of
   large quantities of data from various measurement devices. Data reported
   from these sources are often not interoperable with datasets and
   software used by analysts and other organizations in the same domain,
   introducing barriers for collaboration on large-scale projects. This
   poses a particular problem for cross-device comparisons and machine
   learning applications, which rely on large quantities of data from
   multiple sources. To address these challenges, the open-source
   Time-Series Data Pipelines (Tsdat) Python framework was developed by
   Pacific Northwest National Laboratory, with strategic guidance and
   direction provided by the National Renewable Energy Laboratory and
   Sandia National Laboratories to facilitate collaboration and accelerate
   advancements in the marine energy domain through the development of an
   open-source ecosystem of tools. This paper will describe the Tsdat
   framework and the data standards within which it operates. A beta
   version of Tsdat has been released and is being used by several projects
   in marine energy, wind energy, and building energy systems.
CT OCEANS Conference
CY SEP 20-23, 2021
CL ELECTR NETWORK
ZB 0
Z8 0
ZS 0
TC 0
ZA 0
ZR 0
Z9 0
U1 0
U2 0
BN 978-0-692-93559-0
DA 2021-01-01
UT WOS:000947273302129
ER

PT J
AU Lee, Mo-se
   Kang, Min-su
   Kim, Hong-joon
   Kim, Jae-hun
TI Real-Time Data Processing Architecture for a Smart Cities
Z1 스마트시티를 위한 실시간 데이터 처리 아키텍처
SO The Journal of Korean Institute of Communications and Information
   Sciences
S1 한국통신학회논문지
VL 46
IS 2
BP 401
EP 409
DI 10.7840/kics.2021.46.2.401
DT research-article
PD 2021
PY 2021
AB Smart City uses IoT-based ICT to solve various urban problems such as
   traffic, environment, and facilities, so that citizens can enjoy a
   convenient and comfortable life. The IoT-based smart city service is
   based on artificial intelligence platform technology that collects,
   stores and processes vast amounts of data generated in a smart city, and
   makes predictions and inferences by machine learning. In this paper, we
   briefly review technologies for real-time big data analysis platform
   development for application to a smart city and propose an architecture
   for real-time big data processing. In order to verify the big data
   processing architecture for a smart city, a TensorFlow-based real-time
   data analysis module and a data visualization module were added to
   construct the overall data flow. The platform proposed in this paper
   enables real-time data processing for vast amounts of data generated in
   a smart city, and the smart city manager can apply the desired machine
   learning model and check the results of prediction and inference through
   the dashboard. It is estimated that the proposed real-time big data
   analysis platform will contribute to solving urban problems.
AK 스마트시티는 교통, 환경, 시설 등의 다양한 도시 문제를 사물인터넷 기반의 정보통신기술을 활용하여 해결함으로써 시민들의 편리하고
   쾌적한 생활을 누릴 수 있도록 한다. IoT 기반의 스마트시티 서비스는 스마트시티에서 발생하는 방대한 양의 데이터를 수집, 저장,
   처리하고 기계학습에 의한 예측 및 추론을 하는 인공지능 플랫폼 기술을 기반으로 한다. 본 논문에서는 스마트시티에 적용하기 위한
   실시간 빅데이터 분석 플랫폼 개발 기술들에 대해간략히 살펴보고 실시간 빅데이터 처리를 위한 아키텍처를 제안하고자 한다.
   스마트시티를 위한 빅데이터 처리 아키텍처에 대한 모의 검증을 진행하기 위해 텐서플로우 기반 실시간 데이터 분석 모듈과 데이터
   시각화 모듈을 추가하여 전체적인 데이터 플로우를 구성하였다. 본 논문에서 제안한 플랫폼을 사용하면 스마트시티에서 발생하는방대한
   데이터들에 대한 실시간 데이터 처리가 가능하며 스마트시티 관리자가 원하는 기계학습 모델을 적용하여대시보드를 통해 예측 및 추론에
   대한 결과를 확인할 수 있다. 이로써 제안한 실시간 빅데이터 분석 플랫폼이 도시 문제 해결에 기여할 수 있을 것으로 사료된다.
ZB 0
ZR 0
Z8 0
ZS 0
ZA 0
TC 0
Z9 0
U1 0
U2 4
SN 1226-4717
DA 2021-06-22
UT KJD:ART002684105
ER

PT B
AU Machmouchi, Hassan
   Desper, Deane
   Sambasivam, Samuel
   Calongne, Cynthia
Z2  
TI [not available]
DT Dissertation/Thesis
PD Jun 22 2023
PY 2023
ZA 0
TC 0
ZB 0
Z8 0
ZR 0
ZS 0
Z9 0
U1 1
U2 3
BN 9798534683547
UT PQDT:64705582
ER

PT J
AU Min, Lee Kyoung
   Lee, Kyung-Hee
   Wan-sup, Cho，
TI Apache NiFi-based ETL Process for Building Data Lakes
Z1 데이터 레이크 구축을 위한 Apache NiFi기반 ETL 프로세스
SO The Korea Journal of BigData
S1 한국빅데이터학회 학회지
VL 6
IS 1
BP 145
EP 151
DI 10.36498/kbigdt.2021.6.1.145
DT research-article
PD 2021
PY 2021
AB In recent years, digital data has been generated in all areas of human
   activity, and there are many attempts to safely store and process the
   data to develop useful services. A data lake refers to a data repository
   that is independent of the source of the data and the analytical
   framework that leverages the data. In this paper, we designed a tool to
   safely store various big data generated by smart cities in a data lake
   and ETL it so that it can be used in services, and a web-based tool
   necessary to use it effectively. Implement.A series of processes (ETLs)
   that quality-check and refine source data, store it safely in a data
   lake, and manage it according to data life cycle policies are often
   significant for costly infrastructure and development and maintenance.It
   is a labor-intensive technology. The mounting technology makes it
   possible to set and execute ETL work monitoring and data life cycle
   management visually and efficiently without specialized knowledge in the
   IT field. Separately, a data quality checklist guide is needed to store
   and use reliable data in the data lake. In addition, it is necessary to
   set and reserve data migration and deletion cycles using the data life
   cycle management tool to reduce data management costs.
AK 최근 들어 인간의 모든 활동 영역에서 디지털 데이터가 생성되고 있고 데이터를 안전하게 저장하고 가공하여 유용한 서비스를 개발하려는
   시도가 많아지고 있다. 데이터 레이크는 데이터의 출처나 데이터를 활용하는 분석 프레임워크에 독립된 데이터 저장소를 말한다. 본
   논문에서는 스마트시티에서 생성되는 다양한빅데이터를 데이터 레이크에 안전하게 저장하고 서비스에서 활용할 수 있게 ETL 하는 도구와
   이를 효과적으로 사용하는데 필요한 웹기반 도구를 설계하고 구현한다.원천 데이터를 품질검사하고 정제하여 데이터 레이크에 안전하게
   저장한 다음 데이터 수명주기 정책에따라 관리하는 일련의 과정(ETL)은 대부분 비용이 많이 드는 인프라와 개발 및 유지 관리에
   상당한 노력이 필요한 기술이다. 구현기술을 통해 IT분야 전문지식이 없어도 가시적이고 효율적으로 ETL 작업 모니터링, 데이터
   수명주기 관리 설정과 실행이 가능하다. 이와는 별개로 데이터 레이크에 신뢰할 수 있는 데이터를 저장하고 사용하려면 데이터 품질검사
   리스트 가이드가 필요하다. 또한, 데이터 수명주기 관리 도구를 통해 데이터 마이그레이션 및 삭제 주기를 설정하고 예약하여 데이터
   관리 비용을 줄일 수 있어야 한다.
TC 0
ZB 0
ZR 0
ZA 0
Z8 0
ZS 0
Z9 0
U1 0
U2 1
SN 2508-1829
DA 2022-03-25
UT KJD:ART002772456
ER

PT J
AU Rae, Cha Byung
   Park, Sun
   HYUN, SEO JAE
   Kim, Jong Won
   Byeong-Chun, Shin，
TI Design and Utilization of Connected Data Architecture-based AI Serviceof
   Mass Distributed Abyss Storage
Z1 대용량 분산 Abyss 스토리지의 CDA (Connected Data Architecture) 기반 AI 서비스의 설계 및 활용
SO Smart Media Journal
S1 스마트미디어저널
VL 10
IS 1
BP 99
EP 107
DI 10.30693/smj.2021.10.1.99
DT research-article
PD 2021
PY 2021
AB In addition to the 4th Industrial Revolution and Industry 4.0, the
   recent megatrends in the ICT field are Big-data, IoT, Cloud Computing,
   and Artificial Intelligence. Therefore, rapid digital transformation
   according to the convergence of various industrial areas and ICT fields
   is an ongoing trend that is  due to the development of technology of AI
   services suitable for the era of the 4th industrial revolution and the
   development of subdivided technologies such as (Business Intelligence),
   IA (Intelligent Analytics, BI + AI), AIoT (Artificial Intelligence of
   Things), AIOPS (Artificial Intelligence for IT Operations), and RPA 2.0
   (Robotic Process Automation + AI). This study aims to integrate and
   advance various machine learning services of infrastructure-side GPU,
   CDA (Connected Data Architecture) framework, and AI based on mass
   distributed Abyss storage in accordance with these technical situations.
   Also, we want to utilize AI business revenue model in  various
   industries.
AK 4차 산업혁명, Industry 4.0 과 더불어 최근 ICT 분야의 메가트렌드는 빅데이터, IoT, 클라우드 컴퓨팅, 그리고
   인공지능이라고 할 수 있다. 따라서, 4차 산업혁명 시대에 알맞은 AI 서비스들의 기술 개발과 다양한 산업 영역에서 ICT 분야의
   융합에 따른 BI (Business Intelligence), IA (Intelligent Analytics, BI + AI),
   AIoT (Artificial Intelligence of Things), AIOPS (Artificial Intelligence
   for IT Operations), RPA 2.0 (Robotic Process Automation + AI) 등의 세분화된 기술
   발전으로 급속한 디지털 전환 (Digital Transformation)이 진행되고 있는 추세이다. 본 연구에서는 이러한 기술적
   상황에 따른 대용량 분산 Abyss 스토리지 기반으로 인프라 측면의 GPU, CDA (Connected Data
   Architecture) 프레임워크, 그리고 AI의 다양한 머신러닝 서비스들을 통합 및 고도화를 목표로 하며, AI 비즈니스의
   수익 모델을 다양한 산업 영역에 활용하고자 한다.
TC 0
ZA 0
ZB 0
ZR 0
Z8 0
ZS 0
Z9 0
U1 0
U2 0
SN 2287-1322
DA 2021-06-22
UT KJD:ART002696794
ER

PT C
AU Raje, Satyajeet
   Kervin, Karina
   Issaie, Nergal
   Channapatna, Madhu
GP Assoc Advancement Artificial Intelligence
TI Accelerating Data Discovery with an Ontology-driven Tool for an
   Enterprise-scale Data Lake Environment
SO THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD
   CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE
   ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
SE AAAI Conference on Artificial Intelligence
VL 35
BP 16100
EP 16102
DT Proceedings Paper
PD 2021
PY 2021
AB A large overhead in the analytics process is the time required to find
   relevant data. We present an ontology-driven data discovery application,
   implemented over IBM's Cognitive Enterprise Data Platform (CEDP). CEDP
   contains a large collection of heterogeneous data assets from
   enterprise-wide data sources. The application accelerates the time
   required for data consumers to search and find data relevant for their
   analytics applications.
CT 35th AAAI Conference on Artificial Intelligence / 33rd Conference on
   Innovative Applications of Artificial Intelligence / 11th Symposium on
   Educational Advances in Artificial Intelligence
CY FEB 02-09, 2021
CL ELECTR NETWORK
SP Assoc Advancement Artificial Intelligence
OI Raje, Satyajeet/0000-0002-6983-2054
ZS 0
ZR 0
ZA 0
ZB 0
TC 0
Z8 0
Z9 0
U1 0
U2 3
SN 2159-5399
EI 2374-3468
BN 978-1-57735-866-4
DA 2021-09-20
UT WOS:000681269807225
ER

PT B
AU Rao, T. Ramalingeswara
   Mitra, Pabitra
   Goswami, Adrijit
BE Ravi, V
   Cherukuri, AK
TI The role of data lake in big data analytics: recent developments and
   challenges
SO HANDBOOK OF BIG DATA ANALYTICS, VOL. 1: Methodologies
SE IET COMPUTING SERIES
VL 37
BP 105
EP 123
DT Article; Book Chapter
PD 2021
PY 2021
RI Rao, T/AAE-9233-2020; GOSWAMI, ADRIJIT/AAM-8742-2021
ZB 0
TC 0
ZR 0
Z8 0
ZS 0
ZA 0
Z9 0
U1 0
U2 2
BN 978-1-83953-058-6; 978-1-83953-064-7
DA 2022-02-20
UT WOS:000752483600007
ER

PT R
AU Rosa, Reinaldo R
TI Data Science Strategies for Multimessenger Astronomy
SO Figshare
DI http://dx.doi.org/10.6084/m9.figshare.14275538.v1
DT Data set
PD 2021-04-05
PY 2021
AB Abstract This article aims to identify and suggest data science
   strategies to strengthen scientific research in astronomy. The
   improvements in data workflow performance that can be provided by these
   strategies can be crucial to the multimessenger astronomy (MMA). A
   special focus is given to the treatment of raw data in the context of
   big data networks for BRICS astronomy initiatives. A preliminary design
   of a prototype that incorporates an MMA data cube into a data lake
   system is presented.
Z8 0
ZR 0
ZS 0
ZB 0
TC 0
ZA 0
Z9 0
U1 0
U2 0
DA 2024-11-21
UT DRCI:DATA2021075021465185
ER

PT C
AU Scholly, Etienne
   Favre, Cecile
   Ferey, Eric
   Loudcher, Sabine
BE Filipe, J
   Smialek, M
   Brodsky, A
   Hammoudi, S
TI HOUDAL: A Data Lake Implemented for Public Housing
SO PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON ENTERPRISE
   INFORMATION SYSTEMS (ICEIS 2021), VOL 1
BP 39
EP 50
DI 10.5220/0010418200390050
DT Proceedings Paper
PD 2021
PY 2021
AB Like all areas of economic activity, public housing is impacted by the
   rise of big data. While Business Intelligence and Data Science analyses
   are more or less mastered by social landlords, combining them inside a
   shared environment is still a challenge. Moreover, processing big data,
   such as geographical open data that sometimes exceed the capacity of
   traditional tools, raises a second issue. To face these problems, we
   propose to use a data lake, a system in which data of any type can be
   stored and from which various analyses can be performed. In this paper,
   we present a real use case on public housing that fueled our motivation
   to introduce a data lake. We also propose a data lake framework that is
   versatile enough to meet the challenges induced by the use case.
   Finally, we present HOUDAL, an implementation of a data lake based on
   our framework, which is operational and used by a social landlord.
CT 23rd International Conference on Enterprise Information Systems (ICEIS)
CY APR 26-28, 2021
CL ELECTR NETWORK
SP INSTICC
ZS 0
ZB 0
Z8 0
TC 0
ZA 0
ZR 0
Z9 0
U1 1
U2 4
BN 978-989-758-509-8
DA 2022-04-29
UT WOS:000783390600003
ER

PT C
AU Schroeer, Christoph
   Frischkorn, Jonas
BE Ahlemann, F
   Schutte, R
   Stieglitz, S
TI Decentralized and Microservice-Oriented Data Integration for External
   Data Sources
SO INNOVATION THROUGH INFORMATION SYSTEMS, VOL III: A COLLECTION OF LATEST
   RESEARCH ON MANAGEMENT ISSUES
SE Lecture Notes in Information Systems and Organization
VL 48
BP 55
EP 60
DI 10.1007/978-3-030-86800-0_4
DT Proceedings Paper
PD 2021
PY 2021
AB Data lakes offer good opportunities to centrally use heterogeneous data
   for analytical questions in companies. However, there are also
   challenges and risks regarding missing reference architectures,
   accessibility or usability. By using modern architecture patterns such
   as microservices, data can alternatively be managed in a technically and
   organizationally decentralized manner. Easily accessible interfaces and
   microservice architecture patterns can maintain important data lake
   characteristics, such as accessibility and the provision of metadata.
   Thus, costs can be saved, data can be held accountable in the respective
   domains, and at the same time interfaces for analytical questions can be
   provided. The paper illustrates the idea in the form of a
   work-in-progress paper using the integration of external data sources as
   an example.
CT 16th International Conference on Business and Information Systems
   Engineering (WI)
CY MAR 09-11, 2021
CL Univ Duisburg Essen, GERMANY
HO Univ Duisburg Essen
ZA 0
ZS 0
ZR 0
TC 0
Z8 0
ZB 0
Z9 0
U1 0
U2 4
SN 2195-4968
EI 2195-4976
BN 978-3-030-86800-0; 978-3-030-86799-7
DA 2022-08-27
UT WOS:000841507100005
ER

PT C
AU Suleykin, Alexander
   Bobkova, Anna
   Panfilov, Peter
   Chumakov, Ilya
GP IEEE
TI Efficient Data Exchange Between Typical Data Lake and DWH Corporate
   Systems
SO INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER AND ENERGY TECHNOLOGIES
   (ICECET 2021)
BP 1999
EP 2004
DI 10.1109/ICECET52533.2021.9698468
DT Proceedings Paper
PD 2021
PY 2021
AB In the last five years, many companies around the world have been
   successfully implemented Apache Hadoop as a main Data Lake storage for
   all data presented in the organization. At the same time, the adoption
   of other Open-Source technologies has been also increasing for years,
   such as classical MPP-based systems for Analytical workloads. Thus, the
   question of efficient and fast data integration between Apache Hadoop
   and other organizational data storage systems is highly important for
   enterprises, where business and decision makers need the minimum delay
   of big heterogeneous data exchange between Hadoop and other storages. In
   this paper, we compare different options for loading data from Apache
   Hadoop, representing the Data Lake of organization, into Open-Source MPP
   Greenplum database with the role of classical data warehouse for
   analytical workloads, and choose the best one. Also, we identify
   potential risks of using different data loading methods.
CT IEEE International Conference on Electrical, Computer, and Energy
   Technologies (ICECET)
CY DEC 09-10, 2021
CL Cape Town, SOUTH AFRICA
SP Aksaray Univ; IEEE; Univ Johannesburg
RI Panfilov, Peter/AAJ-8308-2021; Suleykin, Alexander/AAC-6050-2022
OI Panfilov, Peter/0000-0001-6567-6309; Suleykin,
   Alexander/0000-0003-2294-6449
ZA 0
ZS 0
TC 0
ZR 0
Z8 0
ZB 0
Z9 0
U1 0
U2 4
BN 978-1-6654-4231-2
DA 2022-07-10
UT WOS:000814669100344
ER

PT C
AU Zhang, Shujuan
   Zhang, Lixia
   Gao, Huisheng
   Li, Yang
   Wang, Meili
BE Li, Z
   Cen, F
TI Operation Management Architecture of Power Communication Backbone
   Transmission Network Based on Big Data
SO INTERNATIONAL CONFERENCE ON SMART TRANSPORTATION AND CITY ENGINEERING
   2021
SE Proceedings of SPIE
VL 12050
AR 1205010
DI 10.1117/12.2614374
DT Proceedings Paper
PD 2021
PY 2021
AB The backbone transmission network of power communication provides
   important support for service network and plays a decisive role in
   improving the quality of power communication service. Firstly, the paper
   analyzes the technical characteristics of the backbone transmission
   network of power communication, and gives the general operation and
   management framework; then, the big data characteristics of basic data
   in the operation and management process of power communication backbone
   transmission network are described, and the typical big data
   architecture is given; finally, combined with the general operation
   management framework of power communication and typical big data
   architecture, the operation management architecture of power
   communication backbone transmission network is designed. The research
   results have reference value for the application of big data and machine
   learning technology in the operation and management of power
   communication backbone transmission network.
CT International Conference on Smart Transportation and City Engineering
CY OCT 26-28, 2021
CL Chongqing, PEOPLES R CHINA
SP China Merchants Chongqing Commun Technol Res & Design Inst Co Ltd;
   Chongqing Univ; Chongqing Jiaotong Univ; AEIC Acad Exchange Informat Ctr
ZR 0
Z8 0
ZS 0
TC 0
ZB 0
ZA 0
Z9 0
U1 0
U2 12
SN 0277-786X
EI 1996-756X
BN 978-1-5106-4976-7; 978-1-5106-4975-0
DA 2022-05-19
UT WOS:000792686000035
ER

PT P
AU CHEN Z
   XIN S
TI System for managing big data support for            city-level data
   center, has middleware that provides            distributed services for
   applications of city operation            and maintenance center in
   city-level data            platform
PN CN112148718-A
AE INESA INTELLIGENT CO LTD
AB 
   NOVELTY - The system has that big data basic components               
   includes an analytical database to construct a               
   district-level data lake, a basic library and a                subject
   library in Taichung in a city-level data                and performs
   batch analysis and storage of                structured data. A
   distributed database stores and                queries semi-structured
   data, unstructured data and                large object data. A
   comprehensive search engine                indexes a massive data stored
   in district-level                data lakes, basic libraries, and
   subject libraries                and serves as a search engine for
   various                departments to search and query a bottom layer. A
   big data development tool (2) is connected to the                big
   data basic components in which the big data                development
   tool includes the resource management                tools for unified
   control and management of                computing resources, storage
   resources, and data                access resources in the big data
   basic components.                A middleware provides distributed
   services for                applications of a city operation/maintenance
   center                in a city-level data platform.
   USE - System for managing big data support for                city-level
   data center.
   ADVANTAGE - The system realizes unified control and               
   management on computing resources, storage                resources and
   data access resources.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a big  
   data support management system. (Drawing includes               
   non-English language text)1Big data basic component2Big data development
   tool11Analytical database12Distributed database14Comprehensive search
   engine21Resource management tool24Middleware
Z9 0
U1 0
U2 0
DA 2021-01-21
UT DIIDW:202103767X
ER

PT P
AU SUN S
   LAI K
   LI Q
   SI H
TI Method for processing multi-source data, involves            collecting
   data of data sources, converting collected            data, implementing
   classification and caching of data            stream, consuming data
   stream and transferring obtained            data stream to blockchain
PN CN112100265-A
AE BOYA CHAIN BEIJING TECHNOLOGY CO LTD; NANJING BOYA BLOCKCHAIN RES INST
   CO LTD; UNIV PEKING
AB 
   NOVELTY - The method involves collecting (S101) data                from
   multiple data sources, and converting the                collected data
   into a data stream with a unified                format. The
   classification and caching of the data                stream is
   implemented (S102), and a data stream                output interface is
   provided. The data stream is                obtained (S103) through the
   data stream interface,                and the obtained data stream is
   consumed. The data                stream is obtained (S104) through the
   data stream                output interface, and the obtained data
   stream is                transferred to the blockchain. The multiple
   data                sources are comprised with a relational database    
              and a non-relational database.
   USE - Method for processing multi-source data of big                data
   architecture and blockchain e.g. pre-arranged                private
   chain, consortium chain or public chain                (all claimed).
   ADVANTAGE - The method enables realizing data acquisition               
   of different data sources and converting the                collected
   data into the data stream with uniform                format so as to
   facilitate various data query and                fast reading of an
   analysis tool. The method                enables quickly and
   conveniently transferring the                classified stored data
   stream so as to satisfy                chain application requirements.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device    
   for processing multi-source data of big data                architecture
   and blockchain.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                the method for processing multi-source data
   of big                data architecture and blockchain. (Drawing
   includes                non-English language text)101Step for collecting
   data from multiple                data sources, and converting the
   collected data                into data stream with unified
   format102Step for implementing classification and                caching
   of the data stream and providing data                stream output
   interface103Step for obtaining data stream through                data
   stream interface and consuming obtained data               
   stream104Step for obtaining data stream through                data
   stream output interface, and transferring                obtained data
   stream to blockchain
Z9 0
U1 0
U2 0
DA 2021-01-13
UT DIIDW:2020C8907M
ER

PT P
AU DI S
TI Integration method of heterogeneous data source,            involves
   determining data and time stamp of write            request, and
   combining data written into specific file            with operation mark
   and timestamp for data merging to            obtain final result data
PN CN111966750-A; CN111966750-B
AE BEIJING HAIZHIWANGJU INFORMATION TECHNOL; BEIJING HAIZHI TECHNOLOGY
   GROUP CO LTD
AB 
   NOVELTY - The method involves determining (S1) the data               
   and the time stamp of the write request according                to the
   information of the user calling data                interface, the
   operation mark, where the operation                mark comprises three
   types appending, updating and                deleting three types, the
   timestamp is the time                when the write request is reached,
   and the above                information is appended to a specific file
   in the                data lake. The data written into a specific file
   is                combined (S2) with the operation mark and timestamp   
           for data merging to obtain the final result                data.
   USE - Integration method of heterogeneous data                source
   based on data lake.
   ADVANTAGE - The method solves the problems that the               
   existing data lake data integration technology                cannot
   support the data updating operation, cannot                keep the data
   of the data lake consistent with the                original data, and
   cannot effectively solve the                problems of low query
   performance and caused by                large files of the big data
   cluster. The                heterogeneous data synchronization device
   provided                by the invention allows a user to set data      
   synchronization of different data sources in an                interface
   interaction configuration, support                incremental
   synchronization, replace traditional                code configuration
   for data synchronization, and                reduce the threshold of
   user integrated data.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
       heterogeneous data source integration device.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating
   of                the integration method of heterogeneous data          
   source. (Drawing includes non-English language               
   text)S1Step for determining the data and the time                stamp
   of the write request according to the                information of the
   user calling data                interfaceS2Step for combining the data
   written into a                specific file with the operation mark and
   timestamp                for data merging to obtain the final result    
              data
Z9 0
U1 0
U2 0
DA 2020-12-17
UT DIIDW:2020B8044G
ER

PT P
AU VENKATESAN M
   BHAIYA A
   SHANKAR M
TI Artificial Intelligence (AI)-based automatic data            remediation
   system for remediating data for big data            has processor that
   identifies anomalous data points in            dataset that correspond
   to anomalies based on anomaly            type detected in dataset
PN US2020349169-A1; IN202014018379-A; US11093519-B2; IN531450-B
AE ACCENTURE GLOBAL SOLUTIONS LTD
AB 
   NOVELTY - The data remediation system (100) has at least               
   one processor, a non-transitory processor readable                medium
   that stores machine-readable instructions                causing
   processor to determine whether a dataset is                to be treated
   for one or more anomalies including                missing values and
   outliers in dataset via                executing a statistical check on
   dataset. The                processor identifies portions of dataset
   that                include the anomalies based on statistical check,   
   classifies anomalies into one of the anomaly types               
   including point, contextual and time trend                anomalies
   based on number of attributes of the                dataset. The
   processor identifies anomalous data                points in dataset
   that correspond to anomalies                based on anomaly type
   detected in dataset, obtains                expected values for
   anomalous data points by                employing autoencoders,
   generates a transformed                dataset from the dataset by
   replacing data points                corresponding to anomalies in
   dataset with the                expected values, and uploads transformed
   dataset to                data lake (150).
   USE - AI-based automatic data remediation system for               
   remediating data for big data.
   ADVANTAGE - The high quality data can automatically lead               
   to improved results since quality of results                produced
   from the dataset depend on the integrity                of dataset. The
   transformation of dataset with                anomalies remedied affords
   a technical improvement                in the functioning of computer
   system. The problems                with data quality issues are
   efficiently addressed                via automatic data remediation
   which capitalizes on                the improvements in the AI and
   machine learning                (ML) techniques.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:an AI-based data remediation method; anda non-transitory
   processor-readable storage                medium.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   AI-based automatic data remediation system.100Data remediation
   system108Data remediation engine150Data lake156User input170Data store
Z9 0
U1 0
U2 0
DA 2020-11-19
UT DIIDW:2020A73130
ER

PT P
AU DU Z
   LI S
   WU M
TI Heterogeneous data analysis method based on template filtering comprises
   decoding and coding original data to obtain decoding coding data ,
   obtaining converted data and performing index calculation to conversion
   data to obtain target data
PN CN111814000-A
AE NEUSOFT GROUP SHANGHAI CO LTD; NEUSOFT CORP
AB 
   NOVELTY - Heterogeneous data analysis method based on template filtering
   comprises decoding and coding the original data to obtain the decoding
   coding data, performing data cleaning to the decoding coding data to
   obtain the cleaning data, sending the cleaning data into the data lake,
   performing attribute extraction to the cleaning data in the data lake,
   performing the type conversion to the extracted cleaning data, obtaining
   the converted data, and performing index calculation to the conversion
   data to obtain the target data.
   USE - Used as heterogeneous data analysis method based on template
   filtering.
   ADVANTAGE - The method can be utilizes for any source, any type of data
   through template plugging and unplugging, separates the query data from
   the production data by flexibly combining the final result, reduces the
   pressure of the production system, and reduces the invasion of the
   system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for
   heterogeneous data analysis system based on template filtering.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating
   the heterogeneous data analysis method based on template filtering
   (Drawing includes non-English language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2020A4930R
ER

PT P
AU WANG F
   CHEN L
TI Processing data integration method for            heterogeneous data
   source comprises e.g. abstract            mapping of data sources in
   multiple heterogeneous            databases and performing data
   management on the changed            data and storing in integrated data
   lake
PN CN111767332-A; CN111767332-B
AE SHANGHAI SENYI MEDICAL TECHNOLOGY CO LTD
AB 
   NOVELTY - Processing data integration method for               
   heterogeneous data source comprises abstract                mapping of
   data sources in multiple heterogeneous                databases to
   obtain metadata of each meta-model                under the mapping
   relationship, where each of the                meta-models corresponds
   to data source, copying                each heterogeneous database to
   the replicate                database, and setting up change capture on
   the                replicated database to obtain change table that      
   records the change data in each heterogeneous                database,
   and converting the read change data in                each heterogeneous
   database into a data format that                is unified with the
   metadata, and performing data                management on the changed
   data and the metadata                that have been converted in a
   unified data format,                and storing in an integrated data
   lake. The data                structure supported by the replication
   database                includes DB2, Oracle(RTM: Object-relational     
   database management system (ORDBMS)), Sqlserver                and/or
   Mysql(RTM: Relational database management                system)
   database.
   USE - The method is useful for processing data               
   integration method for heterogeneous data                source.
   ADVANTAGE - The method: realizes data integration,               
   sharing, and establishing data standards; is                convenient
   for subsequent data applications; and                has good
   scalability.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included             
   for:data integration system with heterogeneous                data
   sources; anddata integration terminal for heterogeneous               
   data source.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for processing data integration method                for
   heterogeneous data source (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2020-11-09
UT DIIDW:2020A2153C
ER

PT P
AU WU Q
TI Method for realizing data convergence of            heterogeneous data
   source in data lake environment            comprises searching different
   data by searching            sub-module inside inquiring processing
   module, and            displaying data
PN CN111737326-A; CN111737326-B
AE IREADYIT BEIJING CO LTD
AB 
   NOVELTY - Method for realizing data convergence of               
   heterogeneous data source in data lake environment               
   comprises searching different data by the searching               
   sub-module inside the inquiring processing module,               
   operating the operator by the terminal system                module in
   the inquiring processing module,                displaying the data by
   the display screen module in                the inquiring processing
   module, providing multiple                communication modes and
   communication services by                coordination controller in the
   communication                processor module, driving the transceiver
   unit of                different transmission mediums by the transceiver
   unit module in the communication processor module,               
   displaying the function by the display module in                the
   application module, starting and guiding the                correct user
   by the guide module in the application                module, performing
   the interface operation                effectively, giving feedback
   information in time                when the error operation of the user
   has tolerance                or remedial measures by the feedback module
   in the                application module.
   USE - The method is useful for realizing data                convergence
   of heterogeneous data source in data                lake environment.
   ADVANTAGE - The method realizes the complexity and               
   feasibility of data access, integration and sharing                based
   on the existing theory and technology, and                overcomes the
   problem of more convenient                convergence of heterogeneous
   database.
Z9 0
U1 0
U2 0
DA 2020-10-24
UT DIIDW:202099545D
ER

PT P
AU XU Y
TI Method for constructing water conservancy and hydropower big data
   architecture, involves storing sorted out effective data in hard disk,
   classifying data stored hard disk, and modifying each node of big data
   architecture construction model
PN CN111723055-A
AE XU Y
AB 
   NOVELTY - The method involves presetting a big data architecture
   construction model, and presetting each node of the big data
   architecture construction model. A sorted out effective data is stored
   in the hard disk, and the effective data stored is classified in the
   hard disk. A classified effective data is extracted, the keywords of
   each classified data are extracted, and a folder for naming through the
   keywords of the effective data is created. A named data classification
   is printed on paper, and the printed paper is pasted on each node of the
   big data architecture construction model. A valid data is updated, the
   updated data is extracted, and each node of the big data architecture
   construction model is modified through the updated data.
   USE - Method for constructing water conservancy and hydropower big data
   architecture.
   ADVANTAGE - The method is convenient for the worker to understand,
   effectively guides the work of the worker, improves water conservancy,
   and hydropower engineering construction performance, and quality control
   level.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202098501H
ER

PT P
AU ABDULLAH-AL M M
TI Computer system for processing big data stored in            data lake,
   has computer which generates first            presentation information
   for presenting recommend            command, and executes process on
   basis of recommend            command
PN WO2020183538-A1; JP2021504623-X
AE HITACHI LTD
AB 
   NOVELTY - The system has a memory (112) that is                connected
   to a processor (111). A computer manages                history
   information. The history information                storing information
   pertaining to a history command                is a command used for the
   process in which                data-lake (130) is utilized. A candidate
   command is                specified by referring to the history
   information                on the basis of the filtering condition when
   a                recommend request that includes a filtering            
   condition is received from a user. The degree of               
   importance of the candidate command is calculated.                The
   recommend command to be presented to the user                is selected
   on the basis of the degree of                importance. The first
   presentation information for                presenting the recommend
   command is generated. The                process is executed on the
   basis of the recommend                command, when an execution request
   for the                recommend command is received.
   USE - Computer system for processing big data stored                in
   data lake.
   ADVANTAGE - The recommendation technique for commands for               
   implementing real-time analysis can be provided.                The
   relationship between the data can be presented                as visual
   information to the user, by applying tag                to the column of
   data blocks stored in the data.                Thus, the user can easily
   and quickly specify the                data to be analyzed. The user can
   grasp the                structure and contents of the data as visual   
   information, and can reduce the time required for               
   generating command since the relationship between                the
   data can be grasped. Since the appropriate                command can be
   recommended for each user, the time                required for analysis
   can be shortened. Thus, the                real-time analysis can be
   realized. The high degree                of analysis can be realized
   because recommendation                of an appropriate command, even to
   a new user or a                beginner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   processing method executed by computer system for               
   processing big data stored in data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   computer system for processing big data stored in                data
   lake. (Drawing includes non-English language                text)Data
   lake management server (100)Database server (101)Processor (111)Memory
   (112)Data-lake (130)
Z9 0
U1 0
U2 0
DA 2020-09-30
UT DIIDW:2020911780
ER

PT P
AU WU Q
   WANG Y
   WANG M
   GAO Z
TI Implementing heterogeneous data management in data            lake
   environment comprises dividing data lake into            original data
   pools for storing original data, and            uploading heterogeneous
   data collected from each system            to original data pool
PN CN111666263-A
AE IREADYIT BEIJING CO LTD
AB 
   NOVELTY - Implementing heterogeneous data management in               
   data lake environment comprises (i) dividing the                data
   lake into original data pools for storing                original data,
   where the original data pool                comprises a simulated data
   pool for storing                monitoring data, storing temporary data
   generated                when the application is executed by using      
   application data pool, (ii) uploading heterogeneous                data
   collected from each system to original data                pool,
   classifying the uploaded heterogeneous data                information
   via classification procedures by using                original data
   pool, (iii) transmitting classified                heterogeneous data
   information to simulation data                pool, application data
   pool, object data pool, and                document data pool for
   storage by using original                data pool, (iv) connecting data
   transfer port in                simulated data pool application data
   pool object                data pool document data pool with the data
   search                port of data lake system, and searching           
   heterogeneous data information in data lake via                data
   search port of data lake system.
   USE - The method is useful for implementing                heterogeneous
   data management in data lake                environment.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method for implementing heterogeneous data                management
   in data lake environment (Drawing                includes non-English
   language text).
Z9 0
U1 0
U2 0
DA 2020-10-05
UT DIIDW:2020923942
ER

PT P
AU WU Q
   WANG Y
   WANG M
   GAO Z
TI Accessing uniform data in heterogeneous data            storage
   environment of data lake comprises dividing            data lake into
   simulation data pool for storing            monitoring data, and
   converting access request into            data request script of
   predetermined format
PN CN111666283-A
AE IREADYIT BEIJING CO LTD
AB 
   NOVELTY - Accessing uniform data in heterogeneous data               
   storage environment of data lake comprises (i)                dividing
   the data lake into simulation data pool                for storing
   monitoring data, using application data                pool to store
   temporary data generated, (ii)                providing access request
   to the classification                program when the data lake accesses
   the database to                be accessed by the data lake, (iii)
   judging the                home operation for access request of the data
   lake,                and converting the access request into data request
   script of the predetermined format through the data                unit
   belonging to the home, (iv) sending the                converted data
   request script to the to-be accessed                data base, analyzing
   the request information of                data request script, searching
   the corresponding                requirement data in data base to be
   accessed, and                (v) transmitting the demand data to data
   unit,                sending the data request script by the data base to
   be accessed, and transmitting the demand data to                the data
   lake providing the access request by the                data unit.
   USE - The method is useful for accessing uniform                data in
   heterogeneous data storage environment of                data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram for       
   accessing uniform data in heterogeneous data                storage
   environment of data lake (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2020-10-05
UT DIIDW:202092393H
ER

PT J
AU Canbay, Yavuz
   Vural, Yilmaz
   Sagiroglu, Seref
TI Conceptual Model Suggestions for Privacy Preserving Big Data Publishing
SO JOURNAL OF POLYTECHNIC-POLITEKNIK DERGISI
VL 23
IS 3
BP 785
EP 798
DT Article
PD SEP 2020
PY 2020
AB Recent developments in IT has increased the speed of data production and
   processing, as a result, big data concept with components such as
   volume, velocity, variety and value has emerged. In order to get more
   benefit from big data, it is necessary to share or publish the data by
   preserving or respecting privacy. The literature reviews report that
   there is no model that facilitates publishing big data by preserving
   privacy. Designing Privacy Preserving Big Data Publishing (PPBDP) models
   is important to direct all the parties and to meet the requirements of
   them correctly, and to create the right infrastructures and services. In
   addition, it is necessary to consider some factors such as cost and
   security when designing these models.
   In this study, privacy preserving data publishing models were reviewed,
   compared based on various criteria and then evaluated based on privacy
   risk levels. Finally, big data architecture based new conceptual models
   were then established for the first time according to these evaluations
   and privacy risk levels. It is expected that the proposed models might
   contribute to the literature on some issues, such as publishing big data
   with preserving privacy, minimizing privacy risks and obtaining maximum
   benefit from the big data.
RI Canbay, Yavuz/AAH-3353-2019; sagiroglu, seref/AAJ-5739-2020
ZA 0
ZR 0
ZS 0
ZB 0
Z8 0
TC 0
Z9 0
U1 0
U2 6
SN 1302-0900
EI 2147-9429
DA 2020-07-14
UT WOS:000545278700022
ER

PT P
AU HUA S
   CAO Z
TI Method for implementing system data structure            based on
   metadata and data analysis technology in big            data management,
   involves analyzing data ability, data            panorama and data
   popularity, and analyzing metadata            link
PN CN111611458-A; CN111611458-B
AE PRIMETON INFORMATION TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves entering system                information
   and recording the basic situation of                the system. The
   technical metadata is collected                through standard metadata
   collection tools. The                data flow is collected in the
   system by simulating                the business scenario of the system,
   and business                metadata is collected from the business
   scenario of                the system as the source. The metadata link
   is                analyzed through the association of business          
   metadata. The business meaning of business metadata                and
   technical metadata are identified. The data                ability, data
   panorama and data popularity are                analyzed through the
   analysis strategy of                clustering, summarizing and
   statistically sorting                metadata of different dimensions.
   USE - Method for implementing system data structure                based
   on metadata and data analysis technology in                big data
   management.
   ADVANTAGE - The authenticity and effectiveness of the data              
   are guaranteed with higher accuracy. The method                provides
   strong support for enterprises in the                field of big data
   governance, and has good                promotion and application value.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating 
   the method for implementing system data                architecture.
   (Drawing includes non-English                language text)
Z9 0
U1 0
U2 0
DA 2020-09-21
UT DIIDW:2020871404
ER

PT P
AU HINRICHS A L
   DIGIOVANNI A E
   MARITZ W S
   SARDELLA A M
TI System for generating subjective wellbeing            analytics score,
   has processor that analyzes document            in bin associated with
   particular subjective wellbeing            dimension to determine score
   for subjective wellbeing            dimension and overall score
PN US2020265115-A1; US11062093-B2
AE TSG TECHNOLOGIES LLC
AB 
   NOVELTY - The system has processor that collects (302)               
   document from document source. The processor                inserts the
   document into a queue for processing,                stores the document
   in a data lake. The processor                performs (306) natural
   language processing on the                document to obtain text from
   the document and                assign the document to subjective
   wellbeing                dimension by comparing the text from the
   document                with a subjective wellbeing dimension filter for
   each subjective wellbeing dimension. The processor               
   inserts (308) the document into bin which is                associated
   with a particular subjective wellbeing                dimension. The
   processor analyzes each document in                each bin associated
   with the particular subjective                wellbeing dimension to
   determine a score for each                subjective wellbeing dimension
   and an overall score                that is based on each score for each
   subjective                wellbeing dimension.
   USE - System for generating subjective wellbeing               
   analytics score based on document such as social                media
   post, blog post, forum post, traditional news                media
   article or academic research paper.
   ADVANTAGE - The system can utilize big data, computing               
   devices, and statistics to better understand the               
   relationship between how people feel about their                quality
   of life and how that feeling manifests in                or impacts real
   world outcomes.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a method for generating subjective wellbeing               
   analytics score; anda non-transitory computer-readable storage          
   medium storing program for generating subjective               
   wellbeing analytics score.
   DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart for         
   generating the subjective wellbeing analytics               
   score.302Step for collecting document from document               
   source304Step for performing processing on the               
   document306Step for performing natural language               
   processing308Step for inserting the document into               
   bin310Step for determining score for subjective                wellbeing
   dimension and overall score
Z9 0
U1 0
U2 0
DA 2020-08-31
UT DIIDW:202079835D
ER

PT P
AU PANDARI K
   RAMASUBRAMANIAN A
   BALASUBRAMANIAN V A
TI Method for ingesting and consuming data utilizing            a trading
   partner manager (TPM) interface, involves            providing the TPM
   interface with multiple selectable            predefined data fabric
   configurations by a user            operating a user computing device
PN US10740324-B1
AE OPTUM INC
AB 
   NOVELTY - The method involves providing the TPM                interface
   by one or more processors of a data                fabric computing
   device. The TPM interface includes                the multiple
   selectable predefined data fabric                configurations by a
   user operating a user computing                device. The multiple
   selectable predefined data                fabric configurations are used
   to define a set of                data fabric configurations. The user
   input                originating from the TPM interface is received     
   (402) by the one or more processors of the data                fabric
   computing device. The user input includes                one or more
   selections of the multiple selectable                predefined data
   fabric configurations that defined                the set of data fabric
   configurations with data                source, a data landing zone, a
   data source type,                and a delivery channel.
   USE - Method for ingesting and consuming data                utilizing a
   TPM interface such as healthcare data                in big data
   platforms that allow easy user                interaction with the data.
   ADVANTAGE - Method allows a user to selects and utilizes               
   the functions of Data fabric computing system                without the
   need for detailed knowledge of the                lower-level database
   management processes                traditionally required to design a
   database                system.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:an apparatus for ingesting and consuming data               
   utilizing a TPM interface; anda non-transitory computer-readable storage
   medium for ingesting and consuming data utilizing a                TPM
   interface.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method  
   for ingesting and consuming data utilizing a TPM               
   interface.402Receive user input includes data fabric               
   configurations404Receive a data set from a user defined               
   data source into a data landing zone and based on                the
   data fabric configurations406Ingest the received data set from the data 
   landing zone into a data lake based on the data                fabric
   configurations408Enrich the ingested data set based on the              
    data fabric configurations410Consume the ingested data set
Z9 0
U1 0
U2 0
DA 2020-08-24
UT DIIDW:202075281W
ER

PT P
AU LI G
   LI Y
   ZHANG J
   YU G
   XIA L
   LIU J
TI Data lake based large data acquisition and            management quick
   retrieval system, has front end module            for collecting
   heterogeneous data, and data association            metadata extraction
   module extracting metadata of            heterogeneous data
PN CN111460236-A
AE TIANJIN 712 COMMUNICATION & BROADCASTING
AB 
   NOVELTY - The system has a data collecting front end               
   module for collecting multi-source heterogeneous                data,
   where the collected multi-source                heterogeneous data is
   stored in a data resource                pool module. The data resource
   pool module is                provided with a distributed file system, a
   non-relational database and a relational database.                A data
   processing module stores real-time                processing data in the
   data resource pool module. A                data service module is
   provided with a distributed                full-text retrieval database,
   a distributed                analysis database and a distributed memory
   database                three parts. A data service module provides a
   data                service to a user. A data association metadata      
   extraction module extracts metadata of the                multi-source
   heterogeneous data, where the                extracted data is stored in
   the retrieval database                of the data service module.
   USE - Data lake based large data acquisition and               
   management quick retrieval system.
   ADVANTAGE - The system utilizes large data technology and               
   data processing technology so as to effectively                avoid
   data resource fragmentation, low data                retrieval
   efficiency, data storage resources and                resource waste
   problem.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the
   data                lake based large data acquisition and management    
   quick retrieval system (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2020-08-14
UT DIIDW:202072583U
ER

PT P
AU LI N
   WANG P
   CHEN W
TI Multi-source heterogeneous ecological environment            big data
   processing system based on data lake comprises            ecological
   environment data collection layer to collect            original data of
   ecological environment
PN CN111459908-A
AE INST URBAN ENVIRONMENT CHINESE ACAD SCI
AB 
   NOVELTY - Multi-source heterogeneous ecological               
   environment big data processing system based on                data lake
   comprises an ecological environment data                collection layer
   used to collect original data of                ecological environment,
   where original data                comprises an ecological environment
   data and                ecological environment metadata, A metadata     
   collection module is used to collect ecological               
   environment metadata from multiple sources and                multiple
   structures. A data collection module is                used to collect
   ecological environment data from                multiple sources and
   multiple structures. An                ecological environment data
   cleaning layer is used                to preprocess and standardize data
   acquired by                ecological environment data collection layer.
   An                ecological environment data storage layer is used     
   to classify and hierarchically store data                transmitted by
   ecological environment data cleaning                layer. A data
   classification storage module is used                to store
   standardized data transmitted by                ecological environment
   data cleaning layer.
   USE - Used as multi-source heterogeneous ecological               
   environment big data processing system based on                data
   lake.
   ADVANTAGE - The system improves access and analysis               
   efficiency of environmental data, and reduces                storage
   cost.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
   multi-source heterogeneous ecological environment                large
   data processing method based on data                lake
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the
   architecture of a multi-source heterogeneous                ecological
   environment big data management system                based on a data
   lake
Z9 0
U1 0
U2 0
DA 2020-08-17
UT DIIDW:202072591W
ER

PT J
AU Mykland, Per
TI Collaborative Research: Statistical Inference for High Dimensional and
   High Frequency Data
DT Awarded Grant
PD Jul 01 2020
PY 2020
AB To pursue the promise of the big data revolution, the current project
   will focus on a common form of data, high dimensional high frequency
   data (HDHFD), where a snapshot of the data involves a large number of
   variables, and at the same time new data streams in every fraction of
   milliseconds. With technological advances in data collection, HDHFD
   occurs in medical applications from neuroscience to patient care;
   finance and economics; geosciences such as earthquake data; marine
   science including fishing and shipping; turbulence; internet data; and
   other areas where data streaming is available. The Principal
   Investigators' (PIs') research focuses on how to extract information
   from complex big data and how to turn data into knowledge. In
   particular, the project seeks to develop cutting-edge mathematics and
   statistical methodology to uncover the structure governing HDHFD
   systems. This structure is characterized by a web of dependence across
   both time and dimension, and the role of analysis is to provide guidance
   on how to reduce the complexity while retaining the important features
   of the data architecture. An integral part of this research is also
   about how to quantify the uncertainty in estimates and forecasts in
   HDHFD systems. In addition to developing a general theory, the project
   is concerned with applications to financial data, including risk
   management, forecasting, and portfolio management. More precise
   estimators, with improved margins of error, will be useful in all these
   areas of finance. The results are of interest to main-street investors,
   regulators and policymakers, and the results are entirely in the public
   domain.<br/><br/>The purpose of this project is to explore high
   dimensional high frequency data (HDHFD) from several angles. A
   fundamental approach is to extend the PIs��� contiguity theory. Under a
   contiguous probability, the structure of the observations is often more
   accessible (frequently Gaussian) in local neighborhoods, facilitating
   statistical analysis. This is achieved without altering current models.
   In a contribution to factor modeling of the HDHFD data, the PIs will
   explore time-varying matrix decompositions, including the development of
   a singular value decomposition (SVD) for high frequency data, as a more
   direct path to a factor model. We plan to compare the new SVD with PCA
   based methods, as well as L1 type methods such as nonnegative matrix
   factorization. The PIs have discovered a new way to look at time and
   cross-dimension dependence, originally developed by the PIs in
   connection with their observed asymptotic variance (observed AVAR). They
   will now look into the possibility to "borrow" information across time
   and dimension. This tool will be used for matrix decompositions, as well
   as to develop volatility matrices for the drift part of a financial
   process, which will interface with their planned work on matrix
   decompositions. The PIs will explore a path to an observed AVAR that
   takes place in continuous time, thereby improving accuracy and
   simplifying both implementation and theoretical analysis.<br/><br/>This
   award reflects NSF's statutory mission and has been deemed worthy of
   support through evaluation using the Foundation's intellectual merit and
   broader impacts review criteria.
ZR 0
TC 0
ZS 0
Z8 0
ZB 0
ZA 0
Z9 0
U1 1
U2 1
G1 2015544
DA 2023-12-08
UT GRANTS:15133402
ER

PT J
AU Zhang, Lan
TI Collaborative Research: Statistical Inference for High Dimensional and
   High Frequency Data
DT Awarded Grant
PD Jul 01 2020
PY 2020
AB To pursue the promise of the big data revolution, the current project
   will focus on a common form of data, high dimensional high frequency
   data (HDHFD), where a snapshot of the data involves a large number of
   variables, and at the same time new data streams in every fraction of
   milliseconds. With technological advances in data collection, HDHFD
   occurs in medical applications from neuroscience to patient care;
   finance and economics; geosciences such as earthquake data; marine
   science including fishing and shipping; turbulence; internet data; and
   other areas where data streaming is available. The Principal
   Investigators' (PIs') research focuses on how to extract information
   from complex big data and how to turn data into knowledge. In
   particular, the project seeks to develop cutting-edge mathematics and
   statistical methodology to uncover the structure governing HDHFD
   systems. This structure is characterized by a web of dependence across
   both time and dimension, and the role of analysis is to provide guidance
   on how to reduce the complexity while retaining the important features
   of the data architecture. An integral part of this research is also
   about how to quantify the uncertainty in estimates and forecasts in
   HDHFD systems. In addition to developing a general theory, the project
   is concerned with applications to financial data, including risk
   management, forecasting, and portfolio management. More precise
   estimators, with improved margins of error, will be useful in all these
   areas of finance. The results are of interest to main-street investors,
   regulators and policymakers, and the results are entirely in the public
   domain. <br/><br/>The purpose of this project is to explore high
   dimensional high frequency data (HDHFD) from several angles. A
   fundamental approach is to extend the PIs��� contiguity theory. Under a
   contiguous probability, the structure of the observations is often more
   accessible (frequently Gaussian) in local neighborhoods, facilitating
   statistical analysis. This is achieved without altering current models.
   In a contribution to factor modeling of the HDHFD data, the PIs will
   explore time-varying matrix decompositions, including the development of
   a singular value decomposition (SVD) for high frequency data, as a more
   direct path to a factor model. We plan to compare the new SVD with PCA
   based methods, as well as L1 type methods such as nonnegative matrix
   factorization. The PIs have discovered a new way to look at time and
   cross-dimension dependence, originally developed by the PIs in
   connection with their observed asymptotic variance (observed AVAR). They
   will now look into the possibility to "borrow" information across time
   and dimension. This tool will be used for matrix decompositions, as well
   as to develop volatility matrices for the drift part of a financial
   process, which will interface with their planned work on matrix
   decompositions. The PIs will explore a path to an observed AVAR that
   takes place in continuous time, thereby improving accuracy and
   simplifying both implementation and theoretical analysis.<br/><br/>This
   award reflects NSF's statutory mission and has been deemed worthy of
   support through evaluation using the Foundation's intellectual merit and
   broader impacts review criteria.
ZA 0
ZS 0
TC 0
Z8 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
G1 2015530
DA 2023-12-08
UT GRANTS:15133406
ER

PT P
AU ZHENG W
   WANG H
   TAY W R
   WANG H Q
TI Semiconductor processing dispatching comprises            generating
   current load dispatching processing, using            big data
   architecture to adjust weighting factor, and            generating next
   load dispatching processing period            based on key performance
   indicator
PN CN111356988-A; CN111356988-B; US2021263505-A1; WO2021163986-A1;
   TW202133097-A; US11385627-B2; TW788650-B1
AE YANGTZE MEMORY TECHNOLOGIES CO LTD
AB 
   NOVELTY - Semiconductor processing dispatching comprises               
   creating a load dispatching data mode that                comprises
   facility data for product batches to-be                dispatched to
   multiple work stations, using load                balancing model and
   generating load dispatching                profile based on load
   scheduling data mode, where                the load balancing model
   comprises at least one                target functions and at least one
   weighting factor                in target function, generating a current
   load                dispatching processing based on load dispatch       
   profile, obtaining current key performance                indicator
   (KPI) set of completed processing of the                product batch,
   using big data architecture to                automatically adjust the
   weighting factor of the                target function for adjusting the
   load balancing                model to generate next load dispatching
   the next                processing period based on the current KPI.
   USE - The method is useful for semiconductor                processing
   dispatching.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included fora
   semiconductor processing dispatching system;                anda
   non-transitory computer-readable storage                medium
   comprising a set of instructions for                semiconductor
   processing dispatching.
   DESCRIPTION Of DRAWING(S) - The drawing shows a functional/structural   
   block diagram of an exemplary load scheduling                system
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2020-07-22
UT DIIDW:2020636181
ER

PT P
AU YU C S
   CHAN K H
TI Data storage and retrieval system such as data            lakes which
   are compliant with data regulations has            gateway module to
   process instream transactions to            render data storage module
   compliant with privacy data            regulations
PN US2020193057-A1
AE AMARIS.AI PTE LTD
AB 
   NOVELTY - The data storage and retrieval system has a               
   data storage module comprising a data lake for                storing
   unstructured data fragments, and a gateway                module to
   process instream transactions. Processing                of instream
   transaction comprises identifying any                privacy data
   elements in the instream transaction,                encrypting the
   privacy data element or elements,                and storing the
   instream transaction as data                fragments in the data
   storage module. Processing                the instream transactions
   renders the data storage                module compliant with privacy
   data                regulations.
   USE - Data storage and retrieval system such as data               
   lakes which are compliant with data regulations and               
   provides a total customer view.
   ADVANTAGE - Provides an improved data storage system i.e.               
   data lakes with enhanced data privacy protection                while
   providing a total 360 degree customer view.                Privacy
   leakage is prevented and theft management                is enhanced by
   encryption, such as by tokenization                as well as personally
   identifiable information                (PII) masking of data in transit
   and in storage.                Avoids customers from having to perform
   multiple                data entries. Provides omnichannel support and  
   employs highly efficient artificial intelligence                (AI)
   natural language search, for example, using                deep learning
   attention. Analytics can be deployed                to answer questions
   using semantics natural                language processing (NLP),
   enabling a better                understanding of trends and
   motivations. Enables                improved predictability of customer
   requirements,                including optimizing delivery and timing of
   needs                as well as suggesting better packaging, options and
   sharing offers that can cater to the needs of                customers
   and their families as well.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
              for storing data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of         
   tokenization.400Tokenization process405Download mobile payment
   application410Remote system415Make a payment for a purchase425Merchant
   acquirer system
Z9 0
U1 0
U2 0
DA 2020-07-06
UT DIIDW:202054354J
ER

PT P
AU ZENG J
   LIU X
TI Method of data collection and query based on            unbounded big
   data lake, involves obtaining            relationship map, according to
   attribution and spatial            association between conservancy
   element data, and            displaying target conservancy element data
PN CN111258961-A
AE GUANGDONG WATER RESOURCES DEPT; GUANGZHOU LINKCM TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves getting (S101) keywords               
   entered by the user. The keywords include keywords                of
   water conservancy element data to be queried.                The target
   water element data related to the                keyword in the water
   data lake is found. The water                conservancy data lake
   stores water conservancy                element data of multiple water
   conservancy business                systems and the relationship map
   between the water                conservancy element data. The
   relationship map is                obtained (S102), according to
   attribution                association and spatial association between
   the                water conservancy element data. The target water     
             conservancy element data is displayed (S103).
   USE - Method of data collection and query based on               
   unbounded big data lake using computer device                (claimed).
   ADVANTAGE - The unified query and analysis of water               
   conservancy element data is realized, and the                efficiency
   of finding water conservancy element                data is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a water element data query device; anda computer-readable
   storage medium storing                program for data collection and
   query based on                unbounded big data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the method of data collection and query based on               
   unbounded big data lake. (Drawing includes                non-English
   language text)S101Step for getting keywords entered by the              
   userS102Step for finding target water element                data
   related to the keyword in the water data                lakeS103Step for
   displaying target water                conservancy element data as a
   result
Z9 0
U1 0
U2 0
DA 2020-06-29
UT DIIDW:2020546845
ER

PT P
AU CHEN G
TI Method for constructing semantic data lake of            multi-source
   heterogeneous data, involves writing into            graph database
   according to resource description            framework (RDF) description
   and reference to            frame
PN CN111221785-A
AE ZHONGYUN KAIYUAN DATA TECHNOLOGY SHANGHA
AB 
   NOVELTY - The method involves constructing a main frame               
   and confirmed an attributes and parameters of the                main
   frame. A graph database of a data lake server                is stored.
   A semantics of a content of an imported                data file is
   extracted. A RDF description is                created and saved an
   established RDF description to                a document database of the
   data lake server. The                file corresponding to the RDF
   description and the                main frame are semantically
   associated and written                into the graph database according
   to the RDF                description and reference to the relevant main
   frame. An edges represent a relationship between                entities
   and the and attributes.
   USE - Method for constructing semantic data lake of               
   multi-source heterogeneous data.
   ADVANTAGE - The operation that is intervened manually is               
   realized. The retrieval of the semantic data lake                is
   convenient and convenient to obtain the                retrieval result
   file in detail.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
   method for constructing semantic data lake of               
   multi-source heterogeneous data. (Drawing includes               
   non-English language text)
Z9 0
U1 0
U2 0
DA 2020-06-22
UT DIIDW:202051365C
ER

PT P
AU CHEN G
TI Method for importing multi-source heterogeneous            data into
   data lake, involves obtaining access            interface address of
   external file type data, importing            and saving external file
   type data to distributed file            system of local data lake
   server
PN CN111221791-A
AE ZHONGYUN KAIYUAN DATA TECHNOLOGY SHANGHA
AB 
   NOVELTY - The method involves obtaining the access               
   interface information of an external data source.                The
   access interface information of an external                data source
   is obtained, and connecting a local                data lake server to
   the external data source. The                non-relational data of the
   external data source is                converted into relational data
   and save it to the                relational database of the local data
   lake server                or directly import the relational data of the
   external data source and save it to the relational               
   database of the local data lake server. The access               
   interface information of the external data source                is
   obtained, the local data lake server is                connected with
   the external data source, the                non-relational data of the
   external data source is                imported, and save it to the
   document database of                the geodata lake server. The access
   interface                address of the external file type data is
   obtained,                directly import the external file type data,
   and                save it to the distributed file system of the local  
                data lake server.
   USE - Method for importing multi-source                heterogeneous
   data into data lake.
   ADVANTAGE - The method can solve the problem of               
   multivariate and heterogeneous data to be saved,               
   facilitate the collection, management and                application of
   multi-source heterogeneous data and                expand, meet various
   requirements of the                organizational structure, and ensure
   the security                of data access and the flexibility when
   importing                data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the   
     data lake. (Drawing includes non-English language                text)
Z9 0
U1 0
U2 0
DA 2020-06-22
UT DIIDW:2020513656
ER

PT P
AU JIN X
   ZHANG X
   QIAN F
   FENG Z
TI Safe and efficient vulnerability management system            comprises
   local threat and vulnerability management            platform for
   implements internet asset auditing, and            centralized
   management of scanning engines
PN CN111199042-A
AE EXTRA HIGH VOLTAGE POWER TRANSMISSION CO; GUANGDONG NANFANG ELECTRIC
   POWER COMMUNI
AB 
   NOVELTY - Safe and efficient vulnerability management               
   system comprises threat intelligence system,                internet
   intelligence intelligent collection system                based on big
   data architecture, focusing on the                intelligence collected
   by the company's                self-research, sharing with friends,
   industry                sharing and the Internet Unified integration,
   and                according to the characteristics of the industry to  
   share relevant information to third-party systems                or
   products. The local threat and vulnerability                management
   platform, which mainly implements                internet asset
   auditing, centralized management of                scanning engines and
   vulnerability management                support functions. A scanning
   equipment comprises                existing vulnerability scanning
   equipment, WEB                scanning tools and baseline scanning
   tools, through                the threat and vulnerability management
   platform                for centralized scheduling and driving, at the
   same                time, comprises the software probe specially        
   customized for this solution to identify and                monitor the
   user's assets and equipment.
   USE - Used as safe and efficient vulnerability                management
   system.
   ADVANTAGE - The system imports other risk data for unified              
   analysis, combines with continuous monitoring of                local
   risks, provides customers with vulnerability                management
   processes with rapid response, orderly                repair and
   continuous optimization management                capabilities.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation
   of the safe and efficient vulnerability management                system
   (Drawing includes non-English language                text).
Z9 0
U1 0
U2 0
DA 2020-06-11
UT DIIDW:2020480933
ER

PT P
AU QIN L
   LIANG S
   CHEN S
   YU X
   LI S
   ZHOU Y
   OUYANG J
   GAO L
TI Multi-dimensional visualization platform for use            in
   distribution network has data integration and            management
   module for accessing integrated distribution            source data
   network, and distribution power grid            multi-source data for
   monitoring
PN CN111177101-A; CN111177101-B
AE ELECTRIC POWER RES INST GUANGDONG POWER
AB 
   NOVELTY - The multi-dimensional visualization platform               
   comprises a data integration and management module,                used
   for accessing the integrated distribution                source data
   network. The distribution power grid                multi-source data
   for monitoring and abnormality                processing. The data
   storage module for accessing                the data integration. The
   management module                collects the distribution power grid
   multi-source                data. The distribution network for
   classifying                multi-source data. Multiple types of source
   data to                relational database based on multidimensional    
   visualization platform distribution power grid of                the big
   data architecture used according to the                distribution
   network. The real-time database and                HDFS distributed file
   system for storage. A data                fusion module for the data
   storage module in the                distribution power grid
   multi-source data fusion                for archives data, service data
   fusion and running                data fusion.
   USE - Multi-dimensional visualization platform for                use in
   distribution network based on big data                architecture.
   ADVANTAGE - The power distribution control is improved and              
   upgraded to realize distribution of electric                network by
   the early warning management module and                data processing
   visualization of large                comprehensive display module. The
   operation process                is easy.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   multi-dimensional visualization platform. (Drawing               
   includes non-English language text).
Z9 0
U1 0
U2 0
DA 2020-06-15
UT DIIDW:202045248C
ER

PT P
AU ZHANG Z
   HU M
   MA L
   JIANG Y
   CHEN H
TI Mobile terminal bog data processing method,            involves
   utilizing calculation engine and processing            framework to
   process terminal data, performing stream            process on terminal
   data, and performing batch            processing on terminal data
PN CN111147664-A; CN111147664-B
AE SOUTHERN COAST GUANGDONG SCI & TECHNOLOG
AB 
   NOVELTY - The method involves obtaining mobile terminal               
   data. Flow data is analyzed based on a number of                streams,
   duration, bytes, number of users, and                time/space dynamic
   packets. Relevant data of a                mobile terminal application
   is collected to                complete subtasks for a mobile data flow
   user. A                calculation engine and a processing framework are
   utilized to process the mobile terminal data.                Stream
   process is performed on the mobile terminal                data by
   utilizing the computing engine. Batch                processing is
   performed on the mobile terminal data                by the processing
   framework.
           USE - Mobile terminal bog data processing                method.
   ADVANTAGE - The method enables realizing collection,               
   storing, processing, displaying and management of                the
   mobile terminal data in a convenient manner and                providing
   safe, stable and effective environment                for users and
   operators.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:(1) a mobile terminal bog data processing               
   device;(2) a mobile terminal big data architecture;               
   and(3) a storage medium including instructions                for
   performing a mobile terminal bog data                processing method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   mobile                terminal bog data processing device. (Drawing     
             includes non-English language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202042752S
ER

PT P
AU LIU R
   ZHANG Q
TI Petroleum big data based integration synergy            management
   application platform, has integrated            collaborative management
   platform provided with            integrated technology research
   platform and different            dynamic tracking analysis platforms
PN CN111027923-A
AE UNIV XIAN SHIYOU
AB 
   NOVELTY - The platform has a big data architecture                system
   and an integrated collaborative management                platform
   bidirectionally connected through a HTTP                protocol. The
   big data architecture system is                provided with a data
   management system, a data                storage system, a data source
   system, a data                acquisition system, a data transmission
   system and                a data monitoring system. The integrated      
   collaborative management platform is provided with                an
   integrated technology research platform,                different
   dynamic tracking analysis platforms, a                knowledge base
   focusing platform, a real-time                monitoring platform and a
   remote decision-making                platform.
   USE - Petroleum big data based integration synergy               
   management application platform.
   ADVANTAGE - The platform realizes data resource sharing,               
   improves petroleum-related field working efficiency                of a
   worker and promotes development of each                enterprise.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   petroleum big data based integration synergy                management
   application platform. (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2020337321
ER

PT P
AU DANG B
   MENG F
   LIU G
   FU D
   WANG Z
   WANG J
   LIU W
   ZHAO M
   MIAO G
   HUANG W
   WANG M
TI Automatic meter operation monitoring and            intelligent fault
   self-diagnosis system, has            distributed cluster collecting and
   pre-processing            heterogeneous data, and system module formed
   with            display layer, control layer and logic layer
PN CN110988535-A
AE ANYANG POWER SUPPLY CO STATE GRID HENAN
AB 
   NOVELTY - The system has an analysis transformer for               
   analyzing power factor, corresponding user setting               
   corresponding segment value power factor, power                factor
   statistical analysis, maximum power factor,                maximum time,
   minimum value, minimum value                occurrence time and limit
   time. An electric                quantity record transformer determines
   quantity                electric quantity of the analysis transformer
   for                calculating ratio of transformer loss quantity and   
   supplied electric quantity. A data source                collection unit
   is divided into a real-time data                source collection and a
   non-real-time data source                collection. A non-real-time
   data source realizes                day timing synchronization and
   provided with a data                architecture design and a system
   architecture                design. A server processes a data
   architecture. A                distributed cluster collects and
   pre-processes                heterogeneous data to a service request. A
   system                module is formed with a display layer, a control  
                layer and a logic layer.
   USE - Automatic meter operation monitoring and               
   intelligent fault self-diagnosis system.
   ADVANTAGE - The system realizes power supply enterprise               
   economic operation, distribution production                management,
   line loss management and electric power                marketing service
   so as to improve marketing                management automation and
   informatization                level.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   automatic meter operation monitoring and                intelligent
   fault self-diagnosis system (Drawing                includes non-English
   language text).
Z9 0
U1 0
U2 0
DA 2020-04-29
UT DIIDW:2020312821
ER

PT P
AU CAI H
   HUANG J
   ZHANG B
   YU H
   LEI L
   JIANG L
TI Associated data based autonomous data lake            construction
   system, has heterogeneous data source            included with
   structured database, semi-structured            JavaScript object
   notation file and unstructured            Scanned picture
PN CN110941612-A; CN110941612-B
AE UNIV SHANGHAI JIAOTONG
AB 
   NOVELTY - The system has a knowledge correction and               
   fusion module performing word sense disambiguation                on
   encapsulated knowledge and knowledge fusion                process to
   obtain instance knowledge graph. The                instance concept
   extraction module performs concept                extraction on the
   instance knowledge graph in the                data lake and extracts
   obtained concepts that are                automatically clustered to
   obtain the                instance-associated conceptual model generated
   from                the bottom of the instance data. The meta-model     
   verification and evolution module verifies the                initial
   meta-model based on the instance-associated                conceptual
   model and promotes the evolution of the                initial
   meta-model to obtain the final Unified                management
   semantic metamodel. The heterogeneous                data source is
   included with structured database,                semi-structured
   JavaScript object notation                (JSON)(RTM: Lightweight
   computer data-interchange                format) file and unstructured
   Scanned                picture.
   USE - Associated data based autonomous data lake               
   construction system.
   ADVANTAGE - The system generate a real-time updated               
   catalog index and an instance knowledge graph and                quickly
   locates through the catalog while                constructing the data
   lake, so that the data lake                is obtained with autonomous
   capabilities and                managed and retrieved by external users.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an     
   associated data based autonomous data lake                construction
   method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation
   of the associated data based autonomous data lake               
   construction system (Drawing includes non-English               
   language text).
Z9 0
U1 0
U2 0
DA 2020-04-20
UT DIIDW:2020274688
ER

PT J
AU Pedro Luiz Pizzigatti Corrêa; Rosa Virginia Encinas Quille
TI Information system for analysis of large volumes of contaminated areas
   data
DT Awarded Grant
PD Mar 01 2020
PY 2020
AB In this PhD research project proposes the development of high volume
   data management and system analysis for the management of contaminated
   areas to digital plan transformations, intelligent cities, and
   sustainable. Applications of this development involve soil remediation
   methods, emergency actions, among other types of interventions made by
   specialists and/or entities working on these concerning areas. The data
   to be used will vary from georeferenced data from different sources,
   such as historical data, dynamic data (i.e. optical, digital, laser, and
   radar airborne sensors) as well as other new studies. The
   characteristics of these data can be classified by a large amount of
   information (volume), potentially obtained by sensor technologies, where
   new elements are inserted in real time (velocity). The nature of the
   information is also very diverse, from historical data, to
   physical-chemical analysis of the materials present in the areas
   investigated (variety). Therefore, this data will be studied and
   processed by means of Data Science techniques, where a methodology of
   data analysis in the big data architecture will be proposed considering
   its applicability in the context of the smart and sustainable cities.
   (AU)
ZA 0
TC 0
Z8 0
ZB 0
ZS 0
ZR 0
Z9 0
U1 0
U2 0
G1 19/21693-0
DA 2023-12-08
UT GRANTS:16385505
ER

PT P
AU CVIJETINOVIC M
   VANCE G J
   MATURANA F P
TI System for collecting and analyzing performance            data of
   nozzle of pick and place nozzle, has            notification component
   for generating notification data            in response to determining
   that movement of location of            vector point satisfies criterion
PN EP3614320-A1; US2020068759-A1; CN110858339-A; US10624251-B2;
   US2020205324-A1; US11147200-B2; CN110858339-B
AE ROCKWELL AUTOMATION TECHNOLOGIES INC
AB 
   NOVELTY - The system has a data aggregation component                for
   generating performance vector data for a nozzle                of
   respective nozzles based on production data,                where the
   performance vector data defines a                location of a vector
   point of the nozzle within an                x-y plane. The data
   aggregation component                determines the location of the
   vector point as a                plot of a total number of rejects for
   the nozzle on                a y-axis of the x-y plane and percentage of
   rejects                for the nozzle on an x-axis of the x-y plane. A  
   vector analysis component tracks movement of the                location
   of the vector point based on analysis of                the performance
   vector data. A notification                component generates
   notification data directed to a                specified client device
   in response to determining                that the movement of the
   location of the vector                point satisfies a defined
   criterion.
   USE - System for collecting and analyzing                performance
   data of a nozzle of a pick and place                nozzle.
   ADVANTAGE - The system allows edge devices to               
   automatically detect and communicate with a cloud               
   platform upon installation at a facility, thus               
   simplifying integration with cloud-based data                storage,
   analysis or reporting applications used by                an enterprise.
   The system stores heterogeneous data                in a raw format
   without extensive preprocessing so                as to perform data
   streaming and analytics on a                data lake to facilitate
   system flexibility and                scalability.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the     
   following:a method for tracking nozzle performance                dataa
   non-transitory computer-readable medium                comprising a set
   of instructions for collecting and                analyzing nozzle
   performance data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an     
   electronic assembly line.102Stencil printer104Solder paste inspection
   station106Pick and place machines108Connection reflow oven110Automated
   optical inspection                station
Z9 0
U1 0
U2 0
DA 2020-03-04
UT DIIDW:2020172214
ER

PT C
AU da Silva, Rodrigo Dantas
   Pereira de Araujo, Jean Jar
   Pires de Paiva, Alvaro Ferreira
   de Medeiros Valentim, Ricardo Alexsandro
   Coutinho, Karilany Dantas
   de Paiva, Jailton Carlos
   Roussanaly, Azim
   Boyer, Anne
GP ACM
TI A Big Data Architecture to a Multiple Purpose in Healthcare
   Surveillance: The Brazilian Syphilis Case
SO PROCEEDINGS OF THE 10TH EURO-AMERICAN CONFERENCE ON TELEMATICS AND
   INFORMATION SYSTEMS (EATIS 2020)
DI 10.1145/3401895.3402092
DT Proceedings Paper
PD 2020
PY 2020
AB For many decades society did need to monitor and assess the standard of
   living of the population. In the 1950s, the United Nations (UN) saw this
   need and proposed 12 areas that should be evaluated, the first of which
   is listed under "Health and Demography", which focuses on what is
   expressed as the level of a population's health. Decades have passed and
   great results have been gained from similar initiatives such as reducing
   mortality from infectious diseases and even eradicating some others. In
   the age of the digital society, needs have grown. Monitoring demands
   that once perished from data to become concrete now suffer from the
   opposite effect, the excess of data from everywhere. Healthcare systems
   around the world use many different information systems, collecting and
   generating hundreds of data at unimaginable speed. We are billions of
   people on the planet and most of us are connected to the virtual world,
   sharing information, experiences and events with some kind of cloud. In
   this information age, the ability to aggregate and process this data is
   a major factor in raising public health to a new level. The development
   of tools capable of analyzing a large volume of data in seconds and
   producing knowledge for targeted decision making can help in the fight
   against specific diseases, in the process of continuing education of
   professionals, in the formation of new professionals, in the elaboration
   of new policies. with the specific locoregional look, in the analysis of
   hidden trends in front of so much information faced in everyday life and
   other possibilities. The present work proposes an architecture capable
   of storing and manipulating seeking to standardize the variables in
   order to allow to correlate this large amount of data in a systematic
   way, providing to several services and researchers the possibility of
   consuming health, social, economic and educational data for the
   promotion of public health.
CT 10th Euro-American Conference on Telematics and Information Systems
   (EATIS)
CY NOV 25-27, 2020
CL Aveiro, PORTUGAL
RI Paiva, Jailton/OHU-2520-2025; Anne, Boyer/AAB-8771-2022; Ferreira Pires de Paiva, Álvaro/; Coutinho, Karilany Dantas/; Paiva, Jailton Carlos/; Valentim, Ricardo/HHM-8045-2022
OI Ferreira Pires de Paiva, Álvaro/0000-0003-0708-051X; Coutinho, Karilany
   Dantas/0000-0002-2051-8611; Paiva, Jailton Carlos/0000-0002-2080-9945;
   Valentim, Ricardo/0000-0002-9216-8593
ZB 0
ZS 0
ZR 0
TC 0
Z8 0
ZA 0
Z9 0
U1 0
U2 0
BN 978-1-4503-7711-9
DA 2020-01-01
UT WOS:000717043600058
ER

PT C
AU Daki, Houda
   El Hannani, Asmaa
   Ouahmane, Hassan
BE Essaaidi, M
   Zbakh, M
   Ouacha, A
TI Big Data Architectures Benchmark for Forecasting Electricity Consumption
SO PROCEEDINGS OF 2020 5TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND
   ARTIFICIAL INTELLIGENCE: TECHNOLOGIES AND APPLICATIONS (CLOUDTECH'20)
BP 157
EP 162
DI 10.1109/CloudTech49835.2020.9365912
DT Proceedings Paper
PD 2020
PY 2020
AB Now a day, educational institutions present one of the highest power
   consuming sector due to their new activities and occupancy pattern. This
   enormous amount of energy consumption at the university need a huge
   effort to reduce it. Smart grid is among the efficient solution to save
   energy and balance supply and demand. For the same purpose, the National
   School of Applied Sciences of El Jadida-Morocco wants take advantage
   from smart grid to maintain the balance between energy production and
   consumption. Despite of all added value of this smart grid solution for
   the school, it has the issue of managing energy production surplus,
   because it cannot inject it into Moroccan electrical infrastructure
   neither store it using storage devices. So, to overcome this challenge
   the system need to predict electrical consumption to be able to produce
   exactly the same value. Recently, Big Data contributed very well in
   analysing electrical consumption data using many tools and advanced
   techniques. It process, interprets and analyzes huge quantity of data to
   make it more profitable and valuable. For that reason, the school will
   take refuge in Big data technology to implement a custom system to
   predict electrical energy consumption by analyze all factors that
   influence electrical energy use. In this paper, we propose a benchmark
   of the main Big Data architectures in the field and that will cover all
   electrical energy data processing from data collection, data storage,
   data analytic and data visualization. The aim of this benchmark is to
   choose the optimal architecture in term of fault tolerance, resource
   management, data storage and data modelling to forecast electricity
   consumption in educational institutions.
CT 5th International Conference on Cloud Computing and Artificial
   Intelligence - Technologies and Applications (CloudTech)
CY NOV 24-26, 2020
CL ELECTR NETWORK
SP Higher Natl Sch Comp Sci & Syst Anal, ENSIAS; Mohamed V Univ Rabat;
   Moroccan Assoc Cloud Comp; IEEE
RI El Hannani, Asmaa/IYS-5432-2023
OI El Hannani, Asmaa/0000-0003-4765-0222
Z8 0
ZR 0
ZA 0
ZS 0
ZB 0
TC 0
Z9 0
U1 0
U2 3
BN 978-1-7281-6175-4
DA 2021-08-04
UT WOS:000674949000025
ER

PT B
AU Dandapanthula, Sridhar
   Kost, Charles
   Schmitt, Alexa
   Ahmed, lshboul
Z2  
TI [not available]
DT Dissertation/Thesis
PD Jun 21 2023
PY 2023
ZS 0
ZA 0
Z8 0
ZB 0
TC 0
ZR 0
Z9 0
U1 0
U2 3
BN 9798691246876
UT PQDT:67018326
ER

PT R
AU Darmont, Jerome
   Favre, Cecile
   Loudcher, Sabine
   Nous, Camille
TI Data Lakes for Digital Humanities
SO Zenodo
DI http://dx.doi.org/10.5281/ZENODO.4059940
DT Data set
PD 2020-11-01
PY 2020
AB Traditional data in Digital Humanities projects bear various formats
   (structured, semi-structured, textual) and need substantial
   transformations (encoding and tagging, stemming, lemmatization, etc.) to
   be managed and analyzed. To fully master this process, we propose the
   use of data lakes as a solution to data siloing and big data variety
   problems. We describe data lake projects we currently run in close
   collaboration with researchers in humanities and social sciences and
   discuss the lessons learned running these projects. {"references": ["P.
   Liu, S. Loudcher, J. Darmont, E. Perrin, J.P. Girard, M.O. Rousset,
   \"Metadata model for an archeological data lake\", Digital Humanities
   (DH 2020), Ottawa, Canada, July 2020 (https://dh2020.adho.org/).", "P.N.
   Sawadogo, E. Scholly, C. Favre, E. Ferey, S. Loudcher, J. Darmont,
   \"Metadata Systems for Data Lakes: Models and Features\", 1st
   International Workshop on BI and Big Data Applications (BBIGAP@ADBIS
   2019), Bled, Slovenia, September 2019; Communications in Computer and
   Information Science, Vol. 1064, Springer, Heidelberg, Germany,
   440-451.", "P.N. Sawadogo, T. Kibata, J. Darmont, \"Metadata Management
   for Textual Documents in Data Lakes\", 21st International Conference on
   Enterprise Information Systems (ICEIS 2019), Heraklion, Crete-Greece,
   May 2019, 72-83; INSTICC, Set\u00fabal, Portugal (Vol. 1).", "P.N.
   Sawadogo, J. Darmont, \"On Data Lake Architectures and Metadata
   Management\", Journal of Intelligent Information Systems, 2020"]}
   Copyright: Creative Commons Attribution 4.0 International Open Access
TC 0
ZA 0
ZB 0
ZS 0
Z8 0
ZR 0
Z9 0
U1 0
U2 0
DA 2024-12-11
UT DRCI:DATA2020232020490826
ER

PT R
AU Darmont, Jerome
   Favre, Cecile
   Loudcher, Sabine
   Nous, Camille
TI Data Lakes for Digital Humanities
SO Zenodo
DI http://dx.doi.org/10.5281/ZENODO.4059939
DT Data set
PD 2020-11-01
PY 2020
AB Traditional data in Digital Humanities projects bear various formats
   (structured, semi-structured, textual) and need substantial
   transformations (encoding and tagging, stemming, lemmatization, etc.) to
   be managed and analyzed. To fully master this process, we propose the
   use of data lakes as a solution to data siloing and big data variety
   problems. We describe data lake projects we currently run in close
   collaboration with researchers in humanities and social sciences and
   discuss the lessons learned running these projects. {"references": ["P.
   Liu, S. Loudcher, J. Darmont, E. Perrin, J.P. Girard, M.O. Rousset,
   \"Metadata model for an archeological data lake\", Digital Humanities
   (DH 2020), Ottawa, Canada, July 2020 (https://dh2020.adho.org/).", "P.N.
   Sawadogo, E. Scholly, C. Favre, E. Ferey, S. Loudcher, J. Darmont,
   \"Metadata Systems for Data Lakes: Models and Features\", 1st
   International Workshop on BI and Big Data Applications (BBIGAP@ADBIS
   2019), Bled, Slovenia, September 2019; Communications in Computer and
   Information Science, Vol. 1064, Springer, Heidelberg, Germany,
   440-451.", "P.N. Sawadogo, T. Kibata, J. Darmont, \"Metadata Management
   for Textual Documents in Data Lakes\", 21st International Conference on
   Enterprise Information Systems (ICEIS 2019), Heraklion, Crete-Greece,
   May 2019, 72-83; INSTICC, Set\u00fabal, Portugal (Vol. 1).", "P.N.
   Sawadogo, J. Darmont, \"On Data Lake Architectures and Metadata
   Management\", Journal of Intelligent Information Systems, 2020"]}
   Copyright: Creative Commons Attribution 4.0 International Open Access
ZR 0
Z8 0
ZB 0
ZS 0
TC 0
ZA 0
Z9 0
U1 0
U2 0
DA 2024-12-11
UT DRCI:DATA2020232020490825
ER

PT B
AU de Melo, Pedro Junqueira Teixeira Coelho
   Reis, Luís Paulo
   Capa, Francisco
Z2  
TI [not available]
DT Dissertation/Thesis
PD Aug 23 2024
PY 2024
ZR 0
ZB 0
TC 0
ZS 0
Z8 0
ZA 0
Z9 0
U1 0
U2 0
BN 9798383274170
UT PQDT:91337070
ER

PT B
AU Fortem, Mbah Johnas
   McKeeby, Jon W
   Burchell, Jodine M
   Case, Steven V
Z2  
TI [not available]
DT Dissertation/Thesis
PD Jul 15 2023
PY 2023
TC 0
ZR 0
ZA 0
ZB 0
Z8 0
ZS 0
Z9 0
U1 0
U2 1
BN 9798645423957
UT PQDT:67934042
ER

PT C
AU Jian, Zhong
BE Xu, Z
   Choo, KKR
   Dehghantanha, A
   Parizi, R
   Hammoudeh, M
TI Research on the Data Mining Technology in College Students' Attendance
   System Based on the Big Data Architecture
SO CYBER SECURITY INTELLIGENCE AND ANALYTICS
SE Advances in Intelligent Systems and Computing
VL 928
BP 162
EP 167
DI 10.1007/978-3-030-15235-2_25
DT Proceedings Paper
PD 2020
PY 2020
AB With the development and improvement of the information technologies,
   the increasing of the upper application systems and the rapid expansion
   of the data accumulated in the campus information environment, a typical
   campus big data environment has initially been formed. Because of the
   characteristics of the higher education, students' mobility is great and
   their learning environment is uncertain, so that the students'
   attendance mostly used the manual naming. The student attendance system
   based on the big data architecture is relying on the campus network, and
   adopting the appropriate sensors. Through the data mining technology,
   combined with the campus One Card solution, we can realize the
   management of the attendance without naming in class. It can not only
   strengthen the management of the students, but can also improve the
   management levels of the colleges and universities.
CT International Conference on Cyber Security Intelligence and Analytics
   (CSIA)
CY FEB 21-22, 2019
CL Shenyang, PEOPLES R CHINA
Z8 0
ZB 0
ZS 0
ZA 0
TC 0
ZR 0
Z9 0
U1 0
U2 29
SN 2194-5357
EI 2194-5365
BN 978-3-030-15235-2; 978-3-030-15234-5
DA 2019-10-28
UT WOS:000490430400025
ER

PT R
AU Jimenez, Patricia
TI Data lakes for clustering
SO Mendeley Data
VL 2
DI http://dx.doi.org/10.17632/KD9RR3VCR6.2
DT Data set
VN 2
PD 2021-01-05
PY 2020
AB This dataset describes the on-line materials that accompany article
   "ROMULO: A Clustering Proposal in the Context of Data Lakes", by
   Patricia Jimenez, Juan C. Roldan, and Rafael Corchuelo, which was
   submitted for evaluation to Big Data Research. The materials are
   organised into the following folders: - "data-lakes": each subfolder
   corresponds to a data lake, and each CSV file inside a data-lake
   corresponds to a dataset. The last column of the datasets is called
   "clazz", but it is set to "0" in all cases. A few of the original
   datasets had a class, but it was removed to ensure that neither RoMULO
   nor the other competitors use it since they all are unsupervised
   proposals. - "results": it provides the results of testing RoMULO and
   other competitors on the previous data lakes. The results consist of
   several "*-results.csv" files that provide effectiveness and efficiency
   results for each proposal used in the experimentation. - "system": it
   provides the python code required to run and test RoMULO. There is a
   "launch.cmd" script that launches the experimentation. COMPETITORS
   ------------------- The implementation of AffinityPropagation,
   Meanshift, and OPTICS-XI is available in SckitLearn. The implementation
   of GSPPCA is available from the authors at
   https://github.com/pamattei/GSPPCA. THe implementation of PQC is
   available from the authors at https://github.com/racaes/PQC.
ZB 0
ZA 0
ZS 0
TC 0
Z8 0
ZR 0
Z9 0
U1 0
U2 0
DA 2024-11-21
UT DRCI:DATA2021006020865720
ER

PT R
AU Jimenez, Patricia
TI Data lakes for clustering
SO Mendeley Data
DI https://doi.org/10.17632/KD9RR3VCR6
DT Data set
PD 2025-02-27
PY 2020
AB This dataset describes the on-line materials that accompany article
   "R��MULO: A Clustering Proposal in the Context of Data Lakes", by
   Patricia Jim��nez, Juan C. Rold��n, and Rafael Corchuelo, which was
   submitted for evaluation to Big Data Research. The materials are
   organised into the following folders: - "data-lakes": each subfolder
   corresponds to a data lake, and each CSV file inside a data-lake
   corresponds to a dataset. The last column of the datasets is called
   "clazz", but it is set to "0" in all cases. A few of the original
   datasets had a class, but it was removed to ensure that neither R��MULO
   nor the other competitors use it since they all are unsupervised
   proposals. - "results": it provides the results of testing R��MULO and
   other competitors on the previous data lakes. The results consist of
   several "*-results.csv" files that provide effectiveness and efficiency
   results for each proposal used in the experimentation. - "system": it
   provides the python code required to run and test R��MULO. There is a
   "launch.cmd" script that launches the experimentation. COMPETITORS
   ------------------- The implementation of AffinityPropagation,
   Meanshift, and OPTICS-XI is available in SckitLearn. The implementation
   of GSPPCA is available from the authors at
   https://github.com/pamattei/GSPPCA. THe implementation of PQC is
   available from the authors at https://github.com/racaes/PQC. Copyright:
   Creative Commons Attribution 4.0 International
ZS 0
ZA 0
ZR 0
TC 0
Z8 0
ZB 0
Z9 0
U1 0
U2 0
DA 2024-12-11
UT DRCI:DATA2020088018863806
ER

PT C
AU Karageorgou, Ioanna
   Liakos, Panagiotis
   Delis, Alex
BE Wu, XT
   Jermaine, C
   Xiong, L
   Hu, XH
   Kotevska, O
   Lu, SY
   Xu, WJ
   Aluru, S
   Zhai, CX
   Al-Masri, E
   Chen, ZY
   Saltz, J
TI A Sentiment Analysis Service Platform for Streamed Multilingual Tweets
SO 2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
BP 3262
EP 3271
DI 10.1109/BigData50022.2020.9377837
DT Proceedings Paper
PD 2020
PY 2020
AB Micro-blogging and social-media platforms are now prominent forums for
   disseminating information, opinions and commentaries. Among these,
   Twitter enjoys an in-excess of 330M base of users who continually
   produce and consume information snippets. Users collectively create a
   voluminous and multilingual corpus in a very broad range of topics on a
   daily basis. The discourse generated in the blogosphere is often of
   prime interest and importance to individuals, organizations, and
   companies. These actors would certainly like to periodically receive an
   overall assessment of demonstrated "sentiments" on specific issues by
   automatically classifying tweets expressed in different languages in
   conjunction with big-data analytics. In this paper, we propose a
   scalable service platform that employs multilingual sentiment analysis
   to classify streamed-tweets and yields analytics for selected topics in
   real-time. We discuss the main component of our Spark-enabled platform
   as we seek to offer an effective big-data service that can: 1)
   dynamically handle voluminous as well as high-rate tweet traffic through
   a multicomponent application exploiting the latest software
   developments, 2) accurately identify messages originated by non-genuine
   user-accounts, and 3) utilize the Spark machine-learning library (MLib)
   to successfully classify streamed multi-lingual messages in real-time,
   using multiple potentially distributed executors. To empower our service
   platform, we have adopted training sets and developed sentiment analysis
   (SA) models for English, French, and Greek that help classify streamed
   tweets with high accuracy. While experimenting with our distributed
   analytical platform, we establish both accurate and real-time
   classification for tweets expressed in the above European languages.
CT 8th IEEE International Conference on Big Data (Big Data)
CY DEC 10-13, 2020
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc; IBM; Ankura
RI Liakos, Panagiotis/AAE-4569-2020
ZB 0
ZR 0
Z8 0
TC 0
ZA 0
ZS 0
Z9 0
U1 0
U2 3
SN 2639-1589
BN 978-1-7281-6251-5
DA 2021-07-25
UT WOS:000662554703049
ER

PT J
AU Kim, Jongmin
   Kwon，Oh-Heum
   Song, Ha-Joo
TI Design of a Data Hub for Analysis of Food Big Data
Z1 식품 빅데이터 분석을 위한 데이터 허브의 설계
SO Database Research
S1 데이타베이스연구
VL 36
IS 3
BP 153
EP 163
DT research-article
PD 2020
PY 2020
AB In this paper, we propose a data storage system for analysis of sea food
   big data. Existing food analysis stems are based on data warehouses that
   are constructed on relational databases. Therefore data analysis should
   be performed on the predefined database schema and data losses can
   happen while extracting data from the source to the data warehouse.The
   proposed system connects various data sources with different types and
   provides flexible processing of the data which is stored in its original
   form as it is done in data lakes.Users can store data in relational
   databases, non-SQL databases, and files with their selection. Files are
   ingested by Elasticsearch so that they can be efficiently retrieved
   later even though they are stored in its raw format. The data files are
   stored in HDFS to support massive data analysis. Users can easily access
   and analyze the data using a web browser via Jupiter notebook or
   Google’s Colab interface.
AK 본 논문에서는 식품 빅데이터의 분석을 위한 데이터 저장시스템을 제안한다. 기존의 식품관련 분석시스템에서는 관계형데이터베이스 기반의
   데이터웨어하우스를 사용하였고 정형화된 데이터에 기반한 식품 추천을 목적으로 하였다. 따라서 미리 정의된 스키마만을 사용해서
   분석해야 하는 한계가 있고, 데이터 소스에서 웨어하우스로 데이터를 추출하는 과정에서 데이터의 소실이 빈번하게 발생한다. 제안하는
   시스템은 데이터레이크 형식으로 다양한 데이터 소스를 연계하고 원데이터를 그대로 유지하여 유연한 데이터 처리가 가능하도록
   하였다.아울러 관계데이터베이스 또는 NoSQL 데이터베이스 데이터를 적재하여 사용할 수 있도록 하였다. 파일 데이터는
   Elasticsearch를 사용하여 원데이터의 인입(Ingestion)과 검색이 단순하게 이루어지도록 하였다. 원데이터 파일은
   HDFS에 저장하여 대규모 데이터 분석이 가능하도록 하였다. 사용자 측면에서는 Jupyter 노트북과 Google Colab을
   통해 공유된 데이터를 웹브라우저를 통해 간편하게 접근하여 분석할 수 있도록 하였다.
ZA 0
ZR 0
ZB 0
TC 0
Z8 0
ZS 0
Z9 0
U1 0
U2 0
SN 1598-9798
DA 2021-06-22
UT KJD:ART002665460
ER

PT C
AU Latreche, Othmane
   Boukraa, Doulkifli
BE Wu, XT
   Jermaine, C
   Xiong, L
   Hu, XH
   Kotevska, O
   Lu, SY
   Xu, WJ
   Aluru, S
   Zhai, CX
   Al-Masri, E
   Chen, ZY
   Saltz, J
TI Self-Service, On-Demand Creation of OLAP Cubes over Big Data: a
   Metadata-Driven Approach
SO 2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
BP 2907
EP 2914
DI 10.1109/BigData50022.2020.9378026
DT Proceedings Paper
PD 2020
PY 2020
AB Nowadays, huge amounts of data are continuously created from different
   sources in different formats, and stored into data lakes for further
   use. A typical use case is to conduct online analyses in order to gain
   business insights and make better decisions. However, there are many
   obstacles to overcome when it comes on analysing Big Data online, such
   as dealing with schema-free data, conciliating different data formats,
   managing different locations, and allowing BI professionals and analysts
   to create their analytical data by themselves. In this paper, we propose
   some solutions to overcome these obstacles. We propose a data lake
   metadata model as well as a metadata-driven approach to create OLAP
   cubes from data lakes on-demand and in a self-service manner. We apply
   our work to Twitter social network and we present a proof-of-concept
   dedicated application.
CT 8th IEEE International Conference on Big Data (Big Data)
CY DEC 10-13, 2020
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc; IBM; Ankura
RI BOUKRAA, Doulkifli/A-8548-2011
ZR 0
Z8 0
ZB 0
ZS 0
TC 0
ZA 0
Z9 0
U1 0
U2 6
SN 2639-1589
BN 978-1-7281-6251-5
DA 2021-07-25
UT WOS:000662554703005
ER

PT C
AU Liu, Ruoran
   Isah, Haruna
   Zulkernine, Farhana
GP IEEE
TI A Big Data Lake for Multilevel Streaming Analytics
SO 2020 1ST INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS AND PRACTICES,
   IBDAP
BP 103
EP 108
DT Proceedings Paper
PD 2020
PY 2020
AB Large organizations are seeking to create new architectures and scalable
   platforms to effectively handle data management challenges due to the
   explosive nature of data rarely seen in the past. These data management
   challenges are largely posed by the availability of streaming data at
   high velocity from various sources in multiple formats. The changes in
   data paradigm have led to the emergence of new data analytics and
   management architecture. This paper focuses on storing high volume,
   velocity and variety data in the raw formats in a data storage
   architecture called a data lake. First, we present our study on the
   limitations of traditional data warehouses in handling recent changes in
   data paradigms. We discuss and compare different open source and
   commercial platforms that can be used to develop a data lake. We then
   describe our end-to-end data lake design and implementation approach
   using the Hadoop Distributed File System (HDFS) on the Hadoop Data
   Platform (HDP). Finally, we present a real-world data lake development
   use case for data stream ingestion, staging, and multilevel streaming
   analytics which combines structured and unstructured data. This study
   can serve as a guide for individuals or organizations planning to
   implement a data lake solution for their use cases.
CT 1st International Conference on Big Data Analytics and Practices (IBDAP)
CY SEP 25-26, 2020
CL ELECTR NETWORK
RI Isah, Haruna/AAB-3693-2019; liu, ruoran/KOQ-5146-2024
ZR 0
ZB 0
TC 0
ZS 0
ZA 0
Z8 0
Z9 0
U1 0
U2 0
BN 978-1-7281-8106-6
DA 2020-01-01
UT WOS:001338048900019
ER

PT C
AU Lutskiv, Andriy
   Popovych, Nataliya
BE Lytvyn, V
   Vysotska, V
   Hamon, T
   Grabar, N
   Sharonova, N
   Cherednichenko, O
   Kanishcheva, O
TI Big Data Approach to Developing Adaptable Corpus Tools
SO COMPUTATIONAL LINGUISTICS AND INTELLIGENT SYSTEMS (COLINS 2020), VOL I:
   MAIN CONFERENCE
SE CEUR Workshop Proceedings-Series
VL 2604
DT Proceedings Paper
PD 2020
PY 2020
AB Thesis deals with the development of corpus tools which allow building
   corpus of religious and historical texts. It is foreseen that the corpus
   has the features of data ingestion, text data preprocessing, statistics
   calculation, qualitative and quantitative text analysis. All these
   features are customizable. With Big Data approach is meant that corpus
   tools are treated as the data platform and the corpus itself is treated
   as a combination of data lake and data warehouse solutions. There have
   been suggested the ways for resolving algorithmic, methodological and
   architectural problems which arise while building corpus tool. The
   effectiveness of natural language processing and natural language
   understanding methods, libraries and tools on the example of building
   historical and religious texts' corpora have been checked. There have
   been created the workflows which comprise data extraction from sources,
   data transformation, data enrichment and loading into corpus storage
   with proper qualitative and quantitative characteristics. Data
   extraction approaches which are common for ingestion into data lake were
   used. Transformations and enrichments were realized by means of natural
   language processing and natural language understanding techniques.
   Calculation of statistical characteristics was done by means of machine
   learning techniques. Finding keywords and relations between them became
   possible thanks to the employment of latent semantic analysis, terms and
   N-gram frequencies, term frequency-inverse document frequencies.
   Computation complexity and number of information noise were reduced by
   singular value decomposition. The influence of singular value
   decomposition parameters on the text processing accuracy has been
   analyzed. The results of corpus-based computational experiment for
   religious text concept analysis have been shown. The architectural
   approaches to building corpus-based data platform and the usage of
   software tools, frameworks and specific libraries have been suggested.
CT 4th International Conference on Computational Linguistics and
   Intelligent Systems (COLINS)
CY APR 23-24, 2020
CL Lviv, UKRAINE
SP Lviv Polytechn Natl Univ; Natl Tech Univ, Kharkiv Polytechn Inst; Univ
   Paris 13, Inst Galilee; Politechnika Slaska; Ukrainian Sci & Educ IT
   Soc; Lviv IT Cluster; SoftServe; Manning Publicat Co; Kharkiv IT
   Cluster; Fortifier; Envion Software; PI MINDS; SSA Grp; SYTOSS
OI Lutskiv, Andriy/0000-0002-9250-4075
Z8 0
TC 0
ZB 0
ZS 0
ZA 0
ZR 0
Z9 0
U1 0
U2 0
SN 1613-0073
BN *****************
DA 2021-06-10
UT WOS:000651107900028
ER

PT C
AU Staegemann, Daniel
   Volk, Matthias
   Jamous, Naoum
   Venkatesh, Ranjan
   Hart, Stefan Willi
   Bosse, Sascha
   Turowski, Klaus
GP Assoc Informat Syst
TI Improving the Quality Validation of the ETL Process using Test
   Automation
SO AMCIS 2020 PROCEEDINGS
DT Proceedings Paper
PD 2020
PY 2020
AB Due to the growing amount of data that are produced, the concept of big
   data analytics gained plenty of interest among researchers and
   practitioners. However, there are many challenges related to its use.
   One of those is the data storing and subsequently the corresponding
   extraction of required data. Since this process is prone to error, it
   requires thorough quality validation. To facilitate this procedure, an
   automated approach is proposed, using the example of a financial
   institution, replacing its formerly applied manual method. As a result,
   the time needed for a complete factory acceptance test could be reduced
   by 39 percent.
CT Conference of the Association-for-Information-Systems (AMCIS)
CY AUG 10-14, 2020
CL ELECTR NETWORK
SP Assoc Informat Syst
RI Staegemann, Daniel/MWO-8533-2025
ZA 0
Z8 0
ZB 0
ZS 0
TC 0
ZR 0
Z9 0
U1 0
U2 3
BN 978-1-7336325-4-6
DA 2020-09-07
UT WOS:000559924502015
ER

PT C
AU Xu, Liyuan
   Qian, Li
   Chang, Zhijun
   Wu, Zhenxin
BE Zheng, Q
   Zheng, X
   Zhao, X
   Yan, W
   Zhang, N
   Wang, L
TI The Bidirectional Data Flow Based On The Data-Lake
SO 2020 13TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2020)
BP 971
EP 976
DT Proceedings Paper
PD 2020
PY 2020
AB In order to comply with the rapid development of our data resources,
   this paper focuses on the collection, analysis and integration of our
   multi-source heterogeneous data resources based on the data-lake
   structure (short for the forward data flow), and publishes the standard
   data from our data-lake to many characteristic field centers through the
   micro service (short for the reverse data flow). So as to keep the data
   fresh by bidirectional data flow, and to achieve the optimal utilization
   rate. It effectively supports the data display and knowledge service of
   many characteristic field centers, and supports scientific researchers
   with convenience of utilizing knowledge resources in their professional
   field. At present, the bidirectional data flow mechanism has established
   friendly cooperation with more than 10 characteristic field centers, and
   has provided regular and continuous data flow services, the amount of
   data reached to 18 million.
CT 13th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI)
CY OCT 17-19, 2020
CL ELECTR NETWORK
SP IEEE; IEEE Engn Med & Biol Soc
Z8 0
ZB 0
ZS 0
ZR 0
TC 0
ZA 0
Z9 0
U1 0
U2 7
BN 978-0-7381-0545-1
DA 2021-06-04
UT WOS:000651203900176
ER

PT P
AU DIAO Y
   DONG W
   HE K
   HU L
   JIA D
   LI Y
   LIU K
   PEI H
   SHENG W
   TANG J
   YE X
TI Random matrix construction method for reactive            power
   optimization in distribution network, involves            obtaining
   reactive power optimization control sequence            by big data
   method of reactive power optimization in            distribution network
PN CN107133684-A; CN107133684-B
AE CHINA ELECTRIC POWER RES INST; STATE GRID CORP CHINA; STATE GRID JIANGSU
   ELECTRIC POWER CO
AB 
   NOVELTY - The method involves obtaining a random matrix               
   original data source containing historical load                data. The
   load data is extracted to construct load                random matrix
   data set. The cumulative load                function is calculated, to
   construct the load                random matrix. The singular value
   equivalent                transformation and normalized transformation
   are                combined to determine covariance matrix. The         
   average spectral radius is determined and reactive                power
   optimization control sequence is obtained by                the big data
   method of reactive power optimization                in distribution
   network, according to average                spectral radius.
   USE - Random matrix construction method for reactive               
   power optimization in distribution network.
   ADVANTAGE - The random matrix construction method can               
   quantitatively reflect the distribution                characteristics
   of the load spectrum, for load                distribution feature
   comparison and matching, used                for distribution network
   reactive power                optimization, under big data architecture.
   DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating
   a random matrix construction method for reactive                power
   optimization in distribution network.                (Drawing includes
   non-English language text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:201763796E
ER

PT J
AU Wei,  Peng
AA Atkins,  Ella; Schnell,  Thomas; Rozier,  Kristin; Hunter,  George
TI PFI:BIC: Pre-Departure Dynamic Geofencing, En-Route Traffic Alerting, 
   Emergency Landing and Contingency Management for Intelligent
   Low-Altitude Airspace UAS Traffic Management
DT Awarded Grant
PD Aug 15 2017
PY 2017
AB With the development of numerous civilian Unmanned Aerial System (UAS)
   applications, a large number of unmanned aircraft of various types need
   to be safely operated in low-altitude airspace. These UAS also need to
   safely share this space with manned aviation traffic, such as general
   aviation and helicopters. The FAA forecasts 7 million UAS sales
   (commercial and hobbyist combined) by 2020. This project advances and
   integrates the investigators' novel concepts of operations and core
   algorithms into an intelligent UAS Traffic Management system (UTM). This
   UTM system would also break market feasibility barriers for new UAS
   applications such as urban on-demand air transportation and UAS cargo
   delivery. Finally, the insights gained during the development of the
   proposed UTM could have profound impact on design and implementation of
   other human-centered smart service systems and cyber-physical systems
   that support civil aviation, e.g., air traffic infrastructures, operator
   ground support systems, communication, navigation and surveillance
   devices, and vehicle technologies. <br/>This research proposes an
   intelligent UTM system integrating big data architecture and computation
   power that will coordinate pre-departure UAS flight plans, detect
   potential collisions in real time, generate recommendations to resolve
   potential conflictions, proactively control any risk to people and
   objects on the ground during an emergency landing, and identify the
   cause of collisions. The aim of these capabilities is to minimize the
   number of collisions and mitigate the impact of each accident. This will
   be achieved using large-scale optimization, aircraft guidance and
   control, predictive modeling, system verification and validation, and
   advanced visualization techniques for information presentation and
   decision support. The proposed system has a pre-departure flight plan
   coordination module that queries the approved flight plan database and
   performs conformance checking for every newly requested flight plan to
   achieve conflict-free pre-departure traffic coordination. An en route
   traffic monitoring and alerting module receives real-time aircraft
   position data and active flight plans, performs automated prediction for
   potential collision, and generate recommendations to resolve collisions.
   An emergency landing and contingency management module queries multiple
   databases such as terrain maps, obstacle data, airspace data, public
   safety data and real-time aircraft position data to suggest emergency
   landing site and calculate the corresponding landing path to minimize
   the impact risk to people and objects on the ground. Finally, the
   advanced human machine interface will provide information visualization
   and decision support in an intuitive way to reduce cognitive
   inefficiencies and maximize human-in-the-loop performance to augment UAS
   traffic controller capabilities. The proposed system will serve as a
   complementary component of an ongoing NASA UTM.  The research plan has
   three phases: (Phase 1) Identification and synthesis of intelligent UTM
   user requirements, (Phase 2) Development of the intelligent UTM core
   algorithms and system prototype, and (Phase 3) intelligent UTM testing,
   evaluation, and integration.<br/>This academe-industry partnership is
   lead by a multidisciplinary academic research team: Iowa State
   University (lead institution), University of Iowa (Iowa City, IA),and
   University of Michigan (Ann Arbor, MI),) with primary industrial
   partners Rockwell Collins (Cedar Rapids, IA) and Mosaic ATM (small
   business, Leesburg, VA) together with broader context partners the
   Federal Aviation Administration William J. Hughes Technical Center (FAA
   Tech Center) (government agency, Egg Harbor Township, NJ). The partners
   will also receive feedback from the FAA Iowa office and Uber Elevate.
   This partnership will ensure that the proposed UTM system meets FAA
   regulations, user requirements, and market needs.
ZR 0
ZA 0
ZS 0
ZB 0
TC 0
Z8 0
Z9 0
U1 0
U2 1
G1 1718420
DA 2023-12-08
UT GRANTS:13732062
ER

PT J
TI Collaborative Research:  Statistical Inference for High-Frequency Data
DT Awarded Grant
PD Jul 01 2017
PY 2017
AB To pursue the promise of the big data revolution, the current project is
   concerned with a particular form of such data, high frequency data
   (HFD), where series of observations can see data updates in fractions of
   milliseconds. With technological advances in data collection, HFD occurs
   in medicine (from neuroscience to patient care), finance and economics,
   geosciences (such as earthquake data), marine science (fishing and
   shipping), and other areas. The research focuses on how to extract
   information from complex big data and how to turn data into knowledge.
   In particular, the project aims to develop cutting-edge mathematics and
   statistical methodology to uncover the dependence structure governing a
   HFD system. The new dependence structure will permit the "borrowing" of
   information from adjacent time periods, and also from other series from
   a panel of data. It is expected that the results will lead to more
   efficient estimators and better prediction and that this approach will
   form a new paradigm for HFD. In addition to developing a general theory,
   the project is concerned with applications to financial data, including
   risk management, forecasting, and portfolio management. More precise
   estimators, with improved margins of error, will be useful in all these
   areas of finance. The results are expected to be of interest to
   investors, regulators, and policymakers, and the results are entirely in
   the public domain. <br/><br/>The goal of this project is to create a
   unified framework for inference in high frequency data, based on
   dividing the observations and the parameter process into blocks. The
   work pursues two paths, both involving the fundamental structure of the
   data architecture. A "within block" approach uses contiguity to make the
   structure of the observations more accessible in local neighborhoods.
   The "between block" approach sets up a tool for using stochastic
   calculus to study the relationship between parameters in blocks that are
   adjacent (in time and space). It also permits the integration of high
   and low frequency models. This is achieved without altering current
   models. A final part of the project is devoted to further study of the
   observed asymptotic variance, in particular work on tuning parameters
   and inferential interpretation. Both the "within block" and "between
   block" approaches are formulated to cover general time varying
   "parameters" that are usually estimated from high frequency data series,
   not only volatility, but also skewness (leverage effect), regression
   coefficients, and parameter dynamics (such as volatility of volatility).
   In both cases, the observed data and also parameter processes may have
   large dimension (large panel size) in addition to high frequency
   observation. The within block approach permits contiguity to be stated
   jointly for the latent underlying processes and the
   microstructure/observation noise. For the between block approach, the
   investigators will further develop a new way to look at the dependence
   relationships between the parameters.
TC 0
ZR 0
ZB 0
ZS 0
Z8 0
ZA 0
Z9 0
U1 0
U2 0
G1 1713129
DA 2023-12-08
UT GRANTS:13756735
ER

PT J
TI Collaborative Research:  Statistical Inference for High-Frequency Data
DT Awarded Grant
PD Jul 01 2017
PY 2017
AB To pursue the promise of the big data revolution, the current project is
   concerned with a particular form of such data, high frequency data
   (HFD), where series of observations can see data updates in fractions of
   milliseconds. With technological advances in data collection, HFD occurs
   in medicine (from neuroscience to patient care), finance and economics,
   geosciences (such as earthquake data), marine science (fishing and
   shipping), and other areas. The research focuses on how to extract
   information from complex big data and how to turn data into knowledge.
   In particular, the project aims to develop cutting-edge mathematics and
   statistical methodology to uncover the dependence structure governing a
   HFD system. The new dependence structure will permit the "borrowing" of
   information from adjacent time periods, and also from other series from
   a panel of data. It is expected that the results will lead to more
   efficient estimators and better prediction and that this approach will
   form a new paradigm for HFD. In addition to developing a general theory,
   the project is concerned with applications to financial data, including
   risk management, forecasting, and portfolio management. More precise
   estimators, with improved margins of error, will be useful in all these
   areas of finance. The results are expected to be of interest to
   investors, regulators, and policymakers, and the results are entirely in
   the public domain. <br/><br/>The goal of this project is to create a
   unified framework for inference in high frequency data, based on
   dividing the observations and the parameter process into blocks. The
   work pursues two paths, both involving the fundamental structure of the
   data architecture. A "within block" approach uses contiguity to make the
   structure of the observations more accessible in local neighborhoods.
   The "between block" approach sets up a tool for using stochastic
   calculus to study the relationship between parameters in blocks that are
   adjacent (in time and space). It also permits the integration of high
   and low frequency models. This is achieved without altering current
   models. A final part of the project is devoted to further study of the
   observed asymptotic variance, in particular work on tuning parameters
   and inferential interpretation. Both the "within block" and "between
   block" approaches are formulated to cover general time varying
   "parameters" that are usually estimated from high frequency data series,
   not only volatility, but also skewness (leverage effect), regression
   coefficients, and parameter dynamics (such as volatility of volatility).
   In both cases, the observed data and also parameter processes may have
   large dimension (large panel size) in addition to high frequency
   observation. The within block approach permits contiguity to be stated
   jointly for the latent underlying processes and the
   microstructure/observation noise. For the between block approach, the
   investigators will further develop a new way to look at the dependence
   relationships between the parameters.
ZR 0
ZB 0
Z8 0
TC 0
ZA 0
ZS 0
Z9 0
U1 0
U2 0
G1 1713118
DA 2023-12-08
UT GRANTS:13742675
ER

PT P
AU JAVED W
   LEE S
   PAUL S A
   PEREIRA P
   YU B
TI System for providing visualization of big data in            enterprise,
   has visualization system that renders            hierarchical,
   multidimensional view of meta data as            nested icons
PN US2017139974-A1
AE GENERAL ELECTRIC CO
AB 
   NOVELTY - The system has a big data pull infrastructure               
   that is adapted to provide a number of electronic                files.
   A visualization system (500) collects meta                information
   that is associated with the electronic                files. A
   hierarchical, multidimensional view of the                meta data is
   established and is rendered by the                visualization system
   as nested icons. At least one                icon is represented through
   multiple unique visual                characteristics that indicates the
   data that has                not been ingested, the data that has been
   ingested                but not yet validated and the data that has been
                  ingested and validated.
   USE - System for providing visualization of big data                in
   financial management systems, industrial assets                or
   artificial intelligence of enterprise e.g.                business or
   any other type of organization.
   ADVANTAGE - The person's ability to interpret the data               
   efficiently and/or accurately is improved, since                the
   visualization of big data is facilitated                automatically.
   The user access and manipulation is                made easier, by
   supporting both manual and                automatic input mediums and a
   graph database that                store the meta data. The
   technological solutions                are made effective for exploring
   available data                inventory of data across multiple
   businesses. The                effectiveness of a user's ability to
   quickly access                available data in a data lake is improved.
   The                status of available data in a data lake is           
       identified quickly.
   DESCRIPTION Of DRAWING(S) - The drawing shows a front view of           
       visualization display with a legend.500Visualization system510Legend
Z9 0
U1 0
U2 0
DA 2017-01-06
UT DIIDW:201732103G
ER

PT P
AU DING Y
TI Big data architecture based human face            identification
   monitoring system, has data processing            system electrically
   connected with storage system and            network camera management
   system that is connected with            data collecting system
PN CN206149401-U
AE HAINAN SILAN NETWORK TECHNOLOGY CO LTD
AB 
   NOVELTY - The utility model claims a data transmission               
   method based on human face identification                monitoring
   system of large data structure,                comprising multiple data
   acquisition system, the                data collecting system is
   respectively electrically                connected with a network camera
   management system                and different types of camera. said
   network camera                management system are electrically
   connected with                data message system and storage system,
   the data                message system is electrically connected with a 
   data processing system, the data processing system                is
   respectively electrically connected with the                storage
   system and the management system client,                said data
   acquisition system comprising FFmpeg.                alarm device, a
   management system, a database and                some convenient API,
   the FFmpeg API is electrically                connection management
   system, the management system                is respectively
   electrically connected with warning                device, database and
   network camera management                system. The utility model has
   characteristics of                quick identification speed and high   
               precision.
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:201730454V
ER

PT J
AU Hahm, Yukun
TI Data Integration Strategy in Big Data Era: A Public Sector Case Analysis
Z1 빅데이터 시대의 데이터 통합 전략: 공공부문 사례 분석
SO The Journal of Information Technology and Architecture
S1 정보화연구
VL 14
IS 2
BP 115
EP 128
DT research-article
PD 2017
PY 2017
AB Big data has created new technical and business environment which change
   organizationsin many ways. It is time for organizations to cope with
   huge data volume as well as various data typesand the speed of data
   production and utilization. In this context, research on data
   integration for solvingthese problems falls short of academic and
   industry demand. Especially academic research have notprovided enough
   answers on effective decision making support through big data
   integration. By developinga big data integration framework evaluating
   such techniques as Hadoop, data federation, anddata virtualization as
   well as infrastructure like parallel data warehouse, cloud systems,
   logical datawarehouse and data lake and analyzing a public sector data
   integration case, the paper proposes astrategy to select appropriate big
   data integration approaches in various situations.
AK 빅데이터라는 새로운 기술 및 경영 환경은 데이터가 조직에서 활용되는 방법에 많은 변화를 초래하고 있다. 이는 기업들이 이전과 다른
   데이터의 규모는 물론 다양한 형태, 그리고 데이터 생산에서 활용까지 더욱 빠른 속도에 대응해야하기 때문이다. 그러나 빅데이터의
   부상과 함께 데이터의 규모가 상상을 초월할 정도로 커지지는 환경에서 여러 곳에 흩어져 있는 다양한 종류의 데이터를 어떻게 통합하여
   실시간으로 제공해 의사결정을 더욱 효율적이며 효과적으로 할 수 있는지에 대한 연구는 아직까지매우 초기 수준에 있다. 본 연구는
   빅데이터 환경에서 데이터 통합을 위해 사용될 수 있는 다양한 방법들인 하둡, 데이터 페더레이션, 데이터 가상화, 그리고 이들과
   관련된 인프라인 병렬형 데이터웨어하우스, 논리적 데이터 웨어하우스, 클라우드 시스템, 데이터 호수 등의 특징을 분석해 정리하고
   공공기관의 빅데이터 통합 사례를 통해 비즈니스 상황에 맞는 데이터 통합 방법의 선택 전략을 제시하고자 한다.
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
TC 0
Z9 0
U1 0
U2 0
SN 1738-382x
DA 2017-01-01
UT KJD:ART002243254
ER

PT C
AU Leite, Alessandro Ferreira
   Li Weigang
   Fregnani, Jose Alexandre
   de Oliveira, Italo Romani
GP IEEE
TI Big Data Management and Processing in the Context of the System Wide
   Information Management
SO 2017 IEEE 20TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION
   SYSTEMS (ITSC)
SE IEEE International Conference on Intelligent Transportation Systems-ITSC
DT Proceedings Paper
PD 2017
PY 2017
AB The 4D trajectory management program will require a major shift in
   infrastructure and operational management processes to deliver accurate
   and reliable information to trajectory management team. As a result, air
   traffic management operation will demand Network Enabled Operations
   (NEO) concepts such as the System Wide Information Management (SWIM)
   framework to ensure that the decisions are made with the correction
   information at the right time. SWIM provides standards, infrastructure,
   and governance practices to allow information exchanging through
   interoperable services. Consequently, SWIM must provide methods to (a)
   integrate a large variety of data; (b) filter information in a way that
   only the relevant ones are retained to explain the results; (c) enable
   National Airspace System (NAS) operators, pilots, controllers, and
   traffic flow specialists to extract value of air traffic systems in
   real-time; and (d) to seek and explore complex and evolving data's
   relationships. However, SWIM still lacks support to deal with big data
   analytics and to aggregate computing resources on-demand. As the main
   contribution of this paper, we describe the challenges and new focuses
   of SWIM researches. Likewise, we present an architecture to enable big
   data analytics services in SWIM. The proposed architecture relies on big
   data processing frameworks to handle data acquisition and data filtering
   on near-real time taking into account users' objectives; and to
   guarantee that the data go through all the gives stage of the life cycle
   of ATM applications, avoiding the silos that may happen in each data
   analysis stage.
CT 20th IEEE International Conference on Intelligent Transportation Systems
   (ITSC)
CY OCT 16-19, 2017
CL Yokohama, JAPAN
SP IEEE
RI Weigang, Li/ISV-0473-2023; Fregnani, José/H-4238-2012; Romani, Ítalo/Y-1471-2019; Leite, Alessandro/AAP-5824-2020
ZR 0
Z8 0
ZS 0
TC 0
ZB 0
ZA 0
Z9 0
U1 0
U2 1
SN 2153-0009
BN 978-1-5386-1526-3
DA 2018-06-12
UT WOS:000432373000124
ER

PT C
AU Li, Changhua
   Cui, Chenzhou
   He, Boliang
   Fan, Dongwei
   Wang, Jiawei
   Li, Shanshan
   Mi, Linying
   Wan, Wanghui
   Chen, Junyi
   Zhang, Hailong
   Yu, Ce
   Ciao, Jian
   Wang, Chunjuan
   Cao, Zihuang
   Fan, Yufeng
   Hong, Zhi
   Wang, Jianguo
   Yin, Shucheng
   Liu, Liang
   Chen, Xiao
BE Lorente, NPF
   Shortridge, K
   Wayth, R
TI The Design and Application of Astronomy Data Lake in China-VO
SO ASTRONOMICAL DATA ANALYSIS SOFTWARE AND SYSTEMS XXV
SE Astronomical Society of the Pacific Conference Series
VL 512
BP 157
EP 160
DT Proceedings Paper
PD 2017
PY 2017
AB With the coming of many large astronomy observation facilities, such as
   LAMOST, TMT, FAST, LSST, SKA, data storage for these facilities is
   facing big challenges. Astronomy Data Lake, a distributed storage system
   is designed to meet these requirements in the big data era. Astronomy
   Data Lake, basing on a master-slave framework, integrates many
   geographic distributed data storage resources into a single mount point.
   It implements automatic data backup and disaster recovery, with easy
   expansion capability. Based on this system, we developed many data
   storage services, including database storage, private file storage,
   computing data and paper data services for astronomers.
CT 25th Annual Conference on Astronomical Data Analysis Software and
   Systems (ADASS XXV)
CY OCT 25-29, 2015
CL ARC Ctr Excellence All Sky Astrophys (CAASTRO), Sydney, AUSTRALIA
HO ARC Ctr Excellence All Sky Astrophys (CAASTRO)
SP CAASTRO; European Space Agcy; Australian Govt Dept Ind & Sci; European
   So Observat Opt Astron Observ; Smithsonian Astrophys Observ; Elsevier
RI li, changhua/GMW-9966-2022; Wang, Jiawei/LOS-2274-2024; Li, shanshan/ORI-3543-2025; Wang, Chundong/L-6854-2019
ZS 0
ZB 0
Z8 0
TC 0
ZA 0
ZR 0
Z9 0
U1 0
U2 4
SN 1050-3390
BN 978-1-58381-908-1
DA 2018-09-12
UT WOS:000442046200036
ER

PT C
AU Mahanta, Prabal
   Pandey, Himanshu
BE Ciuciu, I
   Debruyne, C
   Panetto, H
   Weichhart, G
   Bollen, P
   Fensel, A
   Vidal, ME
TI Big Data Concept to Address Performance Aware Infrastructure Monitoring
   Challenge for Hybrid Cloud
SO ON THE MOVE TO MEANINGFUL INTERNET SYSTEMS
SE Lecture Notes in Computer Science
VL 10034
BP 185
EP 189
DI 10.1007/978-3-319-55961-2_18
DT Proceedings Paper
PD 2017
PY 2017
AB There has been increasing complexity of cloud infrastructure to sustain
   the growth of enterprise applications and so as the need to constantly
   monitor loads and resource utilization. Numerous sophisticated
   techniques are applied to achieve a unified observation but disparate
   environments, sources and policies restrain the objective to be achieved
   using a standard methodology. The paper tries to present a model for
   standardizing the monitoring platform for applications which are highly
   environment aware and are restraint by governance using a novel
   algorithmic approach. The models tries to instrument APIs to monitor
   single to multitude of parameters to cover the transactions across
   geography. The model also covers a timeline for evolving big data
   analytic methods for application performance monitoring systems for
   environment based applications covering the high data rates and
   computation requirements. The concept of Data Lake brings a unique
   dimension to the model and resource utilization and performance metrics
   for varied workloads and also configuration complexities.
CT OnTheMove (OTM) International Federated Conference
CY OCT 24-28, 2016
CL Rhodes, GREECE
RI Mahanta, Prabal/JNF-0004-2023
Z8 0
ZA 0
ZB 0
ZR 0
TC 0
ZS 0
Z9 0
U1 0
U2 3
SN 0302-9743
EI 1611-3349
BN 978-3-319-55961-2; 978-3-319-55960-5
DA 2018-03-09
UT WOS:000426085500018
ER

PT B
AU Pires, Francisco Miguel Marques
   Neto, Miguel de Castro Simões Ferreira
Z2  
TI [not available]
DT Dissertation/Thesis
PD Aug 09 2024
PY 2024
ZR 0
ZS 0
ZA 0
Z8 0
TC 0
ZB 0
Z9 0
U1 0
U2 0
BN 9798382147833
UT PQDT:88615784
ER

PT C
AU Rabelo, Thomas
   Lama, Manuel
   Vidal, Juan C.
   Amorim, Ricardo
GP IEEE
TI Comparative study of xAPI validation tools
SO 2017 IEEE FRONTIERS IN EDUCATION CONFERENCE (FIE)
SE Frontiers in Education Conference
DT Proceedings Paper
PD 2017
PY 2017
AB Learning Analytics (LA) is currently the most effective way of achieving
   better information and in-depth insights of the learning processes.
   Specifications like Experience API (xAPI) have been defined as part of
   LA initiatives to add interoperability among LA-aware applications.
   Tools that validate the conformance of data to these specifications are
   key components to assure the interoperability among applications. In
   this paper, a comparative evaluation of relevant validation tools of the
   xAPI specification is presented. This comparison focus specially on the
   structural and semantic features of the xAPI specification, revealing
   that most of the currently available tools do not support the semantic
   constraints of the specification.
CT IEEE Frontiers in Education Conference (FIE)
CY OCT 18-21, 2017
CL Indianapolis, IN
SP IEEE; IEEE ASEE; IEEE Comp Soc; Indian
RI Amorim, Ricardo/M-8120-2019; Vidal, Juan/L-7375-2014; Lama, Manuel/AAE-6880-2019
TC 0
Z8 0
ZS 0
ZB 0
ZR 0
ZA 0
Z9 0
U1 0
U2 0
SN 0190-5848
BN 978-1-5090-5920-1
DA 2018-04-11
UT WOS:000426974900295
ER

PT C
AU Saputra, Ferry Astika
   Salman, Muhammad
   Ramli, Kalamullah
   Abdillah, Abid
   Syarif, Iwan
BE AlRasyid, MUH
   Zainudin, A
   Briantoro, H
   Akbar, ZF
   Bagar, FNC
TI Big Data Analysis Architecture for Multi IDS Sensors using Memory based
   Processor
SO 2017 INTERNATIONAL ELECTRONICS SYMPOSIUM ON KNOWLEDGE CREATION AND
   INTELLIGENT COMPUTING (IES-KCIC)
BP 40
EP 45
DT Proceedings Paper
PD 2017
PY 2017
AB The massive internet usage is followed by the rise of cyber-related
   crime such as information stealing, denial-of service (DoS) attack,
   trojan and malware. To cope with the threats, one of most popular choice
   is using Intrusion Detection System (IDS). The logs produced by IDS in a
   day is huge and the limitation of computing power is the main problem to
   process that logs files. In this paper, we propose a big data analysis
   architecture of multi IDS sensors using in-memory data processing.
   Deployed IDS sensors are taking an extra role as computation slave to
   build scalable data analysis platform for network security analysis. So,
   adding more sensors means expanding computational resources. Adding to
   three sensors are helping data computation of clustering algorithm
   faster up to 27% comparing to the computation by using only one sensor.
   This research also introduces the use of memory-based processor, this
   system provides 7,9 times faster data processing than conservative
   MapReduce operation. And moreover, we also have performed botnets
   classification over Spark RDD that give high accuracy result to 99%.
CT International Electronics Symposium on Knowledge Creation and
   Intelligent Computing (IES-KCIC)
CY SEP 26-27, 2017
CL Surabaya, INDONESIA
SP Pens; IEEE Indonesia Sect
RI Syarif, Iwan/M-6965-2019; Sulaman, Muhammad/I-7661-2019; Ramli, Kalamullah/AAI-1405-2019; Saputra, Ferry Astika/N-1115-2018
OI Ramli, Kalamullah/0000-0002-0374-4465; Saputra, Ferry
   Astika/0000-0003-0550-1608
ZB 0
ZA 0
TC 0
ZS 0
Z8 0
ZR 0
Z9 0
U1 0
U2 0
BN 978-1-5386-0716-9
DA 2018-03-09
UT WOS:000425925600007
ER

PT J
AU KABUKA, MANSUR R.
TI Semantic Data Lake for Biomedical Research
DT Awarded Grant
PD Sep 15 2016
PY 2017
AB The SDL-BR System is a distributed computing software solution that
   enables research institutions to manage, integrate, and make available
   large institutional data sets to researchers, and that permits users to
   generate data models specific to particular applications. It uses state
   of the art cluster computing, Semantic Web, and machine learning
   technologies to provide for rapid data ingestion, semantic modeling and
   querying, and search and discovery of data resources through a
   sophisticated, Web-based user interface.
ZR 0
ZS 0
ZB 0
Z8 0
TC 0
ZA 0
Z9 0
U1 0
U2 0
G1 4R44CA206782-02; 9536289; R44CA206782
DA 2023-12-14
UT GRANTS:12238544
ER

PT J
AU KABUKA, MANSUR R.
TI Semantic Data Lake for Biomedical Research
DT Awarded Grant
PD Sep 15 2016
PY 2016
AB The SDL-BR System is a distributed computing software solution that
   enables research institutions to manage, integrate, and make available
   large institutional data sets to researchers, and that permits users to
   generate data models specific to particular applications. It uses state
   of the art cluster computing, Semantic Web, and machine learning
   technologies to provide for rapid data ingestion, semantic modeling and
   querying, and search and discovery of data resources through a
   sophisticated, Web-based user interface.
Z8 0
ZB 0
ZA 0
TC 0
ZS 0
ZR 0
Z9 0
U1 0
U2 0
G1 1R44CA206782-01A1; 9200905; R44CA206782
DA 2023-12-14
UT GRANTS:11219033
ER

PT J
AU KABUKA, MANSUR R.
TI Semantic Data Lake for Biomedical Research
DT Awarded Grant
PD Sep 15 2016
PY 2017
AB The SDL-BR System is a distributed computing software solution that
   enables research institutions to manage, integrate, and make available
   large institutional data sets to researchers, and that permits users to
   generate data models specific to particular applications. It uses state
   of the art cluster computing, Semantic Web, and machine learning
   technologies to provide for rapid data ingestion, semantic modeling and
   querying, and search and discovery of data resources through a
   sophisticated, Web-based user interface.
Z8 0
ZS 0
ZR 0
ZA 0
ZB 0
TC 0
Z9 0
U1 0
U2 0
G1 3R44CA206782-01A1S1; 9443736; R44CA206782
DA 2023-12-14
UT GRANTS:10063784
ER

PT P
AU PAL B
   BANSAL A
   DUTTA S
   KARAR P
   BORAL S
   DEY A
TI Method for detecting faults in rotor driven            equipment based
   on spherical transformation in chemical            industry, involves
   analyzing data by machine learning            engine, and displaying
   fault associated with rotor            driven equipment on user
   interface
PN US2016245686-A1; WO2016137849-A2; WO2016137849-A3; WO2016137849-A4
AE PAL B; BANSAL A; DUTTA S; KARAR P; BORAL S; DEY A; PROPHECY SENSORS LLC
AB 
   NOVELTY - The method involves sampling data at random to               
   estimate a maximum value through a processor.                Sampling
   error is controlled under a predefined                value, where the
   sampling error is associated with                data. The data is
   analyzed through a combination of                Cartesian to Spherical
   transformation, statistics                of extracted entity of
   spherical variables, a large                data analytics engine and a
   machine learning                engine. The Cartesian to spherical
   transformation                is used to make vibrational vectors
   invariant. A                fault associated with a rotor driven
   equipment is                displayed on a user interface.
   USE - Method for detecting faults in a rotor driven               
   equipment based on combination of low frequency                vibration
   data, spherical transformation, machine                learning and big
   data architecture in a chemical                industry, an aviation
   industry and a nuclear power                station.
   ADVANTAGE - The method enables allowing a machine learning              
   engine to process received data to recognize one of                a
   pattern and a deviation to issue alarm and                control
   commands pertaining to the rotor driven                equipment in
   association with a communications                network. The method
   enables supervising support                vector machines with
   associated learning algorithms                to analyze data and
   recognize patterns. The method                enables reducing costs by
   avoiding the need to use                Fast Fourier Transforms and
   reducing sampling                rates.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
        method for predicting rotor driven equipment                issues.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an     
   overall fault detection architecture in a rotor                driven
   equipment using low frequency vibration data                and big data
   architecture.102Sigsbee sensor104BLE sensor106Mobile data
   collector114Data storage topics module122Data storage module
Z9 0
U1 0
U2 0
DA 2016-01-08
UT DIIDW:2016518842
ER

PT P
AU ANTON A
   BERNICO M
   CALVO L
   COOK M K
   KANNEGANTI V R
   PERSCHALL J
   SANIDAS T G
TI System for processing business operations            transactions and
   associated augmented customer data,            comprises multiple
   computer servers interconnected with            a software defined
   network through network switches,            controllers, and network
   interfaces
PN US9363322-B1
AE STATE FARM MUTUAL AUTOMOBILE INSURANCE
AB 
   NOVELTY - The system comprises multiple computer servers               
   interconnected with a software defined network                (112)
   through multiple network switches (110),                controllers, and
   network interfaces. The computer                servers are configured
   for economical large scale                computation and data storage
   with resilience                despite underpinning commodity hardware
   failure and                grow-shrink capacity changes of nodes and    
   associated interconnectivity. A federated database                has a
   distributed in-memory cache and multiple                log-structured
   merge tree databases for resilient                high-throughput data
   accumulation.
   USE - System for processing business operations               
   transactions and associated augmented customer data                using
   a big data architecture.
   ADVANTAGE - The system ensures function-to-data processing              
   to avoid data movement complexity for reduced                development
   time and cost. The federated database                design pattern
   provides eventual consistency to                provide availability
   that spans multiple data                centers without being dependent
   on database                log-based replication.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for processing business operations transactions and               
   associated augmented customer data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   web-scale grid for processing a web-scale               
   data.105Node106Central processing unit110Network switch111Software
   stack112Software defined network
Z9 0
U1 0
U2 0
DA 2016-01-08
UT DIIDW:201633699R
ER

PT P
AU MEACHAM J
   HARRIS M
   BRODMAN G
   CUTHRIELL L
   KORUS H
   TOTH B
   HSIAO J
   ELLIOT M
   SCHIMPF B
   GARLAND M
   NGUYEN E
TI Method for preserving history of derived datasets            at
   computing device for processing large amounts of            data,
   involves creating new-version of derived dataset            in context
   of successful transaction, and adding            new-build catalog entry
   to catalog
PN US2016125000-A1; US9483506-B2
AE PALANTIR TECHNOLOGIES INC
AB 
   NOVELTY - The method involves maintaining a build                catalog
   comprising a set of build catalog entries                at computing
   devices. A new-version of a particular                derived dataset is
   created in context of a                successful transaction. A
   new-build catalog entry                is added to the build catalog,
   where the new-build                catalog entry comprises an identifier
   of the                new-version of the particular derived dataset, and
   the identifier of the new-version of the particular               
   derived dataset is a transaction commit identifier               
   assigned to the successful transaction.
   USE - Method for preserving history of derived                datasets
   at a computing device for processing large                amounts of
   data in a data pipeline computer system.                Uses include but
   are not limited to network device,                mobile device, cell
   phone, smart phone, laptop                computer, desktop computer,
   workstation computer,                personal digital assistant, blade
   server, and                mainframe computer.
   ADVANTAGE - The method enables utilizing a history               
   preserving data pipeline system to be improved on               
   existing data pipeline systems by maintaining build               
   dependency data. The method enables utilizing the                data
   pipeline system to be provided with                heterogeneous data
   from multiple heterogeneous data                sources. The method
   enables utilizing a transaction                service to facilitate
   transactions on multiple                datasets on behalf of multiple
   clients.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the     
   following:a non-transitory storage medium comprising a               
   set of instructions for preserving history of                derived
   datasets at a computing device for                processing large
   amounts of data in a data pipeline                computer systema
   system for preserving history of derived                datasets at a
   computing device for processing large                amounts of data in
   a data pipeline computer                system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   computer-implemented process for preserving history                of a
   derived dataset.1002Step for storing first version of derived           
   dataset in containers1004Step for storing first build catalog           
   entry in build database1006Step for updating other dataset to           
   produce second version of latter dataset resulting                in
   data lake1008Step for storing second version of                derived
   dataset in containers1010Step for storing second build catalog          
        entry in build database
Z9 0
U1 0
U2 0
DA 2016-01-08
UT DIIDW:201626930T
ER

PT C
AU Devin, Florent
   Jourdan, Astrid
   Laiffly, Dominique
   Le Nir, Yannick
BE LeNguyen, M
   Vinh, LS
   Bui, LT
   Nguyen, VG
   Ong, YS
   Hirata, K
TI Cloud data architecture applied to urban management
SO 2016 EIGHTH INTERNATIONAL CONFERENCE ON KNOWLEDGE AND SYSTEMS
   ENGINEERING (KSE)
SE International Conference on Knowledge and Systems Engineering
BP 327
EP 332
DT Proceedings Paper
PD 2016
PY 2016
AB Open Data now provides access to updated data dedicated to urban
   development. The traditional approach of using production data for
   decision on land management needs to be rethought in the light of these
   new opportunities and should therefore addressed towards dynamic
   management of geographic information. Institutional sites such as INSEE
   semantically annotate their data facilitating the discovery of
   contextual data. Linked Open Data cloud network offers semantic access
   to a huge amount of thematic data including an important geographic
   subset. These new opportunities always generate a greater amount of
   usable data. Big Data tools, like Spark, make it easy to handle this
   amount unbounded data via conventional data-mining algorithms. We group
   all these issues under the generic name of Cloud Data that has sense
   only if it is integrated perfectly into a cloud computing environment
   for efficient processing and reactive application (SaaS application,
   Restfull services, D3JS library,...). We offer an example of a dedicated
   application for the Pau urban community in which all Cloud Data is
   integrated as a SaaS (Software as a Service).
CT 8th International Conference on Knowledge and Systems Engineering (KSE)
CY OCT 06-08, 2016
CL Hanoi, VIETNAM
SP Le Quy Don Tech Univ; Japan Adv Inst Sci & Technol; Univ Engn & Technol;
   IEEE
OI Jourdan, Astrid/0000-0003-3879-4216
ZS 0
ZB 0
ZR 0
Z8 0
ZA 0
TC 0
Z9 0
U1 0
U2 2
SN 2164-2508
BN 978-1-4673-8929-7
DA 2017-01-18
UT WOS:000390757200056
ER

PT C
AU Lavanya, K.
   Murali, G.
GP IEEE
TI Efficient Analytical Architecture for Sensor Networks Using Hadoop
SO PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND
   ELECTRONICS SYSTEMS (ICCES)
BP 322
EP 327
DT Proceedings Paper
PD 2016
PY 2016
AB Big Data is the new experience curve in the new economy driven by huge
   data with larger volume, velocity and variety. The real time application
   processing of remote sensing of large volumes of Big Data seems at
   first, and provide extracting and analysis in useful information with an
   effective manner leads a advanced system toward a novel computational
   challenges, such as to analyze, aggregate, and store, where data are
   remotely gathered. The three main units comprise the proposed
   architecture the three units are The proposed architecture can store raw
   data for the analysis of the offline data when required. The proposed
   architecture comprises three main units, such as 1) remote sensing Big
   Data acquisition unit (RSDU); 2) data processing unit (DPU); and 3) data
   analysis decision unit (DADU). First, RSDU acquires data from the
   satellite and sends this data to the Base Station, where initial
   processing takes place. Second, DPU plays a vital role in architecture
   for efficient processing of real-time Big Data by providing filtration,
   load balancing, and parallel processing. Third, DADU is the upper layer
   unit of The proposed structure, which is liable for compilation, storage
   of the results, and generation of determination founded on the outcome
   obtained from DPU. The proposed architecture has the ability of
   dividing, load balancing, and concurrent processing of only useful data.
   For that reason, it outcome in effectively analyzing real time remote
   sensing tremendous data utilizing earth observatory method.
   Additionally, the proposed big data architecture provides the capacity
   of storing and making analysis of incoming raw knowledge to perform
   offline evaluation on mostly stored dumps, when required. Ultimately, a
   targeted analysis of remotely sensed earth observatory tremendous data
   for land and sea field is supplied making use of Hadoop. In addition,
   more than a few algorithms are proposed for each and every stage of
   RSDU, DPU, and DADU to detect land as good as sea discipline to
   complicated the working of architecture.
CT International Conference on Communication and Electronics Systems
   (ICCES)
CY OCT 21-22, 2016
CL Coimbatore, INDIA
SP PPG Inst Technol; Elect Devices Soc; CFY; IEEE
RI Murali, Gunji/B-8441-2016
TC 0
ZS 0
ZR 0
ZA 0
ZB 0
Z8 0
Z9 0
U1 0
U2 1
BN 978-1-5090-1066-0
DA 2017-07-10
UT WOS:000403767600063
ER

PT J
AU Moon,, Su Jung
   Mu-Wook, Pyeon
AU 배상원
   이도림
   한상원
TI Big Data Architecture Design for the Development of Hyper Live Map (HLM)
SO JOURNAL OF THE KOREAN SOCIETY OF SURVEY,GEODESY,PHOTOGRAMMETRY, AND
   CARTOGRAPHY
S1 한국측량학회지
VL 34
IS 2
BP 207
EP 215
DI 10.7848/ksgpc.2016.34.2.207
DT research-article
PD 2016
PY 2016
AB The demand for spatial data service technologies is increasing lately
   with the development of realistic 3D spatial information services and
   ICT (Information and Communication Technology). Research is being
   conducted on the real-time provision of spatial data services through a
   variety of mobile and Web-based contents. Big data or cloud computing
   can be presented as alternatives to the construction of spatial data for
   the effective use of large volumes of data. In this paper, the process
   of building HLM (Hyper Live Map) using multi-source data to acquire
   stereo CCTV and other various data is presented and a big data service
   architecture design is proposed for the use of ﬂexible and scalable
   cloud computing to handle big data created by users through such media
   as social network services and black boxes. The provision of spatial
   data services in real time using big data and cloud computing will
   enable us to implement navigation systems, vehicle augmented reality,
   real-time 3D spatial information, and single picture based positioning
   above the single GPS level using low-cost image-based position
   recognition technology in the future. Furthermore, Big Data and Cloud
   Computing are also used for data collection and provision in U-City and
   Smart-City environment as well, and the big data service architecture
   will provide users with information in real time.
TC 0
ZS 0
Z8 0
ZR 0
ZA 0
ZB 0
Z9 0
U1 0
U2 1
SN 1598-4850
DA 2016-06-23
UT KJD:ART002106287
ER

PT B
AU O'Leary, Daniel E.
BE Hu, F
TI Some Issues of Privacy in a World of Big Data and Data Mining
SO BIG DATA: STORAGE, SHARING, AND SECURITY
BP 289
EP 302
DT Article; Book Chapter
PD 2016
PY 2016
AB This chapter examines privacy issues in the context of big data and
   potential data mining of that data. Issues are analyzed based on five
   emerging unique characterizations associated with big data: the five Vs,
   the Big Data Lake, "thing" data, the quantified self, repurposed data,
   and the generation of knowledge from unstructured communication data,
   e.g., Twitter Tweets. Each of those sets of emerging issues is analyzed
   in detail for their potential impact on privacy.
   As part of this analysis, this chapter identifies "unevenness" (of
   volume, velocity, variety and veracity) as potential concerns of privacy
   and equity. Further, this analysis notes that in some cases systems
   developed to "defend" individuals and organizations from privacy
   infringements, ironically can infringe on the privacy of "adjacent
   others."
   This analysis also notes the importance of governance in emerging
   structures such as the Big Data Lake and repurposed data. Finally, this
   chapter questions the ethical use of some data used to create knowledge
   from the equivalence of digital conversations.
RI O'Leary, Daniel/B-6469-2008
ZB 0
ZR 0
ZA 0
Z8 0
TC 0
ZS 0
Z9 0
U1 0
U2 5
BN 978-1-4987-3487-5; 978-1-4987-3486-8
DA 2016-10-05
UT WOS:000382859100012
D2 10.1201/b19694
ER

PT C
AU Rahman, Md. Abdur
BE Zhou, J
   Salvendy, G
TI Multi-sensory Cyber-Physical Therapy System for Elderly Monitoring
SO HUMAN ASPECTS OF IT FOR THE AGED POPULATION: HEALTHY AND ACTIVE AGING,
   ITAP 2016, PT II
SE Lecture Notes in Computer Science
VL 9755
BP 89
EP 100
DI 10.1007/978-3-319-39949-2_9
DT Proceedings Paper
PD 2016
PY 2016
AB This paper provides an overview of a multi-sensory cyber-physical
   therapy system suitable for old age people with physical impairments,
   which integrates entities in the physical as well as cyber world for
   therapy sensing, therapeutic data computation, interaction between cyber
   and physical world, and in-home therapy support through a cloud-based
   big data architecture. To provide appropriate therapeutic services and
   environment, the CPS uses a multi-modal multimedia sensory framework to
   support therapy recording and playback of a therapy session and
   visualization of effectiveness of an assigned therapy. The physical
   world interaction with the cyber world is stored as a rich gesture
   semantics with the help of multiple media streams, which is then
   uploaded to a tightly synchronized cyber physical cloud environment for
   deducing real-time and historical whole-body Range of Motion (ROM)
   kinematic data.
CT 2nd International Conference on Human Aspects of IT for the Aged
   Population (ITAP) Held as Part of 18th International Conference on
   Human-Computer Interaction (HCI International)
CY JUL 17-22, 2016
CL Toronto, CANADA
RI Abdur Rahman/AAG-9302-2019
OI Abdur Rahman/0000-0002-4105-0368
ZR 0
ZS 0
Z8 0
ZB 0
TC 0
ZA 0
Z9 0
U1 0
U2 2
SN 0302-9743
EI 1611-3349
BN 978-3-319-39948-5; 978-3-319-39949-2
DA 2017-01-04
UT WOS:000389803500009
ER

PT C
AU Rahman, Mohamed Abdur
GP IEEE
TI Demo Abstract: Gesture-based Cyber-Physical In-Home Therapy System in a
   Big Data Environment
SO 2016 ACM/IEEE 7TH INTERNATIONAL CONFERENCE ON CYBER-PHYSICAL SYSTEMS
   (ICCPS)
SE ACM-IEEE International Conference on Cyber-Physical Systems
DT Proceedings Paper
PD 2016
PY 2016
AB This demo provides an overview of a gesture-based cyber-physical therapy
   system, which integrates entities in the physical as well as cyber world
   for therapy sensing, therapeutic data computation, interaction between
   cyber and physical world, and holistic in-home therapy support through a
   cloud-based big data architecture. To provide appropriate therapeutic
   services and environment, the CPS uses a multi-modal multimedia sensory
   framework to support therapy recording and playback of a therapy session
   and visualization of effectiveness of an assigned therapy. The physical
   world interaction with the cyber world is stored as a rich gesture
   semantics with the help of multiple media streams, which is then
   uploaded to a tightly synchronized cyber physical cloud environment for
   deducing real-time and historical whole-body Range of Motion (ROM)
   kinematic data.
CT ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS)
CY APR 11-14, 2016
CL Vienna, AUSTRIA
SP ACM; IEEE
Z8 0
ZS 0
ZB 0
ZR 0
TC 0
ZA 0
Z9 0
U1 0
U2 3
SN 2375-8317
BN 978-1-5090-1772-0
DA 2016-11-16
UT WOS:000386348200010
ER

PT C
AU Stajcer, Marko
   Stajcer, Marko
   Orescanin, Drazen
BE Biljanovic, P
   Butkovic, Z
   Skala, K
   Grbac, TG
   CicinSain, M
   Sruk, V
   Ribaric, S
   Gros, S
   Vrdoljak, B
   Mauher, M
   Tijan, E
   Lukman, D
TI Using MEAN stack for development of GUI in real-time big data
   architecture
SO 2016 39TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION
   TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO)
BP 524
EP 529
DT Proceedings Paper
PD 2016
PY 2016
AB There are several key criteria for successful software development
   branch and one of them is architecture. We describe in detail
   architecture, design and development of graphic user interface for
   real-time fraud detection system in the telecom industry. Our focus is
   on a web application architecture, which includes data model component,
   technical infrastructure component, and components that interact or are
   associated in any way with the system (users, third party components).
CT 39th International Convention on Information and Communication
   Technology, Electronics and Microelectronics (MIPRO)
CY MAY 30-JUN 03, 2016
CL Opatija, CROATIA
SP MIPRO Croatian Soc; IEEE Region 8; Republ Croatia, Minist Sci Educ &
   Sports; Republ Croatia, Minist Maritime Affairs, Transport &
   Infrastructure; Republ Croatia, Minist Entrepreneurship & Crafts; Republ
   Croatia, Minist Public Adm; Croatian Chamber Economy; Univ Rijeka; Univ
   Zagreb; IEEE Croatia Sect; IEEE Croatia Sect Comp Chapter; IEEE Croatia
   Sect Electron Devices Solid State Circuits Joint Chapter; IEEE Croatia
   Sect Educ Chapter; IEEE Croatia Sect Commun Chapter; T Croatian Telecom;
   Ericsson Nikola Tesla; Koncar Elect Ind; HEP Croatian Elect Co; VIPnet;
   Univ Zagreb, Fac Elect Engn & Comp; Rudjer Boskovic Inst; Univ Rijeka,
   Fac Maritime Studies; Univ Rijeka, Fac Engn; Univ Rijeka, Fac Econ; Univ
   Zagreb, Fac Org & Informat; Univ Rijeka, Fac Tourism & Hospital
   Management; Polytechn Zagreb; EuroCloud Croatia; Croatian Regulatory
   Author Network Ind; Croatian Post; Erste&Steiermarkische bank; Selmet;
   CISEx; Kermas energija; Rezultanta; River Publishers; InfoDom; Hewlett
   Packard Croatia; IN2; Transmitters & Commun Co; Storm Comp; Nokia; King
   ICT; Microsoft Croatia; Micro Link; Mjerne tehnologije; Altpro; Danieli
   Automat; ib proCADD; Nomen
OI Oreščanin, ažen/0000-0002-0233-3971
Z8 0
ZB 0
ZS 0
TC 0
ZA 0
ZR 0
Z9 0
U1 0
U2 3
BN 978-953-233-088-5
DA 2017-02-01
UT WOS:000391360600084
ER

PT C
AU Wilder, Nathan
   Smith, Jared M.
   Mockus, Audris
GP IEEE
TI Exploring a Framework for Identity and Attribute Linking Across
   Heterogeneous Data Systems
SO 2016 IEEE/ACM 2ND INTERNATIONAL WORKSHOP ON BIG DATA SOFTWARE
   ENGINEERING (BIGDSE 2016)
BP 19
EP 25
DI 10.1145/2896825.2896833
DT Proceedings Paper
PD 2016
PY 2016
AB Online-activity-generated digital traces provide opportunities for novel
   services and unique insights as demonstrated in, for example, research
   on mining software repositories. The inability to link these traces
   within and among systems, such as Twitter, GitHub, or Reddit, inhibit
   the advances in this area. Furthermore, no single approach to integrate
   data from these disparate sources is likely to work. We aim to design
   Foreseer, an extensible framework, to design and evaluate identity
   matching techniques for public, large, and low accuracy operational
   data. Foreseer consists of three functionally independent components
   designed to address the issues of discovery and preparation, storage and
   representation, and analysis and linking of traces from disparate online
   sources. The framework includes a domain specific language for
   manipulating traces, generating insights, and building novel services.
   We have applied it in a pilot study of roughly 10TB of data from
   Twitter, Reddit, and StackExchange including roughly 6M distinct
   entities and, using basic matching techniques, found roughly 83,000
   matches among these sources. We plan to add additional entity extraction
   and identification algorithms, data from other sources, and design tools
   for facilitating dynamic ingestion and tagging of incoming data on a
   more robust infrastructure using Apache Spark or another distributed
   processing framework. We will then evaluate the utility and
   effectiveness of the framework in applications ranging from identifying
   malicious contributors in software repositories to the evaluation of the
   utility of privacy preservation schemes.
CT IEEE/ACM 2nd International Workshop on Big Data Software Engineering
   (BIGDSE)
CY MAY 16, 2016
CL Austin, TX
SP IEEE; ACM
RI Mockus, Auis/AEY-3361-2022; Smith, Jared/
OI Mockus, Auis/0000-0002-7987-7598; Smith, Jared/0000-0002-3240-2405
ZR 0
TC 0
ZS 0
ZB 0
Z8 0
ZA 0
Z9 0
U1 0
U2 3
BN 978-1-4503-4152-3
DA 2017-07-13
UT WOS:000404434900004
ER

PT C
AU Zhang, Leihan
   Zhao, Jichang
   Xu, Ke
BE Yang, BJ
   Chen, J
   Cai, XQ
   Qin, KD
   Zhou, C
TI Emotion-based Social Computing Platform for Streaming Big-data:
   Architecture and Application
SO 2016 13TH INTERNATIONAL CONFERENCE ON SERVICE SYSTEMS AND SERVICE
   MANAGEMENT
SE International Conference on Service Systems and Service Management
DT Proceedings Paper
PD 2016
PY 2016
AB Exploration of user generated content in the epoch of Web 2.0 brings
   unprecedented challenge to the social computing, which has to provide
   real-time solution in the circumstance of massive data volumes and
   evolving application scenarios. This paper presents an emotion-based
   social computing platform namely ESC for streaming big-data. The main
   aim of ESC is to provide sentiment analysis as the foundation of social
   computing and enable both real-time computation on streaming big-data
   and batch computation on off-line big-data with high performance and low
   risk. Different from conventional data processing technologies, ESC is
   designed as a scalable and QoS-optimized adaptive platform for
   developers to only focus on business models instead of being distracted
   by details of the computing infrastructure. In addition, continuous
   streaming computing is emphasized in ESC to keep tracking on long term
   dynamic evolution in social media, which can provide a valuable proxy
   for in-depth social analytics. The architecture of ESC is implemented by
   distributed storage, sentiment analysis, data parallelism and routing,
   real-time streaming computation, batch computation and distributed
   machine learning. And the evaluation results from real-time and batch
   computations testify the high performance and scalability of ESC.
   Moreover, a few applications based on it further demonstrates its
   usability in enacting on different streaming big-data and variety of
   social computations.
CT 13th International Conference on Service Systems and Service Management
   (ICSSSM)
CY JUN 24-26, 2016
CL Kunming Univ Sci & Technol, Sch Management & Econ, Kunming, PEOPLES R
   CHINA
HO Kunming Univ Sci & Technol, Sch Management & Econ
SP Tsinghua Univ; Inst Elect & Elect Engineers; IEEE Syst; Chinese Univ
   Hong Kong; S China Univ Technol
RI xu, ke/JSL-2305-2023
ZB 0
TC 0
ZS 0
Z8 0
ZA 0
ZR 0
Z9 0
U1 0
U2 4
SN 2161-1890
BN 978-1-5090-2842-9
DA 2017-01-11
UT WOS:000390104400182
ER

PT P
AU BEAK W
   SABHIKHI A
   SANCHEZ M
   SAXENA M
TI Method for using e.g. dark data, within cognitive            information
   processing system environment for            performing cognitive
   inference and learning operations,            involves providing
   information to cognitive inference            and learning system
PN US2015356436-A1; US10325211-B2
AE COGNITIVE SCALE INC
AB 
   NOVELTY - The method involves receiving data from a set               
   of data sources, where the set of data sources                comprises
   a public data source and a private data                source, the
   public data source comprises publicly                available travel
   information and the private data                source comprises
   privately managed company specific                travel information.
   Information is accessed from                the set of data sources
   through a cognitive data                management module. The
   information is provided to a                cognitive inference and
   learning system                (118).
   USE - Method for using hybrid data e.g. big data and                dark
   data, within a cognitive information                processing system
   environment for performing                cognitive inference and
   learning operations for                cognitive applications.
   ADVANTAGE - The method allows a cognitive graph to enable               
   automated agents to access a web more                intelligently,
   enumerate inferences through                utilization of curated
   structured data, and provide                answers to questions by a
   computational knowledge                engine. The method allows a
   cognitive cloud                management console sub-component to
   provide a user                visibility and management controls related
   to a                cloud analytics infrastructure component. The       
   method enables allowing a cluster management                platform to
   manage distributed hardware resources                into a single pool
   of resources that can be used by                application frameworks
   to efficiently manage                workload distribution for batch
   jobs and                long-running services.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a      
            data architecture.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of a client computer.100Information processing system112System
   memory116Operating system118Cognitive inference and learning            
      system142Service provider server
Z9 0
U1 0
U2 0
DA 2015-01-02
UT DIIDW:201578994L
ER

PT J
AU Landman,  Bennett
TI CAREER: Modeling Personalized Brain Development with Big Data
DT Awarded Grant
PD Feb 01 2015
PY 2015
AB Big data offer an opportunity to study specific control populations (age
   / sex / environmental factors / demographics / genetics) and identify
   substantive homogeneous sub-cohorts so that one may understand the roles
   that potential factors play in brain development, differentiating
   abnormal trajectories from normal development. The image processing,
   statistical, and informatics tools to effectively and efficiently use
   big data imaging archives for quantitative population-level research and
   personalized medicine do not yet exist. This research will enable
   discovery science on a scale considerably larger than routinely possible
   with traditional study designs by creating novel informatics resources
   that tie archives of 3-D images into accessible research databases. This
   research will discover genetic and environmental factors that influence
   an individual's brain development and characterize the developing human
   brain through personal developmental trajectories. To accomplish this
   goal, new informatics technologies will be created to enable (1) image
   processing and segmentation based on image content in the context of
   heterogeneous, low quality, and error prone data with minimal human
   oversight and (2) routine archival, query, and image processing of large
   medical imaging datasets. This research will impact the areas of (1)
   informatics via novel computation models, (2) neuroscience via a new
   structural model of brain development, and (3) public health via newly
   accessible data sets for research. The science and technology
   innovations enabled by using big data to understand personalized brain
   development will be communicated in a tiered method. Outreach to the
   K-12 audience will target conceptualizing design criteria, inspiring
   students with interactive demonstrations, and providing capabilities for
   students to apply key concepts in hands-on engineering projects. For
   advanced students and researchers, new accessible course materials and
   online modules will be developed so that others may build upon the
   foundations established by this research.<br/><br/>Novel software, data
   wrangling tools, and resources will be created through two research
   thrusts organized around a novel test bed infrastructure and synthesized
   in a third education/outreach thrust. Thrust 1 (Personal Brain
   Trajectories) will focus on extracting meaningful information from
   medical images when performed at scale through (1) creating automated
   methods robust to variations in image quality, acquisition, and transfer
   errors, and (2) enabling efficient human-in-loop control at scale. The
   research will extend novel statistical models for image content labeling
   while adapting quality control techniques from industrial engineering.
   Thrust 2 (Novel Storage & Processing) will create novel medical imaging
   data models to describe data acquisition / retrieval, storage, cleaning,
   access / security, query and processing by integrating of medical
   imaging standards with big data architecture derived from social network
   and e-commerce communities. This infrastructure will provide practical
   access to petabyte imaging archives, integrate with existing data
   workflows, and effectively function with commodity hardware. The PI will
   develop and release a reference test bed to evaluate new technologies in
   the context of computer-aided detection (CADe) of brain abnormalities
   while considering age, sex, and demographics. Using the test bed,
   researchers and students will be able to efficiently evaluate existing
   and emerging image processing software to screen for potential
   prognostic markers. In Thrust 3 (Education and Outreach), the research
   results will be integrated into two classes targeting undergraduate
   students and interactive online modules created and released through an
   established graduate student/faculty training program. Each summer, an
   undergraduate and high school student will participate in research by
   implementing and extending research contributions within an interactive
   demonstration platform. In the second through fifth summers, a high
   school teacher will assist in the development of curricula targeting
   high school students using the demonstration platform. High school
   students and teachers will be recruited from Nashville Metro schools
   with a high underrepresented minority / reduced cost lunch populations.
   These efforts will create an open-source, open-hardware system for
   public demonstration and K-12 classroom exercises.
RI Landman, Bennett/A-2343-2009
Z8 0
ZB 0
ZR 0
ZA 0
ZS 0
TC 0
Z9 0
U1 0
U2 0
G1 1452485
DA 2023-12-08
UT GRANTS:13589350
ER

PT C
AU Benchara, Fatema Zahra
   Youssfi, Mohamed
   Bouattane, Omar
   Ouajji, Hassan
GP IEEE
TI A Mobile Agent Team Works Model for HPC Big Data Analysis: Fuzzy Logic
   Application
SO 2015 5TH INTERNATIONAL CONFERENCE ON INFORMATION & COMMUNICATION
   TECHNOLOGY AND ACCESSIBILITY (ICTA)
DT Proceedings Paper
PD 2015
PY 2015
AB The aim of this paper is to present a distributed implementation of the
   Type-2 Fuzzy Logic (T2FL) algorithm in a mobile agent based distributed
   computing platform. The proposed algorithm is assigned to be implemented
   on an SPMD (Single Program Multiple Data) architecture which is based on
   a cooperative mobile agent model. It is constituted by a set of mobile
   agents as AVPEs (Agent Virtual Processing Elements) in order to improve
   the processing resources needed for performing the big data image
   segmentation. In this work we focused on the application of this
   algorithm to process the big data MRI (Magnetic Resonance Images) image.
   The input image is splitted into elementary images by the Mobile Team
   leader Agent and encapsulated one per AVPE. Each AVPE perform and
   exchange the segmentation results and maintain asynchronous
   communication with their Team leader agent until the convergence of this
   algorithm. The obtained experimental results in terms of accuracy and
   efficiency analysis of the proposed distributed implementation are
   achieved thanks to the mobile agents several interesting skills
   introduced in this distributed computational model.
CT 5th International Conference on Information and Communication Technology
   and Accessbility (ICTA)
CY DEC 21-23, 2015
CL Marrakech, MOROCCO
RI BOUATTANE, Omar/AAK-1235-2020; OUAJJI, Hassan/AAR-9929-2021; Youssfi, Mohamed/IAP-5429-2023
ZS 0
Z8 0
ZR 0
ZA 0
TC 0
ZB 0
Z9 0
U1 0
U2 0
BN *****************
DA 2016-09-13
UT WOS:000380450100046
ER

PT B
AU da Silva Costa, Carlos Filipe Machado
Z2  
TI BASIS: Uma Arquitetura de Big Data Para Smart CitiesBASIS: A Big Data
   Architecture for Smart Cities
DT Dissertation/Thesis
PD Jan 01 2015
PY 2015
ZA 0
ZR 0
ZS 0
ZB 0
TC 0
Z8 0
Z9 0
U1 0
U2 1
BN 9798368400068
UT PQDT:68419040
ER

PT C
AU Dayal, Umeshwar
   Gupta, Chetan
   Vennelakanti, Ravigopal
   Vieira, Marcos R.
   Wang, Song
BE Rabl, T
   Sachs, K
   Poess, M
   Baru, C
   Jacobson, HA
TI An Approach to Benchmarking Industrial Big Data Applications
SO BIG DATA BENCHMARKING, WBDB 2014
SE Lecture Notes in Computer Science
VL 8991
BP 45
EP 60
DI 10.1007/978-3-319-20233-4_6
DT Proceedings Paper
PD 2015
PY 2015
AB Through the increasing use of interconnected sensors, instrumentation,
   and smart machines, and the proliferation of social media and other open
   data, industrial operations and physical systems are generating ever
   increasing volumes of data of many different types. At the same time,
   advances in computing, storage, communication, and big data technologies
   are making it possible to collect, store, process, analyze and visualize
   enormous volumes of data at scale and at speed. The convergence of
   Operations Technology (OT) and Information Technology (IT), powered by
   innovative data analytics, holds the promise of using insights derived
   from these rich types of data to better manage our systems, resources,
   environment, health, social infrastructure, and industrial operations.
   Opportunities to apply innovative analytics abound in many industries
   (e.g., manufacturing, power distribution, oil and gas exploration and
   production, telecommunication, healthcare, agriculture, mining) and
   similarly in government (e.g., homeland security, smart cities, public
   transportation, accountable care). In developing several such
   applications over the years, we have come to realize that existing
   benchmarks for decision support, streaming data, event processing, or
   distributed processing are not adequate for industrial big data
   applications. One primary reason being that these benchmarks
   individually address narrow range of data and analytics processing needs
   of industrial big data applications. In this paper, we outline an
   approach we are taking to defining a benchmark that is motivated by
   typical industrial operations scenarios. We describe the main issues we
   are considering for the benchmark, including the typical data and
   processing requirements; representative queries and analytics operations
   over streaming and stored, structured and unstructured data; and the
   proposed simulator data architecture.
CT 5th International Workshop on Big Data Benchmarking (WBDB)
CY AUG 05-06, 2014
CL Potsdam, GERMANY
SP Hasso Plattner Inst; SAP SE; HP Suse; Intel; Mellanox; Pivotal; US Natl
   Sci Fdn
TC 0
ZA 0
ZB 0
Z8 0
ZS 0
ZR 0
Z9 0
U1 1
U2 25
SN 0302-9743
EI 1611-3349
BN 978-3-319-20233-4; 978-3-319-20232-7
DA 2015-11-18
UT WOS:000363781600006
ER

PT B
AU Johnson, Bruce
BE Marconi, K
   Lehmann, H
TI Big Data: Architecture and Its Enablement
SO BIG DATA AND HEALTH ANALYTICS
BP 155
EP 175
DT Article; Book Chapter
PD 2015
PY 2015
AB The concept of big data is just that: a concept for the value an
   organization can realize from in-depth analysis of all data. The concept
   of big data is therefore not a database or data architecture but is more
   the solutions that leverage any and all data, wherever they come from.
   In health care, the concepts of big data are enabled only in
   organizations that focus on data-capture, management, and usage. Health
   care data is extremely broad, deep, and complex, yet the needs for data
   access are even greater and ever evolving. To meet such needs, effective
   data architecture must be intertwined with a formal data governance
   program. This combination unlocks analytics and begins to leverage big
   data. It is emerging as a critical best practice to all health care
   analytics efforts.
Z8 0
ZA 0
ZR 0
ZS 0
ZB 0
TC 0
Z9 0
U1 0
U2 2
BN 978-1-4822-2925-7; 978-1-4822-2923-3
DA 2015-06-25
UT WOS:000353153000010
ER

PT C
AU Watanobe, Yutaka
   Mirenkov, Nikolay
BE Chu, W
   Kikuchi, S
   Bhalla, S
TI AIDA: A Language of Big Information Resources
SO DATABASES IN NETWORKED INFORMATION SYSTEMS (DNIS 2015)
SE Lecture Notes in Computer Science
VL 8999
BP 112
EP 121
DT Proceedings Paper
PD 2015
PY 2015
AB Some features of *AIDA language and its environment are provided to show
   a way for possible preparing well-organized information resources which
   are based on integrated-data architecture supporting searching,
   understanding and immediate re-use of the resources needed. A project of
   big information resources of the above mentioned type is presented and
   relations of users and resource unit owners within Global Knowledge
   Market are briefly considered. Some ideas behind knowledge and
   experience transfer with permanent re-evaluating resource unit values
   and examples of the resource types are also provided.
CT 10th International Workshop on Databases in Networked Information
   Systems (DNIS)
CY MAR 23-25, 2015
CL Univ Aizu, Aizuwakamatsu, JAPAN
HO Univ Aizu
SP Univ Aizu, Ctr Strategy Int Programs; Univ Aizu, Grad Dept Informat
   Technol & Project Management
ZS 0
ZR 0
ZA 0
ZB 0
TC 0
Z8 0
Z9 0
U1 0
U2 3
SN 0302-9743
BN 978-3-319-16313-0; 978-3-319-16312-3
DA 2015-10-15
UT WOS:000361698800008
ER

PT C
AU Zheng, Chengyu
   Huang, Liqun
   Huang, Xiaotao
BE Chang, L
   Guiran, C
   Zhen, L
TI An Improved ODS Platform on Large Enterprise-level
SO PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON MECHATRONICS,
   ELECTRONIC, INDUSTRIAL AND CONTROL ENGINEERING
SE AER-Advances in Engineering Research
VL 8
BP 559
EP 562
DT Proceedings Paper
PD 2015
PY 2015
AB With rapid development of big data applications, telecom operators pay
   more attention to the management and analysis of various data.
   Presently, as the telecom operators haven't built a uniform enterprise
   data platform, and application systems and internal data models differ
   from one another, the complexity of interfaces increases gradually,
   which restricts the ability of business systems to perform all their
   business supporting. But now, the ODS increases the maintenance
   efficiency, decreases IT operations and maintenance costs, improves the
   quality of supporting data, assures the operational revenues, improves
   the service quality, provides strong support for operating decision
   making through using the ETOM model, adopting SOA, applying ODS and EDW
   technical framework for full service operation and all professional
   integration according with the general principles, and introducing EAI
   and management mechanism of message control.
CT International Conference on Mechatronics, Electronic, Industrial and
   Control Engineering (MEIC)
CY APR 01-03, 2015
CL Shenyang, PEOPLES R CHINA
ZR 0
Z8 0
ZA 0
ZB 0
TC 0
ZS 0
Z9 0
U1 0
U2 2
SN 2352-5401
BN 978-94-62520-62-2
DA 2015-09-16
UT WOS:000359443600128
ER

PT B
AU Duggirala, Siddhartha
BA Raj, P
   Deka, GC
TI Big Data Architecture: Storage and Computation
SO HANDBOOK OF RESEARCH ON CLOUD INFRASTRUCTURES FOR BIG DATA ANALYTICS
SE Advances in Data Mining and Database Management (ADMDM) Book Series
BP 129
EP 156
DI 10.4018/978-1-4666-5864-6.ch006
DT Article; Book Chapter
PD 2014
PY 2014
AB With the unprecedented increase in data sources, the question of how to
   collect them efficiently, effectively, and elegantly, store them
   securely and safely, leverage those stocked, polished, and maintained
   data in a smarter manner so that industry experts can plan ahead, take
   informed decisions, and execute them in a knowledgeable fashion remains.
   This chapter clarifies several pertinent questions and related issues
   with the unprecedented increase in data sources.
ZS 0
ZA 0
ZB 0
Z8 0
ZR 0
TC 0
Z9 0
U1 0
U2 2
BN 978-1-4666-5865-3; 978-1-4666-5864-6
DA 2015-12-09
UT WOS:000363402500008
D2 10.4018/978-1-4666-5864-6
ER

PT J
AU Silva, Dilshan
Z2  
TI [not available]
DT Dissertation/Thesis
PD Feb 15 2023
PY 2023
ZA 0
ZB 0
Z8 0
ZR 0
TC 0
ZS 0
Z9 0
U1 0
U2 0
UT PQDT:67762557
ER

PT J
AU Zhang, Li Ping
Z2  
TI The research and accomplishment of basic data service interface for the
   decision-making condition-based maintenance system of power transformers
   based on soa
DT Dissertation/Thesis
PD Jan 01 2011
PY 2011
Z8 0
ZR 0
TC 0
ZA 0
ZS 0
ZB 0
Z9 0
U1 0
U2 0
UT PQDT:67102220
ER

PT C
AU Miller, Keith J.
   McLeod, Sarah
   Schroeder, Elizabeth
   Arehart, Mark
   Samuel, Kenneth
   Finley, James
   Jurica, Vanesa
   Polk, John
BE Calzolari, N
   Choukri, K
   Maegaard, B
   Mariani, J
   Odijk, J
   Piperidis, S
   Rosner, M
   Tapias, D
TI Improving Personal Name Search in the TIGR System
SO LREC 2010 - SEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND
   EVALUATION
DT Proceedings Paper
PD 2010
PY 2010
AB This paper describes the development and evaluation of enhancements to
   the specialized information retrieval capabilities of a multimodal
   reporting system. The system enables collection and dissemination of
   information through a distributed data architecture by allowing users to
   input free text documents, which are indexed for subsequent search and
   retrieval by other users. This unstructured data entry method is
   essential for users of this system, but it requires an intelligent
   support system for processing queries against the data. The system,
   known as TIGR ( Tactical Ground Reporting), allows keyword searching and
   geospatial filtering of results, but lacked the ability to efficiently
   index and search person names and perform approximate name matching. To
   improve TIGR's ability to provide accurate, comprehensive results for
   queries on person names we iteratively updated existing entity
   extraction and name matching technologies to better align with the TIGR
   use case. We evaluated each version of the entity extraction and name
   matching components to find the optimal configuration for the TIGR
   context, and combined those pieces into a named entity extraction,
   indexing, and search module that integrates with the current TIGR
   system. By comparing system-level evaluations of the original and
   updated TIGR search processes, we show that our enhancements to personal
   name search significantly improved the performance of the overall
   information retrieval capabilities of the TIGR system.
CT 7th International Conference on Language Resources and Evaluation (LREC)
CY MAY 17-23, 2010
CL Valletta, MALTA
SP CELI Language & Informat Technol; European Media Lab GmBH; Quaero; META
ZA 0
ZR 0
ZS 0
TC 0
Z8 0
ZB 0
Z9 0
U1 0
U2 3
BN 978-2-9517408-6-0
DA 2010-01-01
UT WOS:000356879505055
ER

PT J
AU CHUANG, JEFFREY HSU-MIN
TI Computational Sciences Shared Resource
DT Awarded Grant
PD Aug 01 1997
PY 2023
AB PROJECT SUMMARY COMPUTATIONAL SCIENCESThe JAX Computational Sciences
   (CS) Shared Resource is central to the achievement of the JAX
   CancerCenter's (JAXCC) scientific program objectives. As cancer research
   has become increasingly data intensive, itis vital that investigators be
   capable of interpreting and leveraging vast data sets, both publicly
   available andinternally generated, to understand tumor biology. Such
   data analysis requires access to a dynamic suite ofanalytical tools;
   infrastructure supporting those tools; computational, bioinformatic and
   statistical expertise tomine and analyze the data; and quantitative
   analysts and software engineers to develop and build queries
   andalgorithms. Established in 1998, CS has been operating as a shared
   resource within the JAXCC since 2001 butwas dramatically expanded in
   2013. The 41 member CS group addresses faculty needs by providing
   in-depthexpertise to JAXCC members in support of their independent
   research projects. This includes guidance inexperimental design; support
   for the integration of multi-platform data sets, data analysis software
   applicationsand database development; development and application of
   computational procedures, statistical methods andscientific software;
   and project management. CS also provides training and mentorship
   opportunities incomputational cancer research approaches and manages a
   plethora of analysis pipelines essential for cancergenomic research
   conducted by JAXCC members. Staff include a multi-disciplinary mix of
   computationalbiologists, computer scientists, statisticians,
   bioinformatics software engineers, and research project managers,who
   bring significant depth of expertise in cancer genomics, metabolomics,
   biostatistics, software development,machine learning, single cell
   genomics and integrative analysis, consistent with the needs of JAXCC
   members.CS' three operational groups (Statistics and Analysis,
   Scientific Computing, Research Project Management) arehoused on the Bar
   Harbor, ME and Farmington, CT campuses, and each supports JAXCC members
   on bothcampuses. Functioning in a modular manner, PIs can access the
   right mix of experienced expertise tailored totheir scientific needs.
   The Specific Aims for CS are: 1) To support JAXCC members in developing
   cutting-edgeanalytical procedures for emerging problems in cancer
   genomics, and to carry out integrative analysis infundamental and
   translational cancer research; 2) To develop bioinformatics
   applications, maintain scientificanalysis workflows, and provide data
   architecture and software engineering expertise for the development
   andmanagement of scientific data portals pertaining to specific
   scientific questions addressed by JAXCC members;and 3) To assist in
   resource planning for and management of complex computational projects
   and long-terminformation technology and data science development for
   JAXCC members.
TC 0
ZS 0
ZB 0
ZR 0
ZA 0
Z8 0
Z9 0
U1 0
U2 0
G1 10633078; 5P30CA034196-37; 7970; P30CA034196
DA 2024-07-25
UT GRANTS:17729994
ER

PT J
AU CHUANG, JEFFREY HSU-MIN
TI Computational Sciences Shared Resource
DT Awarded Grant
PD Aug 01 1997
PY 2022
AB PROJECT SUMMARY COMPUTATIONAL SCIENCESThe JAX Computational Sciences
   (CS) Shared Resource is central to the achievement of the JAX
   CancerCenter's (JAXCC) scientific program objectives. As cancer research
   has become increasingly data intensive, itis vital that investigators be
   capable of interpreting and leveraging vast data sets, both publicly
   available andinternally generated, to understand tumor biology. Such
   data analysis requires access to a dynamic suite ofanalytical tools;
   infrastructure supporting those tools; computational, bioinformatic and
   statistical expertise tomine and analyze the data; and quantitative
   analysts and software engineers to develop and build queries
   andalgorithms. Established in 1998, CS has been operating as a shared
   resource within the JAXCC since 2001 butwas dramatically expanded in
   2013. The 41 member CS group addresses faculty needs by providing
   in-depthexpertise to JAXCC members in support of their independent
   research projects. This includes guidance inexperimental design; support
   for the integration of multi-platform data sets, data analysis software
   applicationsand database development; development and application of
   computational procedures, statistical methods andscientific software;
   and project management. CS also provides training and mentorship
   opportunities incomputational cancer research approaches and manages a
   plethora of analysis pipelines essential for cancergenomic research
   conducted by JAXCC members. Staff include a multi-disciplinary mix of
   computationalbiologists, computer scientists, statisticians,
   bioinformatics software engineers, and research project managers,who
   bring significant depth of expertise in cancer genomics, metabolomics,
   biostatistics, software development,machine learning, single cell
   genomics and integrative analysis, consistent with the needs of JAXCC
   members.CS' three operational groups (Statistics and Analysis,
   Scientific Computing, Research Project Management) arehoused on the Bar
   Harbor, ME and Farmington, CT campuses, and each supports JAXCC members
   on bothcampuses. Functioning in a modular manner, PIs can access the
   right mix of experienced expertise tailored totheir scientific needs.
   The Specific Aims for CS are: 1) To support JAXCC members in developing
   cutting-edgeanalytical procedures for emerging problems in cancer
   genomics, and to carry out integrative analysis infundamental and
   translational cancer research; 2) To develop bioinformatics
   applications, maintain scientificanalysis workflows, and provide data
   architecture and software engineering expertise for the development
   andmanagement of scientific data portals pertaining to specific
   scientific questions addressed by JAXCC members;and 3) To assist in
   resource planning for and management of complex computational projects
   and long-terminformation technology and data science development for
   JAXCC members.
TC 0
Z8 0
ZA 0
ZR 0
ZS 0
ZB 0
Z9 0
U1 0
U2 0
G1 10382346; 5P30CA034196-36; 7970; P30CA034196
DA 2024-03-05
UT GRANTS:17446577
ER

PT J
AU CHUANG, JEFFREY HSU-MIN
TI Computational Sciences Shared Resource
DT Awarded Grant
PD Aug 01 1997
PY 2021
AB PROJECT SUMMARY COMPUTATIONAL SCIENCESThe JAX Computational Sciences
   (CS) Shared Resource is central to the achievement of the JAX
   CancerCenter's (JAXCC) scientific program objectives. As cancer research
   has become increasingly data intensive, itis vital that investigators be
   capable of interpreting and leveraging vast data sets, both publicly
   available andinternally generated, to understand tumor biology. Such
   data analysis requires access to a dynamic suite ofanalytical tools;
   infrastructure supporting those tools; computational, bioinformatic and
   statistical expertise tomine and analyze the data; and quantitative
   analysts and software engineers to develop and build queries
   andalgorithms. Established in 1998, CS has been operating as a shared
   resource within the JAXCC since 2001 butwas dramatically expanded in
   2013. The 41 member CS group addresses faculty needs by providing
   in-depthexpertise to JAXCC members in support of their independent
   research projects. This includes guidance inexperimental design; support
   for the integration of multi-platform data sets, data analysis software
   applicationsand database development; development and application of
   computational procedures, statistical methods andscientific software;
   and project management. CS also provides training and mentorship
   opportunities incomputational cancer research approaches and manages a
   plethora of analysis pipelines essential for cancergenomic research
   conducted by JAXCC members. Staff include a multi-disciplinary mix of
   computationalbiologists, computer scientists, statisticians,
   bioinformatics software engineers, and research project managers,who
   bring significant depth of expertise in cancer genomics, metabolomics,
   biostatistics, software development,machine learning, single cell
   genomics and integrative analysis, consistent with the needs of JAXCC
   members.CS' three operational groups (Statistics and Analysis,
   Scientific Computing, Research Project Management) arehoused on the Bar
   Harbor, ME and Farmington, CT campuses, and each supports JAXCC members
   on bothcampuses. Functioning in a modular manner, PIs can access the
   right mix of experienced expertise tailored totheir scientific needs.
   The Specific Aims for CS are: 1) To support JAXCC members in developing
   cutting-edgeanalytical procedures for emerging problems in cancer
   genomics, and to carry out integrative analysis infundamental and
   translational cancer research; 2) To develop bioinformatics
   applications, maintain scientificanalysis workflows, and provide data
   architecture and software engineering expertise for the development
   andmanagement of scientific data portals pertaining to specific
   scientific questions addressed by JAXCC members;and 3) To assist in
   resource planning for and management of complex computational projects
   and long-terminformation technology and data science development for
   JAXCC members.
ZA 0
TC 0
Z8 0
ZB 0
ZS 0
ZR 0
Z9 0
U1 0
U2 0
G1 10133000; 5P30CA034196-35; 7970; P30CA034196
DA 2024-03-01
UT GRANTS:17379759
ER

PT J
AU CHUANG, JEFFREY HSU-MIN
TI Computational Sciences Shared Resource
DT Awarded Grant
PD Jan 01 1800
PY 2020
AB PROJECT SUMMARY COMPUTATIONAL SCIENCESThe JAX Computational Sciences
   (CS) Shared Resource is central to the achievement of the JAX
   CancerCenter's (JAXCC) scientific program objectives. As cancer research
   has become increasingly data intensive, itis vital that investigators be
   capable of interpreting and leveraging vast data sets, both publicly
   available andinternally generated, to understand tumor biology. Such
   data analysis requires access to a dynamic suite ofanalytical tools;
   infrastructure supporting those tools; computational, bioinformatic and
   statistical expertise tomine and analyze the data; and quantitative
   analysts and software engineers to develop and build queries
   andalgorithms. Established in 1998, CS has been operating as a shared
   resource within the JAXCC since 2001 butwas dramatically expanded in
   2013. The 41 member CS group addresses faculty needs by providing
   in-depthexpertise to JAXCC members in support of their independent
   research projects. This includes guidance inexperimental design; support
   for the integration of multi-platform data sets, data analysis software
   applicationsand database development; development and application of
   computational procedures, statistical methods andscientific software;
   and project management. CS also provides training and mentorship
   opportunities incomputational cancer research approaches and manages a
   plethora of analysis pipelines essential for cancergenomic research
   conducted by JAXCC members. Staff include a multi-disciplinary mix of
   computationalbiologists, computer scientists, statisticians,
   bioinformatics software engineers, and research project managers,who
   bring significant depth of expertise in cancer genomics, metabolomics,
   biostatistics, software development,machine learning, single cell
   genomics and integrative analysis, consistent with the needs of JAXCC
   members.CS' three operational groups (Statistics and Analysis,
   Scientific Computing, Research Project Management) arehoused on the Bar
   Harbor, ME and Farmington, CT campuses, and each supports JAXCC members
   on bothcampuses. Functioning in a modular manner, PIs can access the
   right mix of experienced expertise tailored totheir scientific needs.
   The Specific Aims for CS are: 1) To support JAXCC members in developing
   cutting-edgeanalytical procedures for emerging problems in cancer
   genomics, and to carry out integrative analysis infundamental and
   translational cancer research; 2) To develop bioinformatics
   applications, maintain scientificanalysis workflows, and provide data
   architecture and software engineering expertise for the development
   andmanagement of scientific data portals pertaining to specific
   scientific questions addressed by JAXCC members;and 3) To assist in
   resource planning for and management of complex computational projects
   and long-terminformation technology and data science development for
   JAXCC members.
ZB 0
Z8 0
TC 0
ZS 0
ZR 0
ZA 0
Z9 0
U1 0
U2 0
G1 2P30CA034196-34; 9854056; P30CA034196
DA 2023-12-14
UT GRANTS:14986448
ER

EF