FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Asaithambi, Suriya Priya R.
   Venkatraman, Ramanathan
   Venkatraman, Sitalakshmi
TI MOBDA: Microservice-Oriented Big Data Architecture for Smart City
   Transport Systems
SO BIG DATA AND COGNITIVE COMPUTING
VL 4
IS 3
AR 17
DI 10.3390/bdcc4030017
DT Article
PD SEP 2020
PY 2020
AB Highly populated cities depend highly on intelligent transportation
   systems (ITSs) for reliable and efficient resource utilization and
   traffic management. Current transportation systems struggle to meet
   different stakeholder expectations while trying their best to optimize
   resources in providing various transport services. This paper proposes a
   Microservice-Oriented Big Data Architecture (MOBDA) incorporating data
   processing techniques, such as predictive modelling for achieving smart
   transportation and analytics microservices required towards smart cities
   of the future. We postulate key transportation metrics applied on
   various sources of transportation data to serve this objective. A novel
   hybrid architecture is proposed to combine stream processing and batch
   processing of big data for a smart computation of microservice-oriented
   transportation metrics that can serve the different needs of
   stakeholders. Development of such an architecture for smart
   transportation and analytics will improve the predictability of
   transport supply for transport providers and transport authority as well
   as enhance consumer satisfaction during peak periods.
RI Venkatraman, Sitalakshmi/R-3130-2018
OI Venkatraman, Sitalakshmi/0000-0002-2772-133X
ZS 0
Z8 0
TC 22
ZR 0
ZA 0
ZB 0
Z9 25
U1 0
U2 17
EI 2504-2289
DA 2021-10-02
UT WOS:000697677900002
ER

PT J
AU Pau, Marco
   Kapsalis, Panagiotis
   Pan, Zhiyu
   Korbakis, George
   Pellegrino, Dario
   Monti, Antonello
TI MATRYCS-A Big Data Architecture for Advanced Services in the Building
   Domain
SO ENERGIES
VL 15
IS 7
AR 2568
DI 10.3390/en15072568
DT Article
PD APR 2022
PY 2022
AB The building sector is undergoing a deep transformation to contribute to
   meeting the climate neutrality goals set by policymakers worldwide. This
   process entails the transition towards smart energy-aware buildings that
   have lower consumptions and better efficiency performance.
   Digitalization is a key part of this process. A huge amount of data is
   currently generated by sensors, smart meters and a multitude of other
   devices and data sources, and this trend is expected to exponentially
   increase in the near future. Exploiting these data for different use
   cases spanning multiple application scenarios is of utmost importance to
   capture their full value and build smart and innovative building
   services. In this context, this paper presents a high-level architecture
   for big data management in the building domain which aims to foster data
   sharing, interoperability and the seamless integration of advanced
   services based on data-driven techniques. This work focuses on the
   functional description of the architecture, underlining the requirements
   and specifications to be addressed as well as the design principles to
   be followed. Moreover, a concrete example of the instantiation of such
   an architecture, based on open source software technologies, is
   presented and discussed.
RI Kormpakis, Georgios/; Monti, Antonello/ABF-6760-2021; Pan, Zhiyu/KHV-3195-2024; Pellegrino, Dario/; Pau, Marco/ABE-1750-2020; Kapsalis, Panagiotis/
OI Kormpakis, Georgios/0000-0003-4052-4549; Monti,
   Antonello/0000-0003-1914-9801; Pan, Zhiyu/0000-0003-4949-2782;
   Pellegrino, Dario/0000-0001-8154-8710; Pau, Marco/0000-0002-4681-2317;
   Kapsalis, Panagiotis/0000-0002-5571-820X
Z8 0
ZB 0
ZA 0
ZR 0
TC 20
ZS 1
Z9 20
U1 0
U2 6
EI 1996-1073
DA 2022-04-24
UT WOS:000781280600001
ER

PT J
AU Morales-Botello, Maria Luz
   Gachet, Diego
   de Buenaga, Manuel
   Aparicio, Fernando
   Busto, Maria J.
   Ascanio, Juan Ramon
TI Chronic patient remote monitoring through the application of big data
   and internet of things
SO HEALTH INFORMATICS JOURNAL
VL 27
IS 3
AR 14604582211030956
DI 10.1177/14604582211030956
DT Article
PD JUL 2021
PY 2021
AB Chronic patients could benefit from the technological advances, but the
   clinical approaches for this kind of patients are still limited. This
   paper describes a system for chronic patients monitoring both, in home
   and external environments. For this purpose, we used novel technologies
   as big data, cloud computing and internet of things (IoT). Additionally,
   the system has been validated for three use cases: cardiovascular
   disease (CVD), hypertension (HPN) and chronic obstructive pulmonary
   disease (COPD), which were selected for their incidence in the
   population. This system is innovative within e-health, mainly due to the
   use of a big data architecture based on open-source components, also it
   provides a scalable and distributed environment for storage and
   processing of biomedical sensor data. The proposed system enables the
   incorporation of non-medical data sources in order to improve the
   self-management of chronic diseases and to develop better strategies for
   health interventions for chronic and dependents patients.
RI Gachet Páez, Diego/AGJ-3099-2022; Aparicio, Fernando/M-2822-2014; Buenaga, Manuel/AFI-6155-2022
OI Gachet Páez, Diego/0000-0001-6578-2275; 
ZA 0
ZS 0
ZB 1
ZR 0
TC 18
Z8 0
Z9 20
U1 1
U2 58
SN 1460-4582
EI 1741-2811
DA 2021-09-07
UT WOS:000691403000001
PM 34256646
ER

PT C
AU Rao, A. Ravishankar
   Clarke, Daniel
GP IEEE
TI A fully integrated open-source toolkit for mining healthcare big-data:
   architecture and applications
SO 2016 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI)
BP 255
EP 261
DI 10.1109/ICHI.2016.35
DT Proceedings Paper
PD 2016
PY 2016
AB We create an analytics toolkit based on open-source modules that
   facilitate the exploration of healthcare-related datasets. We illustrate
   our framework by providing a detailed analysis of physician and hospital
   ratings data. Our technique should prove valuable to software
   developers, big-data architects, hospital administrators, policy makers
   and patients.
   As an illustration of the capabilities of our toolkit, we examine a
   controversial issue in the medical field regarding the relationship
   between seniority of medical professionals and clinical outcomes. We use
   a publicly available dataset of national hospital ratings in the USA to
   suggest that there is no significant association between experience of
   medical professionals and hospital ratings as defined by the US
   government.
CT IEEE International Conference on Healthcare Informatics (ICHI)
CY OCT 04-07, 2016
CL Chicago, IL
SP IEEE; GE Digital; IEEE Comp Soc; NSF; Computers; Univ Illinois
OI Clarke, Daniel/0000-0003-3471-7416
ZA 0
TC 13
ZR 0
Z8 0
ZB 1
ZS 0
Z9 18
U1 0
U2 7
BN 978-1-5090-6117-4
DA 2016-01-01
UT WOS:000391422100034
ER

PT J
AU Deligiannis, Kimon
   Raftopoulou, Paraskevi
   Tryfonopoulos, Christos
   Platis, Nikos
   Vassilakis, Costas
TI Hydria: An Online Data Lake for Multi-Faceted Analytics in the Cultural
   Heritage Domain
SO BIG DATA AND COGNITIVE COMPUTING
VL 4
IS 2
AR 7
DI 10.3390/bdcc4020007
DT Article
PD JUN 2020
PY 2020
AB Advancements in cultural informatics have significantly influenced the
   way we perceive, analyze, communicate and understand culture. New data
   sources, such as social media, digitized cultural content, and Internet
   of Things (IoT) devices, have allowed us to enrich and customize the
   cultural experience, but at the same time have created an avalanche of
   new data that needs to be stored and appropriately managed in order to
   be of value. Although data management plays a central role in driving
   forward the cultural heritage domain, the solutions applied so far are
   fragmented, physically distributed, require specialized IT knowledge to
   deploy, and entail significant IT experience to operate even for trivial
   tasks. In this work, we present Hydria, an online data lake that allows
   users without any IT background to harvest, store, organize, analyze and
   share heterogeneous, multi-faceted cultural heritage data. Hydria
   provides a zero-administration, zero-cost, integrated framework that
   enables researchers, museum curators and other stakeholders within the
   cultural heritage domain to easily (i) deploy data acquisition services
   (like social media scrapers, focused web crawlers, dataset imports,
   questionnaire forms), (ii) design and manage versatile customizable data
   stores, (iii) share whole datasets or horizontal/vertical data shards
   with other stakeholders, (iv) search, filter and analyze data via an
   expressive yet simple-to-use graphical query engine and visualization
   tools, and (v) perform user management and access control operations on
   the stored data. To the best of our knowledge, this is the first
   solution in the literature that focuses on collecting, managing,
   analyzing, and sharing diverse, multi-faceted data in the cultural
   heritage domain and targets users without an IT background.
RI Tryfonopoulos, Christos/HKW-5651-2023; Vassilakis, Costas/AAH-5948-2019; Raftopoulou, Paraskevi/HKO-5623-2023
OI Tryfonopoulos, Christos/0000-0003-0640-9088; Vassilakis,
   Costas/0000-0001-9940-1821; Raftopoulou, Paraskevi/0000-0003-0663-4322
ZR 0
ZS 0
ZA 0
TC 14
Z8 0
ZB 0
Z9 17
U1 0
U2 13
EI 2504-2289
DA 2020-06-01
UT WOS:000697676000004
ER

PT C
AU Gupta, Maanak
   Patwa, Farhan
   Sandhu, Ravi
GP ACM
TI POSTER: Access Control Model for the Hadoop Ecosystem
SO PROCEEDINGS OF THE 22ND ACM SYMPOSIUM ON ACCESS CONTROL MODELS AND
   TECHNOLOGIES (SACMAT'17)
BP 125
EP 127
DI 10.1145/3078861.3084164
DT Proceedings Paper
PD 2017
PY 2017
AB Apache Hadoop is an important framework for fault-tolerant and
   distributed storage and processing of Big Data. Hadoop core platform
   along with other open-source tools such as Apache Hive, Storm, HBase
   offer an ecosystem to enable users to fully harness Big Data potential.
   Apache Ranger and Apache Sentry provide access control capabilities to
   several ecosystem components by offering centralized policy
   administration and enforcement through plugins. In this work we discuss
   the access control model for Hadoop ecosystem (referred as HeAC) used by
   Apache Ranger (release 0.6) and Sentry (release 1.7.0) along with Hadoop
   2.x native authorization capabilities. This multi-layer model provides
   several access enforcement points to restrict unauthorized users to
   cluster resources. We further outline some preliminary approaches to
   extend the HeAC model consistent with widely accepted access control
   models.
CT 22nd ACM Symposium on Access Control Models and Technologies (SACMAT)
CY JUN 21-23, 2017
CL Indianapolis, IN
SP Assoc Comp Machinery; ACM SIGSAC
RI Gupta, Maanak/ABD-4037-2020
OI Gupta, Maanak/0000-0001-9189-2478
Z8 1
ZR 0
TC 10
ZB 0
ZA 0
ZS 0
Z9 13
U1 0
U2 2
BN 978-1-4503-4702-0
DA 2017-01-01
UT WOS:000614039400014
ER

PT J
AU Silvestri, Stefano
   Tricomi, Giuseppe
   Bassolillo, Salvatore Rosario
   De Benedictis, Riccardo
   Ciampi, Mario
TI An Urban Intelligence Architecture for Heterogeneous Data and
   Application Integration, Deployment and Orchestration
SO SENSORS
VL 24
IS 7
AR 2376
DI 10.3390/s24072376
DT Article
PD APR 2024
PY 2024
AB This paper describes a novel architecture that aims to create a template
   for the implementation of an IT platform, supporting the deployment and
   integration of the different digital twin subsystems that compose a
   complex urban intelligence system. In more detail, the proposed Smart
   City IT architecture has the following main purposes: (i) facilitating
   the deployment of the subsystems in a cloud environment; (ii)
   effectively storing, integrating, managing, and sharing the huge amount
   of heterogeneous data acquired and produced by each subsystem, using a
   data lake; (iii) supporting data exchange and sharing; (iv) managing and
   executing workflows, to automatically coordinate and run processes; and
   (v) to provide and visualize the required information. A prototype of
   the proposed IT solution was implemented leveraging open-source
   frameworks and technologies, to test its functionalities and
   performance. The results of the tests performed in real-world settings
   confirmed that the proposed architecture could efficiently and easily
   support the deployment and integration of heterogeneous subsystems,
   allowing them to share and integrate their data and to select, extract,
   and visualize the information required by a user, as well as promoting
   the integration with other external systems, and defining and executing
   workflows to orchestrate the various subsystems involved in complex
   analyses and processes.
RI Silvestri, Stefano/IUP-0829-2023; Tricomi, Giuseppe/S-6029-2019; Ciampi, Mario/B-3874-2015; Bassolillo, Salvatore Rosario/IUN-2349-2023; De Benedictis, Riccardo/AAU-3550-2020
OI Silvestri, Stefano/0000-0002-9890-8409; Tricomi,
   Giuseppe/0000-0003-3837-8730; Ciampi, Mario/0000-0002-7286-6212;
   Bassolillo, Salvatore Rosario/0000-0002-0411-3729; De Benedictis,
   Riccardo/0000-0003-2344-4088
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
TC 11
Z9 12
U1 2
U2 7
EI 1424-8220
DA 2024-04-17
UT WOS:001201137000001
PM 38610587
ER

PT C
AU Goncalves, Andre
   Portela, Filipe
   Santos, Manuel Filipe
   Rua, Fernando
BE Shakshuki, E
TI Towards of a Real-time Big Data Architecture to Intensive Care
SO 8TH INTERNATIONAL CONFERENCE ON EMERGING UBIQUITOUS SYSTEMS AND
   PERVASIVE NETWORKS (EUSPN 2017) / 7TH INTERNATIONAL CONFERENCE ON
   CURRENT AND FUTURE TRENDS OF INFORMATION AND COMMUNICATION TECHNOLOGIES
   IN HEALTHCARE (ICTH-2017) / AFFILIATED WORKSHOPS
SE Procedia Computer Science
VL 113
BP 585
EP 590
DI 10.1016/j.procs.2017.08.294
DT Proceedings Paper
PD 2017
PY 2017
AB These days the exponential increase in the volume and variety of data
   stored by companies and organizations of various sectors of activity,
   has required to organizations the search for new solutions to improve
   their services and/or products, taking advantage of technological
   evolution. As a response to the inability of organizations to process
   large quantities and varieties of data, in the technological market,
   arise the Big Data. This emerging concept defined mainly by the volume,
   velocity and variety has evolved greatly in part by its ability to
   generate value for organizations in decision making. Currently, the
   health care sector is one of the five sectors of activity where the
   potential of Big Data growth most stands out. However, the way to go is
   still long and in fact there are few organizations, related to health
   care, that are taking advantage of the true potential of Big Data. The
   main target of this research is to produce a real-time Big Data
   architecture to the INTCare system, of the Centro Hospitalar do Porto,
   using the main open source big data solution, the Apache Hadoop. As a
   result of the first phase of this research we obtained a generic
   architecture who can be adopted by other Intensive Care Units. (c) 2017
   The Authors. Published by Elsevier B.V.
CT 8th International Conference on Emerging Ubiquitous Systems and
   Pervasive Networks (EUSPN) / 7th International Conference on Current and
   Future Trends of Information and Communication Technologies in
   Healthcare (ICTH)
CY SEP 18-20, 2017
CL Lund, SWEDEN
RI Gomes, André/KXQ-9620-2024; Santos, Manuel/ABD-3467-2020; Portela, Filipe/G-5324-2012
OI Santos, Manuel/0000-0002-5441-3316; Portela, Filipe/0000-0003-2181-6837
TC 5
ZB 0
Z8 1
ZR 0
ZA 0
ZS 1
Z9 11
U1 0
U2 4
SN 1877-0509
BN *****************
DA 2018-02-01
UT WOS:000419236500080
ER

PT J
AU Park, Sun
   Yang, Chan-Su
   Kim, JongWon
TI Design of Vessel Data Lakehouse with Big Data and AI Analysis Technology
   for Vessel Monitoring System
SO ELECTRONICS
VL 12
IS 8
AR 1943
DI 10.3390/electronics12081943
DT Article
PD APR 2023
PY 2023
AB The amount of data in the maritime domain is rapidly increasing due to
   the increase in devices that can collect marine information, such as
   sensors, buoys, ships, and satellites. Maritime data is growing at an
   unprecedented rate, with terabytes of marine data being collected every
   month and petabytes of data already being made public. Heterogeneous
   marine data collected through various devices can be used in various
   fields such as environmental protection, defect prediction,
   transportation route optimization, and energy efficiency. However, it is
   difficult to manage vessel related data due to high heterogeneity of
   such marine big data. Additionally, due to the high heterogeneity of
   these data sources and some of the challenges associated with big data,
   such applications are still underdeveloped and fragmented. In this
   paper, we propose the Vessel Data Lakehouse architecture consisting of
   the Vessel Data Lake layer that can handle marine big data, the Vessel
   Data Warehouse layer that supports marine big data processing and AI,
   and the Vessel Application Services layer that supports marine
   application services. Our proposed a Vessel Data Lakehouse that can
   efficiently manage heterogeneous vessel related data. It can be
   integrated and managed at low cost by structuring various types of
   heterogeneous data using an open source-based big data framework. In
   addition, various types of vessel big data stored in the Data Lakehouse
   can be directly utilized in various types of vessel analysis services.
   In this paper, we present an actual use case of a vessel analysis
   service in a Vessel Data Lakehouse by using AIS data in Busan area.
OI PARK, SUN/0000-0002-7371-2661; Yang, Chan-Su/0000-0002-6882-7325
TC 7
ZS 0
Z8 0
ZR 0
ZA 0
ZB 0
Z9 10
U1 1
U2 30
EI 2079-9292
DA 2023-05-15
UT WOS:000978958700001
ER

PT C
AU Belcao, Matteo
   Falzone, Emanuele
   Bionda, Enea
   Della Valle, Emanuele
BE Hotho, A
   Blomqvist, E
   Dietze, S
   Fokoue, A
   Ding, Y
   Barnaghi, P
   Haller, A
   Dragoni, M
   Alani, H
TI Chimera: A Bridge Between Big Data Analytics and Semantic Technologies
SO SEMANTIC WEB - ISWC 2021
SE Lecture Notes in Computer Science
VL 12922
BP 463
EP 479
DI 10.1007/978-3-030-88361-4_27
DT Proceedings Paper
PD 2021
PY 2021
AB In the last decades, Knowledge Graph (KG) empowered analytics have been
   used to extract advanced insights from data. Several companies
   integrated legacy relational databases with semantic technologies using
   Ontology-Based Data Access (OBDA). In practice, this approach enables
   the analysts to write SPARQL queries both over KGs and SQL relational
   data sources by making transparent most of the implementation details.
   However, the volume of data is continuously increasing, and a growing
   number of companies are adopting distributed storage platforms and
   distributed computing engines. There is a gap between big data and
   semantic technologies. Ontop, one of the reference OBDA systems, is
   limited to legacy relational databases, and the compatibility with the
   big data analytics engine Apache Spark is still missing. This paper
   introduces Chimera, an open-source software suite that aims at filling
   such a gap. Chimera enables a new type of round-tripping data science
   pipelines. Data Scientists can query data stored in a data lake using
   SPARQL through Ontop and SparkSQL while saving the semantic results of
   such analysis back in the data lake. This new type of pipelines
   semantically enriches data from Spark before saving them back.
CT 20th International Semantic Web Conference (ISWC)
CY OCT 24-28, 2021
CL ELECTR NETWORK
SP IBM; Google; Metaphacts; Oracle; Franz Inc; ebay; Internet & Data Lab;
   imec
OI Belcao, Matteo/0009-0004-4771-1931; Bionda, Enea/0000-0001-7016-7269
TC 10
Z8 0
ZS 0
ZR 0
ZA 0
ZB 0
Z9 10
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-030-88361-4; 978-3-030-88360-7
DA 2021-10-29
UT WOS:000706991800027
ER

PT J
AU Pastor-Galindo, Javier
   Sandlin, Hong-An
   Marmol, Felix Gomez
   Bovet, Gerome
   Perez, Gregorio Martinez
TI A Big Data architecture for early identification and categorization of
   dark web sites
SO FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
VL 157
BP 67
EP 81
DI 10.1016/j.future.2024.03.025
EA MAR 2024
DT Article
PD AUG 2024
PY 2024
AB The dark web has become notorious for its association with illicit
   activities and there is a growing need for systems to automate the
   monitoring of this space. This paper proposes an end -to -end scalable
   architecture for the continuous early identification of new Tor sites
   and the daily analysis of their content. The solution is built using an
   Open Source Big Data stack for data serving with Kubernetes, Kafka,
   Kubeflow, and MinIO, continuously discovering onion addresses in
   different sources (threat intelligence, code repositories, web -Tor
   gateways, and Tor repositories), downloading the HTML from Tor and
   deduplicating the content using MinHash LSH, and categorizing with the
   BERTopic modeling (SBERT embedding, UMAP dimensionality reduction,
   HDBSCAN document clustering and c-TF-IDF topic keywords). In 93 days,
   the system identified 80,049 onion services and characterized 90% of
   them, addressing the challenge of Tor volatility. A disproportionate
   amount of repeated content is found, with only 6.1% unique sites. From
   the HTML files of the dark sites, 31 different low -topics are
   extracted, manually labeled, and grouped into 11 high-level topics. The
   five most popular included sexual and violent content, repositories and
   search engines, carding, cryptocurrencies, and marketplaces. During the
   experiments, we identified 14 sites with 13,946 clones that shared a
   suspiciously similar mirroring rate per day, suggesting an extensive
   common phishing network. Among the related works, this study is the most
   representative characterization of onion services based on topics to
   date.
RI Perez, Gregorio/I-7620-2013; Mármol, Félix/A-7505-2016
ZB 1
TC 8
ZS 0
ZA 0
ZR 0
Z8 0
Z9 9
U1 6
U2 19
SN 0167-739X
EI 1872-7115
DA 2024-05-19
UT WOS:001219503100001
ER

PT C
AU Levandoski, Justin
   Casto, Garrett
   Deng, Mingge
   Desai, Rushabh
   Edara, Pavan
   Hottelier, Thibaud
   Hormati, Amir
   Johnson, Anoop
   Johnson, Jeff
   Kurzyniec, Dawid
   McVeety, Sam
   Ramanathan, Prem
   Saxena, Gaurav
   Shanmugam, Vidya
   Volobuev, Yuri
GP ACM
TI BigLake: BigQuery's Evolution toward a Multi-Cloud Lakehouse
SO COMPANION OF THE 2024 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA,
   SIGMOD-COMPANION 2024
SE International Conference on Management of Data
BP 334
EP 346
DI 10.1145/3626246.3653388
DT Proceedings Paper
PD 2024
PY 2024
AB BigQuery's cloud-native disaggregated architecture has allowed Google
   Cloud to evolve the system to meet several customer needs across the
   analytics and AI/ML workload spectrum. A key customer requirement for
   BigQuery centers around the unification of data lake and enterprise data
   warehousing workloads. This approach combines: (1) the need for core
   data management primitives, e.g., security, governance, common runtime
   metadata, performance acceleration, ACID transactions, provided by an
   enterprise data warehouses coupled with (2) harnessing the flexibility
   of the open source format and analytics ecosystem along with new
   workload types such as AI/ML over unstructured data on object storage.
   In addition, there is a strong requirement to support BigQuery as a
   multi-cloud offering given cloud customers are opting for a multi-cloud
   footprint by default.
   This paper describes BigLake, an evolution of BigQuery toward a
   multi-cloud lakehouse to address these customer requirements in novel
   ways. We describe three main innovations in this space. We first present
   BigLake tables, making open-source table formats (e.g., Apache Parquet,
   Iceberg) first class citizens, providing fine-grained governance
   enforcement and performance acceleration over these formats to BigQuery
   and other open-source analytics engines. Next, we cover the design and
   implementation of BigLake Object tables that allow BigQuery to integrate
   AI/ML for inferencing and processing over unstructured data. Finally, we
   present Omni, a platform for deploying BigQuery on non-GCP clouds,
   focusing on the infrastructure and operational innovations we made to
   provide an enterprise lakehouse product regardless of the cloud provider
   hosting the data.
CT ACM International Conference on Management of Data (SIGMOD)
CY JUN 09-15, 2024
CL Santiago, CHILE
SP Assoc Comp Machinery; ACM SIGMOD
RI , Justin Levandoski/; Edara, Pavan/; Casto, Garrett/; Deng, Mingge/; , Anoop Johnson/; Hormati, Amir/; Desai, Rushabh/; Saxena, Gaurav/MHR-3400-2025; Johnson, Jeff Jeffrey/
OI , Justin Levandoski/0009-0005-7033-0528; Edara,
   Pavan/0009-0001-8943-140X; Casto, Garrett/0009-0008-8808-1779; Deng,
   Mingge/0009-0005-0497-8203; , Anoop Johnson/0009-0006-0891-0166;
   Hormati, Amir/0009-0002-5786-3301; Desai, Rushabh/0009-0001-7406-8433;
   Johnson, Jeff Jeffrey/0009-0009-9657-3121
Z8 0
ZA 0
ZR 0
ZS 0
ZB 0
TC 6
Z9 9
U1 3
U2 6
SN 0730-8078
BN 979-8-4007-0422-2
DA 2024-08-29
UT WOS:001267334100029
ER

PT J
AU Paolucci, Francesco
   Sgambelluri, Andrea
   Silva, Moises Felipe
   Pacini, Alessandro
   Castoldi, Piero
   Valcarenghi, Luca
   Cugini, Filippo
TI Peer-to-peer disaggregated telemetry for autonomic
   machine-learning-driven transceiver operation
SO JOURNAL OF OPTICAL COMMUNICATIONS AND NETWORKING
VL 14
IS 8
BP 606
EP 620
DI 10.1364/JOCN.456666
DT Article
PD AUG 2022
PY 2022
AB Autonomic networking and monitoring will drive the evolution of next
   generation software defined networking (SDN) optical networks towards
   the zero touch networking paradigm. Optical telemetry services will play
   a key role to enable advanced network awareness at device and component
   granularity. Optical disaggregation is pushing the adoption of open
   models, enabling multi-vendor interoperability, including telemetry.
   Moreover, due to whitebox programmability and the adoption of open
   source micro services, it is becoming feasible to monitor data streams
   from optical devices related to quality of transmission key performance
   indicators. Finally, due to mature big data analytics platforms,
   including machine learning and artificial intelligence, the telemetry
   data lake is processed to effectively detect network anomalies. However,
   current centralized telemetry architectures are prone to scalability
   issues, suboptimal soft failure recovery due to operational mode
   limitations, and/or the inability of the SDN controller of tuning finer
   or proprietary transmission parameters. Conversely, a number of soft
   failures might be detected and recovered directly at the optical card
   transmitter, often in a hitless fashion, also relying on optimized
   vendor-proprietary configurations. The paper proposes what we believe to
   be a novel peer-to-peer telemetry (P2PT) service ready for next
   generation digital coherent optics cards, for local processing and soft
   failure recovery at the transceiver agent level. The P2PT architecture,
   workflow, and subscription extensions are conceived to enable direct and
   fast recovery at the transceiver level, resorting to optical signal
   retuning and adaptations. Experimental evaluations, including
   lightweight machine learning detection at the card agent, are provided
   in a multi-vendor disaggregated optical network testbed to assess
   different soft failure use cases and P2PT service scalability. (C) 2022
   Optica Publishing Group
RI Cugini, Filippo/; Valcarenghi, Luca/ABG-4647-2020; Sgambelluri, Anea/; Pacini, Alessano/; Paolucci, Francesco/AAA-3284-2022; Castoldi, Piero/A-2169-2012
OI Cugini, Filippo/0000-0002-9840-0365; Sgambelluri,
   Anea/0000-0001-7435-0458; Pacini, Alessano/0000-0001-5092-6061;
   Paolucci, Francesco/0000-0003-4821-5193; 
ZR 0
ZB 0
Z8 0
ZA 0
ZS 0
TC 9
Z9 9
U1 0
U2 9
SN 1943-0620
EI 1943-0639
DA 2022-07-13
UT WOS:000821580200001
ER

PT J
AU Mostefaoui, Ahmed
   Merzoug, Mohammed Amine
   Haroun, Amir
   Nassar, Anthony
   Dessables, Francois
TI Big data architecture for connected vehicles: Feedback and application
   examples from an automotive group
SO FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
VL 134
BP 374
EP 387
DI 10.1016/j.future.2022.04.020
EA MAY 2022
DT Article
PD SEP 2022
PY 2022
AB Nowadays, using their onboard built-in sensors and communication
   devices, connected vehicles (CVs) can perform numerous measurements
   (speed, temperature, fuel consumption, etc.) and transmit them, in a
   real-time fashion, to dedicated infrastructure, usually via 4G/5G
   wireless communications. This raises many opportunities to develop new
   innovative telematics services, including, among others, driver safety,
   customer experience, location-based services and infotainment. Indeed,
   it is expected that there will be roughly 2 billion connected cars by
   the end of 2025 on the world's roadways, where each of which can produce
   up to 30 terabytes of data per day. Managing this big automotive data,
   in real and batch modes, imposes tight constraints on the underlying
   data management platform. To contribute to this research area, in this
   paper, we report on a real, in-production automotive big data platform;
   specifically, the one deployed by Groupe PSA (a French car manufacturer
   known also as Peugeot-Citroen). In particular, we present the
   technologies and open-source products used within the different
   components of this CV platform to gather, store, process, and leverage
   big automotive data. The proposed architecture is then assessed through
   realistic experiments, and the obtained results are reported and
   analyzed. Finally, we also provide examples of deployed automotive
   applications and reveal the implementation details of one of them (an
   eco-driving service). (c) 2022 Elsevier B.V. All rights reserved.
RI Mostefaoui, Ahmed/; Merzoug, Mohammed Amine/AAI-1281-2020
OI Mostefaoui, Ahmed/0000-0001-8863-4080; Merzoug, Mohammed
   Amine/0000-0002-5316-6456
ZA 0
ZR 0
TC 7
ZB 0
Z8 0
ZS 1
Z9 8
U1 2
U2 21
SN 0167-739X
EI 1872-7115
DA 2022-06-18
UT WOS:000808123100001
ER

PT C
AU Ben Hamadou, Hamdi
   Pedersen, Torben Bach
   Thomsen, Christian
BE Wu, XT
   Jermaine, C
   Xiong, L
   Hu, XH
   Kotevska, O
   Lu, SY
   Xu, WJ
   Aluru, S
   Zhai, CX
   Al-Masri, E
   Chen, ZY
   Saltz, J
TI The Danish National Energy Data Lake: Requirements, Technical
   Architecture, and Tool Selection
SO 2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
BP 1523
EP 1532
DI 10.1109/BigData50022.2020.9378368
DT Proceedings Paper
PD 2020
PY 2020
AB Renewable Energy Sources such as wind and solar do not emit CO2 but
   their production vary considerably depending on time and weather. Thus,
   it is important to use the flexibility in device loads to shift energy
   consumption to follow the production. For example, an Electrical Vehicle
   (EV) can be charged very flexibly between arriving home at 5PM and
   leaving again at 7AM. Utilizing all available energy flexibility
   requires applying machine learning and AI on massive amounts of Big Data
   from many different actors and devices, ranging from private consumers,
   over companies, to energy network operators, and using this to create
   digital solutions to enable and exploit flexibility. The project
   Flexible Energy Denmark (FED) is building the foundation for this for
   the entire Danish society. Specifically, FED collects data from a number
   of Living Labs (LLs) in representative real-life physical environments.
   The data is stored in the Danish National Energy Data Lake, called FED
   Data Lake (FEDDL) to enable efficient and advanced analysis. FEDDL is
   built using only open source tools which can run both on-premise and in
   cloud settings. In this paper, we describe the requirements for FEDDL
   based on a representative LL case study, present its technical
   architecture, and provide a comparison of relevant tools along with the
   arguments for which ones we selected.
CT 8th IEEE International Conference on Big Data (Big Data)
CY DEC 10-13, 2020
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc; IBM; Ankura
OI Thoen, Christian/0000-0002-0862-0509; Pedersen, Torben
   Bach/0000-0002-1615-777X; Ben Hamadou, Hai/0000-0002-4140-4584
TC 4
ZR 0
ZS 0
Z8 0
ZB 0
ZA 0
Z9 5
U1 1
U2 10
SN 2639-1589
BN 978-1-7281-6251-5
DA 2021-07-25
UT WOS:000662554701080
ER

PT J
AU Ch'ng, Eugene
TI Social information landscapes Automated mapping of large multimodal,
   longitudinal social networks
SO INDUSTRIAL MANAGEMENT & DATA SYSTEMS
VL 115
IS 9
SI SI
BP 1724
EP 1751
DI 10.1108/IMDS-02-2015-0055
DT Article
PD 2015
PY 2015
AB Purpose - The purpose of this paper is to present a Big Data solution as
   a methodological approach to the automated collection, cleaning,
   collation, and mapping of multimodal, longitudinal data sets from social
   media. The paper constructs social information landscapes (SIL).
   Design/methodology/approach - The research presented here adopts a Big
   Data methodological approach for mapping user-generated contents in
   social media. The methodology and algorithms presented are generic, and
   can be applied to diverse types of social media or user-generated
   contents involving user interactions, such as within blogs, comments in
   product pages, and other forms of media, so long as a formal data
   structure proposed here can be constructed.
   Findings - The limited presentation of the sequential nature of content
   listings within social media and Web 2.0 pages, as viewed on web
   browsers or on mobile devices, do not necessarily reveal nor make
   obvious an unknown nature of the medium; that every participant, from
   content producers, to consumers, to followers and subscribers, including
   the contents they produce or subscribed to, are intrinsically connected
   in a hidden but massive network. Such networks when mapped, could be
   quantitatively analysed using social network analysis (e.g.
   centralities), and the semantics and sentiments could equally reveal
   valuable information with appropriate analytics. Yet that which is
   difficult is the traditional approach of collecting, cleaning,
   collating, and mapping such data sets into a sufficiently large sample
   of data that could yield important insights into the community structure
   and the directional, and polarity of interaction on diverse topics. This
   research solves this particular strand of problem.
   Research limitations/implications - The automated mapping of extremely
   large networks involving hundreds of thousands to millions of nodes,
   encapsulating high resolution and contextual information, over a long
   period of time could possibly assist in the proving or even disproving
   of theories. The goal of this paper is to demonstrate the feasibility of
   using automated approaches for acquiring massive, connected data sets
   for academic inquiry in the social sciences.
   Practical implications - The methods presented in this paper, together
   with the Big Data architecture can assist individuals and institutions
   with a limited budget, with practical approaches in constructing SIL.
   The software-hardware integrated architecture uses open source software,
   furthermore, the SIL mapping algorithms are easy to implement.
   Originality/value - The majority of research in the literature uses
   traditional approaches for collecting social networks data. Traditional
   approaches can be slow and tedious; they do not yield adequate sample
   size to be of significant value for research. Whilst traditional
   approaches collect only a small percentage of data, the original methods
   presented here are able to collect and collate entire data sets in
   social media due to the automated and scalable mapping techniques.
RI Ch'ng, Eugene/Q-8277-2019
OI Ch'ng, Eugene/0000-0003-3992-8335
ZB 0
ZA 0
ZR 0
TC 5
Z8 0
ZS 0
Z9 5
U1 0
U2 8
SN 0263-5577
EI 1758-5783
DA 2016-02-24
UT WOS:000369166900010
ER

PT C
AU Chen, Fei
   Yan, Zhengzheng
   Gu, Liang
BE Chen, J
   He, D
   Lu, R
TI Towards Low-Latency Big Data Infrastructure at Sangfor
SO EMERGING INFORMATION SECURITY AND APPLICATIONS, EISA 2022
SE Communications in Computer and Information Science
VL 1641
BP 37
EP 54
DI 10.1007/978-3-031-23098-1_3
DT Proceedings Paper
PD 2022
PY 2022
AB As a top cybersecurity vendor, Sangfor needs collects log streams from
   thousands of endpoint detection devices such as NTA, STA, EDR and
   identifies security threats in real-time way everyday. The discovery and
   disposal of network security incidents are highly real-time in nature
   with seconds or even milliseconds response time to prevent possible
   cyber attacks and data leaks. In order to extract more valuable
   information, the log streams are analyzed using stream processing with
   pattern matching like CEP (Complex Event Processing) in memory, and then
   stored in a persistent storage systems such as a data ware-house system
   or a search engine system for data scientists and network security
   engineers to do OLAP (Online Analytical Processing). Sangfor needs to
   build a low-latency big data platform to meet the challenges of massive
   logs.
   More and more open source systems are proposed to solve the problem of
   data processing in a certain aspect. Many decisions must be made to
   balance the benefits when designing a real-time big data infrastructure.
   What's more, how to architecture these systems and construct a one-stack
   unified big data platform have been the key obstacles for big data
   analytics. In this paper, we present the overall architecture of our
   low-latency big data infrastructure and identify four important design
   decisions i.e. message queue, stream processing, OLAP, and data lake. We
   analyze the advantages and disadvantages of existing open source system
   and clarify the reason behind our choices. We also describe the
   improvements and optimizations to make the open-source stacks fit in
   Sangfor's environments, including designing a real-time development
   platform based on Flink and re-architecting Apache Kylin, Clickhouse and
   Presto as a HOLAP system. Then we highlight two important use cases to
   verify the rationality of our infrastructure.
CT 3rd International Conference on Emerging Information Security and
   Applications (EISA)
CY OCT 29-30, 2022
CL Wuhan, PEOPLES R CHINA
SP Cent China Normal Univ, Sch Comp Sci
Z8 0
ZR 0
TC 2
ZS 0
ZA 0
ZB 0
Z9 3
U1 1
U2 4
SN 1865-0929
EI 1865-0937
BN 978-3-031-23097-4; 978-3-031-23098-1
DA 2023-05-09
UT WOS:000972252300003
ER

PT C
AU Chen, Zhe
   Shao, Hangyu
   Li, Yuping
   Lu, Hongru
   Jin, Jiahui
GP IEEE
TI Policy-Based Access Control System for Delta Lake
SO 2022 TENTH INTERNATIONAL CONFERENCE ON ADVANCED CLOUD AND BIG DATA, CBD
SE International Conference on Advanced Cloud and Big Data
BP 60
EP 65
DI 10.1109/CBD58033.2022.00020
DT Proceedings Paper
PD 2022
PY 2022
AB Delta lake is a new generation of data storage solutions. It stores both
   transaction log and data files in one directory, and provides ACID
   transactions, scalable metadata handling, and unifies streaming and
   batch data processing on top of existing data lakes, such as S3, ADLS,
   GCS, and HDFS. Different from data warehouses, delta lakes allow data to
   be stored in the original format, retain complete data information, and
   provide efficient and low-cost storage solutions for data computing and
   analysis businesses. However, Since Delta Lake metadata is scattered in
   different resource files, the lack of a unified metadata view increases
   the difficulty of data governance. Also, Delta Lake adopts an open
   source storage system as the underlying storage, and its basic access
   control does not isolate different users, which may lead the risk of
   data leakage. At present, most common storage systems use data tables'
   row and column fields for access control, while delta lake treats the
   file group as an object. In this paper, aiming at the difficulty of data
   governance, we design a data lake metadata management method to achieve
   unified and efficient management of metadata information in
   heterogeneous data. Then, we design a policy-based data lake access
   control mechanism, combined with the open source permission framework,
   and complete the access request for different users and roles in Delta
   Lake.
CT 10th International Conference on Advanced Cloud and Big Data (CBD)
CY NOV 04-05, 2022
CL Guilin, PEOPLES R CHINA
SP Guangxi Normal Univ; Guilin Univ Technol; SE Univ
RI Jin, Jiahui/JPX-2144-2023
ZB 0
ZS 0
ZA 0
Z8 0
ZR 0
TC 1
Z9 3
U1 1
U2 6
SN 2573-301X
BN 979-8-3503-0971-3
DA 2023-06-01
UT WOS:000976903900011
ER

PT C
AU Ceaparu, Catalin
BE Dzitac, I
   Filip, FG
   Manolescu, MJ
   Dzitac, S
   Kacprzyk, J
   Oros, H
TI IT Solutions for Big Data Processing and Analysis in the Finance and
   Banking Sectors
SO INTELLIGENT METHODS IN COMPUTING, COMMUNICATIONS AND CONTROL
SE Advances in Intelligent Systems and Computing
VL 1243
BP 133
EP 144
DI 10.1007/978-3-030-53651-0_11
DT Proceedings Paper
PD 2021
PY 2021
AB This paper aims to give a general overview of the technologies used by
   two important trends in Business Intelligence nowadays, that continue to
   reshape the Data Architecture landscape worldwide. Bringing equally
   relevant value to businesses today, Fast Data and Big Data complete each
   other in order to enable both quick/short term as well as thorough/long
   term commercial strategies of companies, regardless of the industry they
   are part of. The body and conclusion of this paper will focus on the
   benefits of using the newest FinTech solutions for both aforementioned
   data processing models, while clearly stating the differences between
   the two. Both open source and proprietary type of solutions will be
   presented with the purpose to offer a thorough picture as to what the
   best architectural landscape of Big Data analytics should look like.
CT 8th International Conference on Computers Communications and Control
   (ICCCC)
CY MAY 11-15, 2020
CL ELECTR NETWORK
OI Ceaparu, Catalin/0000-0003-3113-5568
Z8 0
ZR 0
ZA 0
ZS 0
ZB 0
TC 1
Z9 3
U1 1
U2 10
SN 2194-5357
EI 2194-5365
BN 978-3-030-53651-0
DA 2021-04-19
UT WOS:000621675100011
ER

PT C
AU Freymann, Andreas
   Maier, Florian
   Schaefer, Kristian
   Boehnel, Tom
BE Wills, G
   Kacsuk, P
   Chang, V
TI Tackling the Six Fundamental Challenges of Big Data in Research Projects
   by Utilizing a Scalable and Modular Architecture
SO PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON INTERNET OF THINGS,
   BIG DATA AND SECURITY (IOTBDS)
BP 249
EP 256
DI 10.5220/0009388602490256
DT Proceedings Paper
PD 2020
PY 2020
AB Over the last decades the necessity for processing and storing huge
   amounts of data has increased enormously, especially in the fundamental
   research area. Beside the management of large volumes of data, research
   projects are facing additional fundamental challenges in terms of data
   velocity, data variety and data veracity to create meaningful data
   value. In order to cope with these challenges solutions exist. However,
   they often show shortcomings in adaptability, usability or have high
   licence fees. Thus, this paper proposes a scalable and modular
   architecture based on open source technologies using micro-services
   which are deployed using Docker. The proposed architecture has been
   adopted, deployed and tested within a current research project. In
   addition, the deployment and handling is compared with another
   technology. The results show an overcoming of the fundamental challenges
   of processing huge amounts of data and the handling of Big Data in
   research projects.
CT 5th International Conference on Internet of Things, Big Data and
   Security (IoTBDS)
CY MAY 07-09, 2020
CL Prague, CZECH REPUBLIC
OI Freymann, Aneas/0000-0002-3735-4545; Schaefer,
   Kristian/0000-0002-7855-6741
Z8 0
ZS 0
ZB 0
ZA 0
TC 3
ZR 0
Z9 3
U1 1
U2 8
BN 978-989-758-426-8
DA 2021-02-23
UT WOS:000615960700026
ER

PT C
AU Tangsatjatham, Pittayut
   Nupairoj, Natawut
GP IEEE
TI Hybrid Big Data Architecture for High-Speed Log Anomaly Detection
SO 2016 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER SCIENCE AND
   SOFTWARE ENGINEERING (JCSSE)
SE International Joint Conference on Computer Science and Software
   Engineering
BP 538
EP 543
DT Proceedings Paper
PD 2016
PY 2016
AB Log processing can be very challenging, especially for environments with
   lots of servers. In these environments, log data is large, coming at
   high-speed, and have various formats, the classic case of big data
   problem. This makes anomaly detection very difficult due to the fact
   that to get good accuracy, large amount of data must be processed in
   real-time. To solve this problem, this paper proposes a hybrid
   architecture for log anomaly detection using Apache Spark for data
   processing and Apache Flume for data collecting. To demonstrate the
   capabilities of our proposed solution, we implement a SARIMA-based
   anomaly detection as a case study. The experimental results clearly
   indicated that our proposed architecture can support log processing in
   large-scale environment effectively.
CT 13th International Joint Conference on Computer Science and Software
   Engineering (JCSSE)
CY JUL 13-15, 2016
CL Khon Kaen, THAILAND
ZA 0
TC 3
ZR 0
ZB 0
ZS 0
Z8 0
Z9 3
U1 0
U2 1
SN 2372-1642
BN 978-1-5090-2033-1
DA 2016-12-21
UT WOS:000389036200097
ER

PT C
AU Ciangottini, Diego
   Boccali, Tommaso
   Ceccanti, Andrea
   Spiga, Daniele
   Salomoni, Davide
   Tedeschi, Tommaso
   Tracolli, Mirco
BE Biscarat, C
   Campana, S
   Hegner, B
   Roiser, S
   Rovelli, CI
   Stewart, GA
TI First experiences with a portable analysis infrastructure for LHC at
   INFN
SO 25TH INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY AND NUCLEAR
   PHYSICS, CHEP 2021
SE EPJ Web of Conferences
VL 251
AR 02045
DI 10.1051/epjconf/202125102045
DT Proceedings Paper
PD 2021
PY 2021
AB The challenges proposed by the HL-LHC era are not limited to the sheer
   amount of data to be processed: the capability of optimizing the
   analyser's experience will also bring important benefits for the LHC
   communities, in terms of total resource needs, user satisfaction and in
   the reduction of end time to publication. At the Italian National
   Institute for Nuclear Physics (INFN) a portable software stack for
   analysis has been proposed, based on cloud-native tools and capable of
   providing users with a fully integrated analysis environment for the CMS
   experiment. The main characterizing traits of the solution consist in
   the user-driven design and the portability to any cloud resource
   provider. All this is made possible via an evolution towards a
   "python-based" framework, that enables the usage of a set of open-source
   technologies largely adopted in both cloud-native and data-science
   environments. In addition, a "single sign on"-like experience is
   available thanks to the standards-based integration of INDIGO-IAM with
   all the tools. The integration of compute resources is done through the
   customization of a JupyterHUB solution, able to spawn identity-aware
   user instances ready to access data with no further setup actions. The
   integration with GPU resources is also available, designed to sustain
   more and more widespread ML based workflow. Seamless connections between
   the user UI and batch/big data processing framework (Spark, HTCondor)
   are possible. Eventually, the experiment data access latency is reduced
   thanks to the integrated deployment of a scalable set of caches, as
   developed in the context of ESCAPE project, and as such compatible with
   the future scenarios where a data-lake will be available for the
   research community.
   The outcome of the evaluation of such a solution in action is presented,
   showing how a real CMS analysis workflow can make use of the
   infrastructure to achieve its results.
CT 25th International Conference on Computing in High Energy and Nuclear
   Physics (CHEP)
CY MAY 17-21, 2021
CL CERN, ELECTR NETWORK
HO CERN
RI Tedeschi, Tommaso/OMM-4578-2025; Tedeschi, Tommaso/ABB-8842-2021; Ciangottini, Diego/HJH-5914-2023
OI Tedeschi, Tommaso/0000-0002-7125-2905; Tedeschi,
   Tommaso/0000-0002-7125-2905; 
ZB 0
ZA 0
Z8 0
TC 1
ZS 0
ZR 0
Z9 2
U1 0
U2 0
SN 2100-014X
BN *****************
DA 2021-01-01
UT WOS:001329391600053
ER

PT C
AU Revathy, P.
   Mukesh, Rajeswari
BE Niranjan, SK
   Kurian, MZ
   Siddappa, M
TI Analysis of Big Data Security Practices
SO PROCEEDINGS OF THE 2017 3RD INTERNATIONAL CONFERENCE ON APPLIED AND
   THEORETICAL COMPUTING AND COMMUNICATION TECHNOLOGY (ICATCCT)
BP 264
EP 267
DT Proceedings Paper
PD 2017
PY 2017
AB In modern world, huge amount of data is common across all businesses
   which aim to unlock new economy from these sources. Hadoop was developed
   to analyze large scale data repository in a parallel computing
   architecture. The main task in this process is to handle this "Big Data"
   by applying proper strategies. So, present industry is focusing on the
   methods in which this "Big Data" can be used for their business growth.
   There's no suspicion that the setup of Data Lake on hadoop can provide a
   new way of analytics and intuition analysis. Beyond experimentations and
   POCs, today Hadoop is considered more into production. As we are moving
   towards the stage where Hadoop is considered for real-time production
   scenarios and major chunk of the production data is normally sensitive,
   or subject to many control measures, it becomes high priority to
   consider the security aspects in hadoop before deciding on Hadoop
   installation for any enterprise. This paper evaluates various issues in
   Hadoop ecosystem and its popular distributions by top big data players
   in the market. It further intends to investigate and compare the current
   security features being provided in those big data distributions along
   with other open source big data security solutions to help in building
   secure big data environment.
CT 3rd International Conference on Applied and Theoretical Computing and
   Communication Technology (iCATccT)
CY DEC 21-23, 2017
CL Sri Siddhartha Inst Technol, Tumkur, INDIA
HO Sri Siddhartha Inst Technol
SP IEEE Bangalore Sect; IEEE; IEEE Consumer Elect Soc, Malaysia Chapter
RI HITS, Hindustan Institute of Technology and Science/; Mukesh, Rajeswari/HNQ-5487-2023
OI HITS, Hindustan Institute of Technology and Science/0009-0004-3570-2675;
   
ZR 0
ZA 0
ZS 0
TC 2
ZB 0
Z8 0
Z9 2
U1 0
U2 0
BN 978-1-5386-1144-9
DA 2018-07-18
UT WOS:000437178100049
ER

PT J
AU Bohar, Balazs
   Fazekas, David
   Madgwick, Matthew
   Csabai, Luca
   Olbei, Marton
   Korcsmaros, Tamas
   Szalay-Beko, Mate
TI Sherlock: an open-source data platform to store, analyze and integrate
   Big Data for biology.
SO F1000Research
VL 10
BP 409
EP 409
DI 10.12688/f1000research.52791.1
DT Journal Article
PD 2021
PY 2021
AB In the era of Big Data, data collection underpins biological research
   more so than ever before. In many cases this can be as time-consuming as
   the analysis itself, requiring downloading multiple different public
   databases, with different data structures, and in general, spending days
   before answering any biological questions. To solve this problem, we
   introduce an open-source, cloud-based big data platform, called Sherlock
   ( https://earlham-sherlock.github.io/). Sherlock provides a gap-filling
   way for biologists to store, convert, query, share and generate biology
   data, while ultimately streamlining bioinformatics data management. The
   Sherlock platform provides a simple interface to leverage big data
   technologies, such as Docker and PrestoDB. Sherlock is designed to
   analyse, process, query and extract the information from extremely
   complex and large data sets. Furthermore, Sherlock is capable of
   handling different structured data (interaction, localization, or
   genomic sequence) from several sources and converting them to a common
   optimized storage format, for example to the Optimized Row Columnar
   (ORC). This format facilitates Sherlock's ability to quickly and easily
   execute distributed analytical queries on extremely large data files as
   well as share datasets between teams. The Sherlock platform is freely
   available on Github, and contains specific loader scripts for structured
   data sources of genomics, interaction and expression databases. With
   these loader scripts, users are able to easily and quickly create and
   work with the specific file formats, such as JavaScript Object Notation
   (JSON) or ORC. For computational biology and large-scale bioinformatics
   projects, Sherlock provides an open-source platform empowering data
   management, data analytics, data integration and collaboration through
   modern big data technologies.
TC 1
ZA 0
ZR 0
ZB 1
Z8 0
ZS 0
Z9 1
U1 0
U2 0
EI 2046-1402
DA 2022-12-20
UT MEDLINE:36533093
PM 36533093
ER

PT C
AU Sawadogo, Pegdwende N.
   Darmont, Jerome
BE Golfarelli, M
   Wrembel, R
   Kotsis, G
   Tjoa, AM
   Khalil, I
TI Benchmarking Data Lakes Featuring Structured and Unstructured Data with
   DLBench
SO BIG DATA ANALYTICS AND KNOWLEDGE DISCOVERY (DAWAK 2021)
SE Lecture Notes in Computer Science
VL 12925
BP 15
EP 26
DI 10.1007/978-3-030-86534-4_2
DT Proceedings Paper
PD 2021
PY 2021
AB In the last few years, the concept of data lake has become trendy for
   data storage and analysis. Thus, several approaches have been proposed
   to build data lake systems. However, these proposals are difficult to
   evaluate as there are no commonly shared criteria for comparing data
   lake systems. Thus, we introduce DLBench, a benchmark to evaluate and
   compare data lake implementations that support textual and/or tabular
   contents. More concretely, we propose a data model made of both textual
   and CSV documents, a workload model composed of a set of various tasks,
   as well as a set of performance-based metrics, all relevant to the
   context of data lakes. As a proof of concept, we use DLBench to evaluate
   an open source data lake system we previously developed.
CT 23rd International Conference on Big Data Analytics and Knowledge
   Discovery (DaWaK)
CY SEP 27-30, 2021
CL ELECTR NETWORK
SP Software Competence Ctr Hagenberg; JKU Inst Telecooperat; Web Applicat
   Soc
OI Sawadogo, Pegdwendé N/0000-0001-6180-5476
TC 1
ZA 0
Z8 0
ZB 0
ZR 0
ZS 0
Z9 1
U1 0
U2 5
SN 0302-9743
EI 1611-3349
BN 978-3-030-86534-4; 978-3-030-86533-7
DA 2022-12-04
UT WOS:000886498500002
ER

PT J
AU Hachad, Tarik
   Sadiq, Abdelalim
   Ghanimi, Fadoua
TI A New Big Data Architecture for Real-Time Student Attention Detection
   and Analysis
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 11
IS 8
BP 241
EP 247
DT Article
PD AUG 2020
PY 2020
AB Big Data technologies and their analytical methods can help improve the
   quality of education. They can be used to process and analyze classroom
   video streams to predict student attention, this would greatly improve
   the learning-teaching experience. With the increasing number of students
   and the expansion of educational institutions, processing and analyzing
   video streams in real-time become a complicated issue. In this paper, we
   have reviewed the existing systems of student attention detection,
   open-source real-time data stream processing technologies, and the two
   major data stream processing architectures. We also proposed a new Big
   Data architecture for real-time student attention detection.
OI HACHAD, TARIK/0000-0001-9230-8982
Z8 0
ZS 0
TC 0
ZB 0
ZA 0
ZR 0
Z9 1
U1 1
U2 7
SN 2158-107X
EI 2156-5570
DA 2020-09-28
UT WOS:000568849600032
ER

PT C
AU Cardenas, Irvin Steve
   Paladugula, Pradeep Kumar
   Kim, Jong-Hoon
BE Chakrabarti, S
   Paul, R
   Gill, B
   Gangopadhyay, M
   Poddar, S
TI Large Scale Distributed Data Processing for a Network of Humanoid
   Telepresence Robots
SO 2020 IEEE INTERNATIONAL IOT, ELECTRONICS AND MECHATRONICS CONFERENCE
   (IEMTRONICS 2020)
BP 136
EP 144
DT Proceedings Paper
PD 2020
PY 2020
AB We present an open-source data lake architecture implemented to store
   and process data from robotic systems at large scale. In particular, we
   leverage our architecture for the use case of processing data from a
   network of humanoid telepresence robotic avatars that are controlled by
   human operators wearing immersive telepresence control suits. Our
   architecture leverages well-established open-source technologies and
   integrates into existing robot frameworks and middleware such as Robot
   Operating System (ROS) and Data Distribution Service (DDS).
CT IEEE International IOT, Electronics and Mechatronics Conference
   (IEMTRONICS)
CY SEP 09-12, 2020
CL Vancouver, INDIA
SP IEEE; Inst Engn & Management; IEEE Vancouver Sect; SMART; Univ Engn &
   Management
TC 1
ZA 0
ZR 0
ZS 0
Z8 0
ZB 0
Z9 1
U1 0
U2 0
BN 978-1-7281-9615-2
DA 2021-06-16
UT WOS:000655001800025
ER

PT J
AU Nupairoj, Natawut
   Tangsatjatham, Pittayut
TI Hybrid Big Data Architecture for High-Speed Log Anomaly Detection
SO JOURNAL OF INTERNET TECHNOLOGY
VL 18
IS 7
BP 1681
EP 1688
DI 10.6138/JIT.2017.18.7.20170419d
DT Article; Proceedings Paper
PD 2017
PY 2017
AB Anomaly detection in network traffic can be very challenging, especially
   for environments with high-speed networks and lots of servers. In these
   environments, log data of network traffic is usually large, coming at
   highspeed, and have various formats, the classic case of big data
   problem. This makes anomaly detection very difficult due to the fact
   that to get good accuracy, large amount of data must be processed in
   real-time. To solve this problem, this paper proposes a hybrid
   architecture for network traffic anomaly detection using popular big
   data framework including Apache Spark and Apache Flume. To demonstrate
   the capabilities of our proposed solution, we implement a SARIMA-based
   anomaly detection as a case study. The experimental results clearly
   indicated that our proposed architecture allows anomaly detection with
   good accuracy in large-scale environment effectively.
CT 13th International Joint Conference on Computer Science and Software
   Engineering (JCSSE)
CY JUL 13-15, 2016
CL Khon Kaen, THAILAND
ZS 0
ZB 0
ZR 0
TC 1
Z8 0
ZA 0
Z9 1
U1 0
U2 15
SN 1607-9264
EI 2079-4029
DA 2018-02-08
UT WOS:000423459000021
ER

PT P
AU SHANG H
   GUAN W
   LU M
   WANG Y
   CAO Z
   PU A
   YANG H
   DU R
TI Method for publishing online data interface            configuration,
   involves performing interface            configuration encapsulation
   process based on online            configuration table and query engines
   corresponding to            storage components to obtain target
   interface, and            scheduling workflow of target interface
PN CN120234058-A
AE CHINA EVERBRIGHT BANK CO LTD CREDIT CARD
AB 
   NOVELTY - The method involves displaying an application               
   page through a human-computer interaction interface                in
   response to an online interface application                request,
   where the application page is utilized for                configuring an
   interface parameter required by an                online interface. An
   online configuration table is                determined corresponding to
   the online interface                based on the interface parameter in
   response to a                submission request for the application
   page. A                checking result of the online configuration table
   is obtained. Authorization and resource allocation               
   processes are performed on an access tenant of the                online
   interface if the checking result is passed.                Interface
   configuration encapsulation process is                performed based on
   the online configuration table                and a preset rule
   expression engine, a                configuration constraint frame, a
   micro-service                framework and query engines corresponding
   to                different storage components to obtain a target       
   interface. A workflow of the target interface is               
   scheduled.
   USE - Method for publishing an online data interface               
   configuration.
   ADVANTAGE - The method enables improving publishing               
   efficiency and flexibility of an online data                interface.
   DETAILED DESCRIPTION - The application page comprises an interface      
   use, an access tenant, a cluster type, a cluster                number,
   a library, a table, an inquiry engine, a                common inquiry
   condition, an inquiry field, a                processing logic and an
   access user amount. The                storage components comprise HBase
   (RTM:                open-source, column-oriented distributed database  
   system in a Hadoop environment), Elasticsearch                (RTM:
   real-time distributed and open source                full-text search
   and analytics engine), CLICKHOUSE                (RTM: open-source
   column-oriented database                management system), Kudu (RTM:
   distributed columnar                storage engine), Hadoop (RTM: open
   source                framework) distributed file storage system (HDFS) 
   /Hive (RTM: data warehouse system), data Paimon                (RTM:
   open-source Lakehouse storage framework)/Hudi                (RTM: open
   data lakehouse platform), StarRocks                (RTM: open-source
   high performing analytical                database)/Doris (RTM: modern
   data warehouse for                real-time analytics), distributed
   cache Redis (RTM:                Database management system), relational
   database                storing MySQL (RTM: relational database
   management                system), Oracle (RTM: relational database     
   management system) and EverDB (RTM: embedded                database
   system). The query engines comprise                Phoenix java database
   connectivity (JDBC) (RTM:                application program interface)
   query encapsulation,                HBase (RTM: open-source,
   column-oriented                distributed database system in a Hadoop  
   environment) application programming interface                (API)
   encapsulation, Elasticsearch (RTM: real-time                distributed
   and open source full-text search and                analytics engine)
   API interface query                encapsulation, CLICKHOUSE (RTM:
   open-source                column-oriented database management system)  
   interface encapsulation, Impala interface                encapsulation,
   Presto (RTM: distributed query                engine for big data using
   the SQL query                language)/Trino (RTM: highly parallel and  
   distributed query engine) interface encapsulation,               
   StarRocks (RTM: open-source high performing                analytical
   database)/Doris (RTM: modern data                warehouse for real-time
   analytics)universal                interface encapsulation, MySQL (RTM:
   relational                database management system) universal
   interface                encapsulation, Oracle (RTM: relational database
   management system) universal interface                encapsulation,
   EverDB (RTM: embedded database                system) universal
   interface encapsulation and                distributed cache Redis
   universal interface                encapsulation. The micro-service
   framework is a                Spring-Boot (RTM: microservice-based
   framework) for                packaging the target interface into an
   executable                Java archive (JAR) file (RTM: package file
   format)                file.INDEPENDENT CLAIMS are included for: (1) an 
   online data interface configuration publishing                device;
   (2) an electronic device comprising a                storage medium and
   a processor for executing                machine-readable instructions
   for performing a                process of publishing an online data
   interface                configuration; and (3) a computer-readable
   storage                medium storing a computer program for executing a
   process of publishing an online data interface               
   configuration by a processor.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for publishing an online data
   interface                configuration. (Drawing includes non-English   
               language text).
Z9 0
U1 0
U2 0
DA 2025-08-12
UT DIIDW:202569537C
ER

PT P
AU WANG H
   LIN C
   WANG Y
   ZHANG Z
   ZHU C
   SUN S
   LI W
   MIAO X
   CHEN H
   YAO S
   WU Y
   DONG R
   TIAN Y
TI Method for entering multi-source heterogeneous            energy data
   into data lake, involves obtaining            multi-source heterogeneous
   data from internal and            external systems of energy enterprise,
   preprocessing            lake-entering data, and executing lake data
   filing,            deleting and transferring processes
PN CN119645983-A; CN119645983-B
AE GUIZHOU POWER GRID CO LTD
AB 
   NOVELTY - The method involves obtaining multi-source               
   heterogeneous data from internal and external                systems of
   an energy enterprise. The lake-entering                data is
   preprocessed. The multi-source                heterogeneous data is
   entered into the lake through                different entering
   controllers. An Apache Flink                (RTM: open source stream
   processing framework) is                used as the uniform data
   entering engine. Lake data                filing, deleting and
   transferring processes are                executed, where the
   multi-source heterogeneous data                comprises energy
   production, transmission, storage,                consumption and energy
   market transaction full link                full chain data. A
   SeaTunnel(RTM: open-source big                data integration tool) is
   used to provide data                cleaning. A Spark SQLfunction is
   used to define                pre-processing operator.
   USE - Method for entering multi-source heterogeneous               
   energy data into data lake.
   ADVANTAGE - The method enables using the technical               
   framework of Flink (RTM: open source stream                processing
   framework) + Iceberg(RTM: high                performance open-source
   format for large analytic                tables) to realize real-time
   lake entry of                multi-source heterogeneous energy data,
   supporting                data access from different types of data
   source,                ensuring diversification and comprehensiveness of
   data source, adapting multiple data formats, and               
   satisfying the real-time performance and                flexibility
   requirement of energy service                processing.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   computer device comprising a memory and a processor                for
   executing a set of instructions for entering                multi-source
   heterogeneous energy data into data                lake; (2) a
   computer-readable storage medium for                storing a set of
   instructions for entering                multi-source heterogeneous
   energy data into data                lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for entering multi-source
   heterogeneous                energy data into data lake. (Drawing
   includes                non-English language text).
Z9 0
U1 0
U2 0
DA 2025-04-26
UT DIIDW:2025329080
ER

PT J
AU Kretzer, Arthur Raulino
   Barreto Vavassori Benitti, Fabiane
   Siqueira, Frank
TI Challenges and Opportunities in Big Data Analytics for Industry 4.0: A
   Systematic Evaluation of Current Architectures
SO IEEE ACCESS
VL 13
BP 183419
EP 183447
DI 10.1109/ACCESS.2025.3624558
DT Article
PD 2025
PY 2025
AB The current efforts to integrate Big Data Analytics (BDA) into Industry
   4.0 manufacturing systems, despite their usefulness for enhancing
   data-driven decision-making, are constrained by the lack of
   architectural standards for data management. This systematic mapping
   study analyzes many BDA architectures proposed in the literature,
   revealing a fragmented landscape in which the proposed architectures are
   largely conceptual with limited industrial validation. Our analysis
   identifies dominant technological patterns, such as Apache Kafka for
   ingestion, Spark for processing, and Hadoop and Hive for storage, with
   the majority of implementations favoring open-source solutions. Despite
   their theoretical importance, real-time analytics capabilities remain
   underutilized in practice. This study synthesizes a unified conceptual
   reference architecture with eight fundamental layers to provide a
   framework for comparative analysis. We document an imbalance in layer
   development: storage and processing receive comprehensive attention
   while querying, infrastructure management, and monitoring layers remain
   underdeveloped. Implementation approaches show distinct patterns in
   deployment strategies and data handling, with structured and
   semi-structured data well supported, whereas unstructured data
   integration presents ongoing challenges. Future research should focus on
   developing standardized modular frameworks, benchmarking methodologies,
   and integrating modern data lakehouse architectures to bridge the gap
   between theoretical proposals and production-ready systems.
RI Siqueira, Frank/ABB-8351-2021; Raulino Kretzer, Arthur/; Benitti, Fabiane/
OI Raulino Kretzer, Arthur/0000-0003-1656-9464; Benitti,
   Fabiane/0000-0003-2747-9931
ZB 0
Z8 0
ZA 0
ZR 0
TC 0
ZS 0
Z9 0
U1 4
U2 4
SN 2169-3536
DA 2025-11-12
UT WOS:001606717700016
ER

PT J
AU Silva, Danilo
   Moir, Monika
   Dunaiski, Marcel
   Blanco, Natalia
   Murtala-Ibrahim, Fati
   Baxter, Cheryl
   de Oliveira, Tulio
   Xavier, Joicymara S.
CA INFORM Africa Res Study Grp
TI Review of open-source software for developing heterogeneous data
   management systems for bioinformatics applications
SO BIOINFORMATICS ADVANCES
VL 5
IS 1
AR vbaf168
DI 10.1093/bioadv/vbaf168
DT Review
PD 2025
PY 2025
AB In a world where data drive effective decision-making, bioinformatics
   and health science researchers often encounter difficulties managing
   data efficiently. In these fields, data are typically diverse in format
   and subject. Consequently, challenges in storing, tracking, and
   responsibly sharing valuable data have become increasingly evident over
   the past decades. To address the complexities, some approaches have
   leveraged standard strategies, such as using non-relational databases
   and data warehouses. However, these approaches often fall short in
   providing the flexibility and scalability required for complex projects.
   While the data lake paradigm has emerged to offer flexibility and handle
   large volumes of diverse data, it lacks robust data governance and
   organization. The data lakehouse is a new paradigm that combines the
   flexibility of a data lake with the governance of a data warehouse,
   offering a promising solution for managing heterogeneous data in
   bioinformatics. However, the lakehouse model remains unexplored in
   bioinformatics, with limited discussion in the current literature. In
   this study, we review strategies and tools for developing a data
   lakehouse infrastructure tailored to bioinformatics research. We
   summarize key concepts and assess available open-source and commercial
   solutions for managing data in bioinformatics.Availability and
   implementation Not applicable.
RI de Castro Silva, Danilo/; Santos Xavier, Joicymara/; Moir, Monika/AAU-6520-2021; Dunaiski, Marcel/AAC-9387-2022
OI de Castro Silva, Danilo/0000-0001-5740-3968; Santos Xavier,
   Joicymara/0000-0002-4649-6270; Moir, Monika/0000-0003-1095-1910; 
ZS 0
ZR 0
TC 0
Z8 0
ZB 0
ZA 0
Z9 0
U1 3
U2 3
EI 2635-0041
DA 2025-08-12
UT WOS:001543196800001
PM 40761326
ER

PT C
AU Bureva, Veselina
   Atanassov, Krassimir
   Genov, Miroslav
   Sotirov, Sotir
BE Kahraman, C
   Onar, SC
   Cebi, S
   Oztaysi, B
   Tolga, AC
   Sari, IU
TI Index Matrix Representation of Data Storage Structures Using
   Intuitionistic Fuzzy Logic
SO INTELLIGENT AND FUZZY SYSTEMS, INFUS 2024 CONFERENCE, VOL 1
SE Lecture Notes in Networks and Systems
VL 1088
BP 459
EP 466
DI 10.1007/978-3-031-70018-7_51
DT Proceedings Paper
PD 2024
PY 2024
AB In the current research work a big data structure representation using
   extended intuitionistic fuzzy index matrix (EIFIM) is presented. The
   investigation is based on the theories of index matrices, intuitionistic
   fuzzy sets and databases. It is known that big data systems use
   different data structures. The data warehouses are implemented to
   integrate structured datasets. A data lake provides storing capabilities
   for unstructured data. Nowadays, the two concepts are integrated in data
   lakehouse platforms that provide facilities for structured,
   semi-structured and unstructured data. The open-source framework for
   managing and processing huge amounts of data Hadoop is observed. More
   precisely, the HDFS (Hadoop Distributed File System) organization is
   discussed. The aim of the investigation is to present big data structure
   using EIFIM. User access through big data system environment to reach
   files and other resources is analyzed. The intuitionistic fuzzy logic is
   applied to evaluate the big data system processes.
CT International Conference on Intelligent and Fuzzy Systems (INFUS)
CY JUL 16-18, 2024
CL Istanbul Tech Univ, Canakkale, TURKEY
HO Istanbul Tech Univ
SP Canakkale Onsekiz Mart Univ
RI Sotirov, Sotir/M-2488-2013; Bureva, Veselina/; Atanassov, Krassimir/S-2877-2016
OI Bureva, Veselina/0000-0003-4344-4392; 
ZR 0
ZA 0
TC 0
ZS 0
ZB 0
Z8 0
Z9 0
U1 0
U2 1
SN 2367-3370
EI 2367-3389
BN 978-3-031-70017-0; 978-3-031-70018-7
DA 2024-11-13
UT WOS:001331332200050
ER

PT C
AU Salcher, Felix
   Finck, Steffen
   Hellwig, Michael
GP IEEE
TI A Smart Shop Floor Information System Architecture based on the Unified
   Namespace
SO 2024 IEEE INTERNATIONAL CONFERENCE ON ENGINEERING, TECHNOLOGY, AND
   INNOVATION, ICE/ITMC 2024
SE International ICE Conference on Engineering Technology and Innovation
AR 172
DI 10.1109/ICE/ITMC61926.2024.10794387
DT Proceedings Paper
PD 2024
PY 2024
AB Modern research in the field of Smart Manufacturing often focuses on the
   big data aspect, where the goal is to obtain actionable insights from
   the data. In this paper, the focus is shifted back to the Smart Shop
   Floor and how to efficiently derive information with the big data tasks
   that follow as simple as possible. A condensed literature review of the
   existing architectures and frameworks for Smart Manufacturing is
   combined with the experience of practitioners to assess the requirements
   for a Smart Shop Floor Information System Architecture. On this basis,
   an architecture is proposed that consists of eight modular building
   blocks. After a detailed description of the roles and functionalities of
   these building blocks, a reference implementation using readily
   available, open-source tools and technologies is laid out. This
   reference implementation intends to strike the right balance between
   generality and specificity. It provides the reader with a tangible
   starting point for implementing and adapting the proposed architecture
   to their own needs.
CT 30th IEEE International Conference on Engineering, Technology, and
   Innovation
CY JUN 24-28, 2024
CL Funchal, PORTUGAL
SP Institute of Electrical and Electronics Engineers Inc; Universidade do
   Minho; NOVA School of Science and Technology (FCT NOVA); Universidade
   Nova De Lisboa
RI Hellwig, Michael/; Finck, Steffen/V-7919-2019
OI Hellwig, Michael/0000-0002-6731-8166; 
ZB 0
ZR 0
ZA 0
TC 0
Z8 0
ZS 0
Z9 0
U1 2
U2 5
SN 2334-315X
BN 979-8-3503-6244-2; 979-8-3503-6243-5
DA 2025-04-02
UT WOS:001429193000105
ER

PT P
AU XU B
   GENG Z
   YANG Q
   XU H
   XIAO Z
TI Heterogeneous data integration method based on            virtualization
   technology, involves obtaining optimal            integration solution
   of heterogeneous database to be            integrated or data source
   table
PN CN116701504-A
AE YUNNAN POWER GRID CO LTD INFORMATION CEN
AB 
   NOVELTY - The method involves combining the open source               
   language to construct the data source linker                according to
   the heterogeneous database to be                integrated or/and the
   data source table. The                metadata information of the target
   data source in                the data source linker is extracted. A    
   corresponding packaging table for packaging is                generated,
   and a mapping virtual table is                established. The data
   service issue on the mapping                virtual table is performed,
   and an optimization                operation by combining with the data
   virtualization                engine is performed. The optimal
   integration                solution of the heterogeneous database to be 
        integrated or/and the data source table is                obtained.
   USE - Heterogeneous data integration method based on               
   virtualization technology for use in data                integration
   system (claimed) such as data warehouse                system and data
   lake system.
   ADVANTAGE - Reduces the difficulty of data processing by               
   setting uniform data access and virtual data market                and
   autonomous service analysis, simplifies the                data
   integration mode, data development speed, has                high
   flexibility, can access in fast and controlled                mode, and
   has low use cost.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a heterogeneous data integration system based               
   on virtualization technology;a computer device; anda computer readable
   storage medium storing                heterogeneous data integration
   program based on                virtualization technology.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating
   a                method for integrating heterogeneous data based on     
   virtualization technology and a system. (Drawing                includes
   non-English language text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202396227N
ER

PT P
AU ZHANG T
   CHEN Z
   LIU Z
   YU C
   WANG P
   WANG Q
   CHEN W
   LIU Y
   LIU H
TI Method for realizing delta lake data lake index            based on
   Elasticsearch, involves locating storage            position of content
   corresponding to keyword, and            entering positioning position
   according to selection of            user
PN CN116340317-A
AE NANHU LAB
AB 
   NOVELTY - The method involves extracting the source                data,
   and forming a structured Dataset data set                based on Spark.
   The Schema analysis on the                extracted data set is
   performed, and the data                storage address information is
   increased to form an                index structure. The content of the
   index structure                is converted into the support format of
   the search                server and storing in the index database. The 
   keyword input by the user is received by the search               
   server. The search is started based on the index               
   database, and the storage position of the content                is
   located corresponding to the keyword. The                positioning
   position is entered according to the                selection of the
   user.
   USE - Method for realizing delta lake data lake                index
   based on Elasticsearch (RTM: open-source                computer
   programming language developed and                marketed by
   Microsoft(RTM: Company Name)).
   ADVANTAGE - The method enables realizing automatic               
   generating data index scheme on the Delta Lake,                storing
   or searching according to the index scheme,                providing
   efficient index organization and fast                query to support
   high performance of large data                analysis, and realizing
   full fuzzy matching, data                isolation and data fusion
   freely interactive                matching and using full text search
   theory of                Elasticsearch, the multi-source heterogeneous
   data                to reach the purpose of full-text search, and       
       automatically optimizing cache data in the data                lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a  
   method for realizing Deltalake data lake index                based on
   Elasticsearch. (Drawing includes                non-English language
   text)
Z9 0
U1 0
U2 0
DA 2023-07-15
UT DIIDW:202369461J
ER

PT P
AU WU L
   WANG P
   LIN Z
   LI L
   LIU J
   SUN F
TI Semi-supervised flow-shaped collaborative learning            (SMOL)
   based multi-channel invoice data aggregation            processing
   method, involves accessing floor application            invoice data
   into uniform data Application Programming            Interface (API)
PN CN115908031-A
AE INNER MONGOLIA AISINO CO LTD
AB 
   NOVELTY - The method involves outputting standard               
   structured invoice data by using big data                aggregation
   processing based on invoice data from                different channel
   sources.Uniform abstract invoice                data is output by using
   uniform adaptation                processing based on the standard
   structured invoice                data. Different conversion rules are
   configured to                output standardized invoice data based on
   the                uniform abstract invoice data. Checked qualified     
   invoice data is output by using invoice checking                process
   based on the standardized invoice data. The                checked
   qualified invoice data is input into a data                lake to form
   floor application invoice data.
   USE - SMOL based multi-channel invoice data                aggregation
   processing method.
   ADVANTAGE - The method enables realizing accurate               
   regression function training and prediction model                to
   realize precise locating, and combining                Bluetoothand
   Wireless Fidelity (Wi-Fi) data for                training to improve
   training and prediction                precision of regression function,
   and improving                location precision effect of pure Wi-Fi
   data.
   DETAILED DESCRIPTION - The floor application invoice data is accessed   
   into a uniform data Application Programming                Interface
   (API), where original invoice data access                channel
   comprises interface entry, Excel(RTM:                Spreadsheet
   developed by Microsoft for Windows,                macOS, Android and
   iOS) batch introduction, third                party interface
   introduction, image recognition OCR                import, invoice
   two-dimensional (2D) code                recognition introduction and
   rapid expansion                access, and original invoice data
   comprises pure                text format, XMLformat, JSON(RTM: Open
   standard                file format and data interchange format) format
   and                Excelformat. The invoice data is stored in a         
   Hadoop(RTM: Open-source software framework) big                data
   processing middleware by using Hudi(RTM:                transactional
   data lake platform) based on the                checked qualified
   invoice data.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of an SMOL 
   based multi-channel invoice data aggregation                processing
   method. (Drawing includes non-English                language
   text).S101Multi-channel original invoice data               
   accessS102Original invoice data adaptationS103Invoice data
   standardizationS104Invoice data checking floorS105Invoice data docking
   application
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202346039B
ER

PT B
AU Bouziane, Anas
Z2  
TI On-Demand Health Data Provisioning With Custom Temporary Data Views for
   Big Data Platforms
DT Dissertation/Thesis
PD Jan 01 2023
PY 2023
TC 0
ZA 0
ZB 0
Z8 0
ZR 0
ZS 0
Z9 0
U1 0
U2 0
BN 9798302848277
UT PQDT:121301049
ER

PT C
AU Guyot, Alexis
   Leclercq, Eric
   Gillet, Annabelle
   Cullot, Nadine
BE Wrembel, R
   Gamper, J
   Kotsis, G
   Tjoa, AM
   Khalil, I
TI Preventing Technical Errors in Data Lake Analyses with Type Theory
SO BIG DATA ANALYTICS AND KNOWLEDGE DISCOVERY, DAWAK 2023
SE Lecture Notes in Computer Science
VL 14148
BP 18
EP 24
DI 10.1007/978-3-031-39831-5_2
DT Proceedings Paper
PD 2023
PY 2023
AB Data analysts compose various operators provided by data lakes to
   conduct their analyses on big data through complex analytical workflows.
   In this article, we present a formal framework based on type theory to
   prevent technical errors in such compositions of operators. This
   framework uses restrictions on type definitions to transform technical
   errors into type errors. We show how to use this framework to prevent
   errors related to schema or model transformations in analytical
   workflows. We provide an open-source implementation in Scala which can
   be used to detect errors at compile time.
CT 25th International Conference on Big Data Analytics and Knowledge
   Discovery (DaWaK)
CY AUG 28-30, 2023
CL Penang, MALAYSIA
SP Soft Comput Center Hagenburg; Inst telecooperat; Web applicat Soc
RI CULLOT, nadine/; LECLERCQ, Eric/AAY-9094-2020; Gillet, Annabelle/
OI CULLOT, nadine/0000-0003-1307-3287; LECLERCQ, Eric/0000-0001-6382-2288;
   Gillet, Annabelle/0000-0002-4204-9262
Z8 0
ZB 0
ZS 0
ZA 0
ZR 0
TC 0
Z9 0
U1 0
U2 0
SN 0302-9743
EI 1611-3349
BN 978-3-031-39830-8; 978-3-031-39831-5
DA 2024-11-13
UT WOS:001330381900002
ER

PT P
AU OU X
   PHELAN T A
   LEE D
TI Method for performing caching and data access            improvements in
   e.g. environments employing collection            of open-source
   software utilities, involves responding            to read request again
   based on data block returned by            remote data lake, and caching
   data block returned by            the remote data lake within cache
PN US2022300422-A1; US11797447-B2
AE HEWLETT PACKARD ENTERPRISE DEV LP
AB 
   NOVELTY - The method involves receiving a read request               
   from a compute task running on a worker node by a                data
   agent (410). A particular worker node of a                compute
   cluster to which a data block containing                data specified
   by the read request is mapped based                on a mapping function
   is identified (420). Judgment                is made (430) to check
   whether the worker node is                identified. Data is fetched
   (435) from the remote                data agent of the identified worker
   node. The read                request is responded (440) with the data
   returned                by the remote data agent. Determination is made 
   (450) whether a local cache exists within the                worker node
   that contains the data block. The data                block is fetched
   (460) from the remote data lake in                which the file is
   stored. The read request is                responded again (470) based
   on the data block                returned by the remote data lake. The
   data block                returned by the remote data lake is cached
   (480)                within the local cache.
   USE - Method for performing caching and data access               
   improvements in a separate compute and storage                deployment
   large scale data processing environment                e.g. environments
   employing Apache Hadoop (RTM:                Collection of open-source
   software utilities),                Apache Spark (RTM: Multi-language
   engine) and big                data frameworks.
   ADVANTAGE - The method enables providing a cloud computing              
   as a model of service delivery for enabling                convenient
   on-demand network access to a shared                pool of configurable
   computing resources that can                be rapidly provisioned and
   released with minimal                management effort or interaction
   with a provider of                the service. The method enables
   allowing a cloud                consumer to unilaterally provision
   computing                capabilities such as server time and network   
   storage, as needed automatically without requiring                human
   interaction with the service's                provider.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a system
   for performing caching and data                access improvements in a
   separate compute and                storage deployment large scale data
   processing                environment;(2) a non-transitory machine
   readable medium                comprises a set of instructions for
   performing                caching and data access improvements in a
   separate                compute and storage deployment large scale data 
                 processing environment.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                read request processing operation.410Step
   for receiving read request from                compute task running on
   worker node by data                agent420Step for identifying
   particular worker                node of a compute cluster to which a
   data block                containing data specified by the read request
   is                mapped based on a mapping function430Step for checking
   whether worker node is                identified435Step for fetching
   data from remote data                agent of identified worker
   node440Step for responding to read request with                data
   returned by remote data agent450Step for determining whether local cache
   exists within worker node that contains data                block460Step
   for fetching data block from remote                data lake in which
   file is stored470Step for responding to read request again              
   based on data block returned by remote data                lake480Step
   for caching data block returned by                remote data lake
   within local cache
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:2022C1561R
ER

PT J
AU JANOWCZYK, ANDREW ROBERT
TI Histotools: scaling digital pathology curation tools for quality
   control, annotation, labeling, and dataset identification
DT Awarded Grant
PD Sep 21 2022
PY 2023
AB ABSTRACT: With recent approval of whole slide scanners for primary
   diagnosis, wherein routine glasshistopathology slides are digitized and
   presented to clinical pathologists for diagnosis on computer monitors,
   awealth of new untapped data is being created in routine clinical
   practice and placed in growing data lakes. Indigital format, these whole
   slide images (WSIs) can be subjected to digital pathomics, i.e., the
   process ofextracting quantitative image features associated with
   morphology, attributes, and relationships of histologicobjects in WSIs.
   These features can subsequently be employed for discovery in many
   domains such ashistogenomics, which sees associating phenotypical
   presentations with biological pathways and geneontologies. Additionally,
   low-cost non-tissue destructive image-based companion diagnostic assays
   (CDx)can be developed for predicting prognosis and treatment response of
   patients. Unfortunately, unprocessed largedata lakes (e.g., TCGA) are
   not alone sufficient for pathomics, and often require an intractable
   amount of humancuration effort in (i) performing meticulous quality
   control of WSI (i.e., avoid “garbage-in, garbage-out”) andsubsequently
   (ii) precisely annotating (e.g., cell boundary) and labeling (e.g., cell
   type) histologic objects. Toaddress these major limiting factors in
   curating data lakes, we propose developing our small-scale
   HistoToolsprototypes to employ computing clusters and thus enable their
   function at the scale of large digital sliderepositories (DSR): (i)
   HistoQC for robust, reproducible quality control of WSI by identifying
   artifacts (blurriness)and outliers (poorly stained slides) for avoidance
   in downstream analyses, (ii) CohortFinder for identificationand
   compensation of batch affects, (iii) Quick Annotator for rapid computer
   aided annotation generation via acombination of active and machine
   learning, (iv) PatchSorter for improving sub-typing of histologic
   objects withmachine learning. We will evaluate HistoTools for
   improvement of quality control and the efficiency of bothsegmenting and
   labeling histologic objects of interest via (a) onsite curation and
   release of the 14k WSI usedduring our internal validation and (b)
   supported external curation of at least 100k WSI via 24-clinical
   affiliatesfrom every continent, except Antarctica, whom together have
   access to over 20 million WSI during this proposal.Our validation use
   cases are designed to expedite existing onsite projects in the CDx
   space, consisting of 4organs (breast, lung, heart, kidney), 3 diseases
   (cancer, kidney disease, and organ rejection) and WSIs collectedfrom >70
   sites. These cohort characteristics will help ensure the
   generalizability of our tools for curated data lakecreation, with
   open-source and usability study approaches employed to obtain feedback
   from collaborators andthe larger research community. Dissemination
   through consortia (ITCR, NEPTUNE) and websites (Github, TCIA)will
   improve visibility and adoption. The tools and well-curated data sets we
   release are anticipated to bootstrapresearcher-initiated CDx discovery
   projects, along with the creation of their own onsite manicured data
   lakes.Together, this proposal will engender digital pathology based
   precision medicine research.
ZS 0
ZR 0
TC 0
ZB 0
ZA 0
Z8 0
Z9 0
U1 0
U2 0
G1 10708011; 5R01LM013864-02; R01LM013864
DA 2024-07-25
UT GRANTS:17756061
ER

PT J
AU JANOWCZYK, ANDREW ROBERT
TI Histotools: scaling digital pathology curation tools for quality
   control, annotation, labeling, and dataset identification
DT Awarded Grant
PD Sep 21 2022
PY 2022
AB ABSTRACT: With recent approval of whole slide scanners for primary
   diagnosis, wherein routine glasshistopathology slides are digitized and
   presented to clinical pathologists for diagnosis on computer monitors,
   awealth of new untapped data is being created in routine clinical
   practice and placed in growing data lakes. Indigital format, these whole
   slide images (WSIs) can be subjected to digital pathomics, i.e., the
   process ofextracting quantitative image features associated with
   morphology, attributes, and relationships of histologicobjects in WSIs.
   These features can subsequently be employed for discovery in many
   domains such ashistogenomics, which sees associating phenotypical
   presentations with biological pathways and geneontologies. Additionally,
   low-cost non-tissue destructive image-based companion diagnostic assays
   (CDx)can be developed for predicting prognosis and treatment response of
   patients. Unfortunately, unprocessed largedata lakes (e.g., TCGA) are
   not alone sufficient for pathomics, and often require an intractable
   amount of humancuration effort in (i) performing meticulous quality
   control of WSI (i.e., avoid “garbage-in, garbage-out”) andsubsequently
   (ii) precisely annotating (e.g., cell boundary) and labeling (e.g., cell
   type) histologic objects. Toaddress these major limiting factors in
   curating data lakes, we propose developing our small-scale
   HistoToolsprototypes to employ computing clusters and thus enable their
   function at the scale of large digital sliderepositories (DSR): (i)
   HistoQC for robust, reproducible quality control of WSI by identifying
   artifacts (blurriness)and outliers (poorly stained slides) for avoidance
   in downstream analyses, (ii) CohortFinder for identificationand
   compensation of batch affects, (iii) Quick Annotator for rapid computer
   aided annotation generation via acombination of active and machine
   learning, (iv) PatchSorter for improving sub-typing of histologic
   objects withmachine learning. We will evaluate HistoTools for
   improvement of quality control and the efficiency of bothsegmenting and
   labeling histologic objects of interest via (a) onsite curation and
   release of the 14k WSI usedduring our internal validation and (b)
   supported external curation of at least 100k WSI via 24-clinical
   affiliatesfrom every continent, except Antarctica, whom together have
   access to over 20 million WSI during this proposal.Our validation use
   cases are designed to expedite existing onsite projects in the CDx
   space, consisting of 4organs (breast, lung, heart, kidney), 3 diseases
   (cancer, kidney disease, and organ rejection) and WSIs collectedfrom >70
   sites. These cohort characteristics will help ensure the
   generalizability of our tools for curated data lakecreation, with
   open-source and usability study approaches employed to obtain feedback
   from collaborators andthe larger research community. Dissemination
   through consortia (ITCR, NEPTUNE) and websites (Github, TCIA)will
   improve visibility and adoption. The tools and well-curated data sets we
   release are anticipated to bootstrapresearcher-initiated CDx discovery
   projects, along with the creation of their own onsite manicured data
   lakes.Together, this proposal will engender digital pathology based
   precision medicine research.
TC 0
ZS 0
ZA 0
ZB 0
Z8 0
ZR 0
Z9 0
U1 0
U2 0
G1 10520124; 1R01LM013864-01A1; R01LM013864
DA 2023-12-14
UT GRANTS:16276817
ER

PT P
AU SONG S
   DONG X
TI Method for performing intelligent construction for            enterprise
   data, involves supportting switching of            computing engines in
   different scenarios, and            performing real-time query and
   offline query through            Hadoop ecological technology ecology on
   basis of            storage integration
PN CN114691762-A
AE SUZHOU YINGTIANDI INFORMATION TECHNOLOGY
AB 
   NOVELTY - The method involves performing operations at a               
   level of computing and data architecture on a basis                of
   storage integration through a cooperation of a                data
   integration module, a data processing module,                a data
   synchronization module, and a data asset                center. A
   unified management of various data                sources is performed
   in a one-key access mode, and                a database or schema
   required to be accessed is                specified. Uploading is
   performed through                unstructured and semi-structured data,
   a unified                access interface channel is opened for a
   downstream                data processing flow, data expansion and      
   maintenance are performed through a file transfer               
   protocol (FTP) data source acquisition, and a                switching
   of calculation engines of different                scenes is supported
   through Hadoop (RTM: collection                of open-source software
   developed by Apache                Software Foundation) ecological
   technology ecology,                and real-time query and off-line
   query are                performed on the basis of storage              
    integration.
   USE - Method for performing intelligent construction                for
   enterprise data.
   ADVANTAGE - The method enables realizing smooth transition              
   of offline data and real-time data, and ensuring                data
   reliability and abnormal complement. The                method allows a
   data integration module to support                the heterogeneous data
   source of direct number and                off-line synchronization to
   an integrated                collection service of a data lake, thus
   realizing                flexible coding and configuration of different
   data                number requirements, supporting arrangement         
   scheduling of different development languages, and               
   supporting the access of the data to a business                angle of
   understanding for modeling and finishing                into a
   retrievable data development                environment.
Z9 0
U1 0
U2 0
DA 2022-08-09
UT DIIDW:202289641K
ER

PT J
AU MURUGIAH, KARTHIK 
TI Automated ascertainment of bleeding and target lesion revascularization
   after percutaneous coronary intervention (PCI) using electronic health
   record (EHR) data
DT Awarded Grant
PD Jan 25 2022
PY 2023
AB PROJECT SUMMARYPercutaneous coronary intervention (PCI) is the most
   common cardiac procedure with over 650,000 PCIperformed annually in the
   U.S. Post-PCI complications which occur in a significant proportion of
   patients areassociated with an increased risk of morbidity and
   mortality. Reliable ascertainment of post-PCI events isimportant for
   performance measurement, submission to disease registries, clinical
   trials, and for cardiaccatheterization laboratory (CCL) safety
   monitoring. Claims based detection of PCI complications is
   inadequate.Assessing post-PCI events reliably requires an in-depth
   manual chart review, which incurs a significantprovider and
   administrative burden. However, with advances in health information
   technology and nationwideadoption of electronic health record (EHR)
   systems, it possible to utilize EHR for the automatic derivation
   ofclinical events. Dr. Murugiah proposes to create and validate
   automated algorithms which can be applied toEHR data to detect two
   important post-PCI events which are a common focus of clinical trials
   and qualityimprovement efforts – in-hospital bleeding and 1-year target
   lesion revascularization (TLR). Using EHR data ata large health system,
   Dr. Murugiah will develop a hybrid algorithm to detect major bleeding
   post-PCI byleveraging structured data fields such as laboratory values,
   as well as unstructured data such as imagingreports, cardiac
   catheterization reports, and progress notes incorporating Natural
   Language Processing (NLP)techniques (Aim 1). Similarly, using cardiac
   catheterization reports for patients undergoing repeatrevascularization
   within 1 year, an algorithm will be developed to detect TLR (Aim 2).
   Both algorithms will beexternally validated using EHR data from another
   large institution. The final algorithm will be implemented intoa tool
   generating scheduled reports of bleeding and TLR, to be fed back to the
   quality assurance team for theCCL and to individual operators.
   Individual operators will be surveyed to obtain feedback about the
   algorithm,reporting process, and their perceived benefit. The final
   tools will be made open source (Aim 3). An automatedalgorithm for the
   detection of post-PCI events within EHR can reduce administrative
   burden, enable thegeneration of new knowledge from EHR based
   observational studies, and enable pragmatic clinical trials.Further,
   this project can serve as a proof of concept of the utility of hybrid
   tools leveraging both structured dataand clinical text for surveillance
   and quality measurement. Dr. Murugiah has a career interest in studying
   andimproving the treatment for ischemic heart disease using
   multidimensional datasets and EHR data to developreal time risk
   prediction models and decision support tools, and conduct EHR based
   comparative effectivenessstudies and clinical trials. During the award
   period he will leverage the experience of his mentorship teamwhich
   includes national experts in cardiovascular outcomes research, clinical
   informatics, and computationallinguistics. He will also acquire formal
   training in clinical informatics by completing a Master of Health
   Sciencedegree which will provide him the necessary platform to make the
   transition into an independent investigator.
ZA 0
ZR 0
TC 0
ZB 0
ZS 0
Z8 0
Z9 0
U1 0
U2 0
G1 10555326; 5K08HL157727-02; K08HL157727
DA 2024-07-23
UT GRANTS:17710121
ER

PT J
AU MURUGIAH, KARTHIK 
TI Automated ascertainment of bleeding and target lesion revascularization
   after percutaneous coronary intervention (PCI) using electronic health
   record (EHR) data
DT Awarded Grant
PD Jan 25 2022
PY 2022
AB PROJECT SUMMARYPercutaneous coronary intervention (PCI) is the most
   common cardiac procedure with over 650,000 PCIperformed annually in the
   U.S. Post-PCI complications which occur in a significant proportion of
   patients areassociated with an increased risk of morbidity and
   mortality. Reliable ascertainment of post-PCI events isimportant for
   performance measurement, submission to disease registries, clinical
   trials, and for cardiaccatheterization laboratory (CCL) safety
   monitoring. Claims based detection of PCI complications is
   inadequate.Assessing post-PCI events reliably requires an in-depth
   manual chart review, which incurs a significantprovider and
   administrative burden. However, with advances in health information
   technology and nationwideadoption of electronic health record (EHR)
   systems, it possible to utilize EHR for the automatic derivation
   ofclinical events. Dr. Murugiah proposes to create and validate
   automated algorithms which can be applied toEHR data to detect two
   important post-PCI events which are a common focus of clinical trials
   and qualityimprovement efforts – in-hospital bleeding and 1-year target
   lesion revascularization (TLR). Using EHR data ata large health system,
   Dr. Murugiah will develop a hybrid algorithm to detect major bleeding
   post-PCI byleveraging structured data fields such as laboratory values,
   as well as unstructured data such as imagingreports, cardiac
   catheterization reports, and progress notes incorporating Natural
   Language Processing (NLP)techniques (Aim 1). Similarly, using cardiac
   catheterization reports for patients undergoing repeatrevascularization
   within 1 year, an algorithm will be developed to detect TLR (Aim 2).
   Both algorithms will beexternally validated using EHR data from another
   large institution. The final algorithm will be implemented intoa tool
   generating scheduled reports of bleeding and TLR, to be fed back to the
   quality assurance team for theCCL and to individual operators.
   Individual operators will be surveyed to obtain feedback about the
   algorithm,reporting process, and their perceived benefit. The final
   tools will be made open source (Aim 3). An automatedalgorithm for the
   detection of post-PCI events within EHR can reduce administrative
   burden, enable thegeneration of new knowledge from EHR based
   observational studies, and enable pragmatic clinical trials.Further,
   this project can serve as a proof of concept of the utility of hybrid
   tools leveraging both structured dataand clinical text for surveillance
   and quality measurement. Dr. Murugiah has a career interest in studying
   andimproving the treatment for ischemic heart disease using
   multidimensional datasets and EHR data to developreal time risk
   prediction models and decision support tools, and conduct EHR based
   comparative effectivenessstudies and clinical trials. During the award
   period he will leverage the experience of his mentorship teamwhich
   includes national experts in cardiovascular outcomes research, clinical
   informatics, and computationallinguistics. He will also acquire formal
   training in clinical informatics by completing a Master of Health
   Sciencedegree which will provide him the necessary platform to make the
   transition into an independent investigator.
ZR 0
ZB 0
Z8 0
ZS 0
ZA 0
TC 0
Z9 0
U1 0
U2 0
G1 10371710; 1K08HL157727-01A1; K08HL157727
DA 2023-12-14
UT GRANTS:15572751
ER

PT C
AU Iniguez, Luis
   Galar, Mikel
BE Gonzalez, HS
   Lopez, IP
   Bringas, PG
   Quintian, H
   Corchado, E
TI A Scalable and Flexible Open Source Big Data Architecture for Small and
   Medium-Sized Enterprises
SO 16TH INTERNATIONAL CONFERENCE ON SOFT COMPUTING MODELS IN INDUSTRIAL AND
   ENVIRONMENTAL APPLICATIONS (SOCO 2021)
SE Advances in Intelligent Systems and Computing
VL 1401
BP 273
EP 282
DI 10.1007/978-3-030-87869-6_26
DT Proceedings Paper
PD 2022
PY 2022
AB The advancements of Big Data, Internet of Things and Artificial
   Intelligence are causing the industrial revolution known as Industry
   4.0. For automated factories, adopting the necessary technologies for
   its implementation involves a series of challenges such as the lack of a
   proper infrastructure, financial limitations, coordination problems or a
   low understanding of Industry 4.0 implications. Additionally, many
   implementations focus on solving specific problems without taking other
   future or parallel projects into account, leading to continuous
   restructuring and increased complexity, that is, increasing costs. A
   lack of a global view when implementing Industry 4.0 solutions can cause
   difficulties in its adoption, leading to future problems that may be
   unaffordable for Small and Medium-sized Enterprises (SMEs). Traditional
   Big Data architectures offer remarkable solutions to complex data
   issues, but do not cover the complete flow of information that is
   required in Industry 4.0 applications. Therefore, there is a need to
   create solutions for the difficulties that this new digital
   transformation brings to avoid future problems, making it affordable
   also for SMEs. In this work we propose a flexible and scalable Big Data
   architecture that is well-suited for SMEs with automated factories,
   taking the aforementioned difficulties into account.
CT 16th International Conference on Soft Computing Models in Industrial and
   Environmental Applications (SOCO)
CY SEP, 2021
CL Bilbao, SPAIN
SP Startup Ole; Basque Govt, Dept Educ & Univ; Logistar Project DeustoTech;
   Univ Deusto
RI Galar, Mikel/H-4846-2011
ZS 0
Z8 0
TC 0
ZA 0
ZB 0
ZR 0
Z9 0
U1 0
U2 12
SN 2194-5357
EI 2194-5365
BN 978-3-030-87869-6; 978-3-030-87868-9
DA 2021-11-30
UT WOS:000719656700026
ER

PT P
AU GUI H
   FENG K
   WANG Y
   WANG H
TI Petri network based multi-source heterogeneous            data quality
   detection method, involves forming data            quality analysis
   report in form of table and chart            based on feedback of
   library for realizing monitoring            data quality detection
   process
PN CN112540975-A; CN112540975-B
AE CAS BIG DATA ACAD COMPUTING TECHNOLOGY; BIG DATA ACAD ZHONGKE
AB 
   NOVELTY - The method involves configuring multiple               
   heterogeneous data sources connected in data lake                managed
   by a main system through a data source for                configuring.
   Multiple heterogeneous data sources                are connected with
   the data lake. Multiple                heterogeneous data sources are
   connected with a                local multi-source heterogeneous data
   processing                server. Metadata is obtained by performing
   metadata                collection task to multi-source heterogeneous   
   database, where the metadata comprises metadata                table
   information, field information, index                information and
   constraint information. External                table connection is
   established through autonomous                expansion of
   PostgreSQL(RTM: free and open-source                relational database
   management system) database                according to data source
   information and metadata                information. Data quality rule
   is established                according to quality task constructing
   adding                information feedback of a petri network           
       model.
   USE - Petri network based multi-source heterogeneous                data
   quality detection method.
   ADVANTAGE - The method enables generating data quality               
   analysis result for the user to check, establishing                data
   quality knowledge base to improve quality                problem solving
   ability for providing effective                support for enhancement
   of system data quality so                as to increase data value.
   DETAILED DESCRIPTION - Data quality analysis report is formed in form   
   of table and chart to help a user for analyzing               
   integrity, consistency, accuracy, timeliness and                validity
   of data in a management data source                according to
   real-time feedback of message library                for realizing
   real-time monitoring data quality                detection process.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a petri network based multi-source
   heterogeneous                data quality detection method. (Drawing
   includes                non-English language text).
Z9 0
U1 0
U2 0
DA 2021-04-27
UT DIIDW:2021314369
ER

PT C
AU Lansing, Carina
   Levin, Maxwell
   Sivaraman, Chitra
   Fao, Rebecca
   Driscoll, Frederick
GP IEEE
TI Tsdat: An Open-Source Data Standardization Framework for Marine Energy
   and Beyond
SO OCEANS 2021: SAN DIEGO - PORTO
DT Proceedings Paper
PD 2021
PY 2021
AB Many organizations are tasked with the collection and processing of
   large quantities of data from various measurement devices. Data reported
   from these sources are often not interoperable with datasets and
   software used by analysts and other organizations in the same domain,
   introducing barriers for collaboration on large-scale projects. This
   poses a particular problem for cross-device comparisons and machine
   learning applications, which rely on large quantities of data from
   multiple sources. To address these challenges, the open-source
   Time-Series Data Pipelines (Tsdat) Python framework was developed by
   Pacific Northwest National Laboratory, with strategic guidance and
   direction provided by the National Renewable Energy Laboratory and
   Sandia National Laboratories to facilitate collaboration and accelerate
   advancements in the marine energy domain through the development of an
   open-source ecosystem of tools. This paper will describe the Tsdat
   framework and the data standards within which it operates. A beta
   version of Tsdat has been released and is being used by several projects
   in marine energy, wind energy, and building energy systems.
CT OCEANS Conference
CY SEP 20-23, 2021
CL ELECTR NETWORK
ZB 0
Z8 0
ZS 0
TC 0
ZA 0
ZR 0
Z9 0
U1 0
U2 0
BN 978-0-692-93559-0
DA 2021-01-01
UT WOS:000947273302129
ER

PT B
AU Rao, T. Ramalingeswara
   Mitra, Pabitra
   Goswami, Adrijit
BE Ravi, V
   Cherukuri, AK
TI The role of data lake in big data analytics: recent developments and
   challenges
SO HANDBOOK OF BIG DATA ANALYTICS, VOL. 1: Methodologies
SE IET COMPUTING SERIES
VL 37
BP 105
EP 123
DT Article; Book Chapter
PD 2021
PY 2021
RI Rao, T/AAE-9233-2020; GOSWAMI, ADRIJIT/AAM-8742-2021
ZB 0
TC 0
ZR 0
Z8 0
ZS 0
ZA 0
Z9 0
U1 0
U2 2
BN 978-1-83953-058-6; 978-1-83953-064-7
DA 2022-02-20
UT WOS:000752483600007
ER

PT C
AU Suleykin, Alexander
   Bobkova, Anna
   Panfilov, Peter
   Chumakov, Ilya
GP IEEE
TI Efficient Data Exchange Between Typical Data Lake and DWH Corporate
   Systems
SO INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER AND ENERGY TECHNOLOGIES
   (ICECET 2021)
BP 1999
EP 2004
DI 10.1109/ICECET52533.2021.9698468
DT Proceedings Paper
PD 2021
PY 2021
AB In the last five years, many companies around the world have been
   successfully implemented Apache Hadoop as a main Data Lake storage for
   all data presented in the organization. At the same time, the adoption
   of other Open-Source technologies has been also increasing for years,
   such as classical MPP-based systems for Analytical workloads. Thus, the
   question of efficient and fast data integration between Apache Hadoop
   and other organizational data storage systems is highly important for
   enterprises, where business and decision makers need the minimum delay
   of big heterogeneous data exchange between Hadoop and other storages. In
   this paper, we compare different options for loading data from Apache
   Hadoop, representing the Data Lake of organization, into Open-Source MPP
   Greenplum database with the role of classical data warehouse for
   analytical workloads, and choose the best one. Also, we identify
   potential risks of using different data loading methods.
CT IEEE International Conference on Electrical, Computer, and Energy
   Technologies (ICECET)
CY DEC 09-10, 2021
CL Cape Town, SOUTH AFRICA
SP Aksaray Univ; IEEE; Univ Johannesburg
RI Panfilov, Peter/AAJ-8308-2021; Suleykin, Alexander/AAC-6050-2022
OI Panfilov, Peter/0000-0001-6567-6309; Suleykin,
   Alexander/0000-0003-2294-6449
ZA 0
ZS 0
TC 0
ZR 0
Z8 0
ZB 0
Z9 0
U1 0
U2 4
BN 978-1-6654-4231-2
DA 2022-07-10
UT WOS:000814669100344
ER

PT C
AU Liu, Ruoran
   Isah, Haruna
   Zulkernine, Farhana
GP IEEE
TI A Big Data Lake for Multilevel Streaming Analytics
SO 2020 1ST INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS AND PRACTICES,
   IBDAP
BP 103
EP 108
DT Proceedings Paper
PD 2020
PY 2020
AB Large organizations are seeking to create new architectures and scalable
   platforms to effectively handle data management challenges due to the
   explosive nature of data rarely seen in the past. These data management
   challenges are largely posed by the availability of streaming data at
   high velocity from various sources in multiple formats. The changes in
   data paradigm have led to the emergence of new data analytics and
   management architecture. This paper focuses on storing high volume,
   velocity and variety data in the raw formats in a data storage
   architecture called a data lake. First, we present our study on the
   limitations of traditional data warehouses in handling recent changes in
   data paradigms. We discuss and compare different open source and
   commercial platforms that can be used to develop a data lake. We then
   describe our end-to-end data lake design and implementation approach
   using the Hadoop Distributed File System (HDFS) on the Hadoop Data
   Platform (HDP). Finally, we present a real-world data lake development
   use case for data stream ingestion, staging, and multilevel streaming
   analytics which combines structured and unstructured data. This study
   can serve as a guide for individuals or organizations planning to
   implement a data lake solution for their use cases.
CT 1st International Conference on Big Data Analytics and Practices (IBDAP)
CY SEP 25-26, 2020
CL ELECTR NETWORK
RI Isah, Haruna/AAB-3693-2019; liu, ruoran/KOQ-5146-2024
ZR 0
ZB 0
TC 0
ZS 0
ZA 0
Z8 0
Z9 0
U1 0
U2 0
BN 978-1-7281-8106-6
DA 2020-01-01
UT WOS:001338048900019
ER

PT J
AU Landman,  Bennett
TI CAREER: Modeling Personalized Brain Development with Big Data
DT Awarded Grant
PD Feb 01 2015
PY 2015
AB Big data offer an opportunity to study specific control populations (age
   / sex / environmental factors / demographics / genetics) and identify
   substantive homogeneous sub-cohorts so that one may understand the roles
   that potential factors play in brain development, differentiating
   abnormal trajectories from normal development. The image processing,
   statistical, and informatics tools to effectively and efficiently use
   big data imaging archives for quantitative population-level research and
   personalized medicine do not yet exist. This research will enable
   discovery science on a scale considerably larger than routinely possible
   with traditional study designs by creating novel informatics resources
   that tie archives of 3-D images into accessible research databases. This
   research will discover genetic and environmental factors that influence
   an individual's brain development and characterize the developing human
   brain through personal developmental trajectories. To accomplish this
   goal, new informatics technologies will be created to enable (1) image
   processing and segmentation based on image content in the context of
   heterogeneous, low quality, and error prone data with minimal human
   oversight and (2) routine archival, query, and image processing of large
   medical imaging datasets. This research will impact the areas of (1)
   informatics via novel computation models, (2) neuroscience via a new
   structural model of brain development, and (3) public health via newly
   accessible data sets for research. The science and technology
   innovations enabled by using big data to understand personalized brain
   development will be communicated in a tiered method. Outreach to the
   K-12 audience will target conceptualizing design criteria, inspiring
   students with interactive demonstrations, and providing capabilities for
   students to apply key concepts in hands-on engineering projects. For
   advanced students and researchers, new accessible course materials and
   online modules will be developed so that others may build upon the
   foundations established by this research.<br/><br/>Novel software, data
   wrangling tools, and resources will be created through two research
   thrusts organized around a novel test bed infrastructure and synthesized
   in a third education/outreach thrust. Thrust 1 (Personal Brain
   Trajectories) will focus on extracting meaningful information from
   medical images when performed at scale through (1) creating automated
   methods robust to variations in image quality, acquisition, and transfer
   errors, and (2) enabling efficient human-in-loop control at scale. The
   research will extend novel statistical models for image content labeling
   while adapting quality control techniques from industrial engineering.
   Thrust 2 (Novel Storage & Processing) will create novel medical imaging
   data models to describe data acquisition / retrieval, storage, cleaning,
   access / security, query and processing by integrating of medical
   imaging standards with big data architecture derived from social network
   and e-commerce communities. This infrastructure will provide practical
   access to petabyte imaging archives, integrate with existing data
   workflows, and effectively function with commodity hardware. The PI will
   develop and release a reference test bed to evaluate new technologies in
   the context of computer-aided detection (CADe) of brain abnormalities
   while considering age, sex, and demographics. Using the test bed,
   researchers and students will be able to efficiently evaluate existing
   and emerging image processing software to screen for potential
   prognostic markers. In Thrust 3 (Education and Outreach), the research
   results will be integrated into two classes targeting undergraduate
   students and interactive online modules created and released through an
   established graduate student/faculty training program. Each summer, an
   undergraduate and high school student will participate in research by
   implementing and extending research contributions within an interactive
   demonstration platform. In the second through fifth summers, a high
   school teacher will assist in the development of curricula targeting
   high school students using the demonstration platform. High school
   students and teachers will be recruited from Nashville Metro schools
   with a high underrepresented minority / reduced cost lunch populations.
   These efforts will create an open-source, open-hardware system for
   public demonstration and K-12 classroom exercises.
RI Landman, Bennett/A-2343-2009
Z8 0
ZB 0
ZR 0
ZA 0
ZS 0
TC 0
Z9 0
U1 0
U2 0
G1 1452485
DA 2023-12-08
UT GRANTS:13589350
ER

EF