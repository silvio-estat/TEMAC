FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Silva, Danilo
   Moir, Monika
   Dunaiski, Marcel
   Blanco, Natalia
   Murtala-Ibrahim, Fati
   Baxter, Cheryl
   de Oliveira, Tulio
   Xavier, Joicymara S.
CA INFORM Africa Res Study Grp
TI Review of open-source software for developing heterogeneous data
   management systems for bioinformatics applications
SO BIOINFORMATICS ADVANCES
VL 5
IS 1
AR vbaf168
DI 10.1093/bioadv/vbaf168
DT Review
PD 2025
PY 2025
AB In a world where data drive effective decision-making, bioinformatics
   and health science researchers often encounter difficulties managing
   data efficiently. In these fields, data are typically diverse in format
   and subject. Consequently, challenges in storing, tracking, and
   responsibly sharing valuable data have become increasingly evident over
   the past decades. To address the complexities, some approaches have
   leveraged standard strategies, such as using non-relational databases
   and data warehouses. However, these approaches often fall short in
   providing the flexibility and scalability required for complex projects.
   While the data lake paradigm has emerged to offer flexibility and handle
   large volumes of diverse data, it lacks robust data governance and
   organization. The data lakehouse is a new paradigm that combines the
   flexibility of a data lake with the governance of a data warehouse,
   offering a promising solution for managing heterogeneous data in
   bioinformatics. However, the lakehouse model remains unexplored in
   bioinformatics, with limited discussion in the current literature. In
   this study, we review strategies and tools for developing a data
   lakehouse infrastructure tailored to bioinformatics research. We
   summarize key concepts and assess available open-source and commercial
   solutions for managing data in bioinformatics.Availability and
   implementation Not applicable.
RI de Castro Silva, Danilo/; Santos Xavier, Joicymara/; Moir, Monika/AAU-6520-2021; Dunaiski, Marcel/AAC-9387-2022
OI de Castro Silva, Danilo/0000-0001-5740-3968; Santos Xavier,
   Joicymara/0000-0002-4649-6270; Moir, Monika/0000-0003-1095-1910; 
ZS 0
ZR 0
TC 0
Z8 0
ZB 0
ZA 0
Z9 0
U1 3
U2 3
EI 2635-0041
DA 2025-08-12
UT WOS:001543196800001
PM 40761326
ER

PT J
AU Park, Sun
   Yang, Chan-Su
   Kim, JongWon
TI Design of Vessel Data Lakehouse with Big Data and AI Analysis Technology
   for Vessel Monitoring System
SO ELECTRONICS
VL 12
IS 8
AR 1943
DI 10.3390/electronics12081943
DT Article
PD APR 2023
PY 2023
AB The amount of data in the maritime domain is rapidly increasing due to
   the increase in devices that can collect marine information, such as
   sensors, buoys, ships, and satellites. Maritime data is growing at an
   unprecedented rate, with terabytes of marine data being collected every
   month and petabytes of data already being made public. Heterogeneous
   marine data collected through various devices can be used in various
   fields such as environmental protection, defect prediction,
   transportation route optimization, and energy efficiency. However, it is
   difficult to manage vessel related data due to high heterogeneity of
   such marine big data. Additionally, due to the high heterogeneity of
   these data sources and some of the challenges associated with big data,
   such applications are still underdeveloped and fragmented. In this
   paper, we propose the Vessel Data Lakehouse architecture consisting of
   the Vessel Data Lake layer that can handle marine big data, the Vessel
   Data Warehouse layer that supports marine big data processing and AI,
   and the Vessel Application Services layer that supports marine
   application services. Our proposed a Vessel Data Lakehouse that can
   efficiently manage heterogeneous vessel related data. It can be
   integrated and managed at low cost by structuring various types of
   heterogeneous data using an open source-based big data framework. In
   addition, various types of vessel big data stored in the Data Lakehouse
   can be directly utilized in various types of vessel analysis services.
   In this paper, we present an actual use case of a vessel analysis
   service in a Vessel Data Lakehouse by using AIS data in Busan area.
OI PARK, SUN/0000-0002-7371-2661; Yang, Chan-Su/0000-0002-6882-7325
TC 7
ZS 0
Z8 0
ZR 0
ZA 0
ZB 0
Z9 10
U1 1
U2 30
EI 2079-9292
DA 2023-05-15
UT WOS:000978958700001
ER

PT C
AU Bureva, Veselina
   Atanassov, Krassimir
   Genov, Miroslav
   Sotirov, Sotir
BE Kahraman, C
   Onar, SC
   Cebi, S
   Oztaysi, B
   Tolga, AC
   Sari, IU
TI Index Matrix Representation of Data Storage Structures Using
   Intuitionistic Fuzzy Logic
SO INTELLIGENT AND FUZZY SYSTEMS, INFUS 2024 CONFERENCE, VOL 1
SE Lecture Notes in Networks and Systems
VL 1088
BP 459
EP 466
DI 10.1007/978-3-031-70018-7_51
DT Proceedings Paper
PD 2024
PY 2024
AB In the current research work a big data structure representation using
   extended intuitionistic fuzzy index matrix (EIFIM) is presented. The
   investigation is based on the theories of index matrices, intuitionistic
   fuzzy sets and databases. It is known that big data systems use
   different data structures. The data warehouses are implemented to
   integrate structured datasets. A data lake provides storing capabilities
   for unstructured data. Nowadays, the two concepts are integrated in data
   lakehouse platforms that provide facilities for structured,
   semi-structured and unstructured data. The open-source framework for
   managing and processing huge amounts of data Hadoop is observed. More
   precisely, the HDFS (Hadoop Distributed File System) organization is
   discussed. The aim of the investigation is to present big data structure
   using EIFIM. User access through big data system environment to reach
   files and other resources is analyzed. The intuitionistic fuzzy logic is
   applied to evaluate the big data system processes.
CT International Conference on Intelligent and Fuzzy Systems (INFUS)
CY JUL 16-18, 2024
CL Istanbul Tech Univ, Canakkale, TURKEY
HO Istanbul Tech Univ
SP Canakkale Onsekiz Mart Univ
RI Sotirov, Sotir/M-2488-2013; Bureva, Veselina/; Atanassov, Krassimir/S-2877-2016
OI Bureva, Veselina/0000-0003-4344-4392; 
ZR 0
ZA 0
TC 0
ZS 0
ZB 0
Z8 0
Z9 0
U1 0
U2 1
SN 2367-3370
EI 2367-3389
BN 978-3-031-70017-0; 978-3-031-70018-7
DA 2024-11-13
UT WOS:001331332200050
ER

PT P
AU WANG H
   LIN C
   WANG Y
   ZHANG Z
   ZHU C
   SUN S
   LI W
   MIAO X
   CHEN H
   YAO S
   WU Y
   DONG R
   TIAN Y
TI Method for entering multi-source heterogeneous            energy data
   into data lake, involves obtaining            multi-source heterogeneous
   data from internal and            external systems of energy enterprise,
   preprocessing            lake-entering data, and executing lake data
   filing,            deleting and transferring processes
PN CN119645983-A; CN119645983-B
AE GUIZHOU POWER GRID CO LTD
AB 
   NOVELTY - The method involves obtaining multi-source               
   heterogeneous data from internal and external                systems of
   an energy enterprise. The lake-entering                data is
   preprocessed. The multi-source                heterogeneous data is
   entered into the lake through                different entering
   controllers. An Apache Flink                (RTM: open source stream
   processing framework) is                used as the uniform data
   entering engine. Lake data                filing, deleting and
   transferring processes are                executed, where the
   multi-source heterogeneous data                comprises energy
   production, transmission, storage,                consumption and energy
   market transaction full link                full chain data. A
   SeaTunnel(RTM: open-source big                data integration tool) is
   used to provide data                cleaning. A Spark SQLfunction is
   used to define                pre-processing operator.
   USE - Method for entering multi-source heterogeneous               
   energy data into data lake.
   ADVANTAGE - The method enables using the technical               
   framework of Flink (RTM: open source stream                processing
   framework) + Iceberg(RTM: high                performance open-source
   format for large analytic                tables) to realize real-time
   lake entry of                multi-source heterogeneous energy data,
   supporting                data access from different types of data
   source,                ensuring diversification and comprehensiveness of
   data source, adapting multiple data formats, and               
   satisfying the real-time performance and                flexibility
   requirement of energy service                processing.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   computer device comprising a memory and a processor                for
   executing a set of instructions for entering                multi-source
   heterogeneous energy data into data                lake; (2) a
   computer-readable storage medium for                storing a set of
   instructions for entering                multi-source heterogeneous
   energy data into data                lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for entering multi-source
   heterogeneous                energy data into data lake. (Drawing
   includes                non-English language text).
Z9 0
U1 0
U2 0
DA 2025-04-26
UT DIIDW:2025329080
ER

PT C
AU Sawadogo, Pegdwende N.
   Darmont, Jerome
BE Golfarelli, M
   Wrembel, R
   Kotsis, G
   Tjoa, AM
   Khalil, I
TI Benchmarking Data Lakes Featuring Structured and Unstructured Data with
   DLBench
SO BIG DATA ANALYTICS AND KNOWLEDGE DISCOVERY (DAWAK 2021)
SE Lecture Notes in Computer Science
VL 12925
BP 15
EP 26
DI 10.1007/978-3-030-86534-4_2
DT Proceedings Paper
PD 2021
PY 2021
AB In the last few years, the concept of data lake has become trendy for
   data storage and analysis. Thus, several approaches have been proposed
   to build data lake systems. However, these proposals are difficult to
   evaluate as there are no commonly shared criteria for comparing data
   lake systems. Thus, we introduce DLBench, a benchmark to evaluate and
   compare data lake implementations that support textual and/or tabular
   contents. More concretely, we propose a data model made of both textual
   and CSV documents, a workload model composed of a set of various tasks,
   as well as a set of performance-based metrics, all relevant to the
   context of data lakes. As a proof of concept, we use DLBench to evaluate
   an open source data lake system we previously developed.
CT 23rd International Conference on Big Data Analytics and Knowledge
   Discovery (DaWaK)
CY SEP 27-30, 2021
CL ELECTR NETWORK
SP Software Competence Ctr Hagenberg; JKU Inst Telecooperat; Web Applicat
   Soc
OI Sawadogo, Pegdwendé N/0000-0001-6180-5476
TC 1
ZA 0
Z8 0
ZB 0
ZR 0
ZS 0
Z9 1
U1 0
U2 5
SN 0302-9743
EI 1611-3349
BN 978-3-030-86534-4; 978-3-030-86533-7
DA 2022-12-04
UT WOS:000886498500002
ER

PT J
AU Kretzer, Arthur Raulino
   Barreto Vavassori Benitti, Fabiane
   Siqueira, Frank
TI Challenges and Opportunities in Big Data Analytics for Industry 4.0: A
   Systematic Evaluation of Current Architectures
SO IEEE ACCESS
VL 13
BP 183419
EP 183447
DI 10.1109/ACCESS.2025.3624558
DT Article
PD 2025
PY 2025
AB The current efforts to integrate Big Data Analytics (BDA) into Industry
   4.0 manufacturing systems, despite their usefulness for enhancing
   data-driven decision-making, are constrained by the lack of
   architectural standards for data management. This systematic mapping
   study analyzes many BDA architectures proposed in the literature,
   revealing a fragmented landscape in which the proposed architectures are
   largely conceptual with limited industrial validation. Our analysis
   identifies dominant technological patterns, such as Apache Kafka for
   ingestion, Spark for processing, and Hadoop and Hive for storage, with
   the majority of implementations favoring open-source solutions. Despite
   their theoretical importance, real-time analytics capabilities remain
   underutilized in practice. This study synthesizes a unified conceptual
   reference architecture with eight fundamental layers to provide a
   framework for comparative analysis. We document an imbalance in layer
   development: storage and processing receive comprehensive attention
   while querying, infrastructure management, and monitoring layers remain
   underdeveloped. Implementation approaches show distinct patterns in
   deployment strategies and data handling, with structured and
   semi-structured data well supported, whereas unstructured data
   integration presents ongoing challenges. Future research should focus on
   developing standardized modular frameworks, benchmarking methodologies,
   and integrating modern data lakehouse architectures to bridge the gap
   between theoretical proposals and production-ready systems.
RI Siqueira, Frank/ABB-8351-2021; Raulino Kretzer, Arthur/; Benitti, Fabiane/
OI Raulino Kretzer, Arthur/0000-0003-1656-9464; Benitti,
   Fabiane/0000-0003-2747-9931
ZB 0
Z8 0
ZA 0
ZR 0
TC 0
ZS 0
Z9 0
U1 4
U2 4
SN 2169-3536
DA 2025-11-12
UT WOS:001606717700016
ER

PT P
AU GUI H
   FENG K
   WANG Y
   WANG H
TI Petri network based multi-source heterogeneous            data quality
   detection method, involves forming data            quality analysis
   report in form of table and chart            based on feedback of
   library for realizing monitoring            data quality detection
   process
PN CN112540975-A; CN112540975-B
AE CAS BIG DATA ACAD COMPUTING TECHNOLOGY; BIG DATA ACAD ZHONGKE
AB 
   NOVELTY - The method involves configuring multiple               
   heterogeneous data sources connected in data lake                managed
   by a main system through a data source for                configuring.
   Multiple heterogeneous data sources                are connected with
   the data lake. Multiple                heterogeneous data sources are
   connected with a                local multi-source heterogeneous data
   processing                server. Metadata is obtained by performing
   metadata                collection task to multi-source heterogeneous   
   database, where the metadata comprises metadata                table
   information, field information, index                information and
   constraint information. External                table connection is
   established through autonomous                expansion of
   PostgreSQL(RTM: free and open-source                relational database
   management system) database                according to data source
   information and metadata                information. Data quality rule
   is established                according to quality task constructing
   adding                information feedback of a petri network           
       model.
   USE - Petri network based multi-source heterogeneous                data
   quality detection method.
   ADVANTAGE - The method enables generating data quality               
   analysis result for the user to check, establishing                data
   quality knowledge base to improve quality                problem solving
   ability for providing effective                support for enhancement
   of system data quality so                as to increase data value.
   DETAILED DESCRIPTION - Data quality analysis report is formed in form   
   of table and chart to help a user for analyzing               
   integrity, consistency, accuracy, timeliness and                validity
   of data in a management data source                according to
   real-time feedback of message library                for realizing
   real-time monitoring data quality                detection process.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a petri network based multi-source
   heterogeneous                data quality detection method. (Drawing
   includes                non-English language text).
Z9 0
U1 0
U2 0
DA 2021-04-27
UT DIIDW:2021314369
ER

PT C
AU Levandoski, Justin
   Casto, Garrett
   Deng, Mingge
   Desai, Rushabh
   Edara, Pavan
   Hottelier, Thibaud
   Hormati, Amir
   Johnson, Anoop
   Johnson, Jeff
   Kurzyniec, Dawid
   McVeety, Sam
   Ramanathan, Prem
   Saxena, Gaurav
   Shanmugam, Vidya
   Volobuev, Yuri
GP ACM
TI BigLake: BigQuery's Evolution toward a Multi-Cloud Lakehouse
SO COMPANION OF THE 2024 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA,
   SIGMOD-COMPANION 2024
SE International Conference on Management of Data
BP 334
EP 346
DI 10.1145/3626246.3653388
DT Proceedings Paper
PD 2024
PY 2024
AB BigQuery's cloud-native disaggregated architecture has allowed Google
   Cloud to evolve the system to meet several customer needs across the
   analytics and AI/ML workload spectrum. A key customer requirement for
   BigQuery centers around the unification of data lake and enterprise data
   warehousing workloads. This approach combines: (1) the need for core
   data management primitives, e.g., security, governance, common runtime
   metadata, performance acceleration, ACID transactions, provided by an
   enterprise data warehouses coupled with (2) harnessing the flexibility
   of the open source format and analytics ecosystem along with new
   workload types such as AI/ML over unstructured data on object storage.
   In addition, there is a strong requirement to support BigQuery as a
   multi-cloud offering given cloud customers are opting for a multi-cloud
   footprint by default.
   This paper describes BigLake, an evolution of BigQuery toward a
   multi-cloud lakehouse to address these customer requirements in novel
   ways. We describe three main innovations in this space. We first present
   BigLake tables, making open-source table formats (e.g., Apache Parquet,
   Iceberg) first class citizens, providing fine-grained governance
   enforcement and performance acceleration over these formats to BigQuery
   and other open-source analytics engines. Next, we cover the design and
   implementation of BigLake Object tables that allow BigQuery to integrate
   AI/ML for inferencing and processing over unstructured data. Finally, we
   present Omni, a platform for deploying BigQuery on non-GCP clouds,
   focusing on the infrastructure and operational innovations we made to
   provide an enterprise lakehouse product regardless of the cloud provider
   hosting the data.
CT ACM International Conference on Management of Data (SIGMOD)
CY JUN 09-15, 2024
CL Santiago, CHILE
SP Assoc Comp Machinery; ACM SIGMOD
RI , Justin Levandoski/; Edara, Pavan/; Casto, Garrett/; Deng, Mingge/; , Anoop Johnson/; Hormati, Amir/; Desai, Rushabh/; Saxena, Gaurav/MHR-3400-2025; Johnson, Jeff Jeffrey/
OI , Justin Levandoski/0009-0005-7033-0528; Edara,
   Pavan/0009-0001-8943-140X; Casto, Garrett/0009-0008-8808-1779; Deng,
   Mingge/0009-0005-0497-8203; , Anoop Johnson/0009-0006-0891-0166;
   Hormati, Amir/0009-0002-5786-3301; Desai, Rushabh/0009-0001-7406-8433;
   Johnson, Jeff Jeffrey/0009-0009-9657-3121
Z8 0
ZA 0
ZR 0
ZS 0
ZB 0
TC 6
Z9 9
U1 3
U2 6
SN 0730-8078
BN 979-8-4007-0422-2
DA 2024-08-29
UT WOS:001267334100029
ER

PT C
AU Suleykin, Alexander
   Bobkova, Anna
   Panfilov, Peter
   Chumakov, Ilya
GP IEEE
TI Efficient Data Exchange Between Typical Data Lake and DWH Corporate
   Systems
SO INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER AND ENERGY TECHNOLOGIES
   (ICECET 2021)
BP 1999
EP 2004
DI 10.1109/ICECET52533.2021.9698468
DT Proceedings Paper
PD 2021
PY 2021
AB In the last five years, many companies around the world have been
   successfully implemented Apache Hadoop as a main Data Lake storage for
   all data presented in the organization. At the same time, the adoption
   of other Open-Source technologies has been also increasing for years,
   such as classical MPP-based systems for Analytical workloads. Thus, the
   question of efficient and fast data integration between Apache Hadoop
   and other organizational data storage systems is highly important for
   enterprises, where business and decision makers need the minimum delay
   of big heterogeneous data exchange between Hadoop and other storages. In
   this paper, we compare different options for loading data from Apache
   Hadoop, representing the Data Lake of organization, into Open-Source MPP
   Greenplum database with the role of classical data warehouse for
   analytical workloads, and choose the best one. Also, we identify
   potential risks of using different data loading methods.
CT IEEE International Conference on Electrical, Computer, and Energy
   Technologies (ICECET)
CY DEC 09-10, 2021
CL Cape Town, SOUTH AFRICA
SP Aksaray Univ; IEEE; Univ Johannesburg
RI Panfilov, Peter/AAJ-8308-2021; Suleykin, Alexander/AAC-6050-2022
OI Panfilov, Peter/0000-0001-6567-6309; Suleykin,
   Alexander/0000-0003-2294-6449
ZA 0
ZS 0
TC 0
ZR 0
Z8 0
ZB 0
Z9 0
U1 0
U2 4
BN 978-1-6654-4231-2
DA 2022-07-10
UT WOS:000814669100344
ER

PT C
AU Liu, Ruoran
   Isah, Haruna
   Zulkernine, Farhana
GP IEEE
TI A Big Data Lake for Multilevel Streaming Analytics
SO 2020 1ST INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS AND PRACTICES,
   IBDAP
BP 103
EP 108
DT Proceedings Paper
PD 2020
PY 2020
AB Large organizations are seeking to create new architectures and scalable
   platforms to effectively handle data management challenges due to the
   explosive nature of data rarely seen in the past. These data management
   challenges are largely posed by the availability of streaming data at
   high velocity from various sources in multiple formats. The changes in
   data paradigm have led to the emergence of new data analytics and
   management architecture. This paper focuses on storing high volume,
   velocity and variety data in the raw formats in a data storage
   architecture called a data lake. First, we present our study on the
   limitations of traditional data warehouses in handling recent changes in
   data paradigms. We discuss and compare different open source and
   commercial platforms that can be used to develop a data lake. We then
   describe our end-to-end data lake design and implementation approach
   using the Hadoop Distributed File System (HDFS) on the Hadoop Data
   Platform (HDP). Finally, we present a real-world data lake development
   use case for data stream ingestion, staging, and multilevel streaming
   analytics which combines structured and unstructured data. This study
   can serve as a guide for individuals or organizations planning to
   implement a data lake solution for their use cases.
CT 1st International Conference on Big Data Analytics and Practices (IBDAP)
CY SEP 25-26, 2020
CL ELECTR NETWORK
RI Isah, Haruna/AAB-3693-2019; liu, ruoran/KOQ-5146-2024
ZR 0
ZB 0
TC 0
ZS 0
ZA 0
Z8 0
Z9 0
U1 0
U2 0
BN 978-1-7281-8106-6
DA 2020-01-01
UT WOS:001338048900019
ER

PT P
AU XU B
   GENG Z
   YANG Q
   XU H
   XIAO Z
TI Heterogeneous data integration method based on            virtualization
   technology, involves obtaining optimal            integration solution
   of heterogeneous database to be            integrated or data source
   table
PN CN116701504-A
AE YUNNAN POWER GRID CO LTD INFORMATION CEN
AB 
   NOVELTY - The method involves combining the open source               
   language to construct the data source linker                according to
   the heterogeneous database to be                integrated or/and the
   data source table. The                metadata information of the target
   data source in                the data source linker is extracted. A    
   corresponding packaging table for packaging is                generated,
   and a mapping virtual table is                established. The data
   service issue on the mapping                virtual table is performed,
   and an optimization                operation by combining with the data
   virtualization                engine is performed. The optimal
   integration                solution of the heterogeneous database to be 
        integrated or/and the data source table is                obtained.
   USE - Heterogeneous data integration method based on               
   virtualization technology for use in data                integration
   system (claimed) such as data warehouse                system and data
   lake system.
   ADVANTAGE - Reduces the difficulty of data processing by               
   setting uniform data access and virtual data market                and
   autonomous service analysis, simplifies the                data
   integration mode, data development speed, has                high
   flexibility, can access in fast and controlled                mode, and
   has low use cost.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a heterogeneous data integration system based               
   on virtualization technology;a computer device; anda computer readable
   storage medium storing                heterogeneous data integration
   program based on                virtualization technology.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating
   a                method for integrating heterogeneous data based on     
   virtualization technology and a system. (Drawing                includes
   non-English language text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202396227N
ER

PT J
AU Kim, Ka Ram
   Suh,, Jin Hyung
TI Design of Storage and Processing of Aviation Safety Data in a Hybrid
   Data Lake Platform
Z1 항공 안전 데이터의 혼합 데이터레이크 플랫폼에서의 저장과 처리 설계
SO Journal of Creative Information Culture
S1 창의정보문화연구
VL 10
IS 4
BP 251
EP 260
DT research-article
PD 2024
PY 2024
AB In the aviation industry, safety is a key element in protecting lives
   and maintaining business continuity, and for this purpose, it is
   essential to collect, store, and analyze a large amount of safety data.
   Due to the importance of these aviation assets, many aviation-related
   organizations have studied methods for storing and processing aviation
   safety data and providing processed data. However, existing data storage
   and processing methods are diverse and have limitations in quickly
   processing aviation safety data generated in real time. In particular, a
   mixed data processing platform that can accommodate and process both
   structured data such as maintenance records and operation logs and
   unstructured data such as voice and video data is required, and it
   should be structured with a focus on real-time data processing, data
   governance, and data security. In this study, we will examine the
   structure of a mixed data lake platform that can centrally manage
   aviation safety data from various sources by combining a cloud-based
   data lake and an on-premise database.
AK 항공산업에서의 안전은 생명의 보호와 비즈니스 지속성을 유지하는 핵심 요소로 이를 위하여 방대한 양의 안전 데이터를 수집, 저장,
   분석하는 것이 필수적이다. 이러한 항공 자산의 중요성으로 많은 항공 관련 기관에서 항공 안전 데이터의 저장과 처리, 처리된
   데이터의 제공 방법에 대한 연구가 있었으나 기존의 데이터 저장 및 처리 방식으로는 다양하며 실시간으로 발생하는 항공 안전 데이터의
   신속한 처리에 한계가 있다. 특히 생성 데이터가 정비 기록, 운영 로그와 같은 정형 데이터와 음성 및 영상 데이터와 같은 비정형
   데이터를 모두 수용, 처리할 수 있는 혼합 데이터 처리 플랫폼이 필요하며 실시간 데이터 처리, 데이터 거버넌스, 그리고 데이터
   보안성을 중점으로 구성되어야 한다. 본 연구에서는 클라우드 기반의 데이터레이크와 온프레미스 데이터베이스를 결합하여 다양한 소스의
   항공 안전 데이터를 중앙 집중적으로 관리할 수 있는 혼합 데이터레이크 플렛폼 구조를 살펴보고자 한다.
Z8 0
ZS 0
ZR 0
ZB 0
TC 0
ZA 0
Z9 0
U1 0
U2 0
SN 2384-2008
DA 2024-12-27
UT KJD:ART003141570
ER

PT P
AU ZHANG T
   CHEN Z
   LIU Z
   YU C
   WANG P
   WANG Q
   CHEN W
   LIU Y
   LIU H
TI Method for realizing delta lake data lake index            based on
   Elasticsearch, involves locating storage            position of content
   corresponding to keyword, and            entering positioning position
   according to selection of            user
PN CN116340317-A
AE NANHU LAB
AB 
   NOVELTY - The method involves extracting the source                data,
   and forming a structured Dataset data set                based on Spark.
   The Schema analysis on the                extracted data set is
   performed, and the data                storage address information is
   increased to form an                index structure. The content of the
   index structure                is converted into the support format of
   the search                server and storing in the index database. The 
   keyword input by the user is received by the search               
   server. The search is started based on the index               
   database, and the storage position of the content                is
   located corresponding to the keyword. The                positioning
   position is entered according to the                selection of the
   user.
   USE - Method for realizing delta lake data lake                index
   based on Elasticsearch (RTM: open-source                computer
   programming language developed and                marketed by
   Microsoft(RTM: Company Name)).
   ADVANTAGE - The method enables realizing automatic               
   generating data index scheme on the Delta Lake,                storing
   or searching according to the index scheme,                providing
   efficient index organization and fast                query to support
   high performance of large data                analysis, and realizing
   full fuzzy matching, data                isolation and data fusion
   freely interactive                matching and using full text search
   theory of                Elasticsearch, the multi-source heterogeneous
   data                to reach the purpose of full-text search, and       
       automatically optimizing cache data in the data                lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a  
   method for realizing Deltalake data lake index                based on
   Elasticsearch. (Drawing includes                non-English language
   text)
Z9 0
U1 0
U2 0
DA 2023-07-15
UT DIIDW:202369461J
ER

PT C
AU Chen, Zhe
   Shao, Hangyu
   Li, Yuping
   Lu, Hongru
   Jin, Jiahui
GP IEEE
TI Policy-Based Access Control System for Delta Lake
SO 2022 TENTH INTERNATIONAL CONFERENCE ON ADVANCED CLOUD AND BIG DATA, CBD
SE International Conference on Advanced Cloud and Big Data
BP 60
EP 65
DI 10.1109/CBD58033.2022.00020
DT Proceedings Paper
PD 2022
PY 2022
AB Delta lake is a new generation of data storage solutions. It stores both
   transaction log and data files in one directory, and provides ACID
   transactions, scalable metadata handling, and unifies streaming and
   batch data processing on top of existing data lakes, such as S3, ADLS,
   GCS, and HDFS. Different from data warehouses, delta lakes allow data to
   be stored in the original format, retain complete data information, and
   provide efficient and low-cost storage solutions for data computing and
   analysis businesses. However, Since Delta Lake metadata is scattered in
   different resource files, the lack of a unified metadata view increases
   the difficulty of data governance. Also, Delta Lake adopts an open
   source storage system as the underlying storage, and its basic access
   control does not isolate different users, which may lead the risk of
   data leakage. At present, most common storage systems use data tables'
   row and column fields for access control, while delta lake treats the
   file group as an object. In this paper, aiming at the difficulty of data
   governance, we design a data lake metadata management method to achieve
   unified and efficient management of metadata information in
   heterogeneous data. Then, we design a policy-based data lake access
   control mechanism, combined with the open source permission framework,
   and complete the access request for different users and roles in Delta
   Lake.
CT 10th International Conference on Advanced Cloud and Big Data (CBD)
CY NOV 04-05, 2022
CL Guilin, PEOPLES R CHINA
SP Guangxi Normal Univ; Guilin Univ Technol; SE Univ
RI Jin, Jiahui/JPX-2144-2023
ZB 0
ZS 0
ZA 0
Z8 0
ZR 0
TC 1
Z9 3
U1 1
U2 6
SN 2573-301X
BN 979-8-3503-0971-3
DA 2023-06-01
UT WOS:000976903900011
ER

PT C
AU Tunjic, Aleksandar
BE Koricic, M
   Butkovic, Z
   Skala, K
   Car, Z
   CicinSain, M
   Babic, S
   Sruk, V
   Skvorc, D
   Ribaric, S
   Gros, S
   Vrdoljak, B
   Mauher, M
   Tijan, E
   Pale, P
   Huljenic, D
   Grbac, TG
   Janjic, M
TI The Automation of the Data Lake Ingestion Process from Various Sources
SO 2019 42ND INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION
   TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO)
BP 1276
EP 1281
DT Proceedings Paper
PD 2019
PY 2019
AB In a big data environment, it is often necessary to ingest data from
   different sources into a unique storage. Because of low memory price,
   system distribution and failure tolerance, that storage is typically
   HDFS. It enables users to manipulate data with different tools from the
   Hadoop ecosystem. The process of data ingestion seems simple. However,
   because sources can be different database systems, structured,
   semi-structured and unstructured data complicate the ingestion
   procedure. It is usually not enough to just store everything. Data needs
   to be stored in such a way that enables users to quickly access and
   manipulate it. There are many ingestion-specific solutions in the big
   data ecosystem. This paper will describe an implemented system for data
   ingestion from MSSQL, MySQL and Postgres into a Hive database. The
   process starts with creating tables with corresponding metadata,
   continues with the ingestion process and ends with a description of how
   the process is automated. The implementation of Sqoop as an open-source
   tool and Hue, a web user interface from Cloudera, will be described.
CT 42nd International Convention on Information and Communication
   Technology, Electronics and Microelectronics (MIPRO)
CY MAY 20-24, 2019
CL Opatija, CROATIA
SP IEEE; MIPRO Croatian Soc; IEEE Reg 8; IEEE Croatia Sect; IEEE Croatia
   Sect Comp Chapter; IEEE Croatia Sect Electron Devices Solid State
   Circuits Joint Chapter; IEEE Croatia Sect Educ Chapter; IEEE Croatia
   Sect Commun Chapter; Minist Sci & Educ Republ Croatia; Minist Sea
   Transport & Infrastructure Republ Croatia; Minist Econ Entrepreneurship
   & Crafts Republ Croatia; Minist Publ Adm Republ Croatia; Minist Reg Dev
   & EU Funds Republ Croatia; Minist Environm & Energy Republ Croatia;
   Central State Off Dev Digital Soc; Primorje Gorski Kotar County;
   Croatian Regulatory Author Network Ind; Croatian Power Exchange; Univ
   Zagreb; Univ Rijeka; Juraj Dobrila Univ Pula; Rudjer Boskovic Inst
   Zagreb; Univ Zagreb, Fac Elect Engn & Comp; Univ Zagreb, Fac Org &
   Informat; Univ Rijeka, Fac Maritime Studies; Univ Rijeka, Fac Engn; Univ
   Rijeka, Fac Econ & Business; Zagreb Univ Appl Sci; Croatian Acad Engn;
   Ericsson Nikola Tesla; T Croatian Telecom; Koncar Elect Ind; Croatian
   Elect Co; A1; InfoDom; Storm Comp; Transmitters & Commun Co; Brintel;
   Danieli Automat; Mjerne Tehnologije; Selmet; Inst SDT; Nomen
ZR 0
ZB 0
Z8 0
TC 1
ZA 0
ZS 0
Z9 1
U1 0
U2 5
BN 978-953-233-098-4
DA 2019-09-30
UT WOS:000484544500228
ER

PT J
AU Silvestri, Stefano
   Tricomi, Giuseppe
   Bassolillo, Salvatore Rosario
   De Benedictis, Riccardo
   Ciampi, Mario
TI An Urban Intelligence Architecture for Heterogeneous Data and
   Application Integration, Deployment and Orchestration
SO SENSORS
VL 24
IS 7
AR 2376
DI 10.3390/s24072376
DT Article
PD APR 2024
PY 2024
AB This paper describes a novel architecture that aims to create a template
   for the implementation of an IT platform, supporting the deployment and
   integration of the different digital twin subsystems that compose a
   complex urban intelligence system. In more detail, the proposed Smart
   City IT architecture has the following main purposes: (i) facilitating
   the deployment of the subsystems in a cloud environment; (ii)
   effectively storing, integrating, managing, and sharing the huge amount
   of heterogeneous data acquired and produced by each subsystem, using a
   data lake; (iii) supporting data exchange and sharing; (iv) managing and
   executing workflows, to automatically coordinate and run processes; and
   (v) to provide and visualize the required information. A prototype of
   the proposed IT solution was implemented leveraging open-source
   frameworks and technologies, to test its functionalities and
   performance. The results of the tests performed in real-world settings
   confirmed that the proposed architecture could efficiently and easily
   support the deployment and integration of heterogeneous subsystems,
   allowing them to share and integrate their data and to select, extract,
   and visualize the information required by a user, as well as promoting
   the integration with other external systems, and defining and executing
   workflows to orchestrate the various subsystems involved in complex
   analyses and processes.
RI Silvestri, Stefano/IUP-0829-2023; Tricomi, Giuseppe/S-6029-2019; Ciampi, Mario/B-3874-2015; Bassolillo, Salvatore Rosario/IUN-2349-2023; De Benedictis, Riccardo/AAU-3550-2020
OI Silvestri, Stefano/0000-0002-9890-8409; Tricomi,
   Giuseppe/0000-0003-3837-8730; Ciampi, Mario/0000-0002-7286-6212;
   Bassolillo, Salvatore Rosario/0000-0002-0411-3729; De Benedictis,
   Riccardo/0000-0003-2344-4088
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
TC 11
Z9 12
U1 2
U2 7
EI 1424-8220
DA 2024-04-17
UT WOS:001201137000001
PM 38610587
ER

PT P
AU JYOTHY C R
   ABRAHAM S
   RAVEENDRANATH R
   GOPIKA G
   SHIRIN A
TI Artificial intelligence-driven decision support            system for
   digital transformation in traditional            manufacturing
   enterprises, has secure deployment            architecture that supports
   on-premise or cloud-based            environments
PN IN202541041001-A
AE MANGALAM ENG COLLEGE
AB 
   NOVELTY - The system has a data integration layer for               
   collecting operational and enterprise data from               
   enterprise resource planning (ERP), manufacturing               
   execution system (MES), supervisory control and                data
   acquisition (SCADA) and Internet of things                (IoT) systems.
   An artificial intelligence (AI)                engine performs
   predictive, prescriptive and                diagnostic analytics on
   manufacturing data. A                modular user interface (UI) is
   provided with                role-based dashboards and recommendation   
   visualization. A digital twin simulation module is               
   provided for scenario testing. A secure deployment               
   architecture supports on-premise or cloud-based               
   environments.
   USE - AI-driven decision support system for digital               
   transformation in traditional manufacturing                enterprises.
   ADVANTAGE - The system bridges the gap between traditional              
   manufacturing systems and advanced digital                technologies
   using an incremental and modular                approach, avoids a
   complete overhaul of existing                systems, preserves capital
   investment, provides                AI-powered insights to allow good
   decision-making                across maintenance, quality control,
   energy                management and inventory planning, scales across  
   industries and plant sizes, is cloud compatible,                tailored
   with key performance indicators (KPIs)                relevant to roles
   and deployed in low-connectivity                environments using edge
   computing strategies,                empowers mid-sized manufacturers to
   harness digital                tools for competitiveness, flexibility,
   and                sustainable growth, combines predictive analytics,   
   real-time monitoring, and data visualization to                assist
   plant managers, engineers, and executives in                process
   optimization, machine utilization, and                operational
   forecasting, contributes to the                practical adoption of
   Industry 4.0 technologies                within brownfield industrial
   ecosystems, operates                on legacy systems with limited
   connectivity,                fragmented data sources and outdated       
   decision-making processes, increases data centric               
   economy, guides traditional manufacturing                enterprises
   through the process of digital                transformation, enhances
   existing enterprise                software e.g. ERP, MES, and SCADA
   systems using AI                for analytics, forecasting and
   optimization,                aggregates structured and unstructured data
   from                multiple operational silos comprising machine logs, 
   energy meters, maintenance schedules and supply                chain
   flows, processes data to identify                inefficiencies,
   suggests process adjustments,                predicts machine failures,
   offers role specific                dashboards and actionable
   recommendations,                simulates operational scenarios for
   risk-free                experimentation, supports local deployment for 
   data-sensitive industries and secure cloud                deployment or
   on- premise hosting and uses data                encryption and access
   control mechanisms for                cybersecurity compliance, achieves
   modularity,                ensures that manufacturers adopts components 
   incrementally without overhauling their                infrastructure,
   reduces the barrier to Industry 4.0                adoption by merging
   real-time intelligence,                historical context, and strategic
   insight into a                unified decision support system, empowers 
   manufacturers to remain competitive and agile,                augments
   industrial operations without disrupting                core processes,
   collects and aggregates operational                data from IoT
   sensors, machine controllers, energy                meters, and
   maintenance logs in real time into a                central data lake,
   pre-processes data using                techniques e.g. normalization,
   anomaly detection                and imputation to ensure quality,
   identifies                operational bottlenecks, predicts component
   wear,                classifies machine efficiency zones, generates     
   prescriptive suggestions e.g. optimal machine                scheduling,
   energy saving strategies or proactive                inventory ordering,
   features interactive dashboards                for plant managers,
   supervisors and executives and                allows simulation of
   scenarios before implementing                real-world changes.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
              AI-driven decision support system.
Z9 0
U1 0
U2 0
DA 2025-06-21
UT DIIDW:202557041V
ER

PT P
AU SONG S
   DONG X
TI Method for performing intelligent construction for            enterprise
   data, involves supportting switching of            computing engines in
   different scenarios, and            performing real-time query and
   offline query through            Hadoop ecological technology ecology on
   basis of            storage integration
PN CN114691762-A
AE SUZHOU YINGTIANDI INFORMATION TECHNOLOGY
AB 
   NOVELTY - The method involves performing operations at a               
   level of computing and data architecture on a basis                of
   storage integration through a cooperation of a                data
   integration module, a data processing module,                a data
   synchronization module, and a data asset                center. A
   unified management of various data                sources is performed
   in a one-key access mode, and                a database or schema
   required to be accessed is                specified. Uploading is
   performed through                unstructured and semi-structured data,
   a unified                access interface channel is opened for a
   downstream                data processing flow, data expansion and      
   maintenance are performed through a file transfer               
   protocol (FTP) data source acquisition, and a                switching
   of calculation engines of different                scenes is supported
   through Hadoop (RTM: collection                of open-source software
   developed by Apache                Software Foundation) ecological
   technology ecology,                and real-time query and off-line
   query are                performed on the basis of storage              
    integration.
   USE - Method for performing intelligent construction                for
   enterprise data.
   ADVANTAGE - The method enables realizing smooth transition              
   of offline data and real-time data, and ensuring                data
   reliability and abnormal complement. The                method allows a
   data integration module to support                the heterogeneous data
   source of direct number and                off-line synchronization to
   an integrated                collection service of a data lake, thus
   realizing                flexible coding and configuration of different
   data                number requirements, supporting arrangement         
   scheduling of different development languages, and               
   supporting the access of the data to a business                angle of
   understanding for modeling and finishing                into a
   retrievable data development                environment.
Z9 0
U1 0
U2 0
DA 2022-08-09
UT DIIDW:202289641K
ER

PT B
AU Bouziane, Anas
Z2  
TI On-Demand Health Data Provisioning With Custom Temporary Data Views for
   Big Data Platforms
DT Dissertation/Thesis
PD Jan 01 2023
PY 2023
TC 0
ZA 0
ZB 0
Z8 0
ZR 0
ZS 0
Z9 0
U1 0
U2 0
BN 9798302848277
UT PQDT:121301049
ER

PT J
AU MURUGIAH, KARTHIK 
TI Automated ascertainment of bleeding and target lesion revascularization
   after percutaneous coronary intervention (PCI) using electronic health
   record (EHR) data
DT Awarded Grant
PD Jan 25 2022
PY 2023
AB PROJECT SUMMARYPercutaneous coronary intervention (PCI) is the most
   common cardiac procedure with over 650,000 PCIperformed annually in the
   U.S. Post-PCI complications which occur in a significant proportion of
   patients areassociated with an increased risk of morbidity and
   mortality. Reliable ascertainment of post-PCI events isimportant for
   performance measurement, submission to disease registries, clinical
   trials, and for cardiaccatheterization laboratory (CCL) safety
   monitoring. Claims based detection of PCI complications is
   inadequate.Assessing post-PCI events reliably requires an in-depth
   manual chart review, which incurs a significantprovider and
   administrative burden. However, with advances in health information
   technology and nationwideadoption of electronic health record (EHR)
   systems, it possible to utilize EHR for the automatic derivation
   ofclinical events. Dr. Murugiah proposes to create and validate
   automated algorithms which can be applied toEHR data to detect two
   important post-PCI events which are a common focus of clinical trials
   and qualityimprovement efforts – in-hospital bleeding and 1-year target
   lesion revascularization (TLR). Using EHR data ata large health system,
   Dr. Murugiah will develop a hybrid algorithm to detect major bleeding
   post-PCI byleveraging structured data fields such as laboratory values,
   as well as unstructured data such as imagingreports, cardiac
   catheterization reports, and progress notes incorporating Natural
   Language Processing (NLP)techniques (Aim 1). Similarly, using cardiac
   catheterization reports for patients undergoing repeatrevascularization
   within 1 year, an algorithm will be developed to detect TLR (Aim 2).
   Both algorithms will beexternally validated using EHR data from another
   large institution. The final algorithm will be implemented intoa tool
   generating scheduled reports of bleeding and TLR, to be fed back to the
   quality assurance team for theCCL and to individual operators.
   Individual operators will be surveyed to obtain feedback about the
   algorithm,reporting process, and their perceived benefit. The final
   tools will be made open source (Aim 3). An automatedalgorithm for the
   detection of post-PCI events within EHR can reduce administrative
   burden, enable thegeneration of new knowledge from EHR based
   observational studies, and enable pragmatic clinical trials.Further,
   this project can serve as a proof of concept of the utility of hybrid
   tools leveraging both structured dataand clinical text for surveillance
   and quality measurement. Dr. Murugiah has a career interest in studying
   andimproving the treatment for ischemic heart disease using
   multidimensional datasets and EHR data to developreal time risk
   prediction models and decision support tools, and conduct EHR based
   comparative effectivenessstudies and clinical trials. During the award
   period he will leverage the experience of his mentorship teamwhich
   includes national experts in cardiovascular outcomes research, clinical
   informatics, and computationallinguistics. He will also acquire formal
   training in clinical informatics by completing a Master of Health
   Sciencedegree which will provide him the necessary platform to make the
   transition into an independent investigator.
ZA 0
ZR 0
TC 0
ZB 0
ZS 0
Z8 0
Z9 0
U1 0
U2 0
G1 10555326; 5K08HL157727-02; K08HL157727
DA 2024-07-23
UT GRANTS:17710121
ER

PT J
AU MURUGIAH, KARTHIK 
TI Automated ascertainment of bleeding and target lesion revascularization
   after percutaneous coronary intervention (PCI) using electronic health
   record (EHR) data
DT Awarded Grant
PD Jan 25 2022
PY 2022
AB PROJECT SUMMARYPercutaneous coronary intervention (PCI) is the most
   common cardiac procedure with over 650,000 PCIperformed annually in the
   U.S. Post-PCI complications which occur in a significant proportion of
   patients areassociated with an increased risk of morbidity and
   mortality. Reliable ascertainment of post-PCI events isimportant for
   performance measurement, submission to disease registries, clinical
   trials, and for cardiaccatheterization laboratory (CCL) safety
   monitoring. Claims based detection of PCI complications is
   inadequate.Assessing post-PCI events reliably requires an in-depth
   manual chart review, which incurs a significantprovider and
   administrative burden. However, with advances in health information
   technology and nationwideadoption of electronic health record (EHR)
   systems, it possible to utilize EHR for the automatic derivation
   ofclinical events. Dr. Murugiah proposes to create and validate
   automated algorithms which can be applied toEHR data to detect two
   important post-PCI events which are a common focus of clinical trials
   and qualityimprovement efforts – in-hospital bleeding and 1-year target
   lesion revascularization (TLR). Using EHR data ata large health system,
   Dr. Murugiah will develop a hybrid algorithm to detect major bleeding
   post-PCI byleveraging structured data fields such as laboratory values,
   as well as unstructured data such as imagingreports, cardiac
   catheterization reports, and progress notes incorporating Natural
   Language Processing (NLP)techniques (Aim 1). Similarly, using cardiac
   catheterization reports for patients undergoing repeatrevascularization
   within 1 year, an algorithm will be developed to detect TLR (Aim 2).
   Both algorithms will beexternally validated using EHR data from another
   large institution. The final algorithm will be implemented intoa tool
   generating scheduled reports of bleeding and TLR, to be fed back to the
   quality assurance team for theCCL and to individual operators.
   Individual operators will be surveyed to obtain feedback about the
   algorithm,reporting process, and their perceived benefit. The final
   tools will be made open source (Aim 3). An automatedalgorithm for the
   detection of post-PCI events within EHR can reduce administrative
   burden, enable thegeneration of new knowledge from EHR based
   observational studies, and enable pragmatic clinical trials.Further,
   this project can serve as a proof of concept of the utility of hybrid
   tools leveraging both structured dataand clinical text for surveillance
   and quality measurement. Dr. Murugiah has a career interest in studying
   andimproving the treatment for ischemic heart disease using
   multidimensional datasets and EHR data to developreal time risk
   prediction models and decision support tools, and conduct EHR based
   comparative effectivenessstudies and clinical trials. During the award
   period he will leverage the experience of his mentorship teamwhich
   includes national experts in cardiovascular outcomes research, clinical
   informatics, and computationallinguistics. He will also acquire formal
   training in clinical informatics by completing a Master of Health
   Sciencedegree which will provide him the necessary platform to make the
   transition into an independent investigator.
ZR 0
ZB 0
Z8 0
ZS 0
ZA 0
TC 0
Z9 0
U1 0
U2 0
G1 10371710; 1K08HL157727-01A1; K08HL157727
DA 2023-12-14
UT GRANTS:15572751
ER

EF