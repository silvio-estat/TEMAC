FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU O'Leary, Daniel E.
TI Embedding AI and Crowdsourcing in the Big Data Lake
SO IEEE INTELLIGENT SYSTEMS
VL 29
IS 5
BP 70
EP 73
DI 10.1109/MIS.2014.82
DT Editorial Material
PD SEP-OCT 2014
PY 2014
RI O'Leary, Daniel Edmund/B-6469-2008
OI O'Leary, Daniel Edmund/0000-0002-5240-9516
Z8 2
ZS 0
ZB 4
ZA 0
TC 67
ZR 0
Z9 76
U1 1
U2 43
SN 1541-1672
EI 1941-1294
DA 2014-09-01
UT WOS:000345088300010
ER

PT C
AU Matsebula, Fezile
   Mnkandla, Ernest
BE Cornish, DR
TI A BIG DATA ARCHITECTURE FOR LEARNING ANALYTICS IN HIGHER EDUCATION
SO 2017 IEEE AFRICON
SE Africon
BP 951
EP 956
DT Proceedings Paper
PD 2017
PY 2017
AB Data with high volume, velocity, variety and veracity brings the new
   experience curve of analytics. Big data in higher education comes from
   different sources that include blogs, social networks, student
   information systems, learning management systems, research, and other
   machine-generated data. Once the data is analysed it promises better
   student placement processes; more accurate enrolment forecasts, and
   early warning systems that identify and assist students at-risk of
   failing or dropping out. Big data is becoming a key to creating
   competitive advantages in higher education. Like with any organization,
   traditional data processing and analysis of structured and unstructured
   data using RDBMS and data warehousing no longer satisfy big data
   challenges. The lack of adequate conceptual architectures for big data
   tailored for institutions of higher education has led to many failures
   to produce meaningful, accessible, and timely information for decision
   making. Therefore, this calls for the development of conceptual
   architectures for big data in higher education. This paper presents an
   architecture for big data analytics in higher education.
CT IEEE AFRICON Conference - Science, Technology and Innovation for Africa
CY SEP 18-20, 2017
CL Cape Town, SOUTH AFRICA
SP IEEE; mlab; IEEE Reg 8; IEEE S Africa Sect; IES; Univ Pretoria; SAiEE;
   IBM; Altair; CST
RI Mnkandla, Ernest/G-5235-2012; Matsebula, Fezile/JDD-6998-2023; Matsebula, Fezile/
OI Matsebula, Fezile/0000-0001-7646-6243
ZB 0
ZS 0
TC 16
ZA 0
Z8 1
ZR 0
Z9 29
U1 0
U2 17
SN 2153-0025
BN 978-1-5386-2775-4
DA 2017-01-01
UT WOS:000424741600162
ER

PT J
AU Damiani, A.
   Masciocchi, C.
   Lenkowicz, J.
   Capocchiano, N. D.
   Boldrini, L.
   Tagliaferri, L.
   Cesario, A.
   Sergi, P.
   Marchetti, A.
   Luraschi, A.
   Patarnello, S.
   Valentini, V.
TI Building an Artificial Intelligence Laboratory Based on Real World Data:
   The Experience of Gemelli Generator
SO FRONTIERS IN COMPUTER SCIENCE
VL 3
AR 768266
DI 10.3389/fcomp.2021.768266
DT Article
PD DEC 7 2021
PY 2021
AB The problem of transforming Real World Data into Real World Evidence is
   becoming increasingly important in the frameworks of Digital Health and
   Personalized Medicine, especially with the availability of modern
   algorithms of Artificial Intelligence high computing power, and large
   storage facilities.Even where Real World Data are well maintained in a
   hospital data warehouse and are made available for research purposes,
   many aspects need to be addressed to build an effective architecture
   enabling researchers to extract knowledge from data.We describe the
   first year of activity at Gemelli Generator RWD, the challenges we faced
   and the solutions we put in place to build a Real World Data laboratory
   at the service of patients and health researchers. Three classes of
   services are available today: retrospective analysis of existing patient
   data for descriptive and clustering purposes; automation of knowledge
   extraction, ranging from text mining, patient selection for trials, to
   generation of new research hypotheses; and finally the creation of
   Decision Support Systems, with the integration of data from the hospital
   data warehouse, apps, and Internet of Things.
RI Lenkowicz, Jacopo/AAT-8218-2020; Luraschi, Alice/; CESARIO, Alfredo/O-4215-2015; MARCHETTI, ANTONIO/W-2226-2018; Capocchiano, Nikola Dino/AAA-9318-2022
OI Lenkowicz, Jacopo/0000-0002-8366-1474; Luraschi,
   Alice/0000-0001-7400-7182; MARCHETTI, ANTONIO/0009-0008-7589-3196;
   Capocchiano, Nikola Dino/0000-0003-3556-9232
ZA 0
ZS 0
ZB 3
TC 27
Z8 0
ZR 0
Z9 28
U1 0
U2 4
EI 2624-9898
DA 2021-12-30
UT WOS:000733593100001
ER

PT C
AU Giebler, Corinna
   Groger, Christoph
   Hoos, Eva
   Schwarz, Holger
   Mitschang, Bernhard
GP IEEE
TI A Zone Reference Model for Enterprise-Grade Data Lake Management
SO 2020 IEEE 24TH INTERNATIONAL ENTERPRISE DISTRIBUTED OBJECT COMPUTING
   CONFERENCE (EDOC 2020)
SE IEEE International Enterprise Distributed Object Computing
   Conference-EDOC
BP 57
EP 66
DI 10.1109/EDOC49727.2020.00017
DT Proceedings Paper
PD 2020
PY 2020
AB Data lakes are on the rise as data platforms for any kind of analytics,
   from data exploration to machine learning. They achieve the required
   flexibility by storing heterogeneous data in their raw format, and by
   avoiding the need for pre-defined use cases. However, storing only raw
   data is inefficient, as for many applications, the same data processing
   has to be applied repeatedly. To foster the reuse of processing steps,
   literature proposes to store data in different degrees of processing in
   addition to their raw format. To this end, data lakes are typically
   structured in zones. There exists various zone models, but they are
   varied, vague, and no assessments are given. It is unclear which of
   these zone models is applicable in a practical data lake implementation
   in enterprises. In this work, we assess existing zone models using
   requirements derived from multiple representative data analytics use
   cases of a real-world industry case. We identify the shortcomings of
   existing work and develop a zone reference model for enterprise-grade
   data lake management in a detailed manner. We assess the reference
   model's applicability through a prototypical implementation for a
   real-world enterprise data lake use case. This assessment shows that the
   zone reference model meets the requirements relevant in practice and is
   ready for industry use.
CT 24th IEEE International Enterprise Distributed Object Computing
   Conference (IEEE EDOC)
CY OCT 05-08, 2020
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc
RI Schwarz, Holger/AAP-1719-2020
TC 18
Z8 1
ZR 0
ZB 0
ZA 0
ZS 0
Z9 23
U1 0
U2 3
SN 2325-6354
BN 978-1-7281-6473-1
DA 2021-04-20
UT WOS:000630246800007
ER

PT J
AU Murri, Rita
   Masciocchi, Carlotta
   Lenkowicz, Jacopo
   Fantoni, Massimo
   Damiani, Andrea
   Marchetti, Antonio
   Sergi, Paolo Domenico Angelo
   Arcuri, Giovanni
   Cesario, Alfredo
   Patarnello, Stefano
   Antonelli, Massimo
   Bellantone, Rocco
   Bernabei, Roberto
   Boccia, Stefania
   Calabresi, Paolo
   Cambieri, Andrea
   Cauda, Roberto
   Colosimo, Cesare
   Crea, Filippo
   De Maria, Ruggero
   De Stefano, Valerio
   Franceschi, Francesco
   Gasbarrini, Antonio
   Landolfi, Raffaele
   Parolini, Ornella
   Richeldi, Luca
   Sanguinetti, Maurizio
   Urbani, Andrea
   Zega, Maurizio
   Scambia, Giovanni
   Valentini, Vincenzo
CA Gemelli Against Covid Grp
TI A real-time integrated framework to support clinical decision making for
   covid-19 patients
SO COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
VL 217
AR 106655
DI 10.1016/j.cmpb.2022.106655
EA FEB 2022
DT Article
PD APR 2022
PY 2022
AB Background: The COVID-19 pandemic affected healthcare systems worldwide.
   Predictive models developed by Artificial Intelligence (AI) and based on
   timely, centralized and standardized real world patient data could
   improve management of COVID-19 to achieve better clinical outcomes. The
   objectives of this manuscript are to describe the structure and
   technologies used to construct a COVID-19 Data Mart architecture and to
   present how a large hospital has tackled the challenge of supporting
   daily management of COVID-19 pandemic emergency, by creating a strong
   retrospective knowledge base, a real time environment and integrated
   information dashboard for daily practice and early identification of
   critical condition at patient level. This framework is also used as an
   informative, continuously enriched data lake, which is a base for
   several on-going predictive studies. Methods: The information technology
   framework for clinical practice and research was described. It was
   developed using SAS Institute software analytics tool and SAS (R) Vyia
   (R) environment and Open-Source environment R (R) and Python (R) for
   fast prototyping and modeling. The included variables and the source
   extraction procedures were presented. Results: The Data Mart covers a
   retrospective cohort of 5528 patients with SARS-CoV-2 infection. People
   who died were older, had more comorbidities, reported more frequently
   dyspnea at onset, had higher D-dimer, C-reactive protein and urea
   nitrogen. The dashboard was developed to support the management of
   COVID-19 patients at three levels: hospital, single ward and individual
   care level. Interpretation: The COVID-19 Data Mart based on integration
   of a large collection of clinical data and an AI-based integrated
   framework has been developed, based on a set of automated procedures for
   data mining and retrieval, transformation and integration, and has been
   embedded in the clinical practice to help managing daily care. Benefits
   from the availability of a Data Mart include the opportunity to build
   predictive models with a machine learning approach to identify
   undescribed clinical phenotypes and to foster hospital networks. A
   real-time updated dashboard built from the Data Mart may represent a
   valid tool for a better knowledge of epidemiological and clinical
   features of COVID-19, especially when multiple waves are observed, as
   well as for epidemic and pandemic events of the same nature (e. g. with
   critical clinical conditions leading to severe pulmonary inflammation).
   Therefore, we believe the approach presented in this paper may find
   several applications in comparable situations even at region or state
   levels. Finally, models predicting the course of future waves or new
   pandemics could largely benefit from network of DataMarts. (c) 2022 The
   Authors. Published by Elsevier B.V.This is an open access article under
   the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/)
RI Crea, Filippo/AAC-9754-2022; Lenkowicz, Jacopo/AAT-8218-2020; De Stefano, Valerio/AAE-8083-2021; De Vito, Francesco/U-2385-2019; CESARIO, Alfredo/O-4215-2015; Murri, Rita/G-8367-2017; Sanguinetti, Maurizio/A-1453-2019; Masciocchi, Carlotta/AAO-4431-2020; Damiani, Andrea/AAM-9855-2020; Richeldi, Luca/B-6243-2019; Fantoni, Massimo/AAB-7371-2022; Gasbarrini, Antonio/AAB-8487-2019; Scambia, Giovanni/K-7539-2016; Arcuri, Giovanni/MVX-2135-2025; Parolini, Ornella/ABI-7862-2020; Antonelli, Massimo/K-9915-2016; MARCHETTI, ANTONIO/W-2226-2018
OI Gasbarrini, Antonio/0000-0002-6230-1779; Scambia,
   Giovanni/0000-0002-9503-9041; Arcuri, Giovanni/0000-0002-4137-7416;
   Parolini, Ornella/0000-0002-5211-6430; Antonelli,
   Massimo/0000-0003-3007-1670; MARCHETTI, ANTONIO/0009-0008-7589-3196
ZR 0
Z8 0
ZA 0
ZS 0
ZB 2
TC 14
Z9 19
U1 0
U2 17
SN 0169-2607
EI 1872-7565
DA 2022-05-08
UT WOS:000786593500001
PM 35158181
ER

PT J
AU Bilal, Mohsin
   Tsang, Yee Wah
   Ali, Mahmoud
   Graham, Simon
   Hero, Emily
   Wahab, Noorul
   Dodd, Katherine
   Sahota, Harvir
   Wu, Shaobin
   Lu, Wenqi
   Jahanifar, Mostafa
   Robinson, Andrew
   Azam, Ayesha
   Benes, Ksenija
   Nimir, Mohammed
   Hewitt, Katherine
   Bhalerao, Abhir
   Eldaly, Hesham
   Raza, Shan E. Ahmed
   Gopalakrishnan, Kishore
   Minhas, Fayyaz
   Snead, David
   Rajpoot, Nasir
TI Development and validation of artificial intelligence-based prescreening
   of large-bowel biopsies taken in the UK and Portugal: a retrospective
   cohort study
SO LANCET DIGITAL HEALTH
VL 5
IS 11
BP E786
EP E797
DI 10.1016/S2589-7500(23)00148-6
DT Article
PD NOV 2023
PY 2023
AB Background Histopathological examination is a crucial step in the
   diagnosis and treatment of many major diseases. Aiming to facilitate
   diagnostic decision making and improve the workload of pathologists, we
   developed an artificial intelligence (AI)-based prescreening tool that
   analyses whole-slide images (WSIs) of large-bowel biopsies to identify
   typical, non-neoplastic, and neoplastic biopsies. Methods This
   retrospective cohort study was conducted with an internal development
   cohort of slides acquired from a hospital in the UK and three external
   validation cohorts of WSIs acquired from two hospitals in the UK and one
   clinical laboratory in Portugal. To learn the differential histological
   patterns from digitised WSIs of large-bowel biopsy slides, our proposed
   weakly supervised deep-learning model (Colorectal AI Model for
   Abnormality Detection [CAIMAN]) used slide-level diagnostic labels and
   no detailed cell or region-level annotations. The method was developed
   with an internal development cohort of 5054 biopsy slides from 2080
   patients that were labelled with corresponding diagnostic categories
   assigned by pathologists. The three external validation cohorts, with a
   total of 1536 slides, were used for independent validation of CAIMAN.
   Each WSI was classified into one of three classes (ie, typical, atypical
   non-neoplastic, and atypical neoplastic). Prediction scores of image
   tiles were aggregated into three prediction scores for the whole slide,
   one for its likelihood of being typical, one for its likelihood of being
   non-neoplastic, and one for its likelihood of being neoplastic. The
   assessment of the external validation cohorts was conducted by the
   trained and frozen CAIMAN model. To evaluate model performance, we
   calculated area under the convex hull of the receiver operating
   characteristic curve (AUROC), area under the precision-recall curve, and
   specificity compared with our previously published iterative draw and
   rank sampling (IDaRS) algorithm. We also generated heat maps and
   saliency maps to analyse and visualise the relationship between the WSI
   diagnostic labels and spatial features of the tissue microenvironment.
   The main outcome of this study was the ability of CAIMAN to accurately
   identify typical and atypical WSIs of colon biopsies, which could
   potentially facilitate automatic removing of typical biopsies from the
   diagnostic workload in clinics. Findings A randomly selected subset of
   all large bowel biopsies was obtained between Jan 1, 2012, and Dec 31,
   2017. The AI training, validation, and assessments were done between Jan
   1, 2021, and Sept 30, 2022. WSIs with diagnostic labels were collected
   between Jan 1 and Sept 30, 2022. Our analysis showed no statistically
   significant differences across prediction scores from CAIMAN for typical
   and atypical classes based on anatomical sites of the biopsy. At 0.99
   sensitivity, CAIMAN (specificity 0.5592) was more accurate than an
   IDaRS-based weakly supervised WSI-classification pipeline (0.4629) in
   identifying typical and atypical biopsies on cross-validation in the
   internal development cohort (p<0.0001). At 0.99 sensitivity, CAIMAN was
   also more accurate than IDaRS for two external validation cohorts
   (p<0.0001), but not for a third external validation cohort (p=0.10).
   CAIMAN provided higher specificity than IDaRS at some high-sensitivity
   thresholds (0.7763 vs 0.6222 for 0.95 sensitivity, 0.7126 vs 0.5407 for
   0.97 sensitivity, and 0.5615 vs 0.3970 for 0.
   99 sensitivity on one of the external validation cohorts) and showed
   high classification performance in distinguishing between neoplastic
   biopsies (AUROC 0.9928, 95% CI 0<middle dot>9927-0<middle dot>9929),
   inflammatory biopsies (0.9658, 0<middle dot>9655-0<middle dot>9661), and
   atypical biopsies (0.9789, 0<middle dot>9786-0<middle dot>9792). On the
   three external validation cohorts, CAIMAN had AUROC values of 0.9431
   (95% CI 0<middle dot>9165-0<middle dot>9697), 0.9576 (0<middle
   dot>9568-0<middle dot>9584), and 0.9636 (0<middle dot>9615-0<middle
   dot>9657) for the detection of atypical biopsies. Saliency maps
   supported the representation of disease heterogeneity in model
   predictions and its association with relevant histological features.
   Interpretation CAIMAN, with its high sensitivity in detecting atypical
   large-bowel biopsies, might be a promising improvement in clinical
   workflow efficiency and diagnostic decision making in prescreening of
   typical colorectal biopsies.
RI Bilal, Mohsin/AAN-6349-2020; Raza, Shan E Ahmed/LSM-3276-2024; BHALERAO, Abhir/; Ali, Mahmoud/O-1907-2019; Wahab, Noorul/AAW-8308-2021
OI Bilal, Mohsin/0000-0001-8632-2729; Raza, Shan E
   Ahmed/0000-0002-1097-1738; BHALERAO, Abhir/0000-0001-8830-329X; 
TC 15
ZS 0
ZA 0
ZB 4
Z8 1
ZR 0
Z9 17
U1 1
U2 12
EI 2589-7500
DA 2024-02-06
UT WOS:001150091700001
PM 37890902
ER

PT J
AU Chen, Ji
   Chokshi, Sara
   Hegde, Roshini
   Gonzalez, Javier
   Iturrate, Eduardo
   Aphinyanaphongs, Yin
   Mann, Devin
TI Development, Implementation, and Evaluation of a Personalized Machine
   Learning Algorithm for Clinical Decision Support: Case Study With
   Shingles Vaccination
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 22
IS 4
AR e16848
DI 10.2196/16848
DT Article
PD APR 29 2020
PY 2020
AB Background: Although clinical decision support (CDS) alerts are
   effective reminders of best practices, their effectiveness is blunted by
   clinicians who fail to respond to an overabundance of inappropriate
   alerts. An electronic health record (EHR)-integrated machine learning
   (ML) algorithm is a potentially powerful tool to increase the
   signal-to-noise ratio of CDS alerts and positively impact the
   clinician's interaction with these alerts in general.
   Objective: This study aimed to describe the development and
   implementation of an ML-based signal-to-noise optimization system
   (SmartCDS) to increase the signal of alerts by decreasing the volume of
   low-value herpes zoster (shingles) vaccination alerts.
   Methods: We built and deployed SmartCDS, which builds personalized user
   activity profiles to suppress shingles vaccination alerts unlikely to
   yield a clinician's interaction. We extracted all records of shingles
   alerts from January 2017 to March 2019 from our EHR system, including
   327,737 encounters, 780 providers, and 144,438 patients.
   Results: During the 6 weeks of pilot deployment, the SmartCDS system
   suppressed an average of 43.67% (15,425/35,315) potential shingles
   alerts (appointments) and maintained stable counts of weekly shingles
   vaccination orders (326.3 with system active vs 331.3 in the control
   group; P=.38) and weekly user-alert interactions (1118.3 with system
   active vs 1166.3 in the control group; P=.20).
   Conclusions: All key statistics remained stable while the system was
   turned on. Although the results are promising, the characteristics of
   the system can be subject to future data shifts, which require automated
   logging and monitoring. We demonstrated that an automated, ML-based
   method and data architecture to suppress alerts are feasible without
   detriment to overall order rates. This work is the first alert
   suppression ML-based model deployed in practice and serves as
   foundational work in encounter-level customization of alert display to
   maximize effectiveness.
OI Chokshi, Sara/0000-0002-3952-6654; Chen, Ji/0000-0002-4856-0141;
   Iturrate, Eduardo/0000-0001-6434-2518; Gonzalez,
   Javier/0000-0002-7562-6070; Mann, Devin M/0000-0002-2099-0852;
   Aphinyanaphongs, Yin/0000-0001-8605-5392
ZS 0
TC 14
Z8 0
ZB 1
ZA 0
ZR 0
Z9 17
U1 1
U2 5
SN 1439-4456
EI 1438-8871
DA 2020-05-14
UT WOS:000529305600001
PM 32347813
ER

PT C
AU Wibowo, Merlinda
   Sulaiman, Sarina
   Shamsuddin, Siti Mariyam
BA Shi, Y
BE Tan, Y
   Takagi, H
TI Machine Learning in Data Lake for Combining Data Silos
SO DATA MINING AND BIG DATA, DMBD 2017
SE Lecture Notes in Computer Science
VL 10387
BP 294
EP 306
DI 10.1007/978-3-319-61845-6_30
DT Proceedings Paper
PD 2017
PY 2017
AB Data silo can grow to be a large-scale data for years, overlapping and
   has an indefinite quality. It allows an organization to develop their
   own analytical capabilities. Data lake has the ability to solve this
   problem efficiently with the data analysis by using statistical and
   predictive modeling techniques which can be applied to enhance and
   support an organization's business strategy. This study provides an
   overview of the process of decision-making, operational efficiency, and
   creating the solution for an organization. Machine Learning can
   distribute the architecture of data model and integrate the data silo
   with other organizations data to optimize the operational business
   processes within an organization in order to improve data quality and
   efficiency. Testing is done by utilizing the data from the Malaysia's
   and Singapore's Government Open Data on the Air Pollutant Index to
   determine the condition of air pollution levels for the health and
   safety of the population.
CT 2nd International Conference on Data Mining and Big Data (DMBD)
CY JUL 27-AUG 01, 2017
CL Fukuoka, JAPAN
SP Peking Univ, Computat Intelligence Lab; Kyushu Univ, Res Ctr Appl
   Perceptual Sci; IEEE Computat Intelligence Soc; IEEE Syst, Man &
   Cybernet Soc, Japan Chapter
RI Wibowo, Merlinda/AAD-1609-2021; Sulaiman, Sarina/A-1704-2013
Z8 0
TC 8
ZA 0
ZB 0
ZS 0
ZR 0
Z9 14
U1 0
U2 40
SN 0302-9743
EI 1611-3349
BN 978-3-319-61845-6; 978-3-319-61844-9
DA 2018-08-15
UT WOS:000440465200030
ER

PT J
AU Lopez, Ivan Dario
   Grass, Jose Fernando
   Figueroa, Apolinar
   Corrales, Juan Carlos
TI A proposal for a multi-domain data fusion strategy in a climate-smart
   agriculture context
SO INTERNATIONAL TRANSACTIONS IN OPERATIONAL RESEARCH
VL 30
IS 4
SI SI
BP 2049
EP 2070
DI 10.1111/itor.12899
EA OCT 2020
DT Article
PD JUL 2023
PY 2023
AB Agriculture provides food, raw materials, and employment opportunities
   for a significant percentage of the world's population. Climate,
   economic, political, social, and other conditions affect decision making
   in agricultural processes. In many cases, these conditions imply the
   loss of suitability of many areas for some traditional crops. In
   contrast, these areas can produce new crops by taking advantage of
   changing conditions. In this sense, having reliable tools and
   information for decision making is essential in adapting to new
   agricultural productivity scenarios. The above implies having sufficient
   and relevant data sources to reduce the uncertainty in the
   decision-making processes. However, data by nature tend to be diverse in
   structure, storage formats, and access protocols. Data fusion tasks have
   been immersed in a multitude of applications and have been approached
   from different points of view when implementing a suitable solution. We
   propose a multi-domain data fusion strategy to support data analysis
   tasks in agricultural contexts. We also describe all the data sources
   collected, which are the main input to the proposed strategy. The
   combined data sources were also evaluated through a preliminary
   exploratory analysis in a multi-label learning approach. Finally, the
   data fusion strategy is explained through an example in agricultural
   crop production.
RI Figueroa Casas, Apolinar/AAC-3182-2019; Lopez Gomez, Ivan Dario/; lopez, ivan/GSM-8495-2022
OI Figueroa Casas, Apolinar/0000-0003-3586-8187; Lopez Gomez, Ivan
   Dario/0000-0002-9781-6094; 
ZS 0
Z8 0
TC 11
ZR 0
ZA 0
ZB 3
Z9 13
U1 0
U2 25
SN 0969-6016
EI 1475-3995
DA 2020-11-19
UT WOS:000587020700001
ER

PT C
AU Li, Sujie
   Zhang, Guigang
   Wang, Jian
GP IEEE
TI Civil Aircraft Health Management Research based on Big Data and Deep
   Learning Technologies
SO 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT
   (ICPHM)
BP 154
EP 159
DT Proceedings Paper
PD 2017
PY 2017
AB the coupling and correlation degree between aircraft systems is higher,
   and the diagnosis and prognosis of aircraft are more complex. Building a
   platform for storing and analyzing the aviation big data becomes an
   important task for civil aviation. This paper proposes a civil aircraft
   health management big data architecture. The civil aircraft health
   management system includes airborne PHM, ground PHM, remote diagnosis
   system, portable maintenance assistant system, maintenance center,
   automatic test equipment, special test equipment. Airborne PHM collects
   data from multiple types of data sources. Ground PHM provides decision
   making support for civil aircrafts including real-time alarm, health
   management, maintenance plan, spare parts. The paper introduces deep
   learning algorithm and aircraft fault diagnosis and prognosis
   implementation.
CT IEEE International Conference on Prognostics and Health Management
   (ICPHM)
CY JUN 19-21, 2017
CL Dallas, TX
SP IEEE; IEEE Reliabil Soc
ZS 0
TC 9
ZB 0
Z8 2
ZR 0
ZA 0
Z9 13
U1 3
U2 18
BN 978-1-5090-5710-8
DA 2017-01-01
UT WOS:000452639100025
ER

PT C
AU Spangenberg, Norman
   Wilke, Moritz
   Franczyk, Bogdan
BE Shakshuki, E
TI A Big Data architecture for intra-surgical remaining time predictions
SO 8TH INTERNATIONAL CONFERENCE ON EMERGING UBIQUITOUS SYSTEMS AND
   PERVASIVE NETWORKS (EUSPN 2017) / 7TH INTERNATIONAL CONFERENCE ON
   CURRENT AND FUTURE TRENDS OF INFORMATION AND COMMUNICATION TECHNOLOGIES
   IN HEALTHCARE (ICTH-2017) / AFFILIATED WORKSHOPS
SE Procedia Computer Science
VL 113
BP 310
EP 317
DI 10.1016/j.procs.2017.08.332
DT Proceedings Paper
PD 2017
PY 2017
AB The operating room area is still one of the most expensive sections in
   the hospital due to the high resource requirements and the diverse
   uncertainties. However there are few solutions that support monitoring
   and decision-making in operating room management. But with new data
   sources and analytical methods of big data research more improvements
   could be achieved. In this work we utilize surgical phase events
   recognized in surgical device data to learn prediction models and
   trigger online predictions for remaining intervention times in operating
   rooms. To identify the best algorithm for prediction model computation
   with the existing data, we evaluate a set of regression algorithms.
   Based on this methods we propose an architecture approach for the
   integrated processing of real-time data and historic learning data. The
   evaluation and comparison with related work shows that our prototype is
   competitive regarding prediction accuracy. (C) 2017 The Authors.
   Published by Elsevier B.V.
CT 8th International Conference on Emerging Ubiquitous Systems and
   Pervasive Networks (EUSPN) / 7th International Conference on Current and
   Future Trends of Information and Communication Technologies in
   Healthcare (ICTH)
CY SEP 18-20, 2017
CL Lund, SWEDEN
ZB 0
TC 9
Z8 0
ZR 0
ZS 0
ZA 0
Z9 13
U1 0
U2 9
SN 1877-0509
BN *****************
DA 2018-02-01
UT WOS:000419236500040
ER

PT J
AU Atek, Sofiane
   Pesaresi, Cristiano
   Eugeni, Marco
   De Vito, Corrado
   Cardinale, Vincenzo
   Mecella, Massimo
   Rescio, Antonello
   Petronzio, Luca
   Vincenzi, Aldo
   Pistillo, Pasquale
   Bianchini, Filippo
   Giusto, Gianfranco
   Pasquali, Giorgio
   Gaudenzi, Paolo
TI A Geospatial Artificial Intelligence and satellite-based earth
   observation cognitive system in response to COVID-19
SO ACTA ASTRONAUTICA
VL 197
BP 323
EP 335
DI 10.1016/j.actaastro.2022.05.013
EA JUN 2022
DT Article
PD AUG 2022
PY 2022
AB The pandemic emergency caused by the spread of COVID-19 has stressed the
   importance of promptly identifying new epidemic clusters and patterns,
   to ensure the implementation of local risk containment measures and
   provide the needed healthcare to the population. In this framework,
   artificial intelligence, GIS, geospatial analysis and space assets can
   play a crucial role. Social media analytics can be used to trigger Earth
   Observation (EO) satellite acquisitions over potential new areas of
   human aggregation. Similarly, EO satellites can be used jointly with
   social media analytics to systematically monitor well-known areas of
   aggregation (green urban areas, public markets, etc.). The information
   that can be obtained from the Earth Cognitive System 4 COVID-19 (ECO4CO)
   are both predictive, aiming to identify possible new clusters of
   outbreaks, and at the same time supervisorial, by monitoring
   infrastructures (i.e. traffic jams, parking lots) or specific categories
   (i.e. teenagers, doctors, teachers, etc.). In this perspective, the
   technologies described in this paper will allow us to detect critical
   areas where individuals can be involved in risky aggregation clusters.
   The ECO4CO data lake will be integrated with ad hoc data obtained by
   health care structures to understand trends and dynamics, to assess
   criticalities with respect to medical response and supplies, and to test
   possibilities useful to tackle potential future emergencies. The System
   will also provide geographical information on the spread of the
   infection which will allow an appropriate context-specific public health
   response to the epidemic. This project has been co-funded by the
   European Space Agency under its Business Applications programme.
RI Cardinale, Vincenzo/AAB-9296-2019; Mecella, Massimo/N-8699-2019; Atek, Sofiane/GRF-4083-2022
OI Cardinale, Vincenzo/0000-0003-0234-3341; Mecella,
   Massimo/0000-0002-9730-8882; Atek, Sofiane/0000-0001-5213-5343
ZB 0
TC 8
ZS 0
ZA 0
Z8 0
ZR 0
Z9 10
U1 3
U2 25
SN 0094-5765
EI 1879-2030
DA 2022-07-06
UT WOS:000817166900004
PM 35582681
ER

PT J
AU Salierno, Giulio
   Leonardi, Letizia
   Cabri, Giacomo
TI A Big Data Architecture for Digital Twin Creation of Railway Signals
   Based on Synthetic Data
SO IEEE OPEN JOURNAL OF INTELLIGENT TRANSPORTATION SYSTEMS
VL 5
BP 1
EP 18
DI 10.1109/OJITS.2024.3412820
DT Article
PD 2024
PY 2024
AB Industry 5.0 has introduced new possibilities for defining key features
   of the factories of the future. This trend has transformed traditional
   industrial production by exploiting Digital Twin (DT) models as virtual
   representations of physical manufacturing assets. In the railway
   industry, Digital Twin models offer significant benefits by enabling
   anticipation of developments in rail systems and subsystems, providing
   insight into the future performance of physical assets, and allowing
   testing and prototyping solutions prior to implementation. This paper
   presents our approach for creating a Digital Twin model in the railway
   domain. We particularly emphasize the critical role of Big Data in
   supporting decision-making for railway companies and the importance of
   data in creating virtual representations of physical objects in railway
   systems. Our results show that the Digital Twin model of railway switch
   points, based on synthetic data, accurately represents the behavior of
   physical railway switches in terms of data points.
RI Leonardi, Letizia/L-9722-2015; Salierno, Giulio/; Cabri, Giacomo/M-6723-2015
OI Leonardi, Letizia/0000-0003-4035-8560; Salierno,
   Giulio/0000-0002-9617-4448; Cabri, Giacomo/0000-0002-4942-2453
ZA 0
Z8 0
TC 4
ZB 0
ZR 0
ZS 0
Z9 6
U1 1
U2 12
EI 2687-7813
DA 2024-07-28
UT WOS:001273038700001
ER

PT J
AU Di Martino, Beniamino
   Cante, Luigi Colucci
   D'Angelo, Salvatore
   Esposito, Antonio
   Graziano, Mariangela
   Marulli, Fiammetta
   Lupi, Pietro
   Cataldi, Alessandra
TI A Big Data Pipeline and Machine Learning for Uniform Semantic
   Representation of Data and Documents From IT Systems of the Italian
   Ministry of Justice
SO INTERNATIONAL JOURNAL OF GRID AND HIGH PERFORMANCE COMPUTING
VL 14
IS 1
DI 10.4018/IJGHPC.301579
DT Article
PD 2022
PY 2022
AB In this paper, a big data pipeline is presented, taking in consideration
   both structured and unstructured data made available by the Italian
   Ministry of Justice, regarding their telematic civil process. Indeed,
   the complexity and volume of the data provided by the ministry requires
   the application of big data analysis techniques, in concert with machine
   and deep learning frameworks, to be correctly analysed and to obtain
   meaningful information that could support the ministry itself in better
   managing civil processes. The pipeline has two main objectives: to
   provide a consistent workflow of activities to be applied to the
   incoming data, aiming at extracting useful information for the
   ministry's decision making tasks, and to homogenize the incoming data,
   so that they can be stored in a centralized and coherent data lake to be
   used as a reference for further analysis and considerations.
RI Di Martino, Beniamino/O-6876-2015; Esposito, Antonio/AHC-3301-2022; Colucci Cante, Luigi/; Graziano, Mariangela/HPF-2471-2023; Marulli, Fiammetta/AAD-4051-2022; D'Angelo, Salvatore/GQB-4948-2022
OI Di Martino, Beniamino/0000-0001-7613-1312; Esposito,
   Antonio/0000-0002-2004-4815; Colucci Cante, Luigi/0009-0005-5226-6737;
   Graziano, Mariangela/0000-0002-1258-8249; 
ZA 0
ZS 0
ZB 0
TC 5
ZR 0
Z8 0
Z9 6
U1 0
U2 1
SN 1938-0259
EI 1938-0267
DA 2023-02-17
UT WOS:000916579600019
ER

PT J
AU Kachaoui, Jabrane
   Belangour, Abdessamad
TI Enhanced Data Lake Clustering Design based on K-means Algorithm
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
VL 11
IS 4
BP 547
EP 554
DT Article
PD APR 2020
PY 2020
AB In recent years, Big Data requirements have evolved. Organizations are
   trying more than ever to accent their efforts on industrial development
   of all data at their disposal and move further away from underpinning
   technologies. After investing around Data Lake concept, organizations
   must now overhaul their data architecture to face IoT (Internet of
   Things) and AI (Artificial Intelligence) expansion. Efficient and
   effective data mapping treatments could serve in understanding the
   importance of data being transformed and used for decision-making
   process endorsement. As current relational databases are not able to
   manage large amounts of data, organizations headed towards NoSQL (Not
   only Structured Query Language) databases. One such known NoSQL database
   is MongoDB, which has a high scalability. This article mainly put
   forward a new data model able to extract, classify, and then map data
   for the purpose of generating new more structured data that meet
   organizational needs. This can be carried out by calculating various
   metadata attributes weights, which are considered as important
   information. It also processed on data clustering stored into MongoDB.
   This categorization based on data mining clustering algorithm named
   K-Means.
RI Belangour, Abdessamad/KAL-6712-2024
ZB 1
TC 6
ZA 0
ZR 0
Z8 0
ZS 0
Z9 6
U1 0
U2 4
SN 2158-107X
EI 2156-5570
DA 2020-06-16
UT WOS:000537489900072
ER

PT J
AU Simionato, Rafael
   Torres Neto, Jose Rodrigues
   dos Santos, Carla Julciane
   Ribeiro, Bruno Silva
   Britto de Araujo, Fernando Cesar
   de Paula, Antonio Robson
   de Lima Oliveira, Pedro Augusto
   Fernandes, Paulo Silas
   Yi, Jin Hong
TI Survey on connectivity and cloud computing technologies:
   State-of-the-art applied to Agriculture 4.0
SO REVISTA CIENCIA AGRONOMICA
VL 51
SI SI
AR e20207755
DI 10.5935/1806-6690.20200085
DT Article
PD 2020
PY 2020
AB In recent years, agriculture has faced many challenges, from a growing
   global population to be fed, the work power evasion in the sector, to
   sustainability requirements and environmental constraints. To satisfy
   the increasingly demanding stakeholders, the agricultural sector has
   looked for new ways to tackle these issues. In this context, Information
   and Communications Technologies (ICTs) have been applied to help the
   agricultural sector overcome these challenges. This article investigates
   how two ICTs - connectivity and cloud computing - can leverage and
   traverse other ICTs, such as Internet of Things and artificial
   intelligence, enabling the entire productive sector to be supported by
   decision-making systems, which in turn are based on data-driven models.
   Moreover, a successful case study on how cloud computing has helped one
   of SiDi's biggest customers - a global company - improve its operational
   performance by obtaining insights from its data is presented.
ZS 0
ZR 0
ZA 0
ZB 2
TC 3
Z8 0
Z9 6
U1 1
U2 15
SN 0045-6888
EI 1806-6690
DA 2021-09-17
UT WOS:000692719200019
ER

PT J
AU Goldrick, Stephen
   Alosert, Haneen
   Lovelady, Clare
   Bond, Nicholas J.
   Senussi, Tarik
   Hatton, Diane
   Klein, John
   Cheeks, Matthew
   Turner, Richard
   Savery, James
   Farid, Suzanne S.
TI Next-generation cell line selection methodology leveraging data lakes,
   natural language generation and advanced data analytics
SO FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY
VL 11
AR 1160223
DI 10.3389/fbioe.2023.1160223
DT Article
PD JUN 5 2023
PY 2023
AB Cell line development is an essential stage in biopharmaceutical
   development that often lies on the critical path. Failure to fully
   characterise the lead clone during initial screening can lead to lengthy
   project delays during scale-up, which can potentially compromise
   commercial manufacturing success. In this study, we propose a novel cell
   line development methodology, referenced as CLD ( 4 ), which involves
   four steps enabling autonomous data-driven selection of the lead clone.
   The first step involves the digitalisation of the process and storage of
   all available information within a structured data lake. The second step
   calculates a new metric referenced as the cell line manufacturability
   index (MI ( CL )) quantifying the performance of each clone by
   considering the selection criteria relevant to productivity, growth and
   product quality. The third step implements machine learning (ML) to
   identify any potential risks associated with process operation and
   relevant critical quality attributes (CQAs). The final step of CLD ( 4 )
   takes into account the available metadata and summaries all relevant
   statistics generated in steps 1-3 in an automated report utilising a
   natural language generation (NLG) algorithm. The CLD ( 4 ) methodology
   was implemented to select the lead clone of a recombinant Chinese
   hamster ovary (CHO) cell line producing high levels of an
   antibody-peptide fusion with a known product quality issue related to
   end-point trisulfide bond (TSB) concentration. CLD ( 4 ) identified
   sub-optimal process conditions leading to increased levels of trisulfide
   bond that would not be identified through conventional cell line
   development methodologies. CLD ( 4 ) embodies the core principles of
   Industry 4.0 and demonstrates the benefits of increased digitalisation,
   data lake integration, predictive analytics and autonomous report
   generation to enable more informed decision making.
RI Farid, Suzanne/O-3075-2019; Hatton, Diane/AAG-2955-2021
ZR 0
ZS 0
ZA 0
ZB 3
TC 4
Z8 0
Z9 5
U1 0
U2 9
SN 2296-4185
DA 2023-07-05
UT WOS:001013027300001
PM 37342509
ER

PT C
AU Silva, Alecio
   Souza, Gilberto F. M.
GP IEEE
TI Prognosis Smart System AI-based Applied to Equipment Health Monitoring
   in 4.0 Industry Scenario
SO 67TH ANNUAL RELIABILITY & MAINTAINABILITY SYMPOSIUM (RAMS 2021)
SE Reliability and Maintainability Symposium
DI 10.1109/RAMS48097.2021.9605722
DT Proceedings Paper
PD 2021
PY 2021
AB In the age of IIoT - Industrial Internet of Things, data lake, data
   mining, big data, and cloud computing, the smart manufacturing enables
   to make more informed decisions in real-time by using the database
   extracted from sensors in its equipment. During an operational campaign,
   the Health Monitoring System (HMS) also allows an understanding of how
   component degradation is affecting the performance of the equipment.
   Through a structure supported by AI, as data lake and cloud computing,
   the HMS provides to monitored equipment a fault detection system, early
   warning alarms to prevent failures and a calculation of the remaining
   useful life (RUL).
   The purpose of this paper is to present a prognosis smart system based
   on AI applied to HMS to support decision-making regarding operational
   performance of equipment. A Recurrent Neural Network (RNN) procedure is
   developed to continuously analyze the mass of monitoring data generated
   during the machine operation. The ability to learn the behavior patterns
   of the collected signals and in this way to be able to make parameter
   predictions with high accuracy makes artificial neural networks a
   powerful tool to carry out an effective prognosis. Machine operational
   parameters are monitored simultaneously by the prognosis smart system.
   Then, this information is processed by the neural network and used to
   characterize the machine operational condition. Upon detecting a failure
   trend for one or more parameters monitored by recognizing deterioration
   patterns, the prognosis system calculates the remaining useful life
   (RUL) and allows maintainers to take early actions before the failure
   occurrence.
   The proposed methodology is applied as part of a HMS of a hydro
   generator based on parameters registered in operator inspections routes
   designed to identify critical equipment degradation. The registered data
   representing one operational year are used to train the neural network
   regarding normal and abnormal machine condition. After training, the
   neural network is able to predict failure trends for monitored
   temperature parameters of the hydro-generator lubricating system that is
   critical to support equipment performance. Comparing prediction data and
   data collected by the sensors, the developed neural network reached
   about 0,98 RMSE score. The remaining useful life prognosis proved to be
   an important tool to avoid hydro generator components unexpected
   failures which may affect power output and cause penalties to the power
   generation company.
CT 67th Annual Reliability and Maintainability Symposium (RAMS)
CY MAY 24-27, 2021
CL Orlando, FL
RI de Souza, Gilberto/P-1299-2018
ZR 0
ZS 0
TC 5
ZA 0
Z8 0
ZB 0
Z9 5
U1 0
U2 3
SN 0149-144X
BN 978-1-7281-8017-5
DA 2022-05-11
UT WOS:000784131300024
ER

PT C
AU Neves, Ricardo A.
   Cruvinel, Paulo E.
GP IEEE
TI Model for Semantic Base Structuring of Digital Data to Support
   Agricultural Management
SO 2020 IEEE 14TH INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC
   2020)
SE IEEE International Conference on Semantic Computing
BP 337
EP 340
DI 10.1109/ICSC.2020.00067
DT Proceedings Paper
PD 2020
PY 2020
AB This article presents a semantic model for structuring digital databases
   to function in a cloud environment and connect to data sources
   originating from Big Data. The work examines the process of receiving
   structured, semi-structured and unstructured data for use in
   agricultural risk management. It is conceived as an architecture that
   combines Data Mart, Data Warehouse (NoSQL), and Data Lake resources to
   support decision making, through knowledge discovery and applies
   algorithms for data mining by machine learning resources. The
   configuration presented addresses scenarios involving agricultural data,
   obtained from sensors operating in multiple modes.
CT 14th IEEE International Conference on Semantic Computing (ICSC)
CY FEB 03-05, 2020
CL San Diego, CA
SP IEEE; IEEE Comp Soc
RI Cruvinel, Paulo/C-7687-2015
ZR 0
TC 4
ZB 0
Z8 0
ZS 0
ZA 0
Z9 5
U1 1
U2 4
SN 2325-6516
BN 978-1-7281-6332-1
DA 2020-09-17
UT WOS:000565450400058
ER

PT J
AU Elhassouni, Jalil
   El Qadi, Abderrahim
   El Alami, Yasser El Madani
   El Haziti, Mohamed
TI The Implementation of Credit Risk Scorecard Using Ontology Design
   Patterns and BCBS 239
SO CYBERNETICS AND INFORMATION TECHNOLOGIES
VL 20
IS 2
BP 93
EP 104
DI 10.2478/cait-2020-0019
DT Article
PD 2020
PY 2020
AB Nowadays information and communication technologies are playing a
   decisive role in helping the financial institutions to deal with the
   management of credit risk. There have been significant advances in
   scorecard model for credit risk management. Practitioners and policy
   makers have invested in implementing and exploring a variety of new
   models individually. Coordinating and sharing information groups,
   however, achieved less progress. One of several causes of the 2008
   financial crisis was in data architecture and information technology
   infrastructure. To remedy this problem the Basel Committee on Banking
   Supervision (BCBS) outlined a set of principles called BCBS 239. Using
   Ontology Design Patterns (ODPs) and BCBS 239, credit risk scorecard and
   applicant ontologies are proposed to improve the decision making process
   in credit loan. Both ontologies were validated, distributed in Ontology
   Web Language (OWL) files and checked in the test cases using SPARQL.
   Thus, making their (re)usability and expandability easier in financial
   institutions. These ontologies will also make sharing data more
   effective and less costly.
RI EL MADANI EL ALAMI, Yasser/ABG-5957-2021
ZR 0
ZB 0
Z8 0
ZA 0
TC 3
ZS 0
Z9 4
U1 0
U2 7
SN 1311-9702
EI 1314-4081
DA 2020-10-07
UT WOS:000572531700008
ER

PT J
AU Le, Ngoc-Bao-van
   Seo, Yeong-Seok
   Huh, Jun-Ho
TI Artificial Intelligence in Finance: Coffee Commodity Trading Big Data
   for Informed Decision Making
SO IEEE ACCESS
VL 12
BP 91780
EP 91792
DI 10.1109/ACCESS.2024.3409762
DT Article
PD 2024
PY 2024
AB Coffee, the second-largest global soft commodity, can take advantage of
   a comprehensive mining of daily and historical market data for more
   effective informed trading decisions. Advanced ICT and data mining
   technologies can change the trading market operation. The existing
   systems are confronted with certain constraints, including incomplete
   data, insufficient documentation for storage, and a requirement for a
   scalable infrastructure for big data analytics, such as a data warehouse
   or data lakehouse. To address this issue, the paper presents a design
   and implementation of a coffee commodity trading big data warehouse
   capable of analyzing various essential parameters for supporting
   informed decision-making. First, the designed system can automatically
   collect coffee trading data for New York Arabica coffee futures prices
   from selected worldwide reports and financial data portals. Next, the
   Extract, transform, and load (ETL) process is adopted to ingest coffee
   futures trading crawled data into the 3 layers data warehouse. Finally,
   the analytical system will extract and visualize selected key dimensions
   that influence coffee futures prices within different observation
   windows and perspectives. As a result, we implement a prototype of a
   coffee trading data warehouse on the crawled data from January 2000 to
   October 2022 and visualize trends in coffee futures prices based on the
   collected data for informed decision-making. The construction system is
   capable of stably operating and processing large volumes of transaction
   data. This paper will be valuable documentation for reference and
   decision support for coffee commodity trading enterprises and contribute
   to the development of future forecasting algorithms.
RI Le, Ngoc Bao Van/IXW-9767-2023; Huh, Jun-Ho/AAC-1518-2022; Seo, Yeong-Seok/AAF-2849-2019
OI Le, Ngoc Bao Van/0000-0002-3464-1274; Huh, Jun-Ho/0000-0001-6735-6456;
   Seo, Yeong-Seok/0000-0002-5319-7674
ZR 0
TC 3
ZB 0
Z8 0
ZS 0
ZA 0
Z9 3
U1 6
U2 23
SN 2169-3536
DA 2024-07-23
UT WOS:001269900500001
ER

PT C
AU Ostberg, Per-Olov
   Vyhmeister, Eduardo
   Castane, Gabriel G.
   Meyers, Bart
   Van Noten, Johan
TI Domain Models and Data Modeling as Drivers for Data Management: The
   ASSISTANT Data Fabric Approach
SO IFAC PAPERSONLINE
VL 55
IS 10
BP 19
EP 24
DI 10.1016/j.ifacol.2022.09.362
EA OCT 2022
DT Proceedings Paper
PD OCT 2022
PY 2022
AB To develop AI-based models capable of governing or providing decision
   support to complex manufacturing environments, abstractions and
   mechanisms for unified management of data storage and processing
   capabilities are needed. Specifically, as such models tend to include
   and rely on detailed representations of systems, components, and tools
   with complex interactions, mechanisms for simplifying, integrating, and
   scaling management capabilities in the presence of complex data
   requirements (e.g., high volume, velocity, and diversity of data) are of
   particular interest. A data fabric is a system that provides a unified
   architecture for management and provisioning of data. In this work we
   present the background, design requirements, and high-level outline of
   the ASSISTANT data fabric - a flexible data management tool designed for
   use in adaptive manufacturing contexts. The paper outlines the
   implementation of the system with specific focus on the use of domain
   models and the data modeling approach used, as well as provides a
   generic use case structure reusable in many industrial contexts.
   Copyright (C) 2022 The Authors.
CT 10th IFAC Triennial Conference on Manufacturing Modelling, Management
   and Control (MIM)
CY JUN 22-24, 2022
CL Nantes, FRANCE
SP Int Federat Automat Control, Tech Comm 5 2 Management & Control Mfg &
   Logist; Int Federat Automat Control, Tech Comm 1 3 Discrete Event &
   Hybrid Syst; Int Federat Automat Control, Tech Comm 3 2 Computat
   Intelligence Control; Int Federat Automat Control, Tech Comm 5 1 Mfg
   Plant Control; Int Federat Automat Control, Tech Comm 7 4 Transportat
   Syst; Int Federat Automat Control, Tech Comm 9 1 Econ, Business, &
   Financial Syst
ZA 0
ZR 0
TC 2
ZS 1
Z8 0
ZB 0
Z9 3
U1 1
U2 12
SN 2405-8963
DA 2022-12-02
UT WOS:000881681700004
ER

PT J
AU Abouzaid, Ahmed
   Barclay, Peter J.
   Chrysoulas, Christos
   Pitropakis, Nikolaos
TI Building a modern data platform based on the data lakehouse architecture
   and cloud-native ecosystem
SO DISCOVER APPLIED SCIENCES
VL 7
IS 3
AR 166
DI 10.1007/s42452-025-06545-w
DT Article
PD FEB 22 2025
PY 2025
AB In today's Big Data world, organisations can gain a competitive edge by
   adopting data-driven decision-making. However, a modern data platform
   that is portable, resilient, and efficient is required to manage
   organisations' data and support their growth. Furthermore, the change in
   the data management architectures has been accompanied by changes in
   storage formats, particularly open standard formats like Apache Hudi,
   Apache Iceberg, and Delta Lake. With many alternatives, organisations
   are unclear on how to combine these into an effective platform. Our work
   investigates capabilities provided by Kubernetes and other Cloud-Native
   software, using DataOps methodologies to build a generic data platform
   that follows the Data Lakehouse architecture. We define the data
   platform specification, architecture, and core components to build a
   proof of concept system. Moreover, we provide a clear implementation
   methodology by developing the core of the proposed platform, which are
   infrastructure (Kubernetes), ingestion and transport (Argo Workflows),
   storage (MinIO), and finally, query and processing (Dremio). We then
   conducted performance benchmarks using an industry-standard benchmark
   suite to compare cold/warm start scenarios and assess Dremio's caching
   capabilities, demonstrating a 12% median enhancement of query duration
   with caching.
RI AbouZaid, Ahmed/; Pitropakis, Nikolaos/ACW-7211-2022; Chrysoulas, Christos/AAD-8176-2020
OI AbouZaid, Ahmed/0009-0007-5524-5055; Pitropakis,
   Nikolaos/0000-0002-3392-9970; Chrysoulas, Christos/0000-0001-9817-003X
ZS 0
TC 1
ZB 0
ZA 0
Z8 0
ZR 0
Z9 2
U1 3
U2 6
EI 3004-9261
DA 2025-02-27
UT WOS:001427902800002
ER

PT J
AU Maass, Laura
   Badino, Manuel
   Iyamu, Ihoghosa
   Holl, Felix
TI Assessing the Digital Advancement of Public Health Systems Using
   Indicators Published in Gray Literature: Narrative Review
SO JMIR PUBLIC HEALTH AND SURVEILLANCE
VL 10
AR e63031
DI 10.2196/63031
DT Review
PD 2024
PY 2024
AB Background: Revealing the full potential of digital public health (DiPH)
   systems requires a wide-ranging tool to assess their maturity and
   readiness for emerging technologies. Although a variety of indices exist
   to assess digital health systems, questions arise about the inclusion of
   indicators of information and communications technology maturity and
   readiness, digital (health) literacy, and interest in DiPH tools by the
   society and workforce, as well as the maturity of the legal framework
   and the readiness of digitalized health systems. Existing tools
   frequently target one of these domains while overlooking the others. In
   addition, no review has yet holistically investigated the available
   national DiPH system maturity and readiness indicators using a
   multidisciplinary lens. Objective: We used a narrative review to map the
   landscape of DiPH system maturity and readiness indicators published in
   the gray literature. Methods: As original indicators were not published
   in scientific databases, we applied predefined search strings to the
   DuckDuckGo and Google search engines for 11 countries from all
   continents that had reached level 4 of 5 in the latest Global Digital
   Health Monitor evaluation. In addition, we searched the literature
   published by 19 international organizations for maturity and readiness
   indicators concerning DiPH. Results: Of the 1484 identified references,
   137 were included, and they yielded 15,806 indicators. We deemed 286
   indicators from 90 references relevant for DiPH system maturity and
   readiness assessments. The majority of these indicators (133/286, 46.5%)
   had legal relevance (targeting big data and artificial intelligence
   regulation, cybersecurity, national DiPH strategies, or health data
   governance), and the smallest number of indicators (37/286, 12.9%) were
   related to social domains (focusing on internet use and access, digital
   literacy and digital health literacy, or the use of DiPH tools,
   smartphones, and computers). Another 14.3% (41/286) of indicators
   analyzed the information and communications technology infrastructure
   (such as workforce, electricity, internet, and smartphone availability
   or interoperability standards). The remaining 26.2% (75/286) of
   indicators described the degree to which DiPH was applied (including
   health data architecture, storage, and access; the implementation of
   DiPH interventions; or the existence of interventions promoting health
   literacy and digital inclusion). Conclusions:Our work is the first to
   conduct a multidisciplinary analysis of the gray literature on DiPH
   maturity and readiness assessments. Although new methods for
   systematically researching gray literature are needed, our study holds
   the potential to develop more comprehensive tools for DiPH system
   assessments. We contributed toward a more holistic understanding of
   DiPH. Further examination is required to analyze the suitability and
   applicability of all identified indicators in diverse health care
   settings. By developing a standardized method to assess DiPH system
   maturity and readiness, we aim to foster informed decision-making among
   health care planners and practitioners to improve resource distribution
   and continue to drive innovation in health care delivery.
RI Iyamu, Ihoghosa/IWE-5004-2023; Badino, Manuel/; Holl, Felix/Y-9648-2019; Maaß, Laura/AEX-5567-2022
OI Iyamu, Ihoghosa/0000-0003-0271-9468; Badino, Manuel/0000-0003-2193-2168;
   Holl, Felix/0000-0002-4020-9509; Maaß, Laura/0000-0001-7354-8120
ZR 0
ZS 0
ZB 0
Z8 0
TC 2
ZA 0
Z9 2
U1 5
U2 11
SN 2369-2960
DA 2025-01-07
UT WOS:001388074000001
PM 39566910
ER

PT J
AU Sreepathy, H., V
   Rao, B. Dinesh
   Kumar, J. Mohan
   Rao, B. Deepak
TI Design an efficient data driven decision support system to predict
   flooding by analysing heterogeneous and multiple data sources using Data
   Lake
SO METHODSX
VL 11
AR 102262
DI 10.1016/j.mex.2023.102262
EA JUN 2023
DT Article
PD DEC 2023
PY 2023
AB Floods are the most common natural disaster in several countries
   throughout the world. Flooding has a major impact on people's lives and
   livelihoods. The impact of flood disasters on human lives can be
   mitigated by developing effective flood forecasting and prediction
   models. The majority of flood prediction models do not take all
   flood-causing factors into account when they are designed. It is
   difficult to collect and handle some of these flood-causing variables
   since they are heterogeneous in nature. This paper presents a new big
   data architecture called Data Lake, which can ingest and store all
   important flood-causing heterogeneous data sources in their raw format
   for machine learning model creation. The statistical relevance of
   important flood producing factors on flood prediction outcome is
   determined utilizing inferential statistical approaches. The outcome of
   this research is to create flood warning systems that can alert the
   public and government officials so that they can make decisions in the
   event of a severe flood, reducing socioeconomic loss. & BULL; Flood
   causing factors are from heterogeneous sources, so there is no big data
   architecture for handling variety of data sources. & BULL; To provide
   data architectural solution using data lake for collecting and analysing
   heterogeneous flood causing factors. & BULL; Uses inferential
   statistical approach to determine importance of different flood causing
   factors in design of efficient flood prediction models.
OI Jayasubramanian, Mohan Kumar/0000-0002-7559-0071
ZB 0
Z8 0
ZA 0
ZS 0
TC 2
ZR 0
Z9 2
U1 0
U2 15
EI 2215-0161
DA 2023-09-16
UT WOS:001060254000001
PM 37448950
ER

PT C
AU Sousa, Vania
   Barros, Daniela
   Guimaraes, Pedro
   Santos, Antonina
   Santos, Maribel Yasmina
BE Cabanillas, C
   Perez, F
TI Conceptual Formalization of Massive Storage for Advancing
   Decision-Making with Data Analytics
SO INTELLIGENT INFORMATION SYSTEMS, CAISE FORUM 2023
SE Lecture Notes in Business Information Processing
VL 477
BP 121
EP 128
DI 10.1007/978-3-031-34674-3_15
DT Proceedings Paper
PD 2023
PY 2023
AB Data Lakes have been widely used to handle massive amounts of data
   arriving at high velocity and variety. However, if proper data
   management concerns are not addressed, this massive data storage can
   easily turn Data Lakes into Data Swamps. Furthermore, data must be
   associated with the data artefacts created to extract value from it,
   such as pipelines used to collect, treat, or process data and analytical
   artefacts such as analytical dashboards and machine learning models.
   This paper proposes a more comprehensive view of a Data Lake, in which
   all of these resources can be stored and managed. To that end, the
   conceptual meta-model incorporates a data catalog, data at various
   stages of maturity, pipelines, dashboards, and machine learning models.
   The proposed meta-model was instantiated in the ADM.IN (Advanced
   Decision Making in Productive Systems through Intelligent Networks)
   project, showing how vast amounts of data and their related artefacts
   can be managed to support decision-making processes with data analytics.
CT 35th CAiSE Conference on Cyber-Human Systems
CY JUN 12-16, 2023
CL Zaragoza, SPAIN
SP San Jorge Univ, SVIT Res Grp
RI Guimarães, Peo/; Santos, Maribel Yasmina/M-5214-2013; Sousa, Vânia/
OI Guimarães, Peo/0000-0003-3390-8528; Santos, Maribel
   Yasmina/0000-0002-3249-6229; Sousa, Vânia/0009-0002-1279-6651
TC 2
Z8 0
ZR 0
ZA 0
ZB 0
ZS 0
Z9 2
U1 0
U2 0
SN 1865-1348
EI 1865-1356
BN 978-3-031-34673-6; 978-3-031-34674-3
DA 2024-09-15
UT WOS:001284384200015
ER

PT J
AU Geva, Gil A.
   Ketko, Itay
   Nitecki, Maya
   Simon, Shoham
   Inbar, Barr
   Toledo, Itay
   Shapiro, Michael
   Vaturi, Barak
   Votta, Yoni
   Filler, Daniel
   Yosef, Roey
   Shpitzer, Sagi A.
   Hir, Nabil
   Markovich, Michal Peri
   Shapira, Shachar
   Fink, Noam
   Glasberg, Elon
   Furer, Ariel
TI Data Empowerment of Decision-Makers in an Era of a Pandemic:
   Intersection of "Classic" and Artificial Intelligence in the Service of
   Medicine
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 23
IS 9
AR e24295
DI 10.2196/24295
DT Article
PD SEP 10 2021
PY 2021
AB Background: The COVID-19 outbreak required prompt action by health
   authorities around the world in response to a novel threat. With
   enormous amounts of information originating in sources with uncertain
   degree of validation and accuracy, it is essential to provide
   executive-level decision-makers with the most actionable, pertinent, and
   updated data analysis to enable them to adapt their strategy swiftly and
   competently.
   Objective: We report here the origination of a COVID-19 dedicated
   response in the Israel Defense Forces with the assembly of an
   operational Data Center for the Campaign against Coronavirus.
   Methods: Spearheaded by directors with clinical, operational, and data
   analytics orientation, a multidisciplinary team utilized existing and
   newly developed platforms to collect and analyze large amounts of
   information on an individual level in the context of SARS-CoV-2
   contraction and infection.
   Results: Nearly 300,000 responses to daily questionnaires were recorded
   and were merged with other data sets to form a unified data lake. By
   using basic as well as advanced analytic tools ranging from simple
   aggregation and display of trends to data science application, we
   provided commanders and clinicians with access to trusted, accurate, and
   personalized information and tools that were designed to foster
   operational changes and mitigate the propagation of the pandemic. The
   developed tools aided in the in the identification of high-risk
   individuals for severe disease and resulted in a 30% decline in their
   attendance to their units. Moreover, the queue for laboratory
   examination for COVID-19 was optimized using a predictive model and
   resulted in a high true-positive rate of 20%, which is more than twice
   as high as the baseline rate (2.28%, 95% CI 1.63%-3.19%).
   Conclusions: In times of ambiguity and uncertainty, along with an
   unprecedented flux of information, health organizations may find
   multidisciplinary teams working to provide intelligence from diverse and
   rich data a key factor in providing executives relevant and actionable
   support for decision-making.
OI Geva, Gil/0000-0002-0322-8348; Shapiro, Michael/0000-0001-5943-6974;
   Filler, Daniel/0000-0002-0070-816X; Simon, Shoham/0000-0003-2544-1550;
   Ketko, Itay/0000-0001-7435-4424; Nitecki, Maya/0000-0003-1127-4552;
   Inbar, Barr/0000-0002-1978-1826
ZB 0
TC 2
Z8 0
ZA 0
ZS 0
ZR 0
Z9 2
U1 1
U2 19
SN 1439-4456
EI 1438-8871
DA 2021-09-10
UT WOS:000695740200002
PM 34313589
ER

PT C
AU Neves, Ricardo A.
   Cruvinel, Paulo E.
GP IEEE
TI Ontology for Structuring a Digital Databases for Decision Making in
   Grain Production
SO 2021 IEEE 15TH INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC
   2021)
SE IEEE International Conference on Semantic Computing
BP 386
EP 392
DI 10.1109/ICSC50631.2021.00071
DT Proceedings Paper
PD 2021
PY 2021
AB This paper presents an ontology for the structuring of digital databases
   with the objective of acting in a cloud environment and meeting big data
   sources in the agricultural context of grain production. Its conception
   is structured in three stages: the first stage presents an ontological
   architecture aimed at public and private cloud environments, the second
   stage deals with a semantic model at process level, and a pseudocode for
   ontological application is elaborated in the third stage, considering
   the technologies applied to the cloud. This work combines advanced
   features to support decision making from Data Lake storage solutions,
   semantic treatment of big data, as well as the presentation of
   strategies based on machine learning and data quality analysis to obtain
   data and metadata organized for application in a decision model. The
   configuration of the ontology presented meets the diversity of big data
   projects in the grain production context, the characteristics of which
   are based on interoperability in the use of heterogeneous data and its
   integration, elasticity of computational resources, and high
   availability of cloud access.
CT 15th IEEE International Conference on Semantic Computing (ICSC)
CY JAN 27-29, 2021
CL ELECTR NETWORK
SP IEEE; IEEE Comp Soc
RI Cruvinel, Paulo/C-7687-2015
ZS 0
Z8 0
ZA 0
ZB 1
ZR 0
TC 2
Z9 2
U1 0
U2 8
SN 2325-6516
BN 978-1-7281-8899-7
DA 2021-08-04
UT WOS:000668692000067
ER

PT J
AU Ruben Fillottrani, Pablo
   Maria Keet, C.
TI KnowID: An Architecture for Efficient Knowledge-Driven Information and
   Data Access
SO DATA INTELLIGENCE
VL 2
IS 4
BP 487
EP 512
DI 10.1162/dint_a_00060
DT Article
PD FAL 2020
PY 2020
AB Modern information systems require the orchestration of ontologies,
   conceptual data modeling techniques, and efficient data management so as
   to provide a means for better informed decision-making and to keep up
   with new requirements in organizational needs. A major question in
   delivering such systems, is which components to design and put together
   to make up the required "knowledge to data" pipeline, as each component
   and process has trade-offs. In this paper, we introduce a new
   knowledge-to-data architecture, KnowID. It pulls together both recently
   proposed components and we add novel transformation rules between
   Enhanced Entity-Relationship (EER) and the Abstract Relational Model to
   complete the pipeline. KnowID's main distinctive architectural features,
   compared to other ontology-based data access approaches, are that
   runtime use can avail of the closed world assumption commonly used in
   information systems and of full SQL augmented with path queries.
RI Fillottrani, Pablo/A-7817-2008
OI Fillottrani, Pablo/0000-0003-0906-867X
Z8 0
ZS 0
ZB 0
ZA 0
TC 2
ZR 0
Z9 2
U1 0
U2 6
EI 2641-435X
DA 2020-09-01
UT WOS:000691829900002
ER

PT C
AU Li, Haoyuan
   Toor, Salman
BE Ding, W
   Lu, CT
   Wang, F
   Di, L
   Wu, K
   Huan, J
   Nambiar, R
   Li, J
   Ilievski, F
   Baeza-Yates, R
   Hu, X
TI Empowering Data Mesh with Federated Learning
SO 2024 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, BIGDATA
SE IEEE International Conference on Big Data
BP 2340
EP 2342
DI 10.1109/BigData62323.2024.10825390
DT Proceedings Paper
PD 2024
PY 2024
AB The evolution of data architecture has seen the rise of data lakes,
   aiming to solve the bottlenecks of data management and promote
   intelligent decision-making. However, this centralized architecture is
   limited by the proliferation of data sources and the growing demand for
   timely analysis and processing. A new data paradigm, Data Mesh, is
   proposed to overcome these challenges. In this decentralized
   architecture where data is locally preserved by each domain team,
   traditional centralized machine learning cannot conduct effective
   analysis across multiple domains, especially for security-sensitive
   organizations. To this end, we introduce a pioneering approach that
   incorporates Federated Learning into Data Mesh. This applied research
   article emphasizes the benefits of combining two distinct domains to
   achieve the best outcomes for industrial use cases.
CT 2024 IEEE International Conference on Big Data
CY DEC 15-18, 2024
CL Washington, DC
SP Institute of Electrical and Electronics Engineers Inc; National Science
   Foundation; Virginia Tech Corporate Research Center
Z8 0
ZA 0
TC 0
ZB 0
ZR 0
ZS 0
Z9 1
U1 0
U2 0
SN 2639-1589
BN 979-8-3503-6249-7; 979-8-3503-6248-0
DA 2024-01-01
UT WOS:001451321802065
ER

PT J
AU Zhou, Ke
   Meng, En
   Jin, Qingren
   Luo, Bofeng
   Tian, Bing
TI Evaluation of data governance effectiveness in power grid enterprises
   using deep neural network
SO SOFT COMPUTING
VL 27
IS 23
BP 18333
EP 18351
DI 10.1007/s00500-023-09210-9
EA SEP 2023
DT Article
PD DEC 2023
PY 2023
AB In an era of unprecedented technological advancement, the power industry
   is undergoing a transformative evolution, particularly in intelligent
   power grid enterprises. The integration of cutting-edge information
   science and technology has ushered in a new era of automation,
   informatization, and intelligence within these enterprises. While this
   progression promises enhanced production, operation, and management
   capabilities, it also brings forth a daunting challenge: the effective
   governance of the burgeoning volumes of power data. This paper aims to
   conduct in-depth research on evaluating the effectiveness of data
   governance in power grid enterprises based on deep learning to integrate
   more closely with their business systems. The main objective is to
   provide effective and convenient intelligent services for
   decision-making within an innovative power grid enterprise management
   system and strengthen the data architecture of these enterprises in data
   management. First, the deep learning neural network's principle
   structure and training methods are introduced in detail and combined
   with the deep learning neural network. This is a different evaluation
   model for the data governance effectiveness of power grid enterprises
   based on penalty variable weight. The difference probability density of
   the power difference data series in the power grid is taken as the
   evaluation index. The evaluation model for the governance effectiveness
   of different data is modified. Build a different evaluation model of
   power grid enterprise data governance effectiveness based on punishment
   and weight change, comprehensively consider the extent to which the data
   volume of power grid abnormal data in power grid enterprises affects the
   evaluation of data governance effectiveness, and complete the assessment
   of power grid enterprise data governance effectiveness based on deep
   learning. Experimental results underscore the method's efficacy,
   demonstrating an exceptional accuracy rate of 94%. This empirical
   validation highlights the method's efficient evaluation process,
   offering invaluable technical support for enhancing power data
   management's consistency, precision, and reliability within power grid
   enterprises. Moreover, comparative analyses against other methodologies,
   including KNN, SVM, RF, DT, and RNN, reaffirm the superiority of the DNN
   model, solidifying its outstanding performance.
ZA 0
Z8 0
ZS 0
ZB 0
TC 1
ZR 0
Z9 1
U1 3
U2 28
SN 1432-7643
EI 1433-7479
DA 2023-10-14
UT WOS:001074724400003
ER

PT C
AU Kulkarni, Apurva
   Bassin, Pooja
   Parasa, Niharika Sri
   Venugopal, Vinu E.
   Srinivasa, Srinath
   Ramanathan, Chandrashekar
BE Sachdeva, S
   Watanobe, Y
   Bhalla, S
TI Ontology Augmented Data Lake System for Policy Support
SO BIG DATA ANALYTICS IN ASTRONOMY, SCIENCE, AND ENGINEERING, BDA 2022
SE Lecture Notes in Computer Science
VL 13830
BP 3
EP 16
DI 10.1007/978-3-031-28350-5_1
DT Proceedings Paper
PD 2023
PY 2023
AB Analytics of Big Data in the absence of an accompanying framework of
   metadata can be a quite daunting task. While it is true that statistical
   algorithms can do large-scale analyses on diverse data with little
   support from metadata, using such methods on widely dispersed, extremely
   diverse, and dynamic data may not necessarily produce trustworthy
   findings. One such task is identifying the impact of indicators for
   various Sustainable Development Goals (SDGs). One of the methods to
   analyze impact is by developing a Bayesian network for the policymaker
   to make informed decisions under uncertainty. It is of key interest to
   policy-makers worldwide to rely on such models to decide the new
   policies of a state or a country (https://sdgs.un.org/2030agenda). The
   accuracy of the models can be improved by considering enriched data -
   often done by incorporating pertinent data from multiple sources.
   However, due to the challenges associated with volume, variety,
   veracity, and the structure of the data, traditional data lake systems
   fall short of identifying information that is syntactically diverse yet
   semantically connected. In this paper, we propose a Data Lake (DL)
   framework that targets ingesting & processing of data like any
   traditional DL, and in addition, is capable of performing data retrieval
   for applications such as Policy Support Systems (where the selection of
   data greatly affect the output interpretations) by using ontologies as
   the intermediary. We discuss the proof of concept for the proposed
   system and the preliminary results (IIITB Data Lake project Website
   link: http://cads.iiitb.ac.in/wordpress/) based on the data collected
   from the agriculture department of the Government of Karnataka (GoK).
CT 10th International Conference on Big Data Analytics (BDA)
CY DEC 05-07, 2022
CL Univ Aizu, ELECTR NETWORK
HO Univ Aizu
SP Natl Inst Technol Delhi; Indian Inst Technol Delhi
RI Ellampallil Venugopal, Vinu/; Srinivasa, Srinath/AAT-8414-2020; Kulkarni, Apurva/; Bassin, Pooja/; Ramanathan, Chanashekar/
OI Ellampallil Venugopal, Vinu/0000-0003-4429-9932; Kulkarni,
   Apurva/0000-0002-9215-2049; Bassin, Pooja/0000-0002-0611-8734;
   Ramanathan, Chanashekar/0000-0002-3330-8365
Z8 0
TC 1
ZR 0
ZA 0
ZB 0
ZS 0
Z9 1
U1 1
U2 4
SN 0302-9743
EI 1611-3349
BN 978-3-031-28349-9; 978-3-031-28350-5
DA 2023-07-13
UT WOS:001004046900001
ER

PT C
AU Aleisa, Monirah
   Alshahrani, Mona
   Beloff, Natalia
   White, Martin
GP IEEE Comp Soc
TI TAIRA-BSC - Trusting AI in Recruitment Applications through Blockchain
   Smart Contracts
SO 2022 IEEE INTERNATIONAL CONFERENCE ON BLOCKCHAIN (BLOCKCHAIN 2022)
BP 376
EP 383
DI 10.1109/Blockchain55522.2022.00059
DT Proceedings Paper
PD 2022
PY 2022
AB Artificial intelligence (AI) and blockchain technology (BCT) are
   considered two of the most trending and disruptive technologies. BCT,
   although commonly associated with cryptocurrencies, has shown a
   tremendous impact among many other distributed applications domains. BCT
   characteristics, such as the distribution of data storage among
   independent nodes and the use of consensus algorithms offer immutability
   and transparency and remove the need for a central authority making BCT
   trustworthy. However, decision-makers and stakeholders currently lack
   the confidence to overcome uncertainty related to AI technology, which
   affects the acceptance of AI technology in wider application domains,
   such as the recruitment process. Furthermore, current research
   literature does not adequately investigate the role of trust as an
   integral part of an AI-based recruitment application. Therefore, this
   paper aims to investigate how emerging BCT and AI technologies can
   improve decision making and stakeholder trust in a job recruitment
   system that is traditionally focused on just human expert
   decision-making. In this paper we propose the design of a new solution
   for trusting AI in recruitment applications through the use of
   Blockchain Smart Contracts (TAIRA-BSC). TAIRA-BSC integrates Blockchain
   Smart Contracts (BSC) with the Data Lake (DL), Machine Learning (ML) and
   AI technologies in our AI Recruitment Model (AIRM) architecture.
   TAIRA-BSC improves transparency and interoperability in the recruitment
   process while protecting sensitive job candidate data and ensures data
   integrity delivery and traceability in the recruiting process through a
   verifiable decentralized ledger, i.e., the blockchain and associated
   smart contracts. The paper presents a discussion on the state-of-the-art
   of integrating AI with BCT focusing on how BCT can be used to bridge
   trust concerns with AI systems. We also present a conceptual
   architecture TAIRA-BSC proof of concept that is developed to serve as a
   foundation for future studies focused on enhancing trust in AI
   applications through the integration of BCT.
CT 5th IEEE International Conference on Blockchain (Blockchain)
CY AUG 22-25, 2022
CL Espoo, FINLAND
SP IEEE; IEEE Comp Soc; IEEE Tech Comm Scalable Comp; IEEE Technol & Engn
   Management Soc
RI Beloff, Natalia/; White, Martin/; AlShahrani, Mona/HIR-9054-2022
OI Beloff, Natalia/0000-0002-8872-7786; White, Martin/0000-0001-8686-2274;
   AlShahrani, Mona/0000-0002-7848-8136
ZR 0
Z8 0
TC 0
ZB 0
ZS 0
ZA 0
Z9 1
U1 2
U2 36
BN 978-1-6654-6104-7
DA 2022-10-26
UT WOS:000865770500048
ER

PT C
AU Morales, Antonio
   Canovas-Segura, Bernardo
   Campos, Manuel
   Juarez, Jose M.
   Palacios, Francisco
BE Luaces, O
   Gamez, JA
   Barrenechea, E
   Troncoso, A
   Galar, M
   Quintian, H
   Corchado, E
TI Proposal of a Big Data Platform for Intelligent Antibiotic Surveillance
   in a Hospital
SO ADVANCES IN ARTIFICIAL INTELLIGENCE, CAEPIA 2016
SE Lecture Notes in Artificial Intelligence
VL 9868
BP 261
EP 270
DI 10.1007/978-3-319-44636-3_24
DT Proceedings Paper
PD 2016
PY 2016
AB From a technological point of view two kinds of requirements must be
   taken into account when implementing Clinical Decision Support Systems
   (CDSSs) for antibiotic surveillance in a hospital. First, Artificial
   Intelligence (AI) technologies are usually applied to represent and
   reason about existing clinical knowledge, but also to discover new one
   from raw data. Second, at a global decision level, representative
   applications of Business Intelligence (BI) must be also considered. The
   present work introduces the design and implementation of a CDSS platform
   that integrates both AI and BI technologies to assist clinicians in the
   rational use of antibiotics in a hospital. The choice of a Hadoop based
   Big Data architecture provides a suitable solution for the problem of
   integrating, processing and analysing large sets of clinical data. The
   platform facilitates the daily follow-up of antibiotic therapies and
   infections while offering various decision support modules at both
   patient and global level. The system is being tested and evaluated in a
   university hospital.
CT 17th Conference of the Spanish-Association-for-Artificial-Intelligence
   (CAEPIA)
CY SEP 14-16, 2016
CL Salamanca, SPAIN
SP Spanish Assoc Artificial Intelligence; BISITE; Univ Salamanca; Springer
   Team; AEPIA
RI Cánovas-Segura, Bernardo/GWB-9323-2022; MORALES NICOLÁS, ANTONIO/; Juarez, Jose/K-8042-2017; Palacios, Francisco/AAW-5563-2021; Campos, Manuel/H-5226-2015
OI Cánovas-Segura, Bernardo/0000-0002-0777-0441; MORALES NICOLÁS,
   ANTONIO/0000-0002-0872-5351; Campos, Manuel/0000-0002-5233-3769
ZB 1
ZS 0
TC 1
ZR 0
Z8 0
ZA 0
Z9 1
U1 0
U2 5
SN 0302-9743
EI 1611-3349
BN 978-3-319-44636-3; 978-3-319-44635-6
DA 2016-12-07
UT WOS:000387750600024
ER

PT J
AU Iglesias, Felix
   Ros, Frederic
   Thuy, Lynh Hoang Vy
   Gourcy, Laurence
   Moquet, Jean-Sebastien
   Daele, Veronique
   Dupraz, Sebastien
TI A conceptual architecture for AI-assisted Digital Twins in natural
   resource management
SO ECOLOGICAL INFORMATICS
VL 94
AR 103635
DI 10.1016/j.ecoinf.2026.103635
EA JAN 2026
DT Article
PD MAR 2026
PY 2026
AB The management of natural resources is increasingly critical and
   challenging due to complex interactions among environmental, industrial,
   and societal processes. Traditional approaches often fail to integrate
   heterogeneous data, limiting predictive and decision-support
   capabilities. This study presents a conceptual architecture for an
   Artificial Intelligence (AI)-assisted Digital Twin (DT) of the
   Centre-Val de Loire region, designed to unify time-dependent
   multi-source data. Based on the ENVRI Reference Model, it covers
   Science, Information, Computational, Engineering, and Technology layers,
   defining standardized data exchange, communication protocols, and
   prototype functionalities. A proof of concept FIWARE implementation
   supports ingestion, monitoring and analytical services for piezometric
   and meteorological data, exemplified through groundwater dynamics in the
   Beauce aquifer. It integrates daily observations from 53 piezometric
   stations over more than five years, managing approximately 2.8 million
   records in a containerized environment. Results show that the proposed
   DT architecture can enhance sustainability-oriented decision making,
   integrating heterogeneous data and predictive analyses while enabling
   collaboration across scientific and technical domains. Its modular
   design offers a replicable template for future AI-assisted environmental
   DTs, scalable to larger regions. Hence, this work illustrates how DTs
   can improve environmental monitoring and understanding, providing a
   pathway toward resilient, data-driven management of natural resources.
RI ROS, Frédéric/V-2884-2019; Gourcy, Laurence/AAJ-6717-2020
Z8 0
ZR 0
ZB 0
TC 0
ZA 0
ZS 0
Z9 0
U1 0
U2 0
SN 1574-9541
EI 1878-0512
DA 2026-02-13
UT WOS:001681983600001
ER

PT P
AU HE J
   SUN Y
   WEI L
   GENG N
   LI S
   HUANG C
TI Hydropower station power production management            system based
   on industrial digitization, has decision            support layer for
   generating scheduling solution and            risk pre-warning based on
   artificial intelligence and            knowledge map
PN CN121414170-A
AE CWE ELECTRIC POWER DEV CO LTD; CHINA INT WATER & ELECTRIC CORP
AB 
   NOVELTY - The system has a service application layer               
   provided with an investment financing management                module,
   an intelligent building module, an item                management module
   and an intelligent operation and                maintenance module. The
   intelligent operation and                maintenance module is provided
   with a device state                monitoring sub-module, an AI fault
   prediction                sub-module and an energy efficiency
   optimization                sub-module. A data collecting layer is used
   for                collecting operation, construction and operation     
   and maintenance data of a hydropower station in                real
   time. A data management and standard layer is                used for
   establishing a uniform data lake and data                platform,
   cleaning, converting, storing and                authority managing
   multi-source heterogeneous data,                and realizing homologous
   connection of the data. A                decision support layer
   generates scheduling                solution and risk pre-warning based
   on AI and                knowledge map. A security and network layer is
   used                for ensuring security and stability of              
    transnational data transmission.
   USE - System for managing power production of a               
   hydropower station based on industrial                digitization.
   ADVANTAGE - The system realizes inter-department and               
   inter-link data interconnection and                intercommunication by
   establishing uniform data                center and standard governing
   system. The system                combines the BIM, GIS and IoT device
   to construct a                digital twin model and introduce AI
   intelligent                prediction and dynamic scheduling mechanism,
   which                improves the automation and refinement level of    
   construction and operation and maintenance. The                decision
   supporting layer is integrated with a                knowledge map and a
   risk pre-warning model to                realize the dynamic monitoring
   and timely response                to the project progress, cost and
   safety risk in                the overseas complex environment so as to
   obviously                improve the operation efficiency, safety and
   risk                controllability of the hydropower station.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   hydropower station power production management                method
   based on industrial digitization.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   hydropower station power production management                system
   based on industrial digitization. (Drawing                includes
   non-English language text).
Z9 0
U1 0
U2 0
DA 2026-02-15
UT DIIDW:202612914F
ER

PT J
AU Dube, Matthew P.
   Hall, Brendan P.
TI Conceptual Neighborhood Graphs of Discrete Time Intervals
SO ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION
VL 15
IS 1
AR 39
DI 10.3390/ijgi15010039
DT Article
PD JAN 12 2026
PY 2026
AB Temporal reasoning is an important part of the field of time geography
   and spatio-temporal data science. Recent advances in qualitative
   temporal reasoning have developed a set of 74 relations that apply
   between discretized time intervals of at least two pixels each. While
   the identification of specific relations is important, the field of
   qualitative spatial and temporal reasoning relies on conceptual
   neighborhood graphs to address relational similarity. This similarity is
   paramount for generating essential decision support structures, notably
   reasonable aggregations of concepts into single terms and the
   determination of nearest neighbor queries. In this paper, conceptual
   neighborhood graphs of qualitative topological changes, with discretized
   temporal interval relations in the form of translation, isotropic
   scaling, and anisotropic scaling, are identified using data generated
   through a simulation protocol. The outputs of this protocol are compared
   to the extant literature regarding conceptual neighborhood graphs of the
   Allen interval algebra, demonstrating the theoretical accuracy of the
   work. This work supports the development of robust spatio-temporal
   artificial intelligence as well as the future development of
   spatio-temporal query systems upon the spatio-temporal stack data
   architecture.
RI Dube, Matthew/H-7035-2019
ZB 0
ZR 0
TC 0
ZA 0
ZS 0
Z8 0
Z9 0
U1 0
U2 0
EI 2220-9964
DA 2026-01-31
UT WOS:001670563700001
ER

PT J
AU Ngabo-Woods, Harold
   Dunai, Larisa
   Verdu, Isabel Segui
   Liang, Sui
TI A Multimodal Framework for Prognostic Modelling of Mental Health
   Treatment and Recovery Trajectories
SO APPLIED SCIENCES-BASEL
VL 16
IS 2
AR 763
DI 10.3390/app16020763
DT Article
PD JAN 12 2026
PY 2026
AB The clinical management of major depressive disorder is constrained by a
   trial-and-error approach. The clinical management of major depressive
   disorder is constrained by a trial-and-error approach. While
   computational methods have focused on static binary classification
   (e.g., responder vs. non-responder), they ignore the dynamic nature of
   recovery. Building upon the recently proposed prognostic theory of
   treatment response, this article presents a methodological framework for
   its operationalisation. We define a multi-modal data architecture for
   the theory's core constructs-the Patient State Vector (PSV), Therapeutic
   Impulse Function (TIF), and Predicted Recovery Trajectory
   (PRT)-transforming them from abstract concepts into specified
   computational inputs. To model the asynchronous interactions between
   these components, we specify a Time-Aware Long Short-Term Memory (LSTM)
   architecture, providing explicit mathematical formulations for
   time-decay gates to handle irregular clinical sampling. Furthermore, we
   outline a synthetic validation protocol to benchmark this dynamic
   approach against static baselines. By integrating these technical
   specifications with a translational pipeline for Explainable AI (XAI)
   and ethical governance, this paper provides the necessary blueprint to
   transition psychiatry from theoretical prognosis to empirical
   forecasting.
RI SeguiVerdu, Isabel/LGY-6418-2024; Ngabo-Woods, Harold/H-4656-2018; Duani, Larisa/AFJ-8887-2022
Z8 0
TC 0
ZR 0
ZS 0
ZB 0
ZA 0
Z9 0
U1 0
U2 0
EI 2076-3417
DA 2026-01-31
UT WOS:001670125600001
ER

PT J
AU Bahmutsky, Sofia
   Turner, Ian
   Arulnathan, Vivek
   Pelletier, Nathan
TI Advancing life cycle assessment through data science: A critical review
   of algorithms, tools, and data challenges
SO SUSTAINABLE PRODUCTION AND CONSUMPTION
VL 61
BP 25
EP 36
DI 10.1016/j.spc.2025.10.007
EA DEC 2025
DT Article
PD DEC 2025
PY 2025
AB A well-executed life cycle assessment requires thorough data collection
   across all relevant processes, combined with advanced data analysis.
   Common data-related issues in life cycle assessment research include the
   absence of necessary data, low data quality, inconsistencies,
   uncertainty, and failure to account for variations over time and
   location. In this context, data science, the discipline of extracting
   meaningful insights from data, has the potential to address these
   challenges. While the integration of data science with life cycle
   assessment holds significant potential, best use cases depend on the
   goal of the study, as well as the data type and volume required,
   underscoring the necessity of reviewing the intersection of data science
   and life cycle assessment. This study used the Preferred Reporting Items
   for Systematic Reviews and Meta-Analysis (PRISMA) method to identify
   literature addressing the use of data science elements to support life
   cycle assessment. It evaluated which data science techniques are
   appropriate for specific life cycle assessment stages or problem areas
   and the strengths and weaknesses of current data science applications in
   life cycle assessment. Key opportunities identified revolve around
   solutions for dealing with missing or poor-quality data,
   expensive/prohibitive data collection, and improving the accuracy of
   life cycle assessment results. The currently most feasible pathways
   appear to involve use of machine learning techniques, as these types of
   studies were the most conducted and generated tangible results. Extreme
   gradient boosting, random forest, and artificial neural networks were
   particularly prominent algorithm choices. Data collection and
   transferability using ontologies and semantic tools were also
   highlighted as important strategies for improving data flow in life
   cycle assessment, including the integration of a wide variety of
   databases and non-life cycle assessment data.
RI Pelletier, Nathan/U-9312-2019
Z8 0
ZS 0
ZA 0
TC 0
ZR 0
ZB 0
Z9 0
U1 14
U2 14
SN 2352-5509
DA 2025-11-09
UT WOS:001608191100001
ER

PT P
AU HALLUR G G
   VARUN S A
TI System for automating explanatory narrative            generation for
   business intelligence, has natural            language generation module
   for converting analytical            findings into human-readable
   narratives, and            interactive dashboard which presents
   narratives, where            modules are interconnected for seamless
   data            analysis
PN IN202521110535-A
AE UNIV SYMBIOSIS INT
AB 
   NOVELTY - The system has a data monitoring module for               
   tracking key metrics from various data sources. An                event
   detection module is configured to identify                statistically
   significant anomalies. A causal                inference engine analyzes
   anomalies by querying a                data warehouse or data lake to
   determine probable                root causes. A natural language
   generation module                is provided for converting analytical
   findings into                human-readable narratives. An interactive
   dashboard                presents narratives. The modules are
   interconnected                for a seamless data analysis and
   presentation                workflow.
   USE - System for automating explanatory narrative               
   generation for business intelligence, and for                augmenting
   data visualizations with automatically                generated
   explanatory narratives in interactive                dashboards, and for
   descriptive reporting,                displaying data through visual
   elements such as                charts, graphs, and maps.
   ADVANTAGE - The system automatically augments business               
   intelligence dashboards with explanatory                narratives. The
   textual elements are interactively                linked to
   corresponding visualizations, thus                enabling drill-through
   exploration and rapid                comprehension. The system
   transforms static,                descriptive dashboards into
   intelligent,                explanatory, and interactive environments,
   thus                reducing manual analysis effort, accelerating       
   decision making, and clarifying causes behind                observed
   trends. The automated, proactive system                continuously
   monitors primary business metrics for                significant events
   such as anomalies, breaks, or                regime shifts, thus
   initiating analysis without                requiring user prompts. The
   system determines                probable root causes by querying at
   least one                secondary, distinct data source and applying
   causal                inference or attribution logic that extends beyond
   the currently viewed dataset or single                visualization. The
   system generates natural                language narratives that explain
   the detected event                and its likely causes and render those
   narratives                within the dashboard alongside impacted       
   visualizations. The system reduces user effort and                bias
   by replacing manual drill downs and cross                report
   comparisons with automated, reproducible                causal analyses
   driven by artificial intelligence.                The decision velocity
   is improved by delivering                timely, explainable, and
   actionable insights                integrated into existing business
   intelligence                workflows.
Z9 0
U1 0
U2 0
DA 2026-02-08
UT DIIDW:2025C1805W
ER

PT P
AU CHEN W
TI Method for processing data based on artificial            intelligence
   and intelligent car park, involves using            online learning or
   increment learning technology for            periodically re-training
   each artificial intelligence            model and adjusting parameter
PN CN121096164-A
AE TIANJIN RUIYI TECHNOLOGY CO LTD
AB 
   NOVELTY - The method involves collecting multi-source               
   heterogeneous data in real time by multiple                internet of
   things sensing devices arranged in a                car park. The data
   is cleaned, formatted and                space-time aligned to form a
   uniform data lake. An                artificial intelligent algorithm is
   used to perform                depth analysis and model training based
   on the data                lake. An intelligent decision model library
   is                constructed. An analysis result is converted into a   
   specific application service and control                instruction
   based on the intelligent decision model                library. Actual
   effect data of the application                service is taken as
   feedback. Online learning or                increment learning
   technology is used for                periodically re-training each AI
   model and                adjusting a parameter such that prediction     
   precision and decision-making ability of the model                are
   continuously improved along with accumulation                of the data
   to form a closed-loop system from data                analysis to
   application to feedback to                optimization.
   USE - Method for processing data based on artificial               
   intelligence and intelligent car park.
   ADVANTAGE - The method enables periodically performing               
   re-training and parameter optimization on each AI                model,
   so that the prediction precision and                decision ability of
   the model are continuously                improved along with the
   accumulation of the data,                thus forming a               
   data-analysis-application-feedback-optimized closed                loop
   system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for processing data based on
   artificial                intelligence and intelligent car park.
   (Drawing                includes non-English language text).
Z9 0
U1 0
U2 0
DA 2026-01-10
UT DIIDW:2025C1698L
ER

PT P
AU ILLOUZ A
   SHEMESH E
   RISE L
   WEINTRAUB G
TI Method for performing integrity verification of            data obtained
   from cloud data lake, involves            calculating combined hash
   value based on partition hash            values, and calculating
   verified combined hash value            based on hash values obtained
   from metadata table            corresponding to each of partitions
PN US2025363089-A1; US12547606-B2
AE INT BUSINESS MACHINES CORP
AB 
   NOVELTY - The method involves transmitting (602) a               
   request for a data set to the cloud data lake, and               
   receiving (604) multiple file names from the cloud                data
   lake in response to the request. Multiple                partitions of
   the cloud data lake that stores a set                of files that
   satisfy the request are extracted                (606) from the file
   names. A partition hash value                for each of the partitions
   is calculated (608). A                metadata table created by a data
   owner of the set                of files is obtained (610), and the
   metadata table                is verified based on a digital signature
   of the                metadata table corresponding to the data owner.
   The                verified partition hash values are obtained (612)    
   from the metadata table. A combined hash value is               
   calculated based on the partition hash values, and                a
   verified combined hash value is calculated based                on hash
   values obtained from the metadata table                corresponding to
   each of the partitions.
   USE - Method for performing integrity verification                of
   data obtained from cloud data lake used as                primary
   sources for analytics and machine learning                models for
   data-driven decision-making.
   ADVANTAGE - The cloud data lake is a cloud-hosted               
   centralized repository that provides nearly                unlimited
   capacity and scalability for storing                large-scale
   structured and unstructured data. The                control functions
   and the forwarding functions of                network module are
   performed on physically separate                devices, such that the
   control functions manage                multiple different network
   hardware devices.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   computing system having a memory having                computer readable
   instructions and multiple                processors for executing the
   computer readable                instructions;(2) a computer program
   product comprising a                computer readable storage medium
   having program                instructions.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a method
   for performing integrity verification of data                obtained
   from cloud data lake.602Transmitting a request for a data set to        
   the cloud data lake604Receiving multiple file names from the            
   cloud data lake in response to the request606Extracting multiple
   partitions of the                cloud data lake that stores a set of
   files that                satisfy the request from the file
   names608Calculating partition hash value for each                of the
   partitions610Obtaining metadata table created by a data               
   owner of the set of files612Obtaining verified partition hash values    
              from the metadata table
Z9 0
U1 0
U2 0
DA 2025-12-16
UT DIIDW:2025B22079
ER

PT J
AU Santos-Dominguez, Martin
   Hernandez Flores, Nicasio
   Parra-Ramirez, Isaac Alberto
   Arroyo-Figueroa, Gustavo
TI AI-Big Data Analytics Platform for Energy Forecasting in Modern Power
   Systems
SO BIG DATA AND COGNITIVE COMPUTING
VL 9
IS 11
AR 272
DI 10.3390/bdcc9110272
DT Article
PD OCT 31 2025
PY 2025
AB Big Data Analytics is vital for power grids, as it empowers informed
   decision-making, anticipates potential operational and maintenance
   issues, optimizes grid management, supports renewable energy
   integration, ultimately reduces costs, improves customer service,
   monitors consumer behavior, and offers new services. This paper
   describes the AI-Big Data Analytics Architecture based on a data lake
   architecture that uses a reduced and customized set of Hadoop and Spark
   as a cost-effective, on-premises alternative for advanced data analytics
   in power systems. As a case study, a comparative analysis of electricity
   price forecasting models in the day-ahead market for nodes of the
   Mexican national electrical system using statistical, machine learning,
   and deep learning models, is presented. To build and select the best
   forecasting model, a data science and machine learning methodology is
   used. The results show that the Gradient Boosting and Support Vector
   Regression models presented the best performance, with a Mean Absolute
   Percentage Error (MAPE) between 1% and 4% for five-day-ahead electricity
   price forecasting. The implementation of the best forecasting model into
   the Big Data Analytics Platform allows the automation of the calculation
   of the local electricity price forecast per node (every 24, 72, or 120
   h) and its display in a comparative dashboard with actual and forecasted
   data for decision-making on demand. The proposed architecture is a
   valuable tool that allows the future implementation of intelligent
   energy forecasting models in power grids, such as load demand, fuel
   prices, power generation, and consumption, among others.
RI Arroyo Figueroa, Gustavo/O-4911-2016
OI Arroyo Figueroa, Gustavo/0000-0003-0764-045X
ZA 0
Z8 0
ZS 0
ZR 0
ZB 0
TC 0
Z9 0
U1 1
U2 1
EI 2504-2289
DA 2025-12-01
UT WOS:001624040600001
ER

PT C
AU Puertas, Enrique
   Bemposta, Sergio
   Monsalve, Borja
   Lopez, Jose M.
   Corrales-Paredes, Ana
TI Big Data System for Traffic Monitoring and Management at Roundabouts
   using Drones and Artificial Intelligence
SO IFAC PAPERSONLINE
VL 59
IS 10
BP 1534
EP 1539
DI 10.1016/j.ifacol.2025.09.258
EA SEP 2025
DT Proceedings Paper
PD 2025
PY 2025
AB This paper proposes a vehicle detection system for roundabouts based on
   images captured by a drone. This system runs on a Big Data architecture
   to ensure scalability and real-time processing. The system architecture
   is divided into two parts: a detection part, based on drones and
   computer vision, and a communication and processing part, based on a Big
   Data architecture deployed in the cloud. The system is able to
   accurately detect both roundabouts and the vehicles driving on them,
   providing valuable information on traffic conditions. The Big Data
   architecture allows real-time traffic information to be processed and
   analyzed, facilitating informed decision-making to improve traffic flow
   and safety. The evaluation of the system, carried out through
   simulations, has demonstrated its robustness and ability to handle large
   volumes of data in real time. Copyright (C) 2020 The Authors. Published
   by Elsevier B.V. This is an open access article under the CC BY-NC-ND
   license (http://creativecommons.org/licenses/by-nc-nd/4.0/)
CT 11th IFAC Conference on Manufacturing Modelling, Management and Control
   (MIM)
CY JUN 30-JUL 03, 2025
CL Trondheim, NORWAY
SP Int Federat Automat Control, TC 5 2 Management & Control Mfg & Logist;
   Int Federat Automat Control, TC 1 3 Discrete Event & Hybrid Syst; Int
   Federat Automat Control, TC 3 2 Computat Intelligence Control; Int
   Federat Automat Control, TC 5 1 Mfg Plant Control; Int Federat Automat
   Control, TC 7 4 Transportat Syst; Int Federat Automat Control, TC 9 1
   Econ, Business, & Financial Syst
RI Puertas, Enrique/L-5656-2014; Corrales Paredes, Ana/AAD-4733-2022
Z8 0
ZR 0
ZS 0
ZA 0
TC 0
ZB 0
Z9 0
U1 2
U2 2
SN 2405-8963
DA 2025-11-28
UT WOS:001583825700257
ER

PT P
AU VIMALNATH V
TI Cloud-based operations data analytics platform for            managing,
   processing and analyzing large-scale            operational data across
   e.g. manufacturing sectors, has            data ingestion layer for
   capturing structured,            semi-structured, and unstructured
   datasets from            heterogeneous sources in real time
PN IN202541079317-A
AE SAVEETHA ENG COLLEGE
AB 
   NOVELTY - The platform has a data ingestion layer for               
   capturing structured, semi-structured and                unstructured
   datasets from heterogeneous sources in                real time. A
   processing layer performs automated                data cleansing,
   normalization, and transformation                to ensure
   interoperability across multiple                organizational
   functions. A hybrid storage                architecture is provided with
   a data lake for                unstructuring data and a data warehouse
   for                structured analysis-ready datasets. An analytics     
   layer integrates artificial intelligence and                machine
   learning algorithms to deliver predictive                modeling,
   anomaly detection and prescriptive                decision support. A
   visualization module provides                interactive dashboards,
   reporting tools, and                real-time operational monitoring for
                  end-users.
   USE - Cloud-based operations data analytics platform                for
   managing, processing and analyzing large-scale               
   operational data from multiple sources across                diverse
   organizational domains/sectors. Uses                include but are not
   limited to enterprise resource                planning (ERP) systems,
   supply chain transactions,                manufacturing sensors,
   customer interactions and                financial operations across
   manufacturing sectors,                healthcare sectors, finance
   sectors, logistics                sectors, retail sectors and education 
                 sectors.
   ADVANTAGE - The platform integrates real-time               
   visualization dashboards and intelligent reporting               
   mechanisms that empower decision-makers to monitor                key
   operational indicators, identify inefficiencies                and
   implement corrective strategies instantly. The                platform
   supports collaborative analytics, thus                allowing
   cross-functional teams to interact with                shared insights,
   scenarios and forecasts within a                unified environment. The
   platform allows a                transformation logic to ensure
   interoperability of                datasets across different
   organizational functions,                thus enabling consolidated
   analysis.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematuc view of an    
   architecture of cloud-based operations data                analytics
   platform.
Z9 0
U1 0
U2 0
DA 2025-09-26
UT DIIDW:202590525E
ER

PT P
AU PANDE P
   TODD S J
TI Method for performing data confidence based           
   privacy-preserving data aggregation in various            locations,
   such as datacenter, involves generating            confidence score for
   each data point, aggregating data            points into aggregated
   data, and generating aggregated            results
PN US2025217370-A1
AE DELL PROD LP
AB 
   NOVELTY - The method (400) involves ingesting data                points
   into a data confidence fabric implemented in                a computing
   system. Trust insertion technologies                are applied to the
   data points. A confidence score                is generated (402) for
   each data point. The data                points are aggregated (404)
   into aggregated data.                The aggregated results including
   aggregated data                are generated (406). Each of the data
   point is                weighted according to their confidence scores. A
   threshold confidence score, where data points                having a
   confidence score lower than the threshold                score are not
   included in the aggregated or are                included with a first
   weight that is lower than a                second weight given to data
   points with confidence                scores higher than threshold
   score.
   USE - Method for performing data confidence based               
   privacy-preserving data aggregation in various                locations,
   such as a data lake, in a datacenter, or                public cloud
   data storage service.
   ADVANTAGE - The method enables aggregating data using the               
   confidence scores, generating aggregation results                that
   contain more valuable and accurate anonymized                data that
   retains higher utility for analysis,                decision-making,
   machine learning, and other                purposes without violating
   user privacy. The method                enables allowing the location
   based application to                provide valuable and accurate
   traffic insights                while maintaining user privacy. The
   method enables                ensuring that the data confidence fabric  
   (DCF)-enhanced privacy preserving data aggregation               
   ensures that only high-confidence data has a                significant
   impact on the aggregated results.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   non-transitory storage medium for storing a set of               
   instructions for performing data confidence based               
   privacy-preserving data aggregation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a method for performing data confidence
   based                privacy-preserving data aggregation.400Method for
   performing data confidence                based privacy-preserving data
   aggregation402Generating confidence score for each data               
   point404Aggregating the data points into                aggregated
   data406Generating aggregated results
Z9 0
U1 0
U2 0
DA 2025-07-18
UT DIIDW:202566797W
ER

PT P
AU BISWAS A
   SARKAR A
   MUKHERJEE A
   SAMANTA D
   CHOUBEY D
   DUTTA T
TI Decentralized digital twin system for predicting            employee
   turnover in educational institutions, has            smart contract
   engine for triggering human resource            interventions based on
   turnover risk predictions, and            administrator interface for
   enabling proactive human            resource actions
PN IN202531054573-A
AE JIS COLLEGE ENG
AB 
   NOVELTY - The system has a digital twin module for               
   creating and continuously updating virtual               
   representations of employees based on real-time                data
   including performance metrics, engagement                levels and
   external environmental factors. An                artificial
   intelligence engine processes data from                digital twins
   using machine learning algorithms to                identify patterns
   indicative of potential employee                turnover. A smart
   contract engine automatically                triggers human resource
   interventions based on                turnover risk predictions. An
   administrator                interface displays turnover risk insights,
   and                enables proactive human resource actions.
   USE - Decentralized digital twin system for                predicting
   employee turnover in educational                institutions.
   ADVANTAGE - The use of blockchain ensures secure,               
   tamper-proof and decentralized data management,                thus
   making the system particularly suited for                institutions
   seeking to enhance workforce planning,                employee
   satisfaction and long-term organizational                stability
   through intelligent data-driven                decision-making. The
   system enables proactive                intervention strategies by
   creating and                continuously updating digital
   representations of                employees based on real-time
   performance,                engagement and environmental factors.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a
   block                chain data architecture.
Z9 0
U1 0
U2 0
DA 2025-07-27
UT DIIDW:202568407K
ER

PT J
AU ORDÓÑEZ PALACIOS, LUIS EDUARDO
   BUCHELI GUERRERO, VÍCTOR
   CAICEDO BRAVO, EDUARDO
TI E-solar: una herramienta para la evaluación del recurso solar basada en
   una arquitectura big data sobre un ambiente PySpark
X1 E-solar: a tool for solar resource assessment based on a Big Data
   architecture in a PySpark environment
SO Ingeniería y Desarrollo
VL 43
IS 1
BP 6
EP 23
DI 10.14482/inde.43.01.456.089
DT research-article
PD 2025-06
PY 2025
AB Abstract Over time, diverse researchers have created mathematical,
   statistical, and predictive models to evaluate solar resources. However,
   their implementation in technical tools restricts their usability for
   non-technical users. Additionally, data processing to estimate solar
   radiation often necessitates powerful hardware. This study introduces a
   Big Data based tool that employs flat files and satellite images to
   estimate solar radiation in Colombia. A model was developed using
   machine learning techniques and various programming languages. It
   operates within MapR, a distribution of the Hadoop ecosystem with an
   extensive array of Big Data capabilities and utilizes the PySpark API
   for parallel data processing within a computer cluster. The E-Solar
   tool, deployed on a web server, underwent assessment by professionals
   within the energy sector. Usability was analyzed, compliance with recent
   programming standards was confirmed, and profiles of interested users
   were identified. The solar radiation data generated by the tool are
   pivotal for solar projects. Furthermore, the tool lends support to
   researchers and organizations in decision-making for the implementation
   of photovoltaic systems, as it offers pertinent information regarding
   the behavior of solar resources in Colombia.
X4 Resumen Con el tiempo, diversos investigadores han creado modelos
   matemáticos, estadísticos y predictivos para evaluar el recurso solar.
   Sin embargo, su implementación en herramientas técnicas limita su
   utilización por usuarios no técnicos. Además, el procesamiento de datos
   para estimar la radiación solar suele requerir hardware potente. Este
   estudio presenta una herramienta basada en Big data que utiliza archivos
   planos e imágenes de satélite para estimar la radiación solar en
   Colombia. Se desarrolló un modelo con técnicas de aprendizaje automático
   y varios lenguajes de programación. Se ejecuta en MapR, una distribución
   del ecosistema Hadoop con un amplio conjunto de capacidades big data y
   emplea la API de PySpark para procesar datos en paralelo en un clúster
   de computadoras. La herramienta E-solar implementada en un servidor web
   fue evaluada por profesionales del sector energético. Se analizó la
   usabilidad, se verificó la conformidad con estándares de programación
   recientes y se identificaron perfiles de usuarios interesados. Los datos
   de radiación solar generados por la herramienta son fundamentales para
   proyectos solares. Además, la herramienta proporciona apoyo a
   investigadores y organizaciones; y facilita la toma de decisiones en la
   implementación de sistemas fotovol-taicos al ofrecer información
   relevante sobre el comportamiento del recurso solar en Colombia.
ZS 0
TC 0
ZA 0
ZB 0
ZR 0
Z8 0
Z9 0
U1 0
U2 0
SN 2145-9371
DA 2025-08-21
UT SCIELO:S0122-34612025000100006
ER

PT P
AU SRINIVAS K
   PRAVEENA L N S
TI Analytical framework for assessing impact of            e-commerce on
   men's ready-made retail in artificial            intelligence, machine
   learning, and intelligent            decision support systems, has data
   acquisition module            for collecting structured and unstructured
   retail data            from multiple sources
PN IN202541044732-A
AE KONERU LAKSHMAIAH EDUCATION FOUND
AB 
   NOVELTY - The framework has a data acquisition module                for
   collecting structured and unstructured retail                data from
   multiple sources. A pre-processing module                cleanses,
   normalizes and integrates the collected                data into a
   unified data lake. An artificial                intelligence (AI) engine
   comprises sub-modules that                include a customer behavior
   analysis module, a                sentiment analysis module and a
   competitor                benchmarking module to generate real-time
   insights.                A user interface comprises a retail analytics  
   dashboard for visualizing trends, performance                metrics,
   predictive indicators and competitor                comparisons, where
   the framework provides strategic                recommendations based on
   analytical outputs to                enhance retail performance.
   USE - Analytical framework for assessing impact of               
   e-commerce on men's ready-made retail in AI,                machine
   learning, and intelligent decision support                systems. Can
   also be used for collecting,                integrating, and analyzing
   consumer interaction                data from digital and physical
   retail touchpoints                such as e-commerce platforms, mobile
   applications,                physical stores, social media, and customer
   service                channels.
   ADVANTAGE - The framework supports multilingual and               
   multicultural data processing to ensure                adaptability,
   thus allowing the system to                effectively operate in global
   markets. The system                provides a customization toolkit that
   allows                retailers to define custom intent archetypes,     
   scoring logic, and outcome thresholds to align with               
   unique business goals, thus optimizing for revenue,               
   footfall, inventory turnover, or customer lifetime                value,
   and hence ensuring the system remains                relevant across
   diverse retail formats, including                direct-to-consumer,
   marketplace, and franchise                models.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating 
                 an operation for an analytical framework.
Z9 0
U1 0
U2 0
DA 2025-06-29
UT DIIDW:202559160H
ER

PT P
AU JYOTHY C R
   ABRAHAM S
   RAVEENDRANATH R
   GOPIKA G
   SHIRIN A
TI Artificial intelligence-driven decision support            system for
   digital transformation in traditional            manufacturing
   enterprises, has secure deployment            architecture that supports
   on-premise or cloud-based            environments
PN IN202541041001-A
AE MANGALAM ENG COLLEGE
AB 
   NOVELTY - The system has a data integration layer for               
   collecting operational and enterprise data from               
   enterprise resource planning (ERP), manufacturing               
   execution system (MES), supervisory control and                data
   acquisition (SCADA) and Internet of things                (IoT) systems.
   An artificial intelligence (AI)                engine performs
   predictive, prescriptive and                diagnostic analytics on
   manufacturing data. A                modular user interface (UI) is
   provided with                role-based dashboards and recommendation   
   visualization. A digital twin simulation module is               
   provided for scenario testing. A secure deployment               
   architecture supports on-premise or cloud-based               
   environments.
   USE - AI-driven decision support system for digital               
   transformation in traditional manufacturing                enterprises.
   ADVANTAGE - The system bridges the gap between traditional              
   manufacturing systems and advanced digital                technologies
   using an incremental and modular                approach, avoids a
   complete overhaul of existing                systems, preserves capital
   investment, provides                AI-powered insights to allow good
   decision-making                across maintenance, quality control,
   energy                management and inventory planning, scales across  
   industries and plant sizes, is cloud compatible,                tailored
   with key performance indicators (KPIs)                relevant to roles
   and deployed in low-connectivity                environments using edge
   computing strategies,                empowers mid-sized manufacturers to
   harness digital                tools for competitiveness, flexibility,
   and                sustainable growth, combines predictive analytics,   
   real-time monitoring, and data visualization to                assist
   plant managers, engineers, and executives in                process
   optimization, machine utilization, and                operational
   forecasting, contributes to the                practical adoption of
   Industry 4.0 technologies                within brownfield industrial
   ecosystems, operates                on legacy systems with limited
   connectivity,                fragmented data sources and outdated       
   decision-making processes, increases data centric               
   economy, guides traditional manufacturing                enterprises
   through the process of digital                transformation, enhances
   existing enterprise                software e.g. ERP, MES, and SCADA
   systems using AI                for analytics, forecasting and
   optimization,                aggregates structured and unstructured data
   from                multiple operational silos comprising machine logs, 
   energy meters, maintenance schedules and supply                chain
   flows, processes data to identify                inefficiencies,
   suggests process adjustments,                predicts machine failures,
   offers role specific                dashboards and actionable
   recommendations,                simulates operational scenarios for
   risk-free                experimentation, supports local deployment for 
   data-sensitive industries and secure cloud                deployment or
   on- premise hosting and uses data                encryption and access
   control mechanisms for                cybersecurity compliance, achieves
   modularity,                ensures that manufacturers adopts components 
   incrementally without overhauling their                infrastructure,
   reduces the barrier to Industry 4.0                adoption by merging
   real-time intelligence,                historical context, and strategic
   insight into a                unified decision support system, empowers 
   manufacturers to remain competitive and agile,                augments
   industrial operations without disrupting                core processes,
   collects and aggregates operational                data from IoT
   sensors, machine controllers, energy                meters, and
   maintenance logs in real time into a                central data lake,
   pre-processes data using                techniques e.g. normalization,
   anomaly detection                and imputation to ensure quality,
   identifies                operational bottlenecks, predicts component
   wear,                classifies machine efficiency zones, generates     
   prescriptive suggestions e.g. optimal machine                scheduling,
   energy saving strategies or proactive                inventory ordering,
   features interactive dashboards                for plant managers,
   supervisors and executives and                allows simulation of
   scenarios before implementing                real-world changes.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
              AI-driven decision support system.
Z9 0
U1 0
U2 0
DA 2025-06-21
UT DIIDW:202557041V
ER

PT P
AU XU H
   WANG Y
   HU N
TI Method for safely and quickly storing image data            of image
   department, involves constructing data            integrity checking and
   monitoring system, and            establishing distributed checking
   mechanism using            checking algorithm and tool
PN CN119517325-A
AE UNIV GUIZHOU MEDICAL AFFILIATED HOSPITAL
AB 
   NOVELTY - The method involves deeply excavating and               
   analyzing image data using a large data analysis                and
   machine learning technology. A data warehouse                and a data
   lake are established. An auxiliary                decision support is
   provided for clinical diagnosis                and treatment. A strategy
   combining image data                backup and recovery mechanism is
   established by a                snapshot technology, an increment backup
   and a                difference backup. A detailed recovery flow is     
   made. A data integrity checking and monitoring                system is
   constructed. A distributed checking                mechanism is
   established using a checking algorithm                and a tool. An
   intelligent monitoring and                pre-warning is realized using
   AI and machine                learning technology.
   USE - Method for safely and quickly storing image                data of
   image department.
   ADVANTAGE - The method enables realizing efficient storage              
   and security protection of the image data by the               
   technology of distributed storage, encryption, data               
   de-duplication, compression, large data analysis,                machine
   learning, backup and recovery, data                integrity check,
   monitoring and early                warning.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system    
   for safely and quickly storing image data of image               
   department.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating 
   the process for safely and quickly storing image                data of
   image department (Drawing includes                non-English language
   text).
Z9 0
U1 0
U2 0
DA 2025-03-28
UT DIIDW:202523409C
ER

PT P
AU GURJAR R
   DHANERA M
   PARMAR V
   MOURYA V
   JAIN D
   KOTHARI A
   JAIN P
TI System for internet of things-based sales analysis            and
   machine learning, has decision-making and            automation unit
   that utilizes generated insights for            optimizing sales
   strategies, inventory management, and            marketing efforts
PN IN202521008713-A
AE UNIV SAGE
AB 
   NOVELTY - The system has an internet-of-things               
   (IoT)-based data collection unit to gather                real-time
   sales data including transactions,                inventory levels, and
   customer interactions. A                machine learning and data
   analytics module                processes the collected data, identifies
   patterns,                trends, and correlations, and generates
   actionable                insights. A data preprocessing and storage
   unit                cleans, normalizes, and stores preprocessed data in 
   a centralized database or data lake. A                decision-making
   and automation unit utilizes the                generated insights for
   optimizing sales strategies,                inventory management, and
   marketing efforts.
   USE - System for IoT-based sales analysis and                machine
   learning for real-time sales monitoring,                predictive
   analysis, inventory management, and                personalized
   marketing in retail and e-commerce                industries.
   ADVANTAGE - The system ensures scalability, improved               
   customer engagement, and data-driven strategic                planning,
   allowing businesses to remain competitive                in a dynamic
   market. The optimized inventory                management system
   dynamically adjusts inventory                levels based on predicted
   demand fluctuations to                minimize costs and maximize
   revenue. The system                provides a scalable, intelligent, and
   automated                solution for sales optimization, improving
   business                efficiency, customer engagement and revenue     
   generation. The feedback loop mechanism monitors                the
   effectiveness of implemented sales strategies                and refines
   future decision-making based on                stakeholder and customer
   feedback. The data mining                model extracts complex
   relationships and hidden                patterns in sales data,
   enhancing predictive                accuracy. The mobile accessibility
   and remote                monitoring module enables stakeholders to
   access                and monitor sales insights in real-time through   
               mobile devices.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a     
   system for IoT-based sales analysis and machine                learning.
Z9 0
U1 0
U2 0
DA 2025-04-11
UT DIIDW:202521837P
ER

PT P
AU PU C
   WANG M
   WANG H
TI Comprehensive data intelligent processing system            has resource
   calculation module which is provided to            calculate optimal
   solution for data resource calls, and            data lineage analysis
   module that is provided to            analyze flow and changes of data
PN CN119396933-A
AE QIMING INFORMATION TECHNOLOGY CO LTD CHA
AB 
   NOVELTY - The system has a data storage and management               
   module which is provided to collect data, store and               
   manage the collected data. A data analysis module                is
   provided to analyze and process the stored data.                A
   resource calculation module is provided to                calculate the
   optimal solution for data resource                calls. A data lineage
   analysis module is provided                to analyze the flow and
   changes of data. A data                lineage analysis task is created.
   The data is                sorted and associated and the sorted and
   associated                data is stored.
   USE - Comprehensive data intelligent processing                system
   for use in technical field of machine                learning.
   ADVANTAGE - The system aims to achieve unified storage and              
   management of data by integrating the advantages of                data
   lake and data warehouse. The system uses                storage and
   computing separation technology and                deep learning
   algorithms to optimize data                processing flow and improve
   processing efficiency                and accuracy. The system achieves
   accurate tracking                of data flow and changes through
   enhanced lineage                analysis functions, providing
   enterprises with                deeper data insights and decision
   support.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   comprehensive data intelligent processing system.               
   (Drawing includes non-English language text)
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202515632W
ER

PT J
AU Bruno Damásio
AA Carolina Vasconcelos; Flávio Pinheiro; Niclas Frederic Sturm; Sandro
   Mendonça; BEATRIZ AZEVEDO MONTEIRO; Carolina Dinis Ribeiro Raimundo
   Pinto; Carolina Maria de Abreu Braziel Shaul; Matilde Leal Duarte
   Parreira
TI Melhor Eficiência na Avaliação de Riscos e Irregularidades na
   Contratação Pública
DT Awarded Grant
PD Feb 01 2025
PY 2025
AB Corruption not only erodes public trust but also channels resources away
   from public interests. Various risks and irregularities are often linked
   to corrupt practices within public procurement. Identifying these risks
   is crucial to ensure transparency, accountability, and the efficient use
   of public resources. However, detecting such risks in vast amounts of
   data and diverse sources presents significant challenges. As evidenced
   by existing literature, Machine Learning (ML) techniques offer promising
   solutions. Existing studies demonstrate the effectiveness of various ML
   algorithms in detecting procurement irregularities and collusion,
   revealing significant differences in public-private connections and
   identifying proxy indicators for single bidding and municipal spending.
   These techniques have also been employed to predict corruption
   investigations, contract breaches, and profile firms vulnerable to
   market manipulation. Despite access to extensive data sources, the
   Portuguese Court of Auditors has yet to fully leverage these resources
   for their audits. In fact, the European Organization of Supreme Audit
   Institutions has conducted a survey to identify the current situation
   among SAIs in connecting technology to audit processes (EUROSAI, 2023).
   The results showed that the use of artificial intelligence, machine
   learning and deep learning has not yet been integrated into audits in
   the Supreme Audit Institutions. This research project aims to integrate
   ML and data science within public procurement audits at the Court of
   Auditors. This integration will enhance the auditors' capacity to handle
   large datasets, improving efficiency, transparency, and accountability
   in public procurement. To address this challenge, the project team is
   comprised of experts in data science, econometrics, network analysis,
   public administration, digital innovation, and competition policy. The
   team's expertise includes advanced data analysis, machine learning, and
   network science, ensuring a robust approach to developing effective
   models for risk and irregularity detection. Also, the team possesses
   deep domain knowledge from various perspectives, which will
   significantly benefit the model-building process. The project will
   follow a structured plan encompassing three main phases. The first phase
   involves identifying data requirements and gathering relevant data,
   followed by quality assessment and integration into a data lake. The
   second phase focuses on model development, training, and testing using
   various ML methods such as Random Forest, Neural Networks, Support
   Vector Machines, and Logistic Regression, with network science tools
   employed for non-labelled data. The final phase corresponds to deploying
   the model within the Court's systems, developing data visualization
   features, and providing training for staff. This integration will
   significantly improve public administration efficiency by enabling more
   effective and timely identification of procurement irregularities. By
   automating the detection process and leveraging analytical techniques,
   the project will reduce the manual effort required for audits, allowing
   auditors to focus on higher-level analysis and decision-making.
   Furthermore, the improved accuracy and speed of detecting irregularities
   will result in better resource allocation and reduced financial losses
   due to fraud and inefficiencies, ultimately leading to more responsible
   and transparent management of public funds.
TC 0
ZA 0
ZB 0
Z8 0
ZR 0
ZS 0
Z9 0
U1 0
U2 0
G1 2024.07601.IACDC
DA 2025-05-11
UT GRANTS:17900684
ER

PT J
AU João Bruno Morais de Jardim
AA André Figueiredo Barriguinha; Gonçalo Nuno de Castro Oliveira Gomes;
   Miguel de Castro Simões Ferreira Neto; Paulo Simões; Sérgio Guerreiro;
   Mafalda Abranches Amorim Martins
TI AI-Driven Event Management in Oeste
DT Awarded Grant
PD Feb 01 2025
PY 2025
AB Comunidade Intermunicipal do Oeste (OesteCIM) has been at the forefront
   of leveraging data through its Smart Region project . This initiative
   has resulted in a comprehensive data repository encompassing various
   aspects such as monetary transactions, traffic congestion, energy
   consumption, public transport ticket sales, CO2 emissions, and local
   accommodation figures. Despite this rich dataset, there is a significant
   gap in effectively utilizing this data to manage and forecast the impact
   of cultural, sports, and other events . This project aims to develop an
   innovative platform, AI-Driven Event Management for Oeste (AIvents),
   that integrates real-time and historical data analysis, forecasting, and
   natural language querying to enhance event management in OesteCIM . The
   integration of AI and machine learning in public administration,
   particularly in event management , is an emerging field with substantial
   potential. Researchers have explored the transformative impact of these
   technologies in enhancing public sector efficiency and decision-making.
   By cross-referencing diverse data sets, it is possible to derive
   insights into the multifaceted impacts of events. Techniques such as
   difference-in-differences (DiD) analysis enable robust causal inference
   regarding the effects of events on various economic and environmental
   variables. Additionally, advanced forecasting models, including time
   series analysis and machine learning algorithms, can provide predictive
   insights. On top of this, natural language processing (NLP) facilitates
   intuitive interaction with the data and prediction results, making
   sophisticated analytics accessible to non-technical users through
   conversational AI. To harness the full potential of the rich data
   amassed by OesteCIM's Smart Region project, the first step is to
   cross-reference this existing data with event-related data. This
   integration process involves aligning data from various sources and
   ensuring its quality and consistency, creating a comprehensive dataset
   that can support detailed analysis. Building on this foundation, a Data
   Lakehouse will be developed using tools like Micorsoft Fabric,
   facilitating both real-time and historical data analysis . This
   architecture will provide a robust framework for the storage,
   processing, and analysis of large volumes of data, ensuring that
   insights can be derived efficiently and effectively. Interactive
   dashboards will be created for policymakers and event organizers,
   providing them with the tools to visualize data trends and derive
   actionable insights. These dashboards will offer real-time updates and
   historical perspectives, enabling users to make informed decisions based
   on comprehensive data analysis. To assess the impact of events,
   statistical methods such as difference-in-differences (DiD) analysis
   will be implemented, allowing to isolate the effects of events on
   economic and environmental variables. Leveraging the comprehensive
   dataset, m achine learning models will be trained using historical data
   to predict the outcomes of events on various variables such as traffic,
   energy consumption, and CO2 emissions. These models will include
   regression analysis, time series forecasting, and advanced algorithms
   like neural networks and random forests, providing a range of predictive
   insights that can inform event planning and management. In addition to
   these predictive models, simulation capabilities will be developed to
   forecast the impact of potential events at specific locations and times
   . These simulations will enable scenario planning and resource
   allocation by predicting the impacts of hypothetical events, allowing
   policymakers to anticipate challenges and allocate resources more
   efficiently. To enhance user engagement and accessibility, a chatbot
   leveraging natural language processing (NLP) will be created . This
   chatbot will enable users to query the database in natural language,
   providing real-time and historical insights through a user-friendly
   interface . By querying a SQL database, the chatbot will be able to
   answerquestions about past events and provide predictions (previously
   made by the forecast model) for future events, making complex data
   analyses accessible to non-technical stakeholders . This will
   democratize access to sophisticated analytics, supporting informed
   decision-making and enhancing user engagement. Overall, the AI-Driven
   Event Management platform will transform how OesteCIM manages and
   predicts the impacts of events, leveraging AI and machine learning to
   harness the full potential of its data resources . This innovative
   platform will lead to more effective event management and strategic
   planning, ultimately benefiting the community by improving resource
   allocation, reducing negative impacts, and enhancing the overall event
   experience. This platform promises to set a new standard for event
   management in public administration, leveraging the power of AI to
   create smarter, more efficient, and more responsive communities .
ZA 0
Z8 0
ZR 0
TC 0
ZB 0
ZS 0
Z9 0
U1 0
U2 0
G1 2024.07501.IACDC
DA 2025-05-11
UT GRANTS:17900682
ER

PT B
AU Aires, Vitor André Jóia
Z2  
TI Optimising Energy Analytics: a Dimensional Modelling Approach for
   Enhanced Decision-Making
DT Dissertation/Thesis
PD Jan 01 2025
PY 2025
TC 0
ZA 0
ZB 0
Z8 0
ZS 0
ZR 0
Z9 0
U1 0
U2 0
BN 9798265490445
UT PQDT:162943200
ER

PT C
AU El Falah, Zineb
   Abouchabaka, Jaafar
   Rafalia, Najat
BE Koubaa, A
   Mnaouer, AB
   Boulila, W
   Raghay, S
TI An Intelligent Big Data Analysis Approach for Real-Time Data Processing:
   A Case Study on META Stock Price Prediction Using LSTM Model
SO INTERNATIONAL CONFERENCE ON SMART SYSTEMS AND EMERGING TECHNOLOGIES,
   2024
SE Lecture Notes in Networks and Systems
VL 1401
BP 279
EP 291
DI 10.1007/978-3-031-91235-1_25
DT Proceedings Paper
PD 2025
PY 2025
AB Technological progress has made Big Data a major trend, posing
   significant challenges in managing vast datasets through digital
   technologies. These challenges include integrating diverse data,
   ensuring timely processing, and providing effective analysis and
   visualization. Big Data analytics uses AI, deep learning, and machine
   learning to uncover patterns and trends, improving decision-making and
   predictions. Real-time data processing faces additional difficulties
   related to handling high data velocity and volume, requiring robust and
   scalable infrastructure. This paper proposes an intelligent Big Data
   architecture for real-time data processing using Apache Kafka, the
   Elastic Stack, and Apache Spark. The approach is demonstrated through a
   case study predicting META stock prices using a Long Short-Term Memory
   (LSTM) model. By integrating deep learning, the architecture effectively
   addresses Big Data challenges, improving decision-making efficiency. The
   analysis uses stock data from Yahoo Finance, and the model's performance
   was assessed using R-squared (R-2) and Mean Absolute Error (MAE). The
   LSTM model achieved an R-2 score of 0.99 and an MAE of 0.04, showing
   significant improvement over traditional models, particularly in
   handling volatile stock data.
CT 3rd International Conference on Smart Systems and Emerging
   Technologies-SMARTTECH-Annual
CY NOV 19-21, 2024
CL Cadi Ayyad University of Marrakech, Marrakesh, MOROCCO
HO Cadi Ayyad University of Marrakech
SP Institute of Electrical and Electronics Engineers Inc; Prince Sultan
   University; University of Prince Mugrin; Ibn Tofail University of
   Kenitra
RI ABOUCHABAKA, Jaafar/JTV-1015-2023; EL FALAH, Zineb/KYP-0832-2024
TC 0
ZB 0
ZR 0
ZA 0
Z8 0
ZS 0
Z9 0
U1 0
U2 0
SN 2367-3370
EI 2367-3389
BN 978-3-031-91234-4; 978-3-031-91235-1
DA 2025-12-05
UT WOS:001591611300025
ER

PT J
AU Goncalves, Antonio
TI XAI-Compliance-by-Design: A Modular Framework for GDPRand AI Act-Aligned
   Decision Transparency in High-Risk AI Systems
SO Zenodo
DI https://doi.org/10.5281/ZENODO.17813361
DT Software
PD 2025-12-23
PY 2025
AB Software Description ��� XAI-Compliance-by-Design Pipeline �� �� This
   repository provides the full Python implementation of the
   XAI-Compliance-by-Design Pipeline, developed as part of the article
   ���XAI-Compliance-by-Design: A Modular Framework for GDPR- and AI
   Act-Aligned Decision Transparency in High-Risk AI Systems.��� �� The
   software operationalises the modular framework presented in the
   manuscript, integrating explainable artificial intelligence, regulatory
   compliance, and trustworthy Machine Learning Operations (MLOps) into a
   unified, reproducible workflow suitable for high-risk AI
   contexts-particularly in cybersecurity and privacy-critical
   environments. 1. Purpose of the Software The aim of this software
   package is to demonstrate how organisations can design and operate
   high-risk AI systems in a manner that systematically produces traceable,
   auditable and regulation-aligned evidence. The implementation shows how
   technical artefacts can be mapped directly to regulatory obligations
   under: the General Data Protection Regulation (GDPR), the EU Artificial
   Intelligence Act (AI Act), and ISO/IEC 42001:2023 (AI Management
   Systems). Rather than optimising predictive performance, the software
   prioritises accountability, transparency, explainability, documentation
   and continuous monitoring-all requirements of modern European AI
   governance. �� 2. Architectural Overview The implementation follows the
   dual-flow architecture introduced in the article: �� 2.1. Upstream
   Technical Flow Responsible for the operational lifecycle of the AI
   system: environment configuration and RUN_ID registration data
   preprocessing and quality logging model training and evaluation
   explainability (SHAP and LIME) drift monitoring structured storage of
   technical artefacts �� 2.2. Downstream Governance Flow Responsible for
   regulatory and organisational alignment: mapping technical artefacts to
   legal requirements compliance logging audit trail generation continuous
   oversight and policy-as-code mechanisms construction of evidence bundles
   and decision dossiers A central Compliance-by-Design Engine (CDE)
   orchestrates the synchronisation of both flows and manages the
   technical���regulatory correspondence matrix. �� 3. Repository Structure
   The repository is organised in a manner that supports full traceability,
   reproducibility and audit readiness: /data_lake/ Frozen datasets,
   metadata and schema descriptions /models/ Serialized model pipelines
   with lineage hashes /evidence_bundles/ SHAP/LIME outputs, drift reports,
   compliance logs, JSON manifests /decision_dossiers/ Deployment
   decisions, justification records, governance indicators /notebooks/
   Reproducible notebooks illustrating each stage of the pipeline /src/
   Core implementation (preprocessing, training, explainability, logging)
   Each execution is assigned a unique RUN_ID, propagated across all logs,
   filenames and artefacts. �� 4. Key Functional Components �� 4.1. Data
   Handling and Preprocessing synthetic IDS-like dataset generation
   definition of numerical/categorical attributes ColumnTransformer-based
   preprocessing data schema and statistics stored for audit purposes ��
   4.2. Model Training Random Forest classifier (illustrative;
   model-agnostic framework) standard performance metrics: accuracy,
   precision, recall, F1, ROC-AUC versioned model artefacts with SHA-256
   integrity hashing �� 4.3. Explainability Layer SHAP (global summaries,
   feature importance, per-instance explanations) LIME (local explanations
   for selected samples) stability and fidelity indicators full provenance
   logging for reproducibility �� 4.4. Drift Monitoring distributional
   drift indicators stored for post-market monitoring obligations under the
   AI Act �� 4.5. Evidence Bundles and Decision Dossiers Automatically
   generated for each RUN_ID, containing: model documentation explanation
   reports drift analyses performance logs governance interpretations
   deployment eligibility assessment �� These bundles support auditability
   under GDPR, the AI Act and ISO/IEC 42001. �� �� 5. Intended Use �� The
   software is not designed as a high-performance intrusion detection
   system. Instead, its purpose is: to serve as an executable reference
   implementation of the XAI-Compliance-by-Design framework; to demonstrate
   how regulatory requirements can be embedded directly into an MLOps
   pipeline; to allow auditors, researchers and engineers to inspect every
   artefact required for governance, oversight and conformity assessment;
   to provide a template for deploying evidence-centric, transparent and
   governance-aligned high-risk AI systems. �� 6. Reproducibility The
   repository includes: version-locked dependencies deterministic random
   seeds complete artefact lineage structured compliance logs executable
   notebooks This guarantees that any RUN_ID can be reconstructed for
   audit, academic validation or regulatory review. �� 7. Applicability
   Beyond the Case Study Although instantiated on a synthetic IDS-inspired
   dataset, the architecture is domain-agnostic and transferable to any
   high-risk AI context, including: cybersecurity operations (SOC, IDS/IPS,
   threat scoring) financial fraud detection public-sector decision-support
   systems healthcare risk stratification safety-critical industrial
   systems The core design principles-modularity, traceability,
   explainability, compliance-by-design-remain applicable across domains
   Copyright: Creative Commons Attribution 4.0 International
TC 0
Z8 0
ZS 0
ZB 0
ZA 0
ZR 0
Z9 0
U1 0
U2 0
DA 2026-02-17
UT DRCI:DATA2026016034811411
ER

PT J
AU Goncalves, Antonio
TI XAI-Compliance-by-Design: A Modular Framework for GDPRand AI Act-Aligned
   Decision Transparency in High-Risk AI Systems
SO Zenodo
DI https://doi.org/10.5281/ZENODO.17813362
DT Software
PD 2025-12-23
PY 2025
AB Software Description ��� XAI-Compliance-by-Design Pipeline �� �� This
   repository provides the full Python implementation of the
   XAI-Compliance-by-Design Pipeline, developed as part of the article
   ���XAI-Compliance-by-Design: A Modular Framework for GDPR- and AI
   Act-Aligned Decision Transparency in High-Risk AI Systems.��� �� The
   software operationalises the modular framework presented in the
   manuscript, integrating explainable artificial intelligence, regulatory
   compliance, and trustworthy Machine Learning Operations (MLOps) into a
   unified, reproducible workflow suitable for high-risk AI
   contexts-particularly in cybersecurity and privacy-critical
   environments. 1. Purpose of the Software The aim of this software
   package is to demonstrate how organisations can design and operate
   high-risk AI systems in a manner that systematically produces traceable,
   auditable and regulation-aligned evidence. The implementation shows how
   technical artefacts can be mapped directly to regulatory obligations
   under: the General Data Protection Regulation (GDPR), the EU Artificial
   Intelligence Act (AI Act), and ISO/IEC 42001:2023 (AI Management
   Systems). Rather than optimising predictive performance, the software
   prioritises accountability, transparency, explainability, documentation
   and continuous monitoring-all requirements of modern European AI
   governance. �� 2. Architectural Overview The implementation follows the
   dual-flow architecture introduced in the article: �� 2.1. Upstream
   Technical Flow Responsible for the operational lifecycle of the AI
   system: environment configuration and RUN_ID registration data
   preprocessing and quality logging model training and evaluation
   explainability (SHAP and LIME) drift monitoring structured storage of
   technical artefacts �� 2.2. Downstream Governance Flow Responsible for
   regulatory and organisational alignment: mapping technical artefacts to
   legal requirements compliance logging audit trail generation continuous
   oversight and policy-as-code mechanisms construction of evidence bundles
   and decision dossiers A central Compliance-by-Design Engine (CDE)
   orchestrates the synchronisation of both flows and manages the
   technical���regulatory correspondence matrix. �� 3. Repository Structure
   The repository is organised in a manner that supports full traceability,
   reproducibility and audit readiness: /data_lake/ Frozen datasets,
   metadata and schema descriptions /models/ Serialized model pipelines
   with lineage hashes /evidence_bundles/ SHAP/LIME outputs, drift reports,
   compliance logs, JSON manifests /decision_dossiers/ Deployment
   decisions, justification records, governance indicators /notebooks/
   Reproducible notebooks illustrating each stage of the pipeline /src/
   Core implementation (preprocessing, training, explainability, logging)
   Each execution is assigned a unique RUN_ID, propagated across all logs,
   filenames and artefacts. �� 4. Key Functional Components �� 4.1. Data
   Handling and Preprocessing synthetic IDS-like dataset generation
   definition of numerical/categorical attributes ColumnTransformer-based
   preprocessing data schema and statistics stored for audit purposes ��
   4.2. Model Training Random Forest classifier (illustrative;
   model-agnostic framework) standard performance metrics: accuracy,
   precision, recall, F1, ROC-AUC versioned model artefacts with SHA-256
   integrity hashing �� 4.3. Explainability Layer SHAP (global summaries,
   feature importance, per-instance explanations) LIME (local explanations
   for selected samples) stability and fidelity indicators full provenance
   logging for reproducibility �� 4.4. Drift Monitoring distributional
   drift indicators stored for post-market monitoring obligations under the
   AI Act �� 4.5. Evidence Bundles and Decision Dossiers Automatically
   generated for each RUN_ID, containing: model documentation explanation
   reports drift analyses performance logs governance interpretations
   deployment eligibility assessment �� These bundles support auditability
   under GDPR, the AI Act and ISO/IEC 42001. �� �� 5. Intended Use �� The
   software is not designed as a high-performance intrusion detection
   system. Instead, its purpose is: to serve as an executable reference
   implementation of the XAI-Compliance-by-Design framework; to demonstrate
   how regulatory requirements can be embedded directly into an MLOps
   pipeline; to allow auditors, researchers and engineers to inspect every
   artefact required for governance, oversight and conformity assessment;
   to provide a template for deploying evidence-centric, transparent and
   governance-aligned high-risk AI systems. �� 6. Reproducibility The
   repository includes: version-locked dependencies deterministic random
   seeds complete artefact lineage structured compliance logs executable
   notebooks This guarantees that any RUN_ID can be reconstructed for
   audit, academic validation or regulatory review. �� 7. Applicability
   Beyond the Case Study Although instantiated on a synthetic IDS-inspired
   dataset, the architecture is domain-agnostic and transferable to any
   high-risk AI context, including: cybersecurity operations (SOC, IDS/IPS,
   threat scoring) financial fraud detection public-sector decision-support
   systems healthcare risk stratification safety-critical industrial
   systems The core design principles-modularity, traceability,
   explainability, compliance-by-design-remain applicable across domains
   Copyright: Creative Commons Attribution 4.0 International
ZB 0
TC 0
ZR 0
ZS 0
ZA 0
Z8 0
Z9 0
U1 0
U2 0
DA 2026-02-17
UT DRCI:DATA2026016034811412
ER

PT J
AU Kabir, Raihan
   Watanobe, Yutaka
   Ding, Dake
   Islam, Md Rashedul
   Naruse, Keitaro
TI A Comprehensive Survey on Advanced Data Science Platforms for
   Cyber-Physical Systems, Digital Twins, and Robotics
SO IEEE ACCESS
VL 13
BP 177269
EP 177304
DI 10.1109/ACCESS.2025.3619776
DT Article
PD 2025
PY 2025
AB The integration of Cyber-Physical Systems (CPS), Digital Twins (DT), and
   robotics with Advanced Data Science Platforms (ADSP) is rapidly
   transforming industrial and research landscapes by enabling real-time
   data processing, intelligent decision-making, and enhanced automation.
   Over the past decades, with the growing demand for adaptable,
   expandable, and secure platforms, numerous groundbreaking studies have
   emerged in the field of ADSP for CPS, DT, and robotics. However, most
   existing research addresses isolated aspects of AI, CPS, DTs, or
   robotics, lacking a holistic view of how ADSP expands and optimizes
   these technologies. To address this gap, this survey provides a
   comprehensive and structured review of the existing methodologies,
   tools, and techniques at each step of the ADSP workflow, focusing on
   their applications in CPS, DT, and robotics. Firstly, this survey
   analyzes the data ingestion phase, focusing on raw data collection
   architecture, advanced data pre-processing techniques, and data lake
   integration, while identifying integration and scalability challenges.
   Secondly, the development methodologies and data analysis techniques
   within experimental workflows are explored by highlighting widely used
   tools and real-world case studies. Thirdly, a detailed overview of
   optimization techniques and deployment strategies is presented,
   including cloud, edge, and hybrid models, supported by practical
   deployment examples. Finally, the continuous learning mechanisms are
   investigated for adaptive system updates, challenges in real-time
   adaptation, and expanding model performance. Additionally, this survey
   focuses on evaluation metrics, benchmark studies, and performance
   comparisons to assess platform efficiency across various domains. This
   study also explores emerging challenges such as data quality &
   availability, platform expandability, model transparency, and security,
   offering insights into future research directions. To ensure the rigor
   and breadth, this survey is conducted on 316 high-quality studies,
   state-of-the-art methodologies, and platforms that are selected using
   the PRISMA methodology from over 600 reviewed publications. By
   presenting a structured overview of ADSPs, this work aims to serve as a
   valuable resource for researchers, engineers, and industry professionals
   aiming to utilize data science for innovation in CPS, DT, and robotics.
RI Kabir, Raihan/MVY-3900-2025; Islam, Rashedul/D-6227-2019; Ding, Dake/; Naruse, Keitaro/
OI Kabir, Raihan/0000-0003-2031-8836; Islam, Rashedul/0000-0001-8676-6338;
   Ding, Dake/0009-0005-3952-3698; Naruse, Keitaro/0000-0002-2029-2472
ZA 0
ZB 0
Z8 0
ZS 0
ZR 0
TC 0
Z9 0
U1 4
U2 4
SN 2169-3536
DA 2025-11-01
UT WOS:001600088200046
ER

PT J
AU Kretzer, Arthur Raulino
   Barreto Vavassori Benitti, Fabiane
   Siqueira, Frank
TI Challenges and Opportunities in Big Data Analytics for Industry 4.0: A
   Systematic Evaluation of Current Architectures
SO IEEE ACCESS
VL 13
BP 183419
EP 183447
DI 10.1109/ACCESS.2025.3624558
DT Article
PD 2025
PY 2025
AB The current efforts to integrate Big Data Analytics (BDA) into Industry
   4.0 manufacturing systems, despite their usefulness for enhancing
   data-driven decision-making, are constrained by the lack of
   architectural standards for data management. This systematic mapping
   study analyzes many BDA architectures proposed in the literature,
   revealing a fragmented landscape in which the proposed architectures are
   largely conceptual with limited industrial validation. Our analysis
   identifies dominant technological patterns, such as Apache Kafka for
   ingestion, Spark for processing, and Hadoop and Hive for storage, with
   the majority of implementations favoring open-source solutions. Despite
   their theoretical importance, real-time analytics capabilities remain
   underutilized in practice. This study synthesizes a unified conceptual
   reference architecture with eight fundamental layers to provide a
   framework for comparative analysis. We document an imbalance in layer
   development: storage and processing receive comprehensive attention
   while querying, infrastructure management, and monitoring layers remain
   underdeveloped. Implementation approaches show distinct patterns in
   deployment strategies and data handling, with structured and
   semi-structured data well supported, whereas unstructured data
   integration presents ongoing challenges. Future research should focus on
   developing standardized modular frameworks, benchmarking methodologies,
   and integrating modern data lakehouse architectures to bridge the gap
   between theoretical proposals and production-ready systems.
RI Siqueira, Frank/ABB-8351-2021; Raulino Kretzer, Arthur/; Benitti, Fabiane/
OI Raulino Kretzer, Arthur/0000-0003-1656-9464; Benitti,
   Fabiane/0000-0003-2747-9931
ZB 0
Z8 0
ZA 0
ZR 0
TC 0
ZS 0
Z9 0
U1 4
U2 4
SN 2169-3536
DA 2025-11-12
UT WOS:001606717700016
ER

PT J
AU Wang, Gaizhi
TI Intelligent Path for Constructing Financial Risk Monitoring Mechanism
   Under the Big Data Environment
SO INTERNATIONAL JOURNAL OF DECISION SUPPORT SYSTEM TECHNOLOGY
VL 17
IS 1
AR 389193
DI 10.4018/IJDSST.389193
DT Article
PD 2025
PY 2025
AB In the era of big data, corporate financial operations generate a large
   amount of heterogeneous information. Traditional risk monitoring systems
   cannot effectively accommodate complex data flows and real-time risk
   changes, which often leads to false positives and delays. This study
   proposes a framework based on 'big data lake-semantic layer-intelligent
   algorithm' to achieve real-time and interpretable financial risk
   monitoring. Through the multi-modal risk representation combined with
   the hybrid flow-batch pipeline and knowledge graph, the real-time
   synchronization of risk scoring and response strategy is realized by
   using a state machine-driven feedback loop and adaptive threshold
   adjustment. The experimental results show that the accuracy of the
   framework is improved by more than 10%, the false alarm rate is reduced
   to 1.8%, and the response time is shortened to 250 milliseconds. This
   study improves the stability and responsiveness of the system through
   multi-modal learning and an adaptive threshold mechanism.
TC 0
ZA 0
ZS 0
ZB 0
ZR 0
Z8 0
Z9 0
U1 3
U2 3
SN 1941-6296
EI 1941-630X
DA 2025-11-17
UT WOS:001612856000002
ER

PT P
AU GOPALAN M
   SAHOO S
TI Real-time data mesh method for change data capture            in ERP
   agnostic real-time data mesh for dynamically            consolidating
   interaction point to supply chain            management, involves
   integrating transformed and            harmonized data into data layer
   of real-time data mesh,            which includes Global Data Lake
PN US2024428166-A1; EP4485304-A1; CN119294959-A; EP4492307-A1;
   AU2024204660-A1; JP2025011046-A; MX2024008565-A1
AE INGRAM MICRO INC
AB 
   NOVELTY - The method involves monitoring transactional               
   systems including enterprise resource planning                (ERP) for
   real-time changes. The changed data is                captured and
   processed using change data capture                mechanisms. The
   captured data is transformed and                harmonized into a
   standardized format that is                compatible with analysis and
   integration processes                for ensuring data consistency and
   compatibility                across the data mesh. The data consistency
   and                compatibility across the data mesh are ensured. The  
   transformed and the harmonized data are integrated                into
   the data layer of the real time data mesh,                which includes
   a global data lake comprising one or                more Purposive
   Datastores (PDSes). The real time                analysis and
   decision-making are enabled based on                up-to-date data
   within the data meshes. The data                layer comprises the
   Purposive Datastores (PDSes)                optimized for efficient
   retrieval and storage of                specific types of data.
   USE - Real-time data mesh method for change data                capture
   in an Enterprise Resource Planning (ERP)                agnostic
   real-time data mesh for dynamically                consolidating
   interaction points in a distribution                ecosystem to manage
   distribution, supply chain,                inventory control, stock
   keeping unit, compliance,                and evolving consumer
   expectations in a global                distribution industry.
   ADVANTAGE - The method enables the distribution model to               
   hold numerous advantages over the                direct-to-consumer
   model, thus enabling                manufacturers to focus on their core
   competencies,                leaving the complexities of logistics and  
   distribution. The method ensures that distributors                offer
   value-added services, such as after-sales                support,
   installation, and training that enhance                the overall
   customer experience. The current pain                points need to be
   addressed, and processes should                be made more efficient
   and streamlined to ensure                the sustainability of
   distribution model in the                evolving market landscape. The
   system can be                configured to encompass features such as   
   subscription management and other customer-centric                areas
   that traditional distribution platforms have                not
   effectively managed. The artificial                intelligence and
   machine learning models are                applied to enhance the change
   data capture process,                facilitating automated analysis,
   and                decision-making within the real-time data            
      mesh.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   system for change data capture in an ERP agnostic               
   real-time data mesh; (2) a computer-readable                medium.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of       
   operating environment of a distribution               
   platform.100Operating Environment110Leveraging
   system120stakeholders130End Customers140Vendors150Resellers
Z9 0
U1 0
U2 0
DA 2025-01-16
UT DIIDW:2024D6915T
ER

PT P
AU WANG R
   LIU M
   SUN F
   LI X
   LIU J
   WANG S
   SHEN H
   YUAN H
   LIU Y
TI Industrial decision support system based on large            data and
   artificial intelligence technology, has            operation
   optimization engine that is provided for            optimizing
   production flow, and security and compliance            module that is
   provided to ensure that system follows            data protection rules
PN CN119149933-A
AE NANJING KEXUN GRAIN IND TECHNOLOGY CO
AB 
   NOVELTY - The system has a data storage and management               
   system that is provided for constructing and                maintaining
   a data lake, supporting the storage of                multiple data
   types including structured data,                semi-structured data and
   non-structuring data, and                realizing the version control
   of the data, access                authority management and safe backup.
   A data                analysis and modeling module is provided with     
   machine learning algorithm such as random forest,                neural
   network and support vector machine using                deep learning
   technique. An operation optimization                engine is provided
   for optimizing the production                flow, stock level, device
   maintenance plan and                logistics arrangement. A security
   and compliance                module is provided to ensure that system
   follows                data protection rules.
   USE - Industrial decision support system based on                large
   data and artificial intelligence technology                for providing
   intelligent decision support for                industrial production
   process.
   ADVANTAGE - The industrial decision supporting system               
   improves the production efficiency and the product               
   quality, helps the enterprise to reduce the                operation
   cost, avoids the potential risk, and                drives the
   enterprise towards the intelligent and                digital
   transformation. The decision maker can                easily obtain the
   key service insight and                intelligent decision suggestion
   through the user                interaction interface to make quick
   response in the                continuously changing market environment.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for te           
   following:1. a decision method using the industrial               
   decision support system; and2. an industrial decision supporting        
          system.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of an        
   industrial decision support system. (Drawing                includes
   non-English language text).
Z9 0
U1 0
U2 0
DA 2025-01-27
UT DIIDW:2024D4872A
ER

PT P
AU GRIFFIS J
   DUKES S
   DUKES S P
TI System for prescriptive intelligent system used in            e.g.
   making of goods for mobile industrial workers, has            processor
   that is configured to provide overview of            industrial asset or
   industrial process, alarm, root            cause analysis, and
   prescribed solution by user            interface
PN WO2024254131-A2; WO2024254131-A3; AU2024286612-A1; CN120882907-A;
   EP4665891-A2; IN202517088167-A; KR2026018810-A
AE EVOQUA WATER TECHNOLOGIES LLC
AB 
   NOVELTY - The system (400) has processor that is               
   configured to apply information standard to                ingestion
   pipeline (404) enabled to process data                from the data lake
   house (402) and use artificial                intelligence algorithms
   which identify actionable                insights in the data by
   orchestration engine (406),                where actionable insights
   comprise solutions                implemented in historical environments
   and feasible                for other environments. A processor is
   configured                to store the data as a knowledge graph across 
   different types of user interface technology                component
   (408). A processor is configured to                provide an overview
   of an industrial asset or an                industrial process, an
   alarm, a root cause                analysis, and a prescribed solution
   by a user                interface, where the user interface coordinates
   outputs from some of the different types of                technology
   components in response to an excursion                associated with
   the industrial asset or the                industrial process.
   USE - System for prescriptive intelligent system                used in
   making of goods and/or providing of                services for mobile
   industrial workers using user                equipment such as mobile
   phone, tablet, laptop,                sensor, internet of things (loT)
   device, autonomous                machine, and any other device equipped
   with                cellular or wireless or wired transceiver.
   ADVANTAGE - The system improves the in-the-moment decision              
   making process by providing a knowledge               
   graph-as-a-service which converts any operator into                an
   expert operator, thus resolving the challenge of                workers
   not being able to readily access and                understand asset
   information to resolve problems                with asset functionality.
   The artificial                intelligence and/or machine learning
   algorithms are                used for allowing the system to
   potentially find                information/insights more easily or find
   information/insights which no individual human or               
   currently available operator have. The same                information
   standards and mapping are applied to                all data from all
   sources ensuring that there are                compatible, consistent,
   and coherent models across                of the sources of data. A
   machine learning                algorithm is an application of
   artificial                intelligence that provides a system with the  
   ability to automatically learn and improve from               
   experience without being explicitly                programmed.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a method for prescriptive intelligent system               
   for mobile industrial workers; anda computer program product for
   prescriptive                intelligent system for mobile industrial    
              workers.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a
   System                for prescriptive intelligent system for mobile    
   industrial workers.400System for prescriptive intelligent system        
   for mobile industrial workers402Data lake house404Ingestion
   pipeline406Orchestration engine408User interface technology component
Z9 0
U1 0
U2 0
DA 2025-01-16
UT DIIDW:2024D03293
ER

PT P
AU WANKHEDE M
   PARSONS L
   MERIYANI M
   CLIFF T
TI Prescriptive intelligent system for design            information of
   goods in e.g. construction industry for            mobile industrial
   worker, has processor that is            configured to provide overview
   of industrial asset or            industrial process, alarm, root cause
   analysis and            prescribed solution by user interface
PN WO2024254414-A2; US2024411294-A1; WO2024254414-A3; IN202517124328-A
AE AVEVA SOFTWARE LLC
AB 
   NOVELTY - The system (400) has a processor that is               
   configured to apply information standard to                ingestion
   pipeline (404) enabled to process data                from the data lake
   house (402) and use artificial                intelligence algorithms
   which identify actionable                insights in the data by
   orchestration engine (406),                where actionable insights
   comprise solutions                implemented in historical environments
   and feasible                for other environments. A processor is
   configured                to store the data as a knowledge graph across 
   different types of user interface technology                component
   (408). A processor is configured to                provide an overview
   of an industrial asset or an                industrial process, an
   alarm, a root cause                analysis, and a prescribed solution
   by a user                interface, where the user interface coordinates
   outputs from some of the different types of                technology
   components in response to an excursion                associated with
   the industrial asset or the                industrial process.
   USE - Prescriptive intelligent system for design               
   information of goods in mechanical, electronical,               
   pneumatic and/or hydraulic industries such as               
   construction, oil refinery, shipbuilding,                aerospace,
   automotive for mobile industrial                worker.
   ADVANTAGE - The system improves the in-the-moment decision              
   making process by providing a knowledge               
   graph-as-a-service which converts any operator into                an
   expert operator, thus resolving the challenge of                workers
   not being able to readily access and                understand asset
   information to resolve problems                with asset functionality.
   The artificial                intelligence and/or machine learning
   algorithms                allows the system to potentially find         
   information/insights more easily or find               
   information/insights which no individual human or               
   currently available operator have. The same                information
   standards and mapping are applied to                all data from all
   sources ensuring that there are                compatible, consistent,
   and coherent models across                of the sources of data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a method for prescriptive intelligent system               
   for mobile industrial workers; anda computer program product for
   prescriptive                intelligent system for mobile industrial    
              workers.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   prescriptive intelligent system.400Prescriptive intelligent
   system402Data lake house404Ingestion pipeline406Orchestration
   engine408User interface technology component
Z9 0
U1 0
U2 0
DA 2025-01-27
UT DIIDW:2024D03163
ER

PT P
AU HUANG X
TI Statistical analysis based medical channel leakage            quantity
   checking system, has visualization and            monitoring module for
   displaying decision support data            and abnormal alarm data and
   realizing real-time            monitoring and transparent management of
   each supply            chain
PN CN119067678-A; CN119067678-B
AE SHANGHAI PHARMEYES MEDICAL TECHNOLOGY CO
AB 
   NOVELTY - The system has a multi-modal sensor module for               
   collecting multi-dimensional data of medicine in                each
   supply chain link, including environment                state, position
   data and transportation condition,                and generating
   multi-modal sensor data. A data lake                module is
   communicated with the multi-modal sensor                module for
   receiving the multi-modal sensor data,                performing data
   standardization and semantic                hierarchy integration
   operations, and generating                integrated semantic data. An
   intelligent analysis                and verification module comprises a
   heterogeneous                data interaction unit for performing data  
   interaction analysis operation to the integrated                semantic
   data to generate interaction analysis                result data. A
   visualization and monitoring module                displays decision
   support data and abnormal alarm                data and realizes
   real-time monitoring and                transparent management of each
   supply chain.
   USE - Statistical analysis based medical channel                leakage
   quantity checking system.
   ADVANTAGE - The system can identify abnormity by               
   collecting and standardizing the medicine                circulation
   data in real-time and utilizing                intelligent analysis and
   causal reasoning, and                realizes multi-target optimization
   to make better                management strategy, dynamically adjusts
   the                management strategy through a feedback mechanism     
   and application of machine learning and improves                response
   ability and transparency of each supply                chain, ensures
   high-efficiency and precise                management of entire process
   from production of the                medicine to a terminal shop for
   providing reliable                guarantee for safety and efficiency of
   the medical                channel.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a           
   statistical analysis based medical channel leakage               
   quantity checking method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a      
   statistical analysis based medical channel leakage               
   quantity checking system. (Drawing includes                non-English
   language text).
Z9 0
U1 0
U2 0
DA 2025-01-11
UT DIIDW:2024C9266K
ER

PT P
AU LU P
   TIAN K
   LIU T
TI Digital service processing system based on            intelligent
   information mining used in enterprises, has            monitoring and
   operation and maintenance layer that is            used to monitor
   system performance and health status            and monitor automated
   management and deployment of            infrastructure
PN CN118838940-A
AE NANJING YUNWANG INTERACTIVE TECHNOLOGY
AB 
   NOVELTY - The system has a data processing and analysis               
   layer that is used for real-time data processing.                An
   information mining and machine learning layer is                used for
   feature extraction, model training and                model evaluation
   from processed data. An                intelligent application and
   decision support layer                is used for personalized
   recommendation, business                forecast trend analysis and data
   anomaly detection.                A system integration and deployment
   layer is used                for integrating microservice architecture, 
   application containerization and container                orchestration
   management. A security and privacy                protection layer is
   used for data encryption to                protect data transmission and
   storage security. A                monitoring and operation and
   maintenance layer is                used to monitor system performance
   and health                status and monitor the automated management
   and                deployment of infrastructure. A continuous           
   optimization and improvement layer is used to                collect
   user feedback and usage data for                optimization and
   improvement.
   USE - Digital service processing system based on               
   intelligent information mining used in                enterprises.
   ADVANTAGE - The system realizes service function, data               
   processing, the user interaction and the                high-efficiency
   cooperation of the system operation                and maintenance
   improve the flexibility and                response speed of the whole
   service process. The                high quality input of the data is
   ensured by the                multi-source integration, deep cleaning
   and                integration mechanism of the data collection and     
   pre-processing layer. The data storage and                management
   layer effectively solves the storage,                management and
   analysis challenges of the large                data and improves the
   data processing efficiency                through the comprehensive
   application of the                distributed database, the data
   warehouse and the                data lake.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a digital   
   service processing method based on intelligent               
   information mining.
   DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the    
   digital service processing system based on                intelligent
   information mining used in enterprises.                (Drawing includes
   non-English language text)
Z9 0
U1 0
U2 0
DA 2024-12-09
UT DIIDW:2024B5148X
ER

PT P
AU SUNDARARAJ G
   YARABOLU V K
TI System for performing predictive generation of            data puddles,
   has processing device used for capturing            data ingestion
   information associated with end-point            device over period of
   time, processing device used to            store data in data puddle
   associated with end-point            device
PN US2024242093-A1
AE BANK OF AMERICA CORP
AB 
   NOVELTY - The system has a processing device for               
   capturing data ingestion information associated                with an
   end-point device over period of time. The                processing
   device is used to determine data                ingestion pattern for
   the end-point device based on                the data ingestion
   information using a machine                learning (ML) subsystem. The
   processing device is                used to generate a query sequence
   for predictive                extraction of data from a data lake based
   on the                data ingestion pattern. The processing device is  
   used to trigger the predictive extraction of the                data
   from the data lake based on the query                sequence. The
   processing device is used to store                the data in a data
   puddle (110A) associated with                the end-point device in
   response to the predictive                extraction.
   USE - System for performing predictive generation of                data
   puddles.
   ADVANTAGE - The system reduces a latency associated with               
   extracting the data from the data lake in response                to the
   request for the data. The system allows a                cloud computing
   environment to offer                infrastructure, platforms and/or
   software as                services for which a cloud consumer does not
   need                to maintain resources on a local computing device.  
   The system trains the ML model using repeated                execution
   cycles of experimentation, testing, and                tuning to modify
   the performance of the ML                algorithm, and refines the
   results in preparation                for deployment of those results
   for consumption or                decision making.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a
   computer program product for storing a                set of
   instructions for performing a method for                performing
   predictive generation of data                puddles;(2) a method for
   performing predictive                generation of data puddles.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a  
   system for performing predictive generation of data               
   puddles.104Centralized cloud data center106Data lake108Entire
   system110AData puddle114AEnd-point device
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202472606R
ER

PT P
AU ZHONG X
   XU H
   XIAO H
   CHEN J
   ZHANG Y
TI Enterprise cooperative office system for use in            e.g. computer
   technology to optimize and simplify            internal and external
   communication, work flow and            resource management of
   enterprise, has decision            execution evaluation report
   specifically execution            effect evaluation, and performance
   index analysis
PN CN117787892-A
AE CHINA DATACOM CORP LTD
AB 
   NOVELTY - The system has a data lake analysis module               
   based on an enterprise data set, adopts a                distributed
   storage and processing frame of                Hadoopand Spark to
   perform a large data processing,                and integrates a machine
   learning technology of                TensorFlow and PyTorch to perform
   data analysis. A                comprehensive data analysis report is
   generated. A                realtime task management module based on an 
   organization optimization strategy, adopts a                prediction
   model of a time sequence analysis and                the machine
   learning to perform a project progress                and a team load
   analysis, and automatically updates                a task priority,
   generates a real-time task                updating report. A
   communication efficiency                analysis result specifically is
   information                transmission speed evaluation, team
   collaboration                degree analysis, feedback timeliness
   analysis. A                decision execution evaluation report
   specifically                is execution effect evaluation, performance
   index                analysis, improved suggestion report.
   USE - Enterprise cooperative office system for use                in
   computer technology, software engineering and                data
   management to optimize and simplify internal                and external
   communication, work flow and resource                management of
   enterprise.
   ADVANTAGE - The method by introducing the data lake               
   analysis module, in combination with the advanced               
   technology of Hadoopand Spark, the processing large                scale
   is obviously improved. The ability of the                complex data
   set fully uses the behavior analysis                and machine learning
   technology in the aspect of                task distribution, realizing
   the high degree of                individuation of the task
   distribution. The                introduction of the cooperation
   decision supporting                module enhances the ability of system
   in processing                the complex decision process. The flexible
   response                strategy aiming at the market change is
   provided,                at the same time, the adding of the
   organization                structure optimizing module, so as to
   improve the                whole organization efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an     
             enterprise cooperative office method.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating  
   the enterprise cooperative office method (Drawing               
   includes non-English language text).
Z9 0
U1 0
U2 0
DA 2024-04-19
UT DIIDW:202434504X
ER

PT P
AU SANDYA V
   MAHENDER A
   RAMJI B
   KANTHI M
   SRINIVAS K
TI Method for revolutionizing data lake database            management and
   unearthing unnecessary tables, involves            dynamically
   generating schema optimization plan for            restructuring data
   lake database by eliminating            identified unnecessary tables,
   and implementing schema            optimization plan or user-defined
   controls
PN IN202441001256-A
AE CMR TECH CAMPUS
AB 
   NOVELTY - The method involves analyzing data stored in a               
   data lake to identify unnecessary tables based on               
   predefined criteria (202), where the predefined                criteria
   include low utilization, redundancy,                outdated data, and
   adaptability to evolving                business requirements. A schema
   optimization plan                is dynamically generated (204) for
   restructuring a                data lake database by eliminating
   identified                unnecessary tables. The schema optimization
   plan or                user-defined controls is automatically
   implemented                (206), where restructuring is performed
   without                interrupting ongoing data lake operations and    
   providing real-time feedback on progress of schema               
   optimization.
   USE - Method for revolutionizing data lake database               
   management and unearthing unnecessary tables.
   ADVANTAGE - The method enables intelligently identifying               
   and eliminating unnecessary tables based on low               
   utilization, redundancy, and adaptability to                evolving
   business requirements to streamlined                resource utilization
   and a flexible data lake                structure. The method enables
   realizing automated                execution coupled with customizable
   controls to                ensure seamless restructuring without
   disrupting                ongoing operations, utilizing real-time
   feedback                mechanisms for aiding informed decision-making.
   The                method enables realizing security measures, and      
   integration of machine learning, thus enhancing               
   efficiency, adaptability, and performance in data                lake
   management.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system    
   for revolutionizing data lake database management                and
   unearthing unnecessary tables.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a
   method for revolutionizing data lake database                management
   and unearthing unnecessary                tables.202Analyzing data
   stored in data lake to                identify unnecessary tables based
   on predefined                criteria204Dynamically generating schema
   optimization                plan for restructuring data lake database by
   eliminating identified unnecessary tables206Automatically implementing
   schema                optimization plan or user-defined controls
Z9 0
U1 0
U2 0
DA 2025-03-20
UT DIIDW:202415429T
ER

PT C
AU Cajas-Cajas, Viviana
   Riofrio-Luzcando, Diego
   Carrion-Jumbo, Joe
   Martinez-Mosquera, Diana
   Morejon-Hidalgo, Patricio
BE Vizuete, MZ
   Botto-Tobar, M
   Casillas, S
   Gonzalez, C
   Sanchez, C
   Gomes, G
   Durakovic, B
TI Data Lake Optimization: An Educational Analysis Case
SO INNOVATION AND RESEARCH-SMART TECHNOLOGIES & SYSTEMS, VOL 1, CI3 2023
SE Lecture Notes in Networks and Systems
VL 1040
BP 299
EP 309
DI 10.1007/978-3-031-63434-5_22
DT Proceedings Paper
PD 2024
PY 2024
AB This study focuses on enhancing the performance of Universidad
   Internacional SEK's (UISEK) Data Lake by addressing challenges in
   computing resource consumption from a prior data lake implementation.
   Notably, data has been sourced from the Canvas Learning Management
   System based on the university's usage since 2019 for both
   implementations. The restructuring, carried out through three layers,
   successfully mitigated previous computing resource challenges.
   Following the CRISP-DM framework, the new approach exhibited substantial
   improvements over the previous version. Results include a 73.1%
   reduction in the estimated size of the last dump, 51.7% more efficient
   storage utilization in the user behavior table, and a 4.3% improvement
   in CPU consumption. Additionally, showcased a 50% reduction in ingestion
   time.
   The study emphasizes the significance of a well-organized Data Lake
   governance structure for streamlined data management. The presented
   improvements lay a solid foundation for future analyses and machine
   learning models. Moreover, the study underscores the role of automated
   processes in maintaining an updated Data Lake, ensuring its relevance
   for decision-making at UISEK. Overall, this work contributes to
   advancing the efficiency and performance of educational-Data Lakes,
   providing valuable insights for decision-making and continuous
   enhancement of the educational experience.
CT 4th International Research and Innovation Congress - Smart Technologies
   and Systems (CI3)
CY AUG 30-SEP 01, 2023
CL Inst Tecnologico Univ Ruminahui, Pichincha, ECUADOR
HO Inst Tecnologico Univ Ruminahui
SP Red Investigac, Innovac & Transferencia Tecnologia; Secretaria Educac
   Super, Ciencia, Tecnologia & Innovac; Lab Comunicac Visual Univ Estatal
   Campinas; Univ Ana G Mendez; Centro Investigaciones Psicopedagogicas &
   Sociologicas; Inst Super Diseno Univ Habana; GDEON; Corporac Ecuatoriana
   Desarrollo Investigac & Acad
RI Martinez-Mosquera, Diana/; Carrión Jumbo, Joe/AAZ-1459-2020; Riofrío-Luzcando, Diego/IXN-0844-2023
OI Martinez-Mosquera, Diana/0000-0002-0573-8640; Carrión Jumbo,
   Joe/0000-0003-3632-5352; Riofrío-Luzcando, Diego/0000-0002-9000-9847
ZB 0
TC 0
Z8 0
ZA 0
ZS 0
ZR 0
Z9 0
U1 0
U2 0
SN 2367-3370
EI 2367-3389
BN 978-3-031-63433-8; 978-3-031-63434-5
DA 2024-09-15
UT WOS:001286646400022
ER

PT C
AU Foschi, Antonio
   Pistilli, Domenico
   Bondani, Gaia
   Rebecchi, Fabio
GP IEEE
TI Developing a Prototype Healthcare Data Platform for Advanced Analytics
   in Rehabilitation Environments
SO 2024 IEEE 8TH FORUM ON RESEARCH AND TECHNOLOGIES FOR SOCIETY AND
   INDUSTRY INNOVATION, RTSI 2024
SE IEEE International Forum
BP 369
EP 374
DI 10.1109/RTSI61910.2024.10761573
DT Proceedings Paper
PD 2024
PY 2024
AB The rapid evolution of digital systems in the healthcare sector presents
   a significant opportunity to enhance rehabilitation outcomes through
   advanced analytics and personalized treatments. This advancement enables
   healthcare professionals to tailor rehabilitation programs to the
   individual needs of patients, leading to more effective and efficient
   recovery processes. Considering this, this paper introduces a prototype
   of data platform specifically designed to harness the potential of
   digital technology in the rehabilitation domain. The platform concept
   proposed herein integrates advanced data management tools and machine
   learning algorithms in order to leverage the vast amounts of data
   generated by healthcare devices. Specifically, the main purpose of this
   platform is to improve rehabilitation processes by offering tailored,
   data-driven insights that support healthcare professionals in making
   informed decisions. Utilizing open-source components, the prototype aims
   to demonstrate interoperability, scalability, and efficiency in
   processing and analyzing complex rehabilitation datasets. This paper
   concentrates on the architectural design, the plan for implementation,
   and the expected challenges and their solutions in the development of
   this platform. Additionally, it outlines the original contributions of
   the research, including advancements in data management techniques and
   interoperability standards. The prototype discussed here is developed as
   part of the "Fit4MedRob" initiative, a scientific project sponsored by
   the Italian Ministry of University and Research, that aims to overhaul
   rehabilitative models for individuals with compromised motor and
   cognitive functions through cutting-edge bio robotic technologies. This
   initiative spans a total of 44 months and seeks to address the unmet
   needs of patients and healthcare providers with innovative robotic and
   digital technologies.
CT 8th IEEE Forum on Research and Technologies for Society and Industry
   Innovation
CY SEP 18-20, 2024
CL Milano, ITALY
SP Institute of Electrical and Electronics Engineers Inc
OI Bondani, Gaia/0009-0000-6402-1546
Z8 0
ZS 0
ZA 0
ZR 0
ZB 0
TC 0
Z9 0
U1 0
U2 0
SN 2687-6809
BN 979-8-3503-6214-5; 979-8-3503-6213-8
DA 2025-09-28
UT WOS:001540356900064
ER

PT C
AU Harby, Ahmed A.
   ElKhodary, Eyad
   Almeida, Ronan
   Sharma, Drishti
   Zulkernine, Farhana
   Alaca, Furkan
   Elganar, Khalid
   Almarzouqi, Amina
   Al-Yateem, Nabeel
   Rahman, Syed Aziz
BE Shahriar, H
   Ohsaki, H
   Sharmin, M
   Towey, D
   Majumder, AKMJA
   Hori, Y
   Yang, JJ
   Takemoto, M
   Sakib, N
   Banno, R
   Ahamed, SI
TI Revolutionizing Healthcare Management: Architecture of a Web-based
   Medical Triage Service
SO 2024 IEEE 48TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE,
   COMPSAC 2024
SE IEEE Annual International Computer Software and Applications Conference
BP 1887
EP 1894
DI 10.1109/COMPSAC61105.2024.00299
DT Proceedings Paper
PD 2024
PY 2024
AB During the COVID-19 pandemic, the traditional emergency healthcare
   systems faced unprecedented strain due to the sharp rise in demands for
   urgent care, scarcity of resources, and increased risks of people
   getting infected while waiting at the emergency care facility. We
   present Triage-Bot, an online medical triage provisioning service, that
   can revolutionize emergency care by decreasing the load on emergency
   departments (ED), reducing healthcare expenses, and improving the
   quality of care. Empowered by artificial intelligence and natural
   language processing, the Triage-Bot service assesses and prioritizes
   patients' needs based on symptoms, medical history, and perceived
   conditions from multimodal video, audio, and text data captured during
   patients' interactions. The captured summarized information with a
   severity ranking is sent to a human expert to suggest the next action on
   the user's part. The diverse data types used by the Triage-Bot in
   communication, authentication, data collection, storage, and analytics
   requires a robust and scalable system architecture for online service
   provisioning. In this paper, we specifically focus on the system design
   and architecture of the Triage-Bot for emergency healthcare settings.
   With integrated electronic medical records (EMR) and online platforms,
   the bot fosters collaboration among healthcare professionals and enables
   swift and informed decision-making even in the face of crises. By
   partially automating and offering a hybrid triage process, the
   Triage-Bot improves resource allocation, reduces healthcare management
   costs for emergency care, minimizes patient waiting times, and improves
   wellbeing. To address the complexities and demands of healthcare data
   management, our proposed system incorporates MongoDB database for
   flexibility, scalability, and versatility in supporting different types
   of data. Additionally, we implement a data linking and analytics
   pipeline utilizing a data Lakehouse system to effectively ingest,
   manage, process, and generate knowledge from heterogeneous data sources.
CT 48th Annual IEEE International Computers, Software, and Applications
   Conference (COMPSAC) - Digital Development for a Better Future
CY JUL 02-04, 2023
CL Osaka Univ, Nakanoshima Ctr, Osaka, JAPAN
HO Osaka Univ, Nakanoshima Ctr
SP IEEE; IEEE Comp Soc
RI Al-Yateem, Nabeel/GQZ-2152-2022; Rahman, Syed/
OI Rahman, Syed/0000-0002-2583-6037
ZR 0
TC 0
ZS 0
Z8 0
ZB 0
ZA 0
Z9 0
U1 0
U2 1
SN 2836-3787
BN 979-8-3503-7697-5; 979-8-3503-7696-8
DA 2024-12-03
UT WOS:001308581200291
ER

PT B
AU Jorge, Francisco Guerreiro Galla Goucha
Z2  
TI A Metadata-Based Framework for ETL Processes and Monitoring
   Implementation in the Insurance Sector
DT Dissertation/Thesis
PD Jan 01 2024
PY 2024
ZS 0
TC 0
ZB 0
Z8 0
ZA 0
ZR 0
Z9 0
U1 0
U2 0
BN 9798346844938
UT PQDT:122547228
ER

PT B
AU Khatiwada, Aamod
Z2  
TI Table Discovery and Integration in Data Lakes
DT Dissertation/Thesis
PD Jan 01 2024
PY 2024
ZS 0
ZR 0
ZA 0
Z8 0
TC 0
ZB 0
Z9 0
U1 0
U2 0
BN 9798302173751
UT PQDT:120879034
ER

PT C
AU Sun, Susan
   Ye, Jeff
   Schwarthoff, Hubert
   Rosin, Jon
   Vakkalagadda, Varalakshmi
   Chang, Jimmy
   Ubbara, Sesidhar Reddy
   Chinthakindi, Anil
GP IEEE
TI Cloud Big Data Lake for Advanced Analytics in Semiconductor
   Manufacturing
SO 2024 35TH ANNUAL SEMI ADVANCED SEMICONDUCTOR MANUFACTURING CONFERENCE,
   ASMC
SE Advanced Semiconductor Manufacturing Conference and Workshop-Proceedings
DI 10.1109/ASMC61125.2024.10545365
DT Proceedings Paper
PD 2024
PY 2024
AB Data driven business intelligence is changing how semiconductor
   manufacturing thrives in the long term. A cloud big data lake is
   designed and implemented based on state-of-the-art cloud architecture
   providing complete services for data ingestion, storage, processing,
   advanced analytics, and machine learning with a high level of security.
   Efficient and effective use of this big data lake and data science
   enables problem solving and decision making to improve productivity and
   performance.
CT 35th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)
CY MAY 13-16, 2024
CL Albany, NY
SP IEEE
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
TC 0
Z9 0
U1 1
U2 4
SN 1078-8743
BN 979-8-3503-8456-7; 979-8-3503-8455-0
DA 2024-07-27
UT WOS:001245033700005
ER

PT P
AU HOLTCAMP V
   MAHAPATRA S
   SURANGE S
   ANDERSON J
   KOH E
   GUO S
   HOFFSWELL J E
   ROSSI R A
   SINHA A R
   DU F
   NARECHANIA A A
TI Computer storage medium for management,            assessment,
   navigation, and discovery of data based on            data quality,
   consumption, and utility metrics, has set            of instructions for
   causing user interface to present            interactive tree view with
   hierarchy of nodes that            represent nested attribute schema
PN US2023289696-A1; US12340333-B2
AE ADOBE INC
AB 
   NOVELTY - The medium (S900) has a set of instructions                for
   receiving (S910) input identifying a dataset                for
   ingestion into a data lake. Attribute quality                metrics are
   generated for each of attributes of the                dataset based on
   sample data from the dataset                (S920). A user interface is
   caused to present an                interactive tree view with a
   hierarchy of nodes                that represent a nested attribute
   schema of the                data set, where each of the nodes visually 
   represents quality of a corresponding attribute                based on
   the attribute quality metrics for the                corresponding
   attribute (S930). The sample data is                ingested from the
   datastore into a landing zone                separate from the lake.
   USE - Computer storage medium for management,                assessment,
   navigation, and discovery of data based                on data quality,
   consumption, and utility metrics                for data fuel modern
   business performance through                reporting and analysis,
   resulting in decision                making.
   ADVANTAGE - The medium enables providing tools to novice               
   and experienced users to quickly and efficiently                navigate
   and discover knowledge from large                datasets. The medium
   enables utilizing a visual                data analysis tool and a data
   selection tool to                present representations of a dataset
   and metrics                and enables users to select an appropriate
   highly                effective subset for use in downstream
   applications                such as preparing a dataset for training a
   machine                learning model, running a digital marketing      
            campaign, or creating an analytics dashboard.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a       
   method for management, assessment, navigation, and               
   discovery of data based on data quality,                consumption, and
   utility metrics for data fuel                modern business performance
   through reporting and                analysis, resulting in decision
   making; (2) a                system for management, assessment,
   navigation, and                discovery of data based on data quality, 
   consumption, and utility metrics for data fuel                modern
   business performance through reporting and                analysis,
   resulting in decision making.
   DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram
   illustrating                a computer storage medium for management,   
   assessment, navigation, and discovery of data based                on
   data quality, consumption, and utility metrics                for data
   fuel modern business performance through                reporting and
   analysis, resulting in decision                making.S900Realizing
   management, assessment,                navigation, and discovery of data
   based on data                quality, consumption, and utility metrics
   for data                fuel modern business performance through
   reporting                and analysis, resulting in decision
   makingS910Receiving input identifying dataset for               
   ingestion into data lakeS920Generating attribute quality metrics for    
   each of attributes of dataset based on sample data                from
   datasetS930Causing user interface to present                interactive
   tree view with hierarchy of nodes that                represent nested
   attribute schema of the data set,                where each of nodes
   visually represents quality of                corresponding attribute
   based on attribute quality                metrics for corresponding
   attribute
Z9 0
U1 0
U2 0
DA 2023-09-29
UT DIIDW:202396958X
ER

PT P
AU BOWLBY E
   SLAVIN I
TI System for implementing home lending data            reservoir
   architecture that enables near real-time            machine learning
   operation, has multi-topic low latency            message broker for
   receiving normalized data record            from system of record
PN US2023222416-A1; US12147923-B2
AE MORGAN CHASE BANK J P
AB 
   NOVELTY - The system has a multi-topic low latency               
   message broker for receiving a normalized data                record
   (130) from a system of record (SOR). A                common semantic
   extract transform load (ETL) module                transforms data from
   the normalized data record for                using and reporting the
   transformed data in                real-time. A first queue subscriber
   communicates                with a business system for a machine
   learning and                artificial intelligence model and a decision
   support systems. A second queue subscriber                communicates
   with an operational store (150), where                the operational
   store provides data within a                predetermined period of
   time. A third queue                subscriber communicates with a data
   lake (152)                through a data pipeline (148).
   USE - System for implementing a home lending data               
   reservoir architecture that enables near real-time               
   machine learning operation and/or artificial                intelligence
   and data analytic operation.
   ADVANTAGE - The system realizes simple complex               
   dependencies between data transformation and end                user
   consumption that are driving data costs. The                system
   increases revenue, controls cost, and                protected from
   encroachment by competition. The                system increases risk
   management capabilities and                regulatory and external
   reporting operation. The                system increases consumer
   originations flow through                lead optimization and better
   targeting of customer                with bank deposit. The system
   increases time to                market for product ideas and decreases
   staffing                churn across business cycles.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method    
   for implementing a home lending data reservoir               
   architecture that enables near real-time machine                learning
   operation and/or artificial intelligence                and data
   analytic operation.
   DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram 
   of a system for implementing a home lending data               
   reservoir architecture that enables near real-time               
   machine learning operation and/or artificial                intelligence
   and data analytic operation.130Normalized data record146Enterprise data
   model mapper148Data pipeline150Operational store152Data lake
Z9 0
U1 0
U2 0
DA 2023-07-23
UT DIIDW:2023737282
ER

PT J
AU Potts, Laramie
TI I-Corps: Artificial Intelligence Generated Labeled Maps from Reality
   Capture Data
DT Awarded Grant
PD Jun 01 2023
PY 2023
AB The broader impact/commercial potential of this I-Corps project is to
   streamline the Architecture, Engineering and Construction community
   workflows so that considerable cost savings are passed on to their
   customers. This I-Corps project will enhance the scientific
   understanding of automatic feature extraction from a variety of multiple
   sensor data and imagery over a wide range of field conditions. This
   project will require innovative algorithms for processing and
   compression of huge datasets. This project has potential impact on
   combating climate change through automatic processing of massive amounts
   of data collected from earth orbiting satellites, drones, or ships.
   Elements from this project can be used to enhance workforce development
   programs to include occupational training on geospatial information
   systems. This I-Corps project���s visualization modules can support
   spatial thinking development in pre-college programs through hands-on
   manipulation of digital models of the built environment. Spatial
   thinking is highly correlated with competency in mathematics. The
   commercial impact of this project will transform the geospatial mapping
   industry by eliminating manual intervention and allowing machines to
   perform repetitive mapping tasks with greater accuracy and speed.
   Accordingly, this project will enable more accurate and efficient data
   collection, analysis, and decision-making processes, relieve human
   stress and strain, and enhance productivity.<br/><br/>This I-Corps
   project is based on the development of an automated data processing
   system that will increase productivity in mapping workflows using
   reality capture data. Architecture, Engineering and Construction
   companies are heavily invested in reality capture technologies to map
   the infrastructure of the built environment from point clouds and
   imagery. Such investments are meant to reduce operational costs
   associated with field data collection and subsequent production of
   design plans for construction and the development of information systems
   for civil asset management projects. However, outdated workflows that
   incorporate reality capture data often become entangled with
   unproductive processes that slow down mapping operations, increase
   human-induced errors, and yield a negative revenue impact. A viable
   solution is an innovative software system that uses artificial
   intelligence algorithms to extract meaningful information from point
   clouds and imagery to produce industry-standard documents and annotated
   plans that satisfy pre-construction approvals/permitting requirements.
   This I-Corps project will be capable of producing digital information
   for intelligent infrastructure asset management, condition assessment,
   risk management, and overall project management. This project reduces
   human error and decreases the mapping production time with significant
   cost saving to customers.<br/><br/>This award reflects NSF's statutory
   mission and has been deemed worthy of support through evaluation using
   the Foundation's intellectual merit and broader impacts review criteria.
ZR 0
Z8 0
ZS 0
ZA 0
TC 0
ZB 0
Z9 0
U1 0
U2 0
G1 2331160
DA 2024-04-26
UT GRANTS:17657981
ER

PT B
AU Cunha, Diogo
Z2  
TI Explorar Data Lakehouse Como Infraestrutura Para Assistência à Autonomia
   no DomicílioExploring Data Lakehouse as Data Infrastructure for Ambient
   Assisted Living
DT Dissertation/Thesis
PD Jan 01 2023
PY 2023
TC 0
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
Z9 0
U1 1
U2 1
BN 9798384417538
UT PQDT:100664997
ER

PT J
AU Li, Haoyuan
Z2  
TI Empowering Data Mesh with Federated Learning
DT Dissertation/Thesis
PD Jan 01 2023
PY 2023
ZA 0
ZR 0
ZS 0
ZB 0
Z8 0
TC 0
Z9 0
U1 0
U2 0
UT PQDT:84966422
ER

PT B
AU Yang, Tianzi
Z2  
TI Assisting Vehicles Automated Locomotion via Ml-Aided Edge-Twin Paradigm
DT Dissertation/Thesis
PD Jan 01 2023
PY 2023
ZB 0
Z8 0
ZS 0
ZR 0
TC 0
ZA 0
Z9 0
U1 1
U2 1
BN 9798342112208
UT PQDT:118976198
ER

PT J
AU FERNANDEZ, SOLEDAD A; Timothy R Huerta
TI Opioid and SUD Data Enclave (O-SUDDEn): Bringing real-time data to the
   opioid crisis
DT Awarded Grant
PD Sep 30 2022
PY 2022
AB ABSTRACTThe widespread availability of synthetic opioids (fentanyl) has
   fueled the rapidly rising rates of unintentional overdose (OD)
   fatalities. Policy makers, state and local agencies, and investigators
   have focused on the Ohioexperience as a bellwether for the experiences
   of other states because of the representativeness of Ohio’sdemographics.
   The lack of timely geospatially-linked longitudinal data sources has
   impeded the ability ofcommunities and state agencies to pivot allocation
   of resources to regions where they are needed most. Limitedintegration
   of environmental risk factors such as sociodemographic characteristics
   fail to support identification ofnew targets for intervention or new
   approaches to emerging threats from changes in local drug supply.
   Webelieve that agile data systems and informatics tools that can be used
   to demonstrate the utility of predictiveanalytics and machine
   intelligence approaches on how to enable data-driven decision making
   will ultimatelyprove translatable across the country and diverse
   localities. Our proposed Opioid and Substance Use DisorderData Enclave
   (O-SUDDEn) will provide a novel and transformative approach to support
   rigorous andreproducible research on the opioid and substance use
   crises. O-SUDDEn will fill several existing gaps in datainfrastructure
   and prediction models that include machine learning, geospatial
   analyses, and community context.The specific aims for O-SUDDEn are as
   follows: Aim 1: Data linkage with establishment of O-SUDDEn. Wewill
   develop a geospatially-sensitive, individual-level secure data lake that
   integrates multiple disparate datasources that meets the requirement of
   a coded-limited set under HIPAA. New data sources include
   real-timeindividual-level longitudinal data from urine drug testing
   (UDT) and community contextual data based on theOhio Opportunity Index,
   an area-level social determinants of health developed and used by our
   team. We willdevelop query and use tools for data harmonization and
   integration, prepare and release data sets fordissemination and
   secondary data analyses; and facilitate secondary use of related
   administrative data togenerate evidence that informs targeted opioid
   interventions. Aim 2: Develop Predictive Models andSurveillance
   algorithms. Geospatial and machine learning will be used to model the
   contribution of opioid,cocaine, and stimulant use on OD, OD death and
   opioid use disorder/substance use disorder (OUD/SUD);temporal
   relationship of real time data including UDT and demographic and
   contextual variables to identify high-risk populations and
   subpopulations or geographic locales. We will validate model
   performances and predictivepower and then disseminate surveillance and
   forecasting algorithms through the O-SUDDEn portal for end-useraccess.
   Aim 3: Deploy a human-centered platform and actional informatics.
   Applying co-design principleswith a human-centered work group consisting
   of end-user stakeholders (community, researchers, and statepolicy
   makers/agencies), we will develop and deploy an end-user tailored portal
   and effective user-friendly toolsto provide actionable insights to
   inform opioid treatment and targeted harm reduction strategies.
ZA 0
ZR 0
ZS 0
TC 0
ZB 0
Z8 0
Z9 0
U1 0
U2 0
G1 10590246; 1R01DA057668-01; R01DA057668
DA 2023-12-14
UT GRANTS:16277911
ER

PT J
AU SLAVOVA, SVETLA STEFANOVA; Jeffery C Talbert
TI Diversity Supplement - Rapid Actionable Data for Opioid Response in
   Kentucky (RADOR-KY)
DT Awarded Grant
PD Sep 30 2022
PY 2023
AB AbstractOpioid use disorder (OUD) is a major public health concern, with
   opioids being involved inapproximately 75% of the 107,622 overdose
   deaths reported by the Centers for Disease Control(CDC) in 2021.
   Overdose rates have risen in recent years and substantial resources
   areinvested in combatting OUD at the national and local levels.
   Coordination of efforts to combatOUD may be the key to reducing overdose
   deaths. To this end, the RADOR-KY project will builda robust state-wide
   surveillance system, integrating diverse data sources to monitor and
   predictdrug overdose mortality and morbidity. This system will be used
   by stakeholders to inform data-driven action, supporting the
   coordination and targeting of prevention and treatment efforts.
   Asproposed in the parent grant for this supplement, the RADOR-KY system
   will integrate severaldata sources related to OUD prevention and
   treatment efforts. However, there are gaps in thedata that is available
   to RADOR-KY. Kentucky does not have an all-payer claims database andso
   data about residential OUD treatment and treatment support systems can
   only be obtainedfrom Medicaid beneficiaries. In this proposal we will
   supplement the RADOR-KY efforts, usingan informatics approach to capture
   data about treatment and recovery support services fundedby the Kentucky
   Opioid Response Effort (KORE), a state-administered and federally
   fundedprogram aimed at reducing overdose death and expanding access to
   treatment and supportservices. KORE funds treatment support services
   (e.g., peer support, recovery communitycenters) and, for uninsured and
   underinsured people, medication and residential treatment.KORE data is
   currently collected for reporting to the Substance Abuse and Mental
   HealthAdministration, but the existing approach is unstandardized,
   leading to inefficiencies, delays,and quality issues that make KORE data
   unsuitable for use in RADOR-KY. The specific aims ofthis project are to
   build a new platform for KORE agencies to submit standardized data
   –improving data ingestion through automated validation – and then to
   create near-real timevisualizations and analyses that will allow KORE
   agencies and other stakeholders to refine andtarget their efforts to
   combat OUD. This data will be integrated into RADOR-KY, supporting
   theaims of the parent grant by improving the surveillance database and
   predictive models. We willshare the code and methods developed for this
   project publicly, allowing other agencies withinand beyond Kentucky to
   improve their analytic and planning capabilities. This will be
   especiallyhelpful for administering State Opioid Response efforts in
   other states.
ZS 0
Z8 0
ZR 0
ZA 0
ZB 0
TC 0
Z9 0
U1 0
U2 1
G1 10789054; 3R01DA057605-01S1; R01DA057605
DA 2024-07-25
UT GRANTS:17760458
ER

PT J
AU KABUKA, MANSUR R.
TI Deep-CDS: Deep Learning Semantic Data Lake for Clinical Decision Support
DT Awarded Grant
PD Sep 10 2022
PY 2023
AB More than 5 million patients are admitted annually to United States ICUs
   with average mortality rate reportedranging from 8-19%, or about 500,000
   deaths annually. Sepsis is the leading cause of in-hospital
   mortality,where one in three inpatient deaths are due to sepsis.
   Incidence of sepsis has been increasing with 1.7 millionsepsis cases and
   270,000 deaths per year. Early identification of deterioration has been
   shown to reduce theneed for patient transfer to higher care units,
   reduce lengths of stay, and improve survival rates. Each hour ofdelay in
   ICU admission has been associated with a 1.5% increased risk of ICU
   death and a 1% increase in riskof hospital death. Many studies support
   that there is an increase in mortality rate for every hour delay
   inantibiotics. Pairing patient risk stratification with appropriate
   levels of hospital intervention is essential to reducerisk of mortality.
   Patients in intermediate units between the levels of monitoring found in
   floor units and ICUs areespecially difficult to predict possibility of
   condition deterioration. Automated monitoring, alerts, and trendanalysis
   are essential to identifying and proactively intervening patients under
   duress. Current methods ofmonitoring patient health have low specificity
   and have significant room for improvement. This project will develop
   Deep-CDS, a cloud-based deep learning system for context-sensitive
   clinicaldecision support in monitoring and predicting the deterioration
   of patient health and progression of sepsis riskfactors in real-time to
   improve outcomes and optimize the management of care across the hospital
   population.To support the clinical care team, Deep-CDS provides team
   members with (a) a clinical care knowledgebase,(b) an early warning
   score for deteriorating health conditions, (c) a model for predicting
   septic conditions, (d)evidence-based clinical practice guidelines, and
   (e) visualization of patient health status trends. Deep-CDSaddresses
   NIGMS Priorities for Small Business Development of Sepsis Diagnostics
   and Therapeutics, NOT-GM-20-028: 1) Diagnostic tools for emergency
   department settings; 2) Predictive clinical algorithms and
   point-of-carediagnostics; 3) Technologies that combine various types of
   data for diagnosis of sepsis patients; and 4) Clinicaldecision support,
   including use of artificial intelligence and machine learning
   approaches, to develop tools for earlyrecognition of sepsis, assessment
   of treatment responses and patient deterioration, and long-term
   prognosisprediction in various care settings.
ZB 0
Z8 0
TC 0
ZS 0
ZA 0
ZR 0
Z9 0
U1 0
U2 0
G1 10747223; 4R44GM143996-02; R44GM143996
DA 2024-07-25
UT GRANTS:17758993
ER

PT J
AU KABUKA, MANSUR R.
TI Deep-CDS: Deep Learning Semantic Data Lake for Clinical Decision Support
DT Awarded Grant
PD Sep 10 2022
PY 2022
AB More than 5 million patients are admitted annually to United States ICUs
   with average mortality rate reportedranging from 8-19%, or about 500,000
   deaths annually. Sepsis is the leading cause of in-hospital
   mortality,where one in three inpatient deaths are due to sepsis.
   Incidence of sepsis has been increasing with 1.7 millionsepsis cases and
   270,000 deaths per year. Early identification of deterioration has been
   shown to reduce theneed for patient transfer to higher care units,
   reduce lengths of stay, and improve survival rates. Each hour ofdelay in
   ICU admission has been associated with a 1.5% increased risk of ICU
   death and a 1% increase in riskof hospital death. Many studies support
   that there is an increase in mortality rate for every hour delay
   inantibiotics. Pairing patient risk stratification with appropriate
   levels of hospital intervention is essential to reducerisk of mortality.
   Patients in intermediate units between the levels of monitoring found in
   floor units and ICUs areespecially difficult to predict possibility of
   condition deterioration. Automated monitoring, alerts, and trendanalysis
   are essential to identifying and proactively intervening patients under
   duress. Current methods ofmonitoring patient health have low specificity
   and have significant room for improvement. This project will develop
   Deep-CDS, a cloud-based deep learning system for context-sensitive
   clinicaldecision support in monitoring and predicting the deterioration
   of patient health and progression of sepsis riskfactors in real-time to
   improve outcomes and optimize the management of care across the hospital
   population.To support the clinical care team, Deep-CDS provides team
   members with (a) a clinical care knowledgebase,(b) an early warning
   score for deteriorating health conditions, (c) a model for predicting
   septic conditions, (d)evidence-based clinical practice guidelines, and
   (e) visualization of patient health status trends. Deep-CDSaddresses
   NIGMS Priorities for Small Business Development of Sepsis Diagnostics
   and Therapeutics, NOT-GM-20-028: 1) Diagnostic tools for emergency
   department settings; 2) Predictive clinical algorithms and
   point-of-carediagnostics; 3) Technologies that combine various types of
   data for diagnosis of sepsis patients; and 4) Clinicaldecision support,
   including use of artificial intelligence and machine learning
   approaches, to develop tools for earlyrecognition of sepsis, assessment
   of treatment responses and patient deterioration, and long-term
   prognosisprediction in various care settings.
ZS 0
ZA 0
Z8 0
TC 0
ZR 0
ZB 0
Z9 0
U1 0
U2 0
G1 10546333; 1R44GM143996-01A1; R44GM143996
DA 2023-12-14
UT GRANTS:16274094
ER

PT C
AU Cakir, Altan
   Ozkaya, Emre
   Akkus, Fatih
   Kucukbas, Ezgi
   Yilmaz, Okan
BE Kahraman, C
   Tolga, AC
   Onar, SC
   Cebi, S
   Oztaysi, B
   Sari, IU
TI Real Time Big Data Analytics for Tool Wear Protection with Deep Learning
   in Manufacturing Industry
SO INTELLIGENT AND FUZZY SYSTEMS: DIGITAL ACCELERATION AND THE NEW NORMAL,
   INFUS 2022, VOL 2
SE Lecture Notes in Networks and Systems
VL 505
BP 148
EP 155
DI 10.1007/978-3-031-09176-6_18
DT Proceedings Paper
PD 2022
PY 2022
AB Industry 4.0 is a motivation that represents the transformation by
   data-driven industrial operations and decision making by digitization of
   manufacturing processes to gain operational advantages in the market.
   Considering how the manufacturing sector is adopting data-driven
   operations is challenging, given that there is not a straightforward
   definition of machine traceability, receiving and storing raw data from
   manufacturing lines, gives an opportunity to analyse the processes in
   real time nature. Thanks to big data management platforms and artificial
   intelligence decision support algorithms, it gives the ability to deeply
   understand the complexity of the processes and, accordingly, to
   eliminate or minimise false methods and reduce the costs that are
   insufficient for production. In addition, one of the biggest preventable
   costs for metal machining processes is the tool breakage and tool
   wearing problems. The motivation of this paper is to discuss data-driven
   decision making possibilities of the tool wearing and optimise breakage
   costs with using artificial intelligence. Furthermore, the analysis
   provides a proof-of-concept that the existence of a digital
   infrastructure combined with the analytical capabilities, such as
   real-time data management and monitoring, and having a highly accurate
   LSTM based time-series integrated artificial intelligent predictive
   model, to deal with inefficiencies in production processes. To this end,
   in this context, by developing the latest advancements in big data
   analytics, we propose a scalable predictive and preventive maintenance
   architecture for metal machining processes domain. We also show the
   opportunities and challenges of utilizing the big data architecture in
   the manufacturing domain.
CT 4th International Conference on Intelligent and Fuzzy Systems (INFUS)
CY JUL 19-21, 2022
CL Bornova, TURKEY
RI Akkus, Mehmet Fatih/; Cakir, Altan/ABD-4450-2020
OI Akkus, Mehmet Fatih/0000-0002-3628-5656; 
TC 0
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
Z9 0
U1 1
U2 7
SN 2367-3370
EI 2367-3389
BN 978-3-031-09176-6; 978-3-031-09175-9
DA 2022-12-10
UT WOS:000889132600018
ER

PT R
AU Damiani, A
   Masciocchi, C
   Lenkowicz, J
   Capocchiano, ND
   Boldrini, L
   Tagliaferri, L
   Cesario, A
   Sergi, P
   Marchetti, A
   Luraschi, A
   Patarnello, S
   Valentini, V
TI Table1_Building an Artificial Intelligence Laboratory Based on Real
   World Data: The Experience of Gemelli Generator.DOCX
SO Figshare
DI https://doi.org/10.3389/fcomp.2021.768266.s001
DT Data set
PD 2021-12-30
PY 2021
AB The problem of transforming Real World Data into Real World Evidence is
   becoming increasingly important in the frameworks of Digital Health and
   Personalized Medicine, especially with the availability of modern
   algorithms of Artificial Intelligence high computing power, and large
   storage facilities.Even where Real World Data are well maintained in a
   hospital data warehouse and are made available for research purposes,
   many aspects need to be addressed to build an effective architecture
   enabling researchers to extract knowledge from data.We describe the
   first year of activity at Gemelli Generator RWD, the challenges we faced
   and the solutions we put in place to build a Real World Data laboratory
   at the service of patients and health researchers. Three classes of
   services are available today: retrospective analysis of existing patient
   data for descriptive and clustering purposes; automation of knowledge
   extraction, ranging from text mining, patient selection for trials, to
   generation of new research hypotheses; and finally the creation of
   Decision Support Systems, with the integration of data from the hospital
   data warehouse, apps, and Internet of Things. Copyright: CC BY 4.0
ZB 0
ZS 0
TC 0
Z8 0
ZR 0
ZA 0
Z9 0
U1 0
U2 0
DA 2024-11-28
UT DRCI:DATA2022008023271401
ER

PT B
AU Jephte, Ioudom Foubi
   Pinheiro, Flávio Luis Portas Portas
Z2  
TI [not available]
DT Dissertation/Thesis
PD Jul 10 2023
PY 2023
ZA 0
Z8 0
TC 0
ZB 0
ZS 0
ZR 0
Z9 0
U1 0
U2 0
BN 9798759929598
UT PQDT:50758602
ER

PT B
AU Machmouchi, Hassan
   Desper, Deane
   Sambasivam, Samuel
   Calongne, Cynthia
Z2  
TI [not available]
DT Dissertation/Thesis
PD Jun 22 2023
PY 2023
ZA 0
TC 0
ZB 0
Z8 0
ZR 0
ZS 0
Z9 0
U1 1
U2 3
BN 9798534683547
UT PQDT:64705582
ER

PT J
AU Fillottrani, Pablo R.
   Jamieson, Stephan
   Keet, C. Maria
TI Connecting Knowledge to Data Through Transformations in KnowID: System
   Description
SO KUNSTLICHE INTELLIGENZ
VL 34
IS 3
SI SI
BP 373
EP 379
DI 10.1007/s13218-020-00675-6
EA JUN 2020
DT Article
PD SEP 2020
PY 2020
AB Intelligent information systems deploy applied ontologies or logic-based
   conceptual data models for effective and efficient data management and
   to assist with decision-making. A core deliberation in the design of
   such systems, is how to link the knowledge to the data. We recently
   designed a novel knowledge-to-data architecture (KnowID) which aims to
   solve this critical step through a set of transformation rules rather
   than a mapping layer, which operate between models represented in EER
   notation and an enhanced relational model called the ARM. This system
   description zooms in on the novel tool for the core component of the
   transformation from the Artificial Intelligence-oriented modelling to
   the relational database-oriented data management. It provides an
   overview of the requirements, design, and implementation of the modular
   transformations module that straightforwardly permits extension with
   other components of the modular KnowID architecture.
RI Fillottrani, Pablo/A-7817-2008
OI Fillottrani, Pablo/0000-0003-0906-867X
ZA 0
ZB 0
ZR 0
Z8 0
ZS 0
TC 0
Z9 0
U1 0
U2 3
SN 0933-1875
EI 1610-1987
DA 2020-07-10
UT WOS:000543697600001
ER

PT P
AU SZCZEPANIK G P
   RUDEK K A
   HANUSIAK T
   KOMNATA K W
TI Method for managing cognitive database of data            lake, involves
   creating entry in knowledge base that            comprise description of
   data lake in which operational            database is associated with
   data lake and categorized            streaming file
PN US2020174966-A1; US11119980-B2
AE INT BUSINESS MACHINES CORP
AB 
   NOVELTY - The method involves categorizing a streaming               
   file that is ingested by a data lake based on a                metadata
   of the streaming file. An operational                database (123) is
   associated with the data lake                based on the categorized
   streaming file. An entry                is created in a knowledge base
   that comprise a                description of the data lake in which the
   operational database associated with the data lake                and
   the categorized streaming file.
   USE - Method for managing cognitive database of data               
   lake.
   ADVANTAGE - The method streamlines and improves the               
   efficiency of accessing and transforming data from                the
   native file formats ingested by the data lake                to
   structured datasets maintained by the                operational
   database of the data lake system. The                building up of the
   knowledge base is quicker and is                more robust and
   accurate. The accuracy of the                automated decision making
   of the machine learning                module is improved to provide the
   correct                inputs.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the          
   following:a system for managing cognitive database of               
   data lake; anda computer program product for managing               
   cognitive database of data lake.
   DESCRIPTION Of DRAWING(S) - The drawing shows a functional block diagram
   of a computing environment.101Data lake system103Processor116External
   device118Display123Operational database
Z9 0
U1 0
U2 0
DA 2020-06-11
UT DIIDW:2020489503
ER

EF