{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fb4f8d",
   "metadata": {},
   "source": [
    "# Consolidação de Base de Dados - Web of Science (Plain Text)\n",
    "Diferente da IEEE e Scopus, a Web of Science exporta os dados em formato de texto estruturado por tags. \n",
    "Cada publicação inicia na tag de tipo de documento (`PT J`, por exemplo) e termina com a tag de fim de registro (`ER`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd50480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 9 arquivos .txt da WoS.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pasta_origem_wos = \"/home/silvio/Projetos/TEMAC/WOS\"\n",
    "\n",
    "if not os.path.exists(pasta_origem_wos):\n",
    "    print(f\"Atenção: O diretório {pasta_origem_wos} não foi encontrado.\")\n",
    "else:\n",
    "    arquivos_txt_wos = [f for f in os.listdir(pasta_origem_wos) if f.endswith('.txt')]\n",
    "    print(f\"Encontrados {len(arquivos_txt_wos)} arquivos .txt da WoS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237eccd6",
   "metadata": {},
   "source": [
    "### 1. Função Analisadora (Parser) da WoS\n",
    "O código abaixo lê as linhas do arquivo e agrupa cada registro em um bloco.\n",
    "Ao mesmo tempo, ele mapeia as tags principais (como **TI** para Título, **DI** para DOI e **UT** para o ID único da Clarivate) para podermos usar como chave na hora de remover as duplicatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e020f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Função de parser carregada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "def parse_wos_file(caminho_arquivo):\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8-sig') as file:\n",
    "        linhas = file.readlines()\n",
    "\n",
    "    header_global = []\n",
    "    registros = []\n",
    "    \n",
    "    bloco_raw_atual = []\n",
    "    metadados_atuais = {}\n",
    "    tag_atual = \"\"\n",
    "    lendo_header = True\n",
    "\n",
    "    for linha in linhas:\n",
    "        # Remove a quebra de linha do final, mas mantém os espaços do início\n",
    "        linha_limpa = linha.rstrip('\\n') \n",
    "        \n",
    "        # Ignora linhas 100% vazias entre os registros\n",
    "        if not linha_limpa.strip():\n",
    "            continue\n",
    "\n",
    "        # Captura o cabeçalho inicial (FN e VR)\n",
    "        if linha_limpa.startswith('FN ') or linha_limpa.startswith('VR '):\n",
    "            if lendo_header:\n",
    "                header_global.append(linha_limpa)\n",
    "            continue\n",
    "\n",
    "        # Quando encontra \"PT \" (Publication Type), começa um novo registro\n",
    "        if linha_limpa.startswith('PT '):\n",
    "            lendo_header = False\n",
    "\n",
    "        if lendo_header:\n",
    "            continue\n",
    "            \n",
    "        # Adiciona a linha ao bloco de texto bruto que será salvo no final\n",
    "        bloco_raw_atual.append(linha_limpa)\n",
    "\n",
    "        # Fim do registro\n",
    "        if linha_limpa == 'ER':\n",
    "            registros.append({\n",
    "                'bloco_texto': '\\n'.join(bloco_raw_atual),\n",
    "                # Salvamos as chaves principais em minúsculo e sem quebras para facilitar a comparação\n",
    "                'DI': metadados_atuais.get('DI', '').strip().lower(), # DOI\n",
    "                'TI': metadados_atuais.get('TI', '').strip().lower(), # Título\n",
    "                'UT': metadados_atuais.get('UT', '').strip().lower()  # ID Único WoS\n",
    "            })\n",
    "            bloco_raw_atual = []\n",
    "            metadados_atuais = {}\n",
    "            tag_atual = \"\"\n",
    "            continue\n",
    "\n",
    "        # Lógica para extrair as Tags e os Valores\n",
    "        # Se a linha não começa com espaço e tem uma tag de 2 letras (Ex: \"TI \")\n",
    "        if not linha_limpa.startswith(' ') and len(linha_limpa) >= 3 and linha_limpa[2] == ' ':\n",
    "            tag_atual = linha_limpa[0:2]\n",
    "            valor = linha_limpa[3:]\n",
    "            metadados_atuais[tag_atual] = valor\n",
    "        elif tag_atual:\n",
    "            # É uma linha de continuação (Ex: segunda linha de um abstract longo)\n",
    "            metadados_atuais[tag_atual] += \" \" + linha_limpa.strip()\n",
    "\n",
    "    return header_global, registros\n",
    "\n",
    "print(\"Função de parser carregada com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c4b75",
   "metadata": {},
   "source": [
    "### 2. Etapa de Debug: Analisando as Tags Extraídas\n",
    "Vamos ler todos os arquivos, processar usando nossa função e imprimir um exemplo do primeiro registro extraído para validar se as tags (TI, DI, UT) foram mapeadas corretamente antes da consolidação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71b63c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lido: savedrecs.txt - 11 publicações encontradas.\n",
      "Lido: savedrecs (5).txt - 10 publicações encontradas.\n",
      "Lido: savedrecs (3).txt - 84 publicações encontradas.\n",
      "Lido: savedrecs (2).txt - 42 publicações encontradas.\n",
      "Lido: savedrecs (4).txt - 654 publicações encontradas.\n",
      "Lido: savedrecs (8).txt - 9 publicações encontradas.\n",
      "Lido: savedrecs (6).txt - 43 publicações encontradas.\n",
      "Lido: savedrecs (7).txt - 20 publicações encontradas.\n",
      "Lido: savedrecs (1).txt - 623 publicações encontradas.\n",
      "\n",
      "--- DEBUG: Exemplo das chaves de indexação do PRIMEIRO registro ---\n",
      "Título (TI): review of open-source software for developing heterogeneous data management syst...\n",
      "DOI (DI)   : 10.1093/bioadv/vbaf168\n",
      "WoS ID (UT): wos:001543196800001\n"
     ]
    }
   ],
   "source": [
    "todos_os_registros = []\n",
    "cabecalho_oficial = []\n",
    "\n",
    "for arquivo in arquivos_txt_wos:\n",
    "    caminho = os.path.join(pasta_origem_wos, arquivo)\n",
    "    header, records = parse_wos_file(caminho)\n",
    "    \n",
    "    if not cabecalho_oficial and header:\n",
    "        cabecalho_oficial = header # Salva o cabeçalho do primeiro arquivo\n",
    "        \n",
    "    todos_os_registros.extend(records)\n",
    "    print(f\"Lido: {arquivo} - {len(records)} publicações encontradas.\")\n",
    "\n",
    "print(\"\\n--- DEBUG: Exemplo das chaves de indexação do PRIMEIRO registro ---\")\n",
    "if todos_os_registros:\n",
    "    exemplo = todos_os_registros[0]\n",
    "    print(f\"Título (TI): {exemplo['TI'][:80]}...\") # Mostra os primeiros 80 caracteres\n",
    "    print(f\"DOI (DI)   : {exemplo['DI']}\")\n",
    "    print(f\"WoS ID (UT): {exemplo['UT']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6f019",
   "metadata": {},
   "source": [
    "### 3. Remoção de Duplicatas e Consolidação\n",
    "Como não estamos usando o Pandas aqui, usaremos um dicionário nativo do Python. Ao usar a chave única do artigo (UT, DOI ou Título) como chave do dicionário, publicações repetidas irão sobrescrever a anterior, mantendo a base final limpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6c1e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resumo da Limpeza (WoS) ---\n",
      "Registros brutos lidos: 1496\n",
      "Registros únicos mantidos: 1290\n",
      "Duplicatas removidas: 206\n"
     ]
    }
   ],
   "source": [
    "# Dicionário para guardar registros únicos\n",
    "registros_deduplicados = {}\n",
    "\n",
    "for reg in todos_os_registros:\n",
    "    # A prioridade de chave única: 1º WoS ID (UT), 2º DOI (DI), 3º Título (TI)\n",
    "    chave_unica = reg['UT']\n",
    "    \n",
    "    if not chave_unica:\n",
    "        chave_unica = reg['DI']\n",
    "        \n",
    "    if not chave_unica:\n",
    "        chave_unica = reg['TI']\n",
    "    \n",
    "    # Adiciona ao dicionário. Se a chave já existir, ele simplesmente sobrescreve (remove a duplicata)\n",
    "    if chave_unica:\n",
    "        registros_deduplicados[chave_unica] = reg['bloco_texto']\n",
    "\n",
    "total_original = len(todos_os_registros)\n",
    "total_final = len(registros_deduplicados)\n",
    "\n",
    "print(\"--- Resumo da Limpeza (WoS) ---\")\n",
    "print(f\"Registros brutos lidos: {total_original}\")\n",
    "print(f\"Registros únicos mantidos: {total_final}\")\n",
    "print(f\"Duplicatas removidas: {total_original - total_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cfdf6",
   "metadata": {},
   "source": [
    "### 4. Geração do Arquivo Final em TXT\n",
    "Reconstruindo o arquivo seguindo exatamente o padrão original exigido por ferramentas de bibliometria ou importadores (como Mendeley/Zotero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039d8646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arquivo final padronizado salvo com sucesso: /home/silvio/Projetos/TEMAC/ajustes_BD/base_wos_consolidada.txt\n"
     ]
    }
   ],
   "source": [
    "arquivo_saida_wos = \"/home/silvio/Projetos/TEMAC/ajustes_BD/base_wos_consolidada.txt\"\n",
    "\n",
    "with open(arquivo_saida_wos, 'w', encoding='utf-8') as f:\n",
    "    # 1. Escreve o cabeçalho \"FN ...\" e \"VR ...\"\n",
    "    for linha_header in cabecalho_oficial:\n",
    "        f.write(linha_header + \"\\n\")\n",
    "    \n",
    "    # 2. Escreve os registros únicos\n",
    "    for bloco_raw in registros_deduplicados.values():\n",
    "        f.write(bloco_raw + \"\\n\\n\") # Adiciona duas quebras para garantir a linha em branco entre registros\n",
    "\n",
    "print(f\"\\nArquivo final padronizado salvo com sucesso: {arquivo_saida_wos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temac (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
