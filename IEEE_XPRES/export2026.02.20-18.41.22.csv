"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Efficient Data Exchange Between Typical Data Lake and DWH Corporate Systems","A. Suleykin; A. Bobkova; P. Panfilov; I. Chumakov","Russian Academy of Sciences, Doctoral School V.A. Trapeznikov Institute of Control Sciences, Moscow, Russia; Department of Business Informatics, Graduate School of Business, HSE University, Moscow, Russia; Department of Business Informatics, Graduate School of Business, HSE University, Moscow, Russia; Digital Marketing&e-Commerce Liebherr Appliances Liebherr-International Deutschland GmbH, Mooscow, Russia","2021 International Conference on Electrical, Computer and Energy Technologies (ICECET)","11 Feb 2022","2021","","","1","6","In the last five years, many companies around the world have been successfully implemented Apache Hadoop as a main Data Lake storage for all data presented in the organization. At the same time, the adoption of other Open-Source technologies has been also increasing for years, such as classical MPP-based systems for Analytical workloads. Thus, the question of efficient and fast data integration between Apache Hadoop and other organizational data storage systems is highly important for enterprises, where business and decision makers need the minimum delay of big heterogeneous data exchange between Hadoop and other storages. In this paper, we compare different options for loading data from Apache Hadoop, representing the Data Lake of organization, into Open-Source MPP Greenplum database with the role of classical data warehouse for analytical workloads, and choose the best one. Also, we identify potential risks of using different data loading methods.","","978-1-6654-4231-2","10.1109/ICECET52533.2021.9698468","RFBR(grant numbers:20-07-00958); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698468","Big Data;Data Lake;Hadoop Distributed File System;Greenplum;Massively Parallel Processing;Data Warehouse","Loading;Distributed databases;Data integration;Cluster computing;Computer architecture;Production;Data warehouses","","","","11","IEEE","11 Feb 2022","9-10 Dec. 2021","9-10 Dec. 2021","IEEE","IEEE Conferences"
"Data Lake Management for Educational Analysis","D. Martinez-Mosquera; V. Beltrán; D. Riofrío-Luzcando; J. Carrión-Jumbo","Department of Informatics and Computer Science, Escuela Politécnica Nacional, Digital School, Universidad Internacional SEK, Quito, Ecuador; Digital School, Universidad Internacional SEK, Quito, Ecuador; Digital School, Universidad Internacional SEK, Quito, Ecuador; Digital School, Universidad Internacional SEK, Quito, Ecuador",2022 IEEE Sixth Ecuador Technical Chapters Meeting (ETCM),"9 Nov 2022","2022","","","1","5","This article presents an approach to managing an educational analytical system in a data lake. This solution covers higher education institutions' requirements for managing large volumes generated by their students and teachers. This work deals with the problem of the lack of organization when implementing a data lake due to the fact that there are no well-known or standardized methods for its administration. Our methodology proposes dividing the data lake into three zones: (1) landing tier, (2) staging tier, and (3) consumption tier, and transforming the data for each zone under the guidance of the Common Data Model and One Data Model. The main goal is to avoid the educational data lake from converting into a data swamp. This methodology was implemented at University as a case study over an open-source data lake environment. The results obtained figures that historical data analysis barriers are overcome thanks to the high capabilities of the data lake. In addition, this approach can be applied to other institutions with great flexibility, with commodity solutions, and regardless of the source data format.","","978-1-6654-8744-3","10.1109/ETCM56276.2022.9935751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935751","big data;common data model;data lake;education;HDFS;one data model","Data lakes;Analytical models;Data analysis;Standards organizations;Education;Organizations;Big Data applications","","1","","18","IEEE","9 Nov 2022","11-14 Oct. 2022","11-14 Oct. 2022","IEEE","IEEE Conferences"
"Building a Data Lake for Smart Building Data: Architecture for Data Quality and Interoperability","J. L. Hernández; S. Martín; P. Kapsalis; K. Katsigarakis; E. Sarmas; V. Marinakis","Energy Division, CARTIF Technology Centre, Boecillo, Valladolid, Spain; Energy Division, CARTIF Technology Centre, Boecillo, Valladolid, Spain; Decision Support Systems Laboratory, National Technical University of Athens, Athens, Greece; Faculty of the Built Environment, University College of London, London, United Kindom; Decision Support Systems Laboratory, National Technical University of Athens, Athens, Greece; Decision Support Systems Laboratory, National Technical University of Athens, Athens, Greece","2023 14th International Conference on Information, Intelligence, Systems & Applications (IISA)","15 Dec 2023","2023","","","1","8","Building data is growing, where sources are heterogeneous and still treated as silos from different building domains (energy, architecture elements or automation networks, among others). This leads to a lock-in when providing capabilities of data exploitation, such as added-value services, artificial intelligence services or machine-learning activities. Assuring data integration via interoperability mechanisms becomes then pivotal in building data management schemas. Under this perspective, this paper presents an architecture of a data lake that integrates heterogeneous data sources from diverse building domains with the aim of homogenising data-sets and creating data-quality procedures to ensure high-quality services to make better decisions. Based on enriched data between static and dynamic data-sets, the data lake ultimate develops business intelligence mechanism to extract knowledge and information.","","979-8-3503-1806-7","10.1109/IISA59645.2023.10345892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10345892","data lake;data quality;interoperability;digitalisation;smart buildings","Smart buildings;Architecture;Data integrity;Soft sensors;Buildings;Pipelines;Metadata","","5","","16","IEEE","15 Dec 2023","10-12 July 2023","10-12 July 2023","IEEE","IEEE Conferences"
"The Automation of the Data Lake Ingestion Process from Various Sources","A. Tunjić","Multicom d.o.o., Zagreb, Croatia","2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","11 Jul 2019","2019","","","1276","1281","In a big data environment, it is often necessary to ingest data from different sources into a unique storage. Because of low memory price, system distribution and failure tolerance, that storage is typically HDFS. It enables users to manipulate data with different tools from the Hadoop ecosystem. The process of data ingestion seems simple. However, because sources can be different database systems, structured, semi-structured and unstructured data complicate the ingestion procedure. It is usually not enough to just store everything. Data needs to be stored in such a way that enables users to quickly access and manipulate it. There are many ingestion-specific solutions in the big data ecosystem. This paper will describe an implemented system for data ingestion from MSSQL, MySQL and Postgres into a Hive database. The process starts with creating tables with corresponding metadata, continues with the ingestion process and ends with a description of how the process is automated. The implementation of Sqoop as an open-source tool and Hue, a web user interface from Cloudera, will be described.","2623-8764","978-953-233-098-4","10.23919/MIPRO.2019.8756864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756864","hadoop;sqoop;hive;data ingestion automation;big data","Tools;Big Data;Metadata;Relational databases;Servers;Password","","1","","8","","11 Jul 2019","20-24 May 2019","20-24 May 2019","IEEE","IEEE Conferences"
"A Big Data Lake for Multilevel Streaming Analytics","R. Liu; H. Isah; F. Zulkernine","School of Computing, Queen’s University, Kingston, Canada; School of Computing, Queen’s University, Kingston, Canada; School of Computing, Queen’s University, Kingston, Canada",2020 1st International Conference on Big Data Analytics and Practices (IBDAP),"5 Nov 2020","2020","","","1","6","Large organizations are seeking to create new architectures and scalable platforms to effectively handle data management challenges due to the explosive nature of data rarely seen in the past. These data management challenges are largely posed by the availability of streaming data at high velocity from various sources in multiple formats. The changes in data paradigm have led to the emergence of new data analytics and management architecture. This paper focuses on storing high volume, velocity and variety data in the raw formats in a data storage architecture called a data lake. First, we present our study on the limitations of traditional data warehouses in handling recent changes in data paradigms. We discuss and compare different open source and commercial platforms that can be used to develop a data lake. We then describe our end-to-end data lake design and implementation approach using the Hadoop Distributed File System (HDFS) on the Hadoop Data Platform (HDP). Finally, we present a real-world data lake development use case for data stream ingestion, staging, and multilevel streaming analytics which combines structured and unstructured data. This study can serve as a guide for individuals or organizations planning to implement a data lake solution for their use cases.","","978-1-7281-8106-6","10.1109/IBDAP50342.2020.9245460","IBM Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9245460","Hadoop Data Platform;Hadoop Distributed File System;NiFi;streaming data;unstructured data","Data analysis;Distributed databases;Organizations;Lakes;Big Data;Data warehouses;Planning","","11","","17","IEEE","5 Nov 2020","25-26 Sept. 2020","25-26 Sept. 2020","IEEE","IEEE Conferences"
"Data Lake Architecture for Storing and Transforming Web Server Access Log Files","E. Zagan; M. Danubianu","Faculty of Electrical Engineering and Computer Science, Ştefan cel Mare University of Suceava, Suceava, Romania; Faculty of Electrical Engineering and Computer Science, Ştefan cel Mare University of Suceava, Suceava, Romania",IEEE Access,"1 May 2023","2023","11","","40916","40929","Web server access log files are text files containing important data about server activities, client requests addressed to a server, server responses, etc. Large-scale analysis of these data can contribute to various improvements in different areas of interest. The main problem lies in storing these files in their raw form, over long time, to allow analysis processes to be run at any time enabling information to be extracted as foundation for high quality decisions. Our research focuses on offering an economical, secure, and high-performance solution for the storage of large amount of raw log files. Proposed system implements a Data Lake (DL) architecture in cloud using Azure Data Lake Storage Gen2 (ADLS Gen2) for extract–load–transform (ELT) pipelines. This architecture allows large volumes of data to be stored in their raw form. Afterwards they can be subjected to transformation and advanced analysis processes without the need of a structured writing scheme. The main contribution of this paper is to provide a solution that is affordable and more accessible to perform web server access log data ingestion, storage and transformation over the newest technology, Data Lake. As derivative contribution, we proposed the use of Azure Blob Trigger Function to implement the algorithm of transforming log files into parquet files leading to 90% reduction in storage space compared to their original size. That means much lower storage costs than if they had been stored as log files. A hierarchical data storage model has also been proposed for shared access to data over different layers in the DL architecture, on top of which Data Lifecycle Management (DLM) rules have been proposed for storage cost efficiency. We proposed ingesting log files into a Data Lake deployed in cloud due to ease of deployment and low storage costs. The aim is to maintain this data in the long term, to be used in future advanced analytics processes by cross-referencing with other organizational or external data. That could bring important benefits. While the proposed solution is explicitly based on ADLS Gen2, it represents an important benchmark in approaching a cloud DL solution offered by any other vendor.","2169-3536","","10.1109/ACCESS.2023.3270368","ANTREPRENORDOC, in the framework of Human Resources Development Operational Programme 2014–2020; European Social Fund(grant numbers:36355/23.05.2019 HRD OP /380/6/13–SMIS,123847); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10107911","Cloud data lake;ADLS Gen2;data lake architecture;web server access log data;Azure function Blob trigger","Computer architecture;Web servers;Big Data applications;Costs;Data models;Companies;Data mining","","7","","40","CCBYNCND","25 Apr 2023","2023","","IEEE","IEEE Journals"
"Solution for detecting sensitive data inside a data lake","S. Tovernić; V. Banović; Z. Hrastić; K. Plantić; A. Šandić; M. Baranović","Faculty of Electrical Engineering and Computing, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, Zagreb, Croatia","2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","2 Jul 2018","2018","","","1284","1288","This paper is the result of a project realized by a team of current master's degree students. The team created an algorithm for recognition of sensitive data, primarily name, surname and OIB (Croatian personal identification number). Same algorithm iterates across given unstructured texts and appoints tags for documents considering the existence of specific sensitive data. This process offers a way for companies to narrow down the search for personal information if a client demands removal of his data. Similar algorithm was implemented for working with server logs as well, which are represented as data streams and analysed in real time. To provide insight on the quantity of sensitive information and how it is distributed across different types of documents the team created a dashboard that shows statistical data accumulated by developed algorithms. The solution is stored on Cloudera, Apache Hadoop-based open source platform designed for data management and analytics, which is deployed on Microsoft Azure cloud infrastructure.","","978-953-233-095-3","10.23919/MIPRO.2018.8400232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8400232","","Electric potential;Software algorithms;Semantics;Tagging;Software;Real-time systems;Teamwork;Microelectronics;Information and communication technology;Servers","","6","","20","","2 Jul 2018","21-25 May 2018","21-25 May 2018","IEEE","IEEE Conferences"
"Open Data Lake to Support Machine Learning on Arctic Big Data","A. M. Olawoyin; C. K. Leung; A. Cuzzocrea","Department of Computer Science, University of Manitoba, Winnipeg, MB, Canada; Department of Computer Science, University of Manitoba, Winnipeg, MB, Canada; iDEA Lab University of Calabria, Rende, Italy",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","5215","5224","The era of big data is evolving with the introduction of the data lake concept. While a data warehouse provides a well-structured model to manage big data, a data lake accepts data of any types and formats with or without schema and provides access to the data for diverse communities of users. A data lake provides flexible, agile, and scalable solution to manage the ever-increasing volume of big data we are witnessing in the world today, including many siloed data collected over the years by researchers through Arctic expeditions. In this paper, we present our conceptual model of a data lake for integrating the diverse huge amount of data collected by researchers during Arctic expedition. We also design a baseline metadata using a data-driven approach to manage the disparately huge structured, semi-structured, and unstructured data collected from the Arctic region. The resulting open data lake not only effectively manages big Arctic data but also supports machine learning on these big data.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671453","big data;data management;data lake;open data;reusability;FAIR principle;CARE principle;Arctic data;Arctic expedition;machine learning;data mining","Renewable energy sources;Visual analytics;Machine learning;Production;Metadata;Lakes;Data warehouses","","23","","78","IEEE","13 Jan 2022","15-18 Dec. 2021","15-18 Dec. 2021","IEEE","IEEE Conferences"
"Toward Sustainable Data Practices: Integrating Open Data With SDG-Based Data Lake Frameworks","A. Kulkarni; C. Ramanathan; V. E. Venugopal","International Institute of Information Technology, Bangalore, Bengaluru, India; International Institute of Information Technology, Bangalore, Bengaluru, India; International Institute of Information Technology, Bangalore, Bengaluru, India",IEEE Technology and Society Magazine,"11 Apr 2024","2024","43","1","62","69","Achieving sustainable development goals (SDGs) necessitates the adoption of sustainable policies, which entails a crucial task of policy formulation. Policymakers must consider multiple factors, such as the present development status, which can be assessed using diverse data points, as well as the policy’s impact and an action plan outlining its implementation [1]. These data points typically originate from various sources, primarily governmental bodies encompassing statistics, budgets, legislation, public services, and geospatial data (maps, satellite imagery), along with domain users comprising individuals, organizations, and governments [2]. The SDGs are intricately crafted, taking into account a multitude of factors and pinpointing key indicators that influence these factors. These factors typically span across diverse data points sourced from various sectors. For instance, in assessing the student dropout rate, it is imperative to consider factors such as the availability of transportation facilities, basic hygiene amenities, access to drinking water, and the effectiveness of government-organized schemes in utilization and impact. Employing an open data framework coupled with advanced artificial intelligence (AI) models has the potential to facilitate in-depth exploration and analysis, providing valuable insights into the complex interplay of these factors and contributing to a more comprehensive understanding of SDG-related challenges and opportunities.","1937-416X","","10.1109/MTS.2024.3365591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496885","","Transportation;Legislation;Big Data applications;Data models;Satellite images;Geospatial analysis;Artificial intelligence;Sustainable development;Water resources;Open data;Climate change","","1","","14","IEEE","11 Apr 2024","March 2024","","IEEE","IEEE Magazines"
"Policy-Based Access Control System for Delta Lake","Z. Chen; H. Shao; Y. Li; H. Lu; J. Jin","School of Software Engineering, Southeast University, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Southeast University, Suzhou, Jiangsu, China; China Information Consulting & Designing Institute CO, LTD, Nanjing, Jiangsu, China; School of Information Engineering, Nanjing Audit University, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Southeast University, Nanjing, Jiangsu, China",2022 Tenth International Conference on Advanced Cloud and Big Data (CBD),"25 Jan 2023","2022","","","60","65","Delta lake is a new generation of data storage solutions. It stores both transaction log and data files in one directory, and provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing on top of existing data lakes, such as S3, ADLS, GCS, and HDFS. Different from data warehouses, delta lakes allow data to be stored in the original format, retain complete data information, and provide efficient and low-cost storage solutions for data computing and analysis businesses. However, Since Delta Lake metadata is scattered in different resource files, the lack of a unified metadata view increases the difficulty of data governance. Also, Delta Lake adopts an open source storage system as the underlying storage, and its basic access control does not isolate different users, which may lead the risk of data leakage. At present, most common storage systems use data tables’ row and column fields for access control, while delta lake treats the file group as an object. In this paper, aiming at the difficulty of data governance, we design a data lake metadata management method to achieve unified and efficient management of metadata information in heterogeneous data. Then, we design a policy-based data lake access control mechanism, combined with the open source permission framework, and complete the access request for different users and roles in Delta Lake.","","979-8-3503-0971-3","10.1109/CBD58033.2022.00020","National Natural Science Foundation of China; Key Laboratory of Computer Network and Information Integration; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024547","Data Lake;Delta Lake;Metadata;Access Control","Access control;Memory;Lakes;Metadata;Lead;Data warehouses;Big Data applications","","3","","16","IEEE","25 Jan 2023","4-5 Nov. 2022","4-5 Nov. 2022","IEEE","IEEE Conferences"
"Connection of Dynamic and Static Data: A Data Lake for Building Digitalisation","J. L. Hernández; D. Arévalo; S. Martín; K. Katsigarakis; G. N. Lilis; D. Rovas; I. De Miguel","Energy Division, CARTIF technology centre, Boecillo, Spain; Energy Division, CARTIF technology centre, Boecillo, Spain; Energy Division, CARTIF technology centre, Boecillo, Spain; IEDE University College London, London, UK; IEDE University College London, London, UK; IEDE University College London, London, UK; Universidad de Valladolid, Valladolid, Spain",2024 IEEE International Workshop on Metrology for Living Environment (MetroLivEnv),"6 Aug 2024","2024","","","262","267","The landscape of building data is expanding, with heterogeneous sources that are often treated as isolated silos across different building domains such as energy, architecture elements, or automation networks. This siloed approach creates a lock-in scenario, limiting the potential for effective data exploitation, including the provision of added-value services, artificial intelligence, or machine-learning activities. To overcome this challenge, ensuring data integration through interoperability mechanisms becomes crucial within building data management frameworks. In line with this perspective, this work introduces a data lake that harmonizes heterogeneous data sources from various building domains. The primary goal is to standardize datasets, ensuring the delivery of high-quality services to facilitate better decision-making. These datasets are enriched by interactions between static and dynamic datasets. This holistic approach aims to break down silos and unlock the full potential of building data for informed decision-making processes.","","979-8-3503-8501-4","10.1109/MetroLivEnv60384.2024.10615827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10615827","","Soft sensors;Buildings;Semantics;Decision making;Sociology;Data collection;Big Data applications","","","","13","IEEE","6 Aug 2024","12-14 June 2024","12-14 June 2024","IEEE","IEEE Conferences"
"Challenges and Opportunities in Big Data Analytics for Industry 4.0: A Systematic Evaluation of Current Architectures","A. R. Kretzer; F. Barreto Vavassori Benitti; F. Siqueira","Department of Informatics and Statistics (INE), Federal University of Santa Catarina (UFSC), Florianópolis, Santa Catarina, Brazil; Department of Informatics and Statistics (INE), Federal University of Santa Catarina (UFSC), Florianópolis, Santa Catarina, Brazil; Department of Informatics and Statistics (INE), Federal University of Santa Catarina (UFSC), Florianópolis, Santa Catarina, Brazil",IEEE Access,"30 Oct 2025","2025","13","","183419","183447","The current efforts to integrate Big Data Analytics (BDA) into Industry 4.0 manufacturing systems, despite their usefulness for enhancing data-driven decision-making, are constrained by the lack of architectural standards for data management. This systematic mapping study analyzes many BDA architectures proposed in the literature, revealing a fragmented landscape in which the proposed architectures are largely conceptual with limited industrial validation. Our analysis identifies dominant technological patterns, such as Apache Kafka for ingestion, Spark for processing, and Hadoop and Hive for storage, with the majority of implementations favoring open-source solutions. Despite their theoretical importance, real-time analytics capabilities remain underutilized in practice. This study synthesizes a unified conceptual reference architecture with eight fundamental layers to provide a framework for comparative analysis. We document an imbalance in layer development: storage and processing receive comprehensive attention while querying, infrastructure management, and monitoring layers remain underdeveloped. Implementation approaches show distinct patterns in deployment strategies and data handling, with structured and semi-structured data well supported, whereas unstructured data integration presents ongoing challenges. Future research should focus on developing standardized modular frameworks, benchmarking methodologies, and integrating modern data lakehouse architectures to bridge the gap between theoretical proposals and production-ready systems.","2169-3536","","10.1109/ACCESS.2025.3624558","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) (Research Organization Registry (ROR) identifier: 00x0ma614) for the Article Processing Charge; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11214321","Big data analytics (BDA);Industry 4.0;Industrial Internet of Things (IIoT);smart manufacturing;cyber-physical systems (CPS);data lake","Fourth Industrial Revolution;Manufacturing;Systematics;Computer architecture;Artificial intelligence;Pipelines;Industrial Internet of Things;Big Data applications;Computer science;Focusing","","","","86","CCBY","22 Oct 2025","2025","","IEEE","IEEE Journals"
"Serverless Analytics with Amazon Athena: Query structured, unstructured, or semi-structured data in seconds without setting up any infrastructure","A. Virtuoso; M. T. Hocanin; A. Wishnick; R. Pathak",NA; NA; NA; NA,"Serverless Analytics with Amazon Athena: Query structured, unstructured, or semi-structured data in seconds without setting up any infrastructure","","2021","","","","","Get more from your data with Amazon Athena’s ease-of-use, interactive performance, and pay-per-query pricingKey FeaturesExplore the promising capabilities of Amazon Athena and Athena’s Query Federation SDKUse Athena to prepare data for common machine learning activitiesCover best practices for setting up connectivity between your application and Athena and security considerationsBook DescriptionAmazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using SQL, without needing to manage any infrastructure. This book begins with an overview of the serverless analytics experience offered by Athena and teaches you how to build and tune an S3 Data Lake using Athena, including how to structure your tables using open-source file formats like Parquet. You’ll learn how to build, secure, and connect to a data lake with Athena and Lake Formation. Next, you’ll cover key tasks such as ad hoc data analysis, working with ETL pipelines, monitoring and alerting KPI breaches using CloudWatch Metrics, running customizable connectors with AWS Lambda, and more. Moving on, you’ll work through easy integrations, troubleshooting and tuning common Athena issues, and the most common reasons for query failure. You will also review tips to help diagnose and correct failing queries in your pursuit of operational excellence. Finally, you’ll explore advanced concepts such as Athena Query Federation and Athena ML to generate powerful insights without needing to touch a single server. By the end of this book, you’ll be able to build and use a data lake with Amazon Athena to add data-driven features to your app and perform the kind of ad hoc data analysis that often precedes many of today’s ML modeling exercises.What you will learnSecure and manage the cost of querying your dataUse Athena ML and User Defined Functions (UDFs) to add advanced features to your reportsWrite your own Athena Connector to integrate with a custom data sourceDiscover your datasets on S3 using AWS Glue CrawlersIntegrate Amazon Athena into your applicationsSetup Identity and Access Management (IAM) policies to limit access to tables and databases in Glue Data CatalogAdd an Amazon SageMaker Notebook to your Athena queriesGet to grips with using Athena for ETL pipelinesWho this book is forBusiness intelligence (BI) analysts, application developers, and system administrators who are looking to generate insights from an ever-growing sea of data while controlling costs and limiting operational burden, will find this book helpful. Basic SQL knowledge is expected to make the most out of this book.","","9781800567863","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163042.pdf&bkn=10163042&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
