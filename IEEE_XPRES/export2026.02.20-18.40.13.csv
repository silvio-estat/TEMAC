"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Data Ingestions as a Service (DIaaS): A Unified Interface for Heterogeneous Data Ingestion, Transformation, and Metadata Management for Data Lake","H. V. Sreepathy; B. Dinesh Rao; M. Kumar Jaysubramanian; B. Deepak Rao","Manipal School of Information Sciences, Manipal Academy of Higher Education, Manipal, India; Manipal School of Information Sciences, Manipal Academy of Higher Education, Manipal, India; Frameworks and Tools, MulticoreWare, Coimbatore, India; Manipal School of Information Sciences, Manipal Academy of Higher Education, Manipal, India",IEEE Access,"29 Oct 2024","2024","12","","156131","156145","Data ingestion tools are critical component of Data Lake. Existing data ingestion tools face challenges of handling large variety, formats, sources of data. There exists void for unified data ingestion interface to handle the above research problems. This study proposes an innovative and integrated framework for data ingestion in a data lake, addressing the challenges posed by heterogeneous data sources, formats, and metadata management. The framework comprises three novel modules: First Unified Data Integration Connectors (UDIC), which provide seamless connectivity and data retrieval capabilities from diverse sources including databases, data warehouses, file systems, cloud storage, and APIs; Second, Adaptive Data Variety Transformation (ADVT), a module that intelligently handles the transformation and processing of structured, semi-structured, and unstructured data types, ensuring efficient ingestion into the data lake; and third, Intelligent Metadata Management (IMM), a module that captures, stores, and manages metadata associated with the ingested data, offering advanced search, discovery, and enrichment functionalities. Comparative study corroborates features offered by the service with existing data ingestion tools to evaluate the novelty and significance of the study. Performance validation shows varying ingestion latencies across different data types: approximately 148.1 microseconds per record for structured data, 234.2 microseconds per record for semi-structured data, 65.6 microseconds per kilobyte (KB) for video data, and 42.7 microseconds per KB for image data. These results underscore the importance of considering data structure and size in optimizing ingestion processes. Overall, this research aims to revolutionize data ingestion in data lake environments by providing a unified solution for handling diverse data sources, formats, and metadata management.","2169-3536","","10.1109/ACCESS.2024.3479736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10716380","Data ingestion;data lake;data sources;data formats;unified interface;data ingestion service;big data","Big Data applications;Metadata;Soft sensors;Real-time systems;Organizations;Java;Internet of Things;Relational databases;File systems;Data lakes","","5","","24","CCBYNCND","14 Oct 2024","2024","","IEEE","IEEE Journals"
"Cloud Big Data Lake for Advanced Analytics in Semiconductor Manufacturing","S. Sun; J. Ye; H. Schwarthoff; J. Rosin; V. Vakkalagadda; J. Chang; S. R. Ubbara; A. Chinthakindi","Data Science, Micron Technology Inc., Manassas, VA, USA; Process Integration Micron Technology Inc., Manassas, VA, USA; Smart Mfg Tech Solutions Micron Technology Inc., Boise, ID, USA; Information Technology Micron Technology Inc., Manassas, VA, USA; Data Science, Micron Technology Inc., Manassas, VA, USA; Smart Mfg Tech Solutions Micron Technology Inc., Boise, ID, USA; Smart Mfg Tech Solutions Micron Technology Inc., Boise, ID, USA; Front-end Central Quality Micron Technology Inc., Manassas, VA, USA",2024 35th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC),"6 Jun 2024","2024","","","1","5","Data driven business intelligence is changing how semiconductor manufacturing thrives in the long term. A cloud big data lake is designed and implemented based on state-of-the-art cloud architecture providing complete services for data ingestion, storage, processing, advanced analytics, and machine learning with a high level of security. Efficient and effective use of this big data lake and data science enables problem solving and decision making to improve productivity and performance.","2376-6697","979-8-3503-8455-0","10.1109/ASMC61125.2024.10545365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545365","Cloud;Data Lake;Semiconductor Manufacturing;Big Data Analytics","Cloud computing;Soft sensors;Scalability;Machine learning;Lakes;Big Data;Semiconductor device manufacture","","","","5","IEEE","6 Jun 2024","13-16 May 2024","13-16 May 2024","IEEE","IEEE Conferences"
"Data Lakehouse in Action: Architecting a modern and scalable data analytics platform","P. Menon",NA,Data Lakehouse in Action: Architecting a modern and scalable data analytics platform,"","2022","","","","","Propose a new scalable data architecture paradigm, Data Lakehouse, that addresses the limitations of current data architecture patternsKey FeaturesUnderstand how data is ingested, stored, served, governed, and secured for enabling data analyticsExplore a practical way to implement Data Lakehouse using cloud computing platforms like AzureCombine multiple architectural patterns based on an organization’s needs and maturity levelBook DescriptionThe Data Lakehouse architecture is a new paradigm that enables large-scale analytics. This book will guide you in developing data architecture in the right way to ensure your organization's success. The first part of the book discusses the different data architectural patterns used in the past and the need for a new architectural paradigm, as well as the drivers that have caused this change. It covers the principles that govern the target architecture, the components that form the Data Lakehouse architecture, and the rationale and need for those components. The second part deep dives into the different layers of Data Lakehouse. It covers various scenarios and components for data ingestion, storage, data processing, data serving, analytics, governance, and data security. The book's third part focuses on the practical implementation of the Data Lakehouse architecture in a cloud computing platform. It focuses on various ways to combine the Data Lakehouse pattern to realize macro-patterns, such as Data Mesh and Data Hub-Spoke, based on the organization's needs and maturity level. The frameworks introduced will be practical and organizations can readily benefit from their application. By the end of this book, you'll clearly understand how to implement the Data Lakehouse architecture pattern in a scalable, agile, and cost-effective manner.What you will learnUnderstand the evolution of the Data Architecture patterns for analyticsBecome well versed in the Data Lakehouse pattern and how it enables data analyticsFocus on methods to ingest, process, store, and govern data in a Data Lakehouse architectureLearn techniques to serve data and perform analytics in a Data Lakehouse architectureCover methods to secure the data in a Data Lakehouse architectureImplement Data Lakehouse in a cloud computing platform such as AzureCombine Data Lakehouse in a macro-architecture pattern such as Data MeshWho this book is forThis book is for data architects, big data engineers, data strategists and practitioners, data stewards, and cloud computing practitioners looking to become well-versed with modern data architecture patterns to enable large-scale analytics. Basic knowledge of data architecture and familiarity with data warehousing concepts are required.","","9781801815109","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163024.pdf&bkn=10163024&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Big Data Pipeline with ML-Based and Crowd Sourced Dynamically Created and Maintained Columnar Data Warehouse for Structured and Unstructured Big Data","K. Ghane","Anagira, Inc.",2020 3rd International Conference on Information and Computer Technologies (ICICT),"14 May 2020","2020","","","60","67","The existing big data platforms take data through distributed processing platforms and store them in a data lake. The architectures such as Lambda and Kappa address the real-time and batch processing of data. Such systems provide real time analytics on the raw data and delayed analytics on the curated data. The data denormalization, creation and maintenance of a columnar dimensional data warehouse is usually time consuming with no or limited support for unstructured data. The system introduced in this paper automatically creates and dynamically maintains its data warehouse as a part of its big data pipeline in addition to its data lake. It creates its data warehouse on structured, semi-structured and unstructured data. It uses Machine Learning to identify and create dimensions. It also establishes relations among data from different data sources and creates the corresponding dimensions. It dynamically optimizes the dimensions based on the crowd sourced data provided by end users and also based on query analysis.","","978-1-7281-7283-5","10.1109/ICICT50521.2020.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092124","Big Data;Data Lake;Data Warehouse;Data Pipeline;Machine Learning;Structured Data;Unstructured Data;Columnar Database","Data warehouses;Lakes;Real-time systems;Big Data;Distributed databases;Data mining","","8","","36","IEEE","14 May 2020","9-12 March 2020","9-12 March 2020","IEEE","IEEE Conferences"
"Corddl: An Efficient and Extensible Connector between Relational Databases and Data Lakes","T. Zhang; H. Liu; C. Yu; P. Wang","Research Center of Big Data Technology Nanhu Laboratory, JiaXing, China; Research Center of Big Data Technology Nanhu Laboratory, JiaXing, China; Research Center of Big Data Technology Nanhu Laboratory, JiaXing, China; Research Center of Big Data Technology Nanhu Laboratory, JiaXing, China","2023 5th International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","2 Apr 2024","2023","","","173","177","In the era of big data, data lakes have been widely used. However, enterprise applications generally do not interact directly with data lakes, but with relational databases. Relational databases may become data silos because they can only store structured data. In this paper, we proposed Corddl, a connector between relational databases and data lakes. For ingesting data from a relational database into a data lake, first, Corddl uses configuration files and parameters to match the data source class and data source instance for the analysis engine. Then, Corddl uses the JDBC driver to realize the connection to the relational database, and do data conversion according to the data type configuration file. Finally, the ingested data is written into the data lake. There is a similar process for ingesting data from a data lake into a relational database. Experimental results show that Corddl has high efficiency and strong extensibility.","2994-2977","979-8-3503-5993-0","10.1109/MLBDBI60823.2023.10481929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481929","big data;data lake;relational database;data ingestion;data conversion","Connectors;Data conversion;Soft sensors;Relational databases;Machine learning;Lakes;Big Data applications","","","","15","IEEE","2 Apr 2024","15-17 Dec. 2023","15-17 Dec. 2023","IEEE","IEEE Conferences"
"Optimizing SaaS Metrics with Advanced Data Processing and Machine Learning Techniques","R. M. Chetan; G. S. Mamatha","Department of Information Science & Engineering, RV College of Engineering, Bengaluru, India; Department of Information Science & Engineering, RV College of Engineering, Bengaluru, India",2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS),"1 Jan 2025","2024","","","1","6","In SaaS applications, optimizing metrics is essential for enhancing efficiency, user satisfaction, and performance. This paper presents a framework integrating advanced data processing and intelligent algorithms within a scalable data lake architecture, leveraging AWS services like Kinesis for real-time data ingestion, Glue for ETL, S3 for storage, and Redshift for warehousing. Amazon SageMaker enables predictive analytics and real-time anomaly detection, with Athena and QuickSight supporting querying and visualization. Experimental results show improved data processing, anomaly detection, and predictive accuracy, contributing to efficient resource allocation and customer retention. This framework provides SaaS providers with a robust solution for achieving performance and scalability in multi-tenant environments.","2767-1097","979-8-3315-0546-2","10.1109/CSITSS64042.2024.10816850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10816850","SaaS;Data Lake;Machine Learning;Real-Time Analytics;Anomaly Detection;Amazon Web Services (AWS) [13];Kinesis Streams;AWS Glue;Amazon S3 [13];Amazon Redshift [13];Amazon SageMaker;Amazon Athena;Amazon QuickSight;Predictive Analytics;Multi-Tenant Environments;Data Processing;ETL (Extract, Transform, Load);Resource Optimization;Customer Retention","Measurement;Web services;Scalability;Computer architecture;Data processing;Big Data applications;Real-time systems;Resource management;Anomaly detection;Optimization","","","","15","IEEE","1 Jan 2025","7-9 Nov. 2024","7-9 Nov. 2024","IEEE","IEEE Conferences"
"Data Engineering with Apache Spark, Delta Lake, and Lakehouse: Create scalable pipelines that ingest, curate, and aggregate complex data in a timely and secure way","M. Kukreja; D. Zburivsky",NA; NA,"Data Engineering with Apache Spark, Delta Lake, and Lakehouse: Create scalable pipelines that ingest, curate, and aggregate complex data in a timely and secure way","","2021","","","","","Understand the complexities of modern-day data engineering platforms and explore strategies to deal with them with the help of use case scenarios led by an industry expert in big dataKey FeaturesBecome well-versed with the core concepts of Apache Spark and Delta Lake for building data platformsLearn how to ingest, process, and analyze data that can be later used for training machine learning modelsUnderstand how to operationalize data models in production using curated dataBook DescriptionIn the world of ever-changing data and schemas, it is important to build data pipelines that can auto-adjust to changes. This book will help you build scalable data platforms that managers, data scientists, and data analysts can rely on. Starting with an introduction to data engineering, along with its key concepts and architectures, this book will show you how to use Microsoft Azure Cloud services effectively for data engineering. You'll cover data lake design patterns and the different stages through which the data needs to flow in a typical data lake. Once you've explored the main features of Delta Lake to build data lakes with fast performance and governance in mind, you'll advance to implementing the lambda architecture using Delta Lake. Packed with practical examples and code snippets, this book takes you through real-world examples based on production scenarios faced by the author in his 10 years of experience working with big data. Finally, you'll cover data lake deployment strategies that play an important role in provisioning the cloud resources and deploying the data pipelines in a repeatable and continuous way. By the end of this data engineering book, you'll know how to effectively deal with ever-changing data and create scalable data pipelines to streamline data science, ML, and artificial intelligence (AI) tasks.What you will learnDiscover the challenges you may face in the data engineering worldAdd ACID transactions to Apache Spark using Delta LakeUnderstand effective design strategies to build enterprise-grade data lakesExplore architectural and design patterns for building efficient data ingestion pipelinesOrchestrate a data pipeline for preprocessing data using Apache Spark and Delta Lake APIsAutomate deployment and monitoring of data pipelines in productionGet to grips with securing, monitoring, and managing data pipelines models efficientlyWho this book is forThis book is for aspiring data engineers and data analysts who are new to the world of data engineering and are looking for a practical guide to building scalable data platforms. If you already work with PySpark and want to use Delta Lake for data engineering, you'll find this book useful. Basic knowledge of Python, Spark, and SQL is expected.","","9781801074322","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163257.pdf&bkn=10163257&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"AI-optimized data Lakes for Real-Time Big Data Management and Autonomous Query Optimization","G. Ramesh; N. V. Sivareddy; L. H. Jasim; P. Murugeswari; B. Kumaraswamy; M. L","Department of CSE, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad, Telangana, India; Department of Computer Science and Engineering, CMR Institute of Technology, Hyderabad; Department of Computers Techniques Engineering, College of Technical Engineering, The Islamic University, Najaf, Iraq, Al Diwaniyah, Iraq; Department of Electrical Communication Engineering, Sethu Institute of Technology, Pulloor, Kariapatti, Tamil Nadu, India; Department of CS & IT, Kalinga University, Raipur, India; Department of Mathematics, Saveetha Institute of Medical and Technical Sciences, Chennai, Tamilnadu, India",2025 International Conference on Metaverse and Current Trends in Computing (ICMCTC),"17 Oct 2025","2025","","","1","5","Due to the massive explosion of big data, efficient data lakes with ‘real-time’ data management and ‘autonomous’ query optimization have become inevitable. One of the problems attached to traditional data lakes is the difficulty of retrieving data, which is slow to query, inefficiently, and with high complexities of schema evolution the ability of the data lake to optimize workloads dynamically. To overcome these problems, the work presented in this paper brings forward the concept of an AI-optimized data lake framework that exploits machine learning and deep learning for intelligent data ingestion, adaptive indexing as well as self-optimizing query execution. The system is proposed to incorporate hierarchical storage management with AI-driven, self-studying indexing mechanisms, and reinforcement learning-aided query optimization to improve data retrieval efficiency. Furthermore, through an integrated AI-powered anomaly detection system, data processing is made robust and reliable in real time. On top of that the framework also involves autonomous governance and security that can use explainable AI for compliance auditing and a dynamic role-based access control (RBAC). Experimental evaluations show that, with up to a 90% time saving, and with real-time decision-making capabilities, it improves over both aspects in a variety of data workloads. This research brings forth the integration of AI-based optimizations for the development of a broad and flexible framework for intelligently and scaleably building large data lake architectures, thus becoming the future-ready solution for any big data ecosystem within healthcare, financial, or IoT-based smart systems.","","979-8-3315-3821-7","10.1109/ICMCTC62214.2025.11196397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11196397","Role-based access control;AI-driven;Healthcare;Hadoop Distributed File System","Access control;Data ingestion;Accuracy;Query processing;Storage management;Medical services;Big Data applications;Real-time systems;Anomaly detection;Indexing","","","","14","IEEE","17 Oct 2025","10-11 April 2025","10-11 April 2025","IEEE","IEEE Conferences"
"Business Intelligence with Databricks SQL: Concepts, tools, and techniques for scaling business intelligence on the data lakehouse","V. Gupta",NA,"Business Intelligence with Databricks SQL: Concepts, tools, and techniques for scaling business intelligence on the data lakehouse","","2022","","","","","Master critical skills needed to deploy and use Databricks SQL and elevate your BI from the warehouse to the lakehouse with confidenceKey FeaturesLearn about business intelligence on the lakehouse with features and functions of Databricks SQLMake the most of Databricks SQL by getting to grips with the enablers of its data warehousing capabilitiesA unique approach to teaching concepts and techniques with follow-along scenarios on real datasetsBook DescriptionIn this new era of data platform system design, data lakes and data warehouses are giving way to the lakehouse – a new type of data platform system that aims to unify all data analytics into a single platform. Databricks, with its Databricks SQL product suite, is the hottest lakehouse platform out there, harnessing the power of Apache Spark™, Delta Lake, and other innovations to enable data warehousing capabilities on the lakehouse with data lake economics. This book is a comprehensive hands-on guide that helps you explore all the advanced features, use cases, and technology components of Databricks SQL. You’ll start with the lakehouse architecture fundamentals and understand how Databricks SQL fits into it. The book then shows you how to use the platform, from exploring data, executing queries, building reports, and using dashboards through to learning the administrative aspects of the lakehouse – data security, governance, and management of the computational power of the lakehouse. You’ll also delve into the core technology enablers of Databricks SQL – Delta Lake and Photon. Finally, you’ll get hands-on with advanced SQL commands for ingesting data and maintaining the lakehouse. By the end of this book, you’ll have mastered Databricks SQL and be able to deploy and deliver fast, scalable business intelligence on the lakehouse.What you will learnUnderstand how Databricks SQL fits into the Databricks Lakehouse PlatformPerform everyday analytics with Databricks SQL Workbench and business intelligence toolsOrganize and catalog your data assetsProgram the data security model to protect and govern your dataTune SQL warehouses (computing clusters) for optimal query experienceTune the Delta Lake storage format for maximum query performanceDeliver extreme performance with the Photon query execution engineImplement advanced data ingestion patterns with Databricks SQLWho this book is forThis book is for business intelligence practitioners, data warehouse administrators, and data engineers who are new to Databrick SQL and want to learn how to deliver high-quality insights unhindered by the scale of data or infrastructure. This book is also for anyone looking to study the advanced technologies that power Databricks SQL. Basic knowledge of data warehouses, SQL-based analytics, and ETL processes is recommended to effectively learn the concepts introduced in this book and appreciate the innovation behind the platform.","","9781803237596","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163377.pdf&bkn=10163377&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Examining Amazon Customer Reviews using PySpark and AWS: A Data Lake Approach","R. S; A. S. Karthik; M. H. S. M. K. Karthik; M. Jayasurya; S. Yashwanth","Dept. of Computer Science and Engineering, Amrita Vishwa Vidyapeetham; Dept. of Computer Science and Engineering, Amrita Vishwa Vidyapeetham; Dept. of Computer Science and Engineering, Amrita Vishwa Vidyapeetham; Dept. of Computer Science and Engineering, Amrita Vishwa Vidyapeetham; Dept. of Computer Science and Engineering, Amrita Vishwa Vidyapeetham",2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),"23 Nov 2023","2023","","","1","6","In the contemporary world, big data analysis with PySpark and AWS has become quite handy. Large multinational companies, including Walmart, trivago, and countless others are using big data. And with the assistance of AWS, which offers a variety of services like glue, Athena, S3, etc., becoming extremely beneficial for cloud storage purposes and also for creating data migration pipelines. Any RDBMS database server might send tens of gigabytes of data to an Amazon S3 bucket using the ETL architecture. Our primary goal in this project is to use PySpark to construct an ETL pipeline for data migration. Amazon wireless devices review analysis, Amazon watches review analysis, Amazon books review analysis, Amazon shoes review analysis, and Amazon musical instruments review analysis are the five separate datasets on which we conducted our analysis. The datasets considered for the analysis are Amazon watch reviews, book reviews, shoe reviews, wireless device reviews and musical instrument reviews . The goal of the project is to obtain the ratio of the number of trustworthy, real reviews to untrusted reviews and a comparison in done in the form of graphs.","2473-7674","979-8-3503-3509-5","10.1109/ICCCNT56998.2023.10307845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307845","Athena;AWS;ETL Glue;Pipeline;PySpark;RDBMS","Wireless communication;Cloud computing;Instruments;Pipelines;Music;Companies;Footwear","","","","17","IEEE","23 Nov 2023","6-8 July 2023","6-8 July 2023","IEEE","IEEE Conferences"
"The Danish National Energy Data Lake: Requirements, Technical Architecture, and Tool Selection","H. B. Hamadou; T. Bach Pedersen; C. Thomsen","Department of Computer Science, Aalborg University, Aalborg, Denmark; Department of Computer Science, Aalborg University, Aalborg, Denmark; Department of Computer Science, Aalborg University, Aalborg, Denmark",2020 IEEE International Conference on Big Data (Big Data),"19 Mar 2021","2020","","","1523","1532","Renewable Energy Sources such as wind and solar do not emit CO2 but their production vary considerably depending on time and weather. Thus, it is important to use the flexibility in device loads to shift energy consumption to follow the production. For example, an Electrical Vehicle (EV) can be charged very flexibly between arriving home at 5PM and leaving again at 7AM. Utilizing all available energy flexibility requires applying machine learning and AI on massive amounts of Big Data from many different actors and devices, ranging from private consumers, over companies, to energy network operators, and using this to create digital solutions to enable and exploit flexibility. The project Flexible Energy Denmark (FED) is building the foundation for this for the entire Danish society. Specifically, FED collects data from a number of Living Labs (LLs) in representative real-life physical environments. The data is stored in the Danish National Energy Data Lake, called FED Data Lake (FEDDL) to enable efficient and advanced analysis. FEDDL is built using only open source tools which can run both on-premise and in cloud settings. In this paper, we describe the requirements for FEDDL based on a representative LL case study, present its technical architecture, and provide a comparison of relevant tools along with the arguments for which ones we selected.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378368","Data Lake;Energy Data;Living Labs;Data Ingestion;Data Governance;Data Security;Open Source;GDPR","Wind;Renewable energy sources;Production;Machine learning;Tools;Big Data;Lakes","","7","","44","IEEE","19 Mar 2021","10-13 Dec. 2020","10-13 Dec. 2020","IEEE","IEEE Conferences"
"Leveraging Scalable Cloud Infrastructure for Autonomous Driving Data Lakes and Real-Time Decision Making","J. Chen","Graduate School of Arts and Sciences, Columbia University, New York, United States",2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA),"1 Jul 2025","2025","","","1750","1753","Autonomous driving technology relies heavily on the effective management of vast datasets generated by various sensors and vehicle systems. As such, leveraging scalable cloud infrastructure becomes paramount for improving data handling and decision-making capabilities. In this paper, we introduce the Autonomous Driving Data Lakes (ADDL) framework, designed to streamline the storage, retrieval, and processing of extensive driving data in real-time. By utilizing cloud technology, ADDL ensures tight integration of data from diverse sources to enhance situational awareness for autonomous systems. Our architecture features robust data pipelines that support real-time analytics and machine learning applications, which are crucial for timely and accurate decision-making. Extensive experiments with large-scale datasets demonstrate how our approach significantly boosts processing efficiency, data accessibility, and decision-making reliability. The findings highlight advancements in autonomous driving technologies, addressing the challenges associated with data management and enhancing operational effectiveness in changing driving environments.","","979-8-3315-0976-7","10.1109/AIITA65135.2025.11048068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11048068","Cloud Infrastructure;Autonomous Driving;Decision Making","Decision making;Pipelines;Machine learning;Big Data applications;Real-time systems;Sensor systems;Telemetry;Reliability;Autonomous vehicles;Intelligent sensors","","","","18","IEEE","1 Jul 2025","28-30 March 2025","28-30 March 2025","IEEE","IEEE Conferences"
"Scalable Data Lakes for AI Workloads: A Multitenant Architecture for Big Data Orchestration","K. K. Goyal","Independent Researcher, California, USA",2025 IEEE International Conference on Computing (ICOCO),"13 Jan 2026","2025","","","266","271","As artificial intelligence (AI) keeps expanding into workloads in real time, high volume, and various domains, traditional data architectures fall short of keeping pace with the overflow of data—structured and unstructured—that exceeds the thresholds of processing power they were designed to handle. This paper takes a step toward introducing a scalable, multitenant data layer specifically made to manage what is termed ""AI overflow""—the big bursts of data that arrive from all directions when using AI, especially when using it within enterprises. This paper presents an orchestration-focused architecture that supports governance of the all-important metadata, provides the necessary ""modular"" components to build a pipeline that operates within real time, and serves up AI-specific ""lineage tracking"" to enable end-to-end data observability, while guaranteeing seamless reusability across multiple tenancies within the same framework without compromising performance.","","979-8-3315-7539-7","10.1109/ICOCO67189.2025.11334100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11334100","AI data overflow;multitenancy;data orchestration;scalable data layers;real-time pipelines","Scalability;Pipelines;Computer architecture;Metadata;Solids;Big Data applications;Real-time systems;Artificial intelligence;Observability;Engines","","","","10","IEEE","13 Jan 2026","6-8 Oct. 2025","6-8 Oct. 2025","IEEE","IEEE Conferences"
"Replication as Lineage Mechanism for Materialized Views in Lakehouse Architectures","D. -I. Sirbu; A. -T. Taleanu; F. Pop","Faculty of Automatic Control and Computers, National University of Scinence and Technology POLITEHNICA Bucharest, Romania; Faculty of Automatic Control and Computers, National University of Scinence and Technology POLITEHNICA Bucharest, Romania; Faculty of Automatic Control and Computers, National University of Scinence and Technology POLITEHNICA Bucharest, Romania",2024 International Conference on INnovations in Intelligent SysTems and Applications (INISTA),"24 Sep 2024","2024","","","1","7","Nowadays, the volume of data collected from online activity is continuously growing, starting from ecommerce websites, mobile applications or even IoT devices and sensors. All this data must be processed by more and more complex Big Data applications that gather and provide most useful information out of it. Consequently, the storage systems have evolved to keep up with all these changes, transitioning from traditional Data warehouses to Data Lakes and now to Lakehouses. All these applications consist of various transformations on stored data that result in another improved version of it, enriched with some new knowledge that brings value to the user. Based on the type of transformation, one can distinguish at least two types. The first can be simply executed on demand at runtime to get some insights from the collected data, such as the events count or some aggregated reports. The other type is more complex by its nature because it involves some complex processing of that data solely or in conjunction with other data. So, the result of such a transformation is both expensive and not feasible to be computed at runtime every time it is needed. Therefore, it must be persisted on storage once it is computed and then, kept up to date with the latest changes. This persisted computed version of the data is named materialized view, and multiple materialized views can be chained in data pipelines if the result of one transformation represents the input of the next one. Replication is the mechanism that adds lineage semantics to the materialized view transformation, independent of how many chained materialized views are kept, such as in the case of the bronzesilver-gold tables in the medallion architecture. Furthermore, this identifiability feature added to the materialized data facilitates both the propagation of all new incoming changes and the at-least-once processing semantics of the system in case of data reprocessing or in the case this reprocessing is the actual purpose of the service. The proposed replication solution consists of two types of system generated ids, one that uniquely identifies a unit of data and one that points to the original unit of data representing its source.","2768-7295","979-8-3503-6813-0","10.1109/INISTA62901.2024.10683854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10683854","Big Data;Replication;Lakehouse;Materialized View;Machine Learning","Technological innovation;Runtime;Soft sensors;Pipelines;Semantics;Computer architecture;Big Data applications","","","","12","IEEE","24 Sep 2024","4-6 Sept. 2024","4-6 Sept. 2024","IEEE","IEEE Conferences"
"Comparison of a FaaS- and SaaS-Based ETL Process in a Cloud Environment","J. Kohler","University of Applied Sciences, Worms, Worms, Germany",2023 IEEE 6th International Conference on Cloud Computing and Artificial Intelligence: Technologies and Applications (CloudTech),"29 Dec 2023","2023","","","01","07","The goal of this paper is to analyze experiences from a FaaS-based (function as a service) cloud-native implementation of an ETL process (extract, transform, load) based on AWS Lambda functions. The actual implementation is outlined and the experiences from that implementation are evaluated. This results in an overview of pros and cons of a cloud-native implementation in general and determines best practices for other implementations in other cloud environments or business domains.","","979-8-3503-0306-3","10.1109/CloudTech58737.2023.10366090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10366090","FaaS;AWS Cloud Services;Cloud Native;Infrastructure as Code;ETL Process;Data Lake;Core Banking Domain","Cloud computing;Batch production systems;Transforms;Banking;Data warehouses;Artificial intelligence;Best practices","","","","29","IEEE","29 Dec 2023","21-23 Nov. 2023","21-23 Nov. 2023","IEEE","IEEE Conferences"
"Data Lakehouse for Time Series Data: A Systematic Literature Review","M. Pohl; N. D. Wijemanne; D. Staegemann; C. Haertel; C. Daase; D. Dreschel; D. S. Walia; A. Osterthun; J. Reibert; K. Turowski","German Aerospace Center (DLR), Institute of Data Science, Jena, Germany; German Aerospace Center (DLR), Institute of Data Science, Jena, Germany; Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany; Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany; Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany; Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany; Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany; German Aerospace Center (DLR), Institute of Data Science, Jena, Germany; German Aerospace Center (DLR), Institute of Data Science, Jena, Germany; Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5833","5842","As data continues to grow exponentially, the fields of data management and analytics must evolve to ensure efficient data ingestion, knowledge extraction, and scalability. The Data Lakehouse architecture, which combines the best features of Data Warehouses and Data Lakes, has emerged as a potential solution. However, to fully leverage the capabilities of Data Lakehouses for time series data, it is crucial to understand the unique challenges and opportunities they present. This literature review examines proposed Data Lakehouse architectures specifically for time series data, exploring their implementation, the software technologies used, and potential real-world applications. The focus is on comparing these architectures to identify the most suitable technologies for similar implementations. Through an in-depth analysis, this study emphasizes the importance of optimizing configurations to enhance system performance and scalability, particularly for data analysis and artificial intelligence (AI) workloads.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825961","Data Lakehouse;Time Series Data;Temporal Data;Virtual Data Warehouse;Literature Review","Data analysis;Costs;Scalability;Time series analysis;Computer architecture;Data warehouses;Big Data applications;Data mining;Artificial intelligence;Systematic literature review","","","","29","IEEE","16 Jan 2025","15-18 Dec. 2024","15-18 Dec. 2024","IEEE","IEEE Conferences"
"Autonomous Data Quality Monitoring with AI Agents: Integrating ML with Cloud Warehouses and Data Lakes","S. Chippagiri; K. Alang; A. Gumber; S. G. Thomas",NA; NA; NA; NA,2025 International Conference on Computing Technologies & Data Communication (ICCTDC),"22 Sep 2025","2025","","","1","7","Since storing data in cloud warehouses and lakes has increased, it is now important to rely on automation for keeping data accurate and trustworthy. It outlines an autonomous way of monitoring data quality by using AI agents with botnet techniques and cloud architecture. Key features of the system are that it constantly scans the data, finds unusual activities, detects any changes in the structure, and responds to any data updates without someone needing to act. Missed values, duplication, inconsistency, and noticing outliers on both structured and semi-structured data is done by using both supervised and unsupervised ML models. AI agents interact by way of a network pipeline and use reinforcement learning to pick the best solutions. easily set up Cloud Genomics in any leading cloud environment, and it works well with data warehouses as well as data lakes. Experimental evidence shows that AI-driven analysis is better at detecting errors, offering faster results, and being more adaptable than hand-made and rule-based ones. In this research, data pipelines can become more flexible and easier to manage in today's business architecture.","","979-8-3315-2798-3","10.1109/ICCTDC64446.2025.11157955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11157955","Data quality;AI agents;machine learning;data lakes;cloud data warehouse;autonomous systems;data monitoring;anomaly detection","Cloud computing;Data integrity;Pipelines;Genomics;Reinforcement learning;Data warehouses;Lakes;Big Data applications;Feature extraction;Monitoring","","","","18","IEEE","22 Sep 2025","4-5 July 2025","4-5 July 2025","IEEE","IEEE Conferences"
"AWS for Solutions Architects: The definitive guide to AWS Solutions Architecture for migrating to, building, scaling, and succeeding in the cloud","S. Shrivastava; N. Srivastav; A. Artasanchez; I. Sayed; D. S. C. Ph.D",NA; NA; NA; NA; NA,"AWS for Solutions Architects: The definitive guide to AWS Solutions Architecture for migrating to, building, scaling, and succeeding in the cloud","","2023","","","","","Become a master Solutions Architect with this comprehensive guide, featuring cloud design patterns and real-world solutions for building scalable, secure, and highly available systems Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesGain expertise in automating, networking, migrating, and adopting cloud technologies using AWSUse streaming analytics, big data, AI/ML, IoT, quantum computing, and blockchain to transform your businessUpskill yourself as an AWS solutions architect and explore details of the new AWS certificationBook DescriptionAre you excited to harness the power of AWS and unlock endless possibilities for your business? Look no further than the second edition of AWS for Solutions Architects! Packed with all-new content, this book is a must-have guide for anyone looking to build scalable cloud solutions and drive digital transformation using AWS.  This updated edition offers in-depth guidance for building cloud solutions using AWS. It provides detailed information on AWS well-architected design pillars and cloud-native design patterns. You'll learn about networking in AWS, big data and streaming data processing, CloudOps, and emerging technologies such as machine learning, IoT, and blockchain. Additionally, the book includes new sections on storage in AWS, containers with ECS and EKS, and data lake patterns, providing you with valuable insights into designing industry-standard AWS architectures that meet your organization's technological and business requirements. Whether you're an experienced solutions architect or just getting started with AWS, this book has everything you need to confidently build cloud-native workloads and enterprise solutions.What you will learnOptimize your Cloud Workload using the AWS Well-Architected FrameworkLearn methods to migrate your workload using the AWS Cloud Adoption FrameworkApply cloud automation at various layers of application workload to increase efficiencyBuild a landing zone in AWS and hybrid cloud setups with deep networking techniquesSelect reference architectures for business scenarios, like data lakes, containers, and serverless appsApply emerging technologies in your architecture, including AI/ML, IoT and blockchainWho this book is forThis book is for application and enterprise architects, developers, and operations engineers who want to become well versed with AWS architectural patterns, best practices, and advanced techniques to build scalable, secure, highly available, highly tolerant, and cost-effective solutions in the cloud. Existing AWS users are bound to learn the most, but it will also help those curious about how leveraging AWS can benefit their organization. Prior knowledge of any computing language is not needed, and there’s little to no code. Prior experience in software architecture design will prove helpful.","","9781803244822","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10251173.pdf&bkn=10251173&pdfType=book","","","","","","","","14 Sep 2023","","","Packt Publishing","Packt Publishing eBooks"
"Building Privacy-Aware Data Pipelines: Balancing Scalability, Compliance, and Performance in Modern Data Architectures","G. R. Mittoor; P. Udayaraju; S. Putteti","The Wendy's Company, Dublin, Ohio, USA; Department of CSE, School of SEAS, SRM University - AP; Zoominfo Technologies, Bentonville, Arkansas, USA",2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS),"25 Apr 2025","2025","","","951","957","In cloud computing, data is generated from various sources in different locations, such as database management systems, data streaming environments, and files, and is automatically transferred to a data lake. Managing that data in the data lake is a challenging problem that involves developing a scalable model to integrate, process, and move it from one source to another with optimized cost and performance. The earlier research used various optimisation, data management and processing tools, but their performance efficiency is poor. This paper has been motivated to improve the overall performance by implementing the Random Forest algorithm for an overview of the incoming data, which is passed into a cost-effective data pipeline that helps to ingest and process massive amounts of incoming data daily. It also involves Apache Spark cloud services integrating the data seamlessly to manage storage and processing. Based on the data privacy and governance policies the RF model monitors and secures sensitive data in the input dataset. The simulation output is obtained by executing the proposed model in the cloud Apache framework, and the efficiency is verified regarding execution time, response time for user query, and accuracy. It is also verified that the data transmission with the cost efficiency for the healthcare dataset simulated in the Spark, S3, and AWS lake provides an aware pipeline model. A large-scale healthcare dataset is used in the simulation to confirm the efficacy of the data pipelining model, and the data transmission rate, claims, and cost efficiency are verified.","","979-8-3315-0574-5","10.1109/ICMLAS64557.2025.10968671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10968671","Data Pipeline;Data Scalability;AWS;Apache Spark;Healthcare Data Pipelining","Radio frequency;Cloud computing;Analytical models;Costs;Accuracy;Computational modeling;Scalability;Medical services;Data models;Pipeline processing","","","","18","IEEE","25 Apr 2025","10-12 March 2025","10-12 March 2025","IEEE","IEEE Conferences"
"Data Engineering with AWS: Learn how to design and build cloud-based data transformation pipelines using AWS","G. Eagar",NA,Data Engineering with AWS: Learn how to design and build cloud-based data transformation pipelines using AWS,"","2021","","","","","The missing expert-led manual for the AWS ecosystem — go from foundations to building data engineering pipelines effortlessly Purchase of the print or Kindle book includes a free eBook in the PDF format.Key FeaturesLearn about common data architectures and modern approaches to generating value from big dataExplore AWS tools for ingesting, transforming, and consuming data, and for orchestrating pipelinesLearn how to architect and implement data lakes and data lakehouses for big data analytics from a data lakes expertBook DescriptionWritten by a Senior Data Architect with over twenty-five years of experience in the business, Data Engineering for AWS is a book whose sole aim is to make you proficient in using the AWS ecosystem. Using a thorough and hands-on approach to data, this book will give aspiring and new data engineers a solid theoretical and practical foundation to succeed with AWS. As you progress, you’ll be taken through the services and the skills you need to architect and implement data pipelines on AWS. You'll begin by reviewing important data engineering concepts and some of the core AWS services that form a part of the data engineer's toolkit. You'll then architect a data pipeline, review raw data sources, transform the data, and learn how the transformed data is used by various data consumers. You’ll also learn about populating data marts and data warehouses along with how a data lakehouse fits into the picture. Later, you'll be introduced to AWS tools for analyzing data, including those for ad-hoc SQL queries and creating visualizations. In the final chapters, you'll understand how the power of machine learning and artificial intelligence can be used to draw new insights from data. By the end of this AWS book, you'll be able to carry out data engineering tasks and implement a data pipeline on AWS independently.What you will learnUnderstand data engineering concepts and emerging technologiesIngest streaming data with Amazon Kinesis Data FirehoseOptimize, denormalize, and join datasets with AWS Glue StudioUse Amazon S3 events to trigger a Lambda process to transform a fileRun complex SQL queries on data lake data using Amazon AthenaLoad data into a Redshift data warehouse and run queriesCreate a visualization of your data using Amazon QuickSightExtract sentiment data from a dataset using Amazon ComprehendWho this book is forThis book is for data engineers, data analysts, and data architects who are new to AWS and looking to extend their skills to the AWS cloud. Anyone new to data engineering who wants to learn about the foundational concepts while gaining practical experience with common data engineering services on AWS will also find this book useful. A basic understanding of big data-related topics and Python coding will help you get the most out of this book but it’s not a prerequisite. Familiarity with the AWS console and core services will also help you follow along.","","9781800569041","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162399.pdf&bkn=10162399&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Data Engineering with Google Cloud Platform: A practical guide to operationalizing scalable data analytics systems on GCP","A. Wijaya",NA,Data Engineering with Google Cloud Platform: A practical guide to operationalizing scalable data analytics systems on GCP,"","2022","","","","","Build and deploy your own data pipelines on GCP, make key architectural decisions, and gain the confidence to boost your career as a data engineerKey FeaturesUnderstand data engineering concepts, the role of a data engineer, and the benefits of using GCP for building your solutionLearn how to use the various GCP products to ingest, consume, and transform data and orchestrate pipelinesDiscover tips to prepare for and pass the Professional Data Engineer examBook DescriptionWith this book, you'll understand how the highly scalable Google Cloud Platform (GCP) enables data engineers to create end-to-end data pipelines right from storing and processing data and workflow orchestration to presenting data through visualization dashboards. Starting with a quick overview of the fundamental concepts of data engineering, you'll learn the various responsibilities of a data engineer and how GCP plays a vital role in fulfilling those responsibilities. As you progress through the chapters, you'll be able to leverage GCP products to build a sample data warehouse using Cloud Storage and BigQuery and a data lake using Dataproc. The book gradually takes you through operations such as data ingestion, data cleansing, transformation, and integrating data with other sources. You'll learn how to design IAM for data governance, deploy ML pipelines with the Vertex AI, leverage pre-built GCP models as a service, and visualize data with Google Data Studio to build compelling reports. Finally, you'll find tips on how to boost your career as a data engineer, take the Professional Data Engineer certification exam, and get ready to become an expert in data engineering with GCP. By the end of this data engineering book, you'll have developed the skills to perform core data engineering tasks and build efficient ETL data pipelines with GCP.What you will learnLoad data into BigQuery and materialize its output for downstream consumptionBuild data pipeline orchestration using Cloud ComposerDevelop Airflow jobs to orchestrate and automate a data warehouseBuild a Hadoop data lake, create ephemeral clusters, and run jobs on the Dataproc clusterLeverage Pub/Sub for messaging and ingestion for event-driven systemsUse Dataflow to perform ETL on streaming dataUnlock the power of your data with Data StudioCalculate the GCP cost estimation for your end-to-end data solutionsWho this book is forThis book is for data engineers, data analysts, and anyone looking to design and manage data processing pipelines using GCP. You'll find this book useful if you are preparing to take Google's Professional Data Engineer exam. Beginner-level understanding of data science, the Python programming language, and Linux commands is necessary. A basic understanding of data processing and cloud computing, in general, will help you make the most out of this book.","","9781800565067","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162346.pdf&bkn=10162346&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Crime Pattern Detection Utilizing Power BI Visualizations on the Microsoft Fabric Data Platform With the Public data.police.uk Dataset","A. Todosijević; P. Dakić; T. Heričko; Ž. Kljajić; V. Todorović","Faculty of Informatics and Computing, Singidunum University, Belgrade, Serbia; Faculty of Informatics and Computing, Singidunum University, Belgrade, Serbia; Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia; Faculty of Business Economics, Pan-European University Apeiron, Banja Luka, Bosnia & Herzegovina; Faculty of Business Studies and Law, MB University, Belgrade, Serbia",2025 15th International Conference on Advanced Computer Information Technologies (ACIT),"9 Oct 2025","2025","","","593","598","This paper presents a big data-driven platform for crime pattern detection using Power BI visualizations on Microsoft Fabric, built upon the public data.police.uk dataset. Law enforcement agencies require scalable analytics to extract actionable insights from large, complex datasets. We implemented a Data Lakehouse architecture to process around 8,500 crime data files i n C SV format from multiple regions, with Python-based metadata cataloging for structured access to crime outcomes, stop-and-search records, and street-level incidents. Dataflows and Notebooks in Fabric addressed regional inconsistencies and enabled efficient data transformation. Power BI reports provided intuitive and interactive visualizations for exploring geographic and temporal crime trends. Performance testing demonstrated up to 40% faster query response times compared to traditional warehouses, and regional crime analysis that previously took days was completed within hours. The results indicated that the platform scaled efficiently while maintaining stable performance under growing data volumes. Our approach demonstrates how unified analytics and visualization environments can democratize access to crime data insights, supporting evidence-based policing and public safety decision-making.","2770-5226","979-8-3315-9544-9","10.1109/ACIT65614.2025.11185634","Erasmus; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11185634","Data Platform;Microsoft Fabric;data.police.uk;Crime Analytics;Data Lakehouse Architecture","Law enforcement;Soft sensors;Decision making;Data visualization;Computer architecture;Metadata;Fabrics;Real-time systems;Time factors;Testing","","3","","14","IEEE","9 Oct 2025","17-19 Sept. 2025","17-19 Sept. 2025","IEEE","IEEE Conferences"
"Essential PySpark for Scalable Data Analytics: A beginner's guide to harnessing the power and ease of PySpark 3","S. Nudurupati",NA,Essential PySpark for Scalable Data Analytics: A beginner's guide to harnessing the power and ease of PySpark 3,"","2021","","","","","Get started with distributed computing using PySpark, a single unified framework to solve end-to-end data analytics at scaleKey FeaturesDiscover how to convert huge amounts of raw data into meaningful and actionable insightsUse Spark's unified analytics engine for end-to-end analytics, from data preparation to predictive analyticsPerform data ingestion, cleansing, and integration for ML, data analytics, and data visualizationBook DescriptionApache Spark is a unified data analytics engine designed to process huge volumes of data quickly and efficiently. PySpark is Apache Spark's Python language API, which offers Python developers an easy-to-use scalable data analytics framework. Essential PySpark for Scalable Data Analytics starts by exploring the distributed computing paradigm and provides a high-level overview of Apache Spark. You'll begin your analytics journey with the data engineering process, learning how to perform data ingestion, cleansing, and integration at scale. This book helps you build real-time analytics pipelines that help you gain insights faster. You'll then discover methods for building cloud-based data lakes, and explore Delta Lake, which brings reliability to data lakes. The book also covers Data Lakehouse, an emerging paradigm, which combines the structure and performance of a data warehouse with the scalability of cloud-based data lakes. Later, you'll perform scalable data science and machine learning tasks using PySpark, such as data preparation, feature engineering, and model training and productionization. Finally, you'll learn ways to scale out standard Python ML libraries along with a new pandas API on top of PySpark called Koalas. By the end of this PySpark book, you'll be able to harness the power of PySpark to solve business problems.What you will learnUnderstand the role of distributed computing in the world of big dataGain an appreciation for Apache Spark as the de facto go-to for big data processingScale out your data analytics process using Apache SparkBuild data pipelines using data lakes, and perform data visualization with PySpark and Spark SQLLeverage the cloud to build truly scalable and real-time data analytics applicationsExplore the applications of data science and scalable machine learning with PySparkIntegrate your clean and curated data with BI and SQL analysis toolsWho this book is forThis book is for practicing data engineers, data scientists, data analysts, and data enthusiasts who are already using data analytics to explore distributed and scalable data analytics. Basic to intermediate knowledge of the disciplines of data engineering, data science, and SQL analytics is expected. General proficiency in using any programming language, especially Python, and working knowledge of performing data analytics using frameworks such as pandas and SQL will help you to get the most out of this book.","","9781800563094","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162348.pdf&bkn=10162348&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Data Serverlessing Approach for IoT Devices Using Python Dimension","A. Dev; K. P. Sharma","Amity University, Greater Noida; Amity University, Greater Noida",2025 IEEE 5th International Conference on ICT in Business Industry & Government (ICTBIG),"13 Jan 2026","2025","","","1","6","Serverless computing has emerged as a transformative model in cloud-based data engineering, enabling organizations to process, manage, and analyze information without the burden of managing infrastructure. This approach dynamically provisions resources in response to incoming events and charges users only for the execution time, making it highly efficient for data-intensive and event-driven applications. Leveraging services such as Azure Functions, Event Hubs, and Data Lake, serverless data pipelines can simplify ingestion, transformation, and analysis tasks while ensuring scalability, reliability, and cost-effectiveness. This study examines the role of serverless architectures in building modern pipelines with Python on Microsoft Azure, outlining their strengths, limitations, and potential applications in real-world scenarios.","","979-8-3315-7981-4","10.1109/ICTBIG68706.2025.11323960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11323960","serverless computing;azure functions;data pipeline;cloud computing;python;event-driven architecture","Costs;Scalability;Pipelines;Buildings;Refining;Serverless computing;Computer architecture;Internet of Things;Security;Reliability","","","","11","IEEE","13 Jan 2026","12-13 Dec. 2025","12-13 Dec. 2025","IEEE","IEEE Conferences"
"A Comprehensive Survey on Advanced Data Science Platforms for Cyber-Physical Systems, Digital Twins, and Robotics","R. Kabir; Y. Watanobe; D. Ding; M. Rashedul Islam; K. Naruse","School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Fukushima, Japan; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Fukushima, Japan; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Fukushima, Japan; Department of Computer Science and Engineering, University of Asia Pacific, Dhaka, Bangladesh; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Fukushima, Japan",IEEE Access,"22 Oct 2025","2025","13","","177269","177304","The integration of Cyber-Physical Systems (CPS), Digital Twins (DT), and robotics with Advanced Data Science Platforms (ADSP) is rapidly transforming industrial and research landscapes by enabling real-time data processing, intelligent decision-making, and enhanced automation. Over the past decades, with the growing demand for adaptable, expandable, and secure platforms, numerous groundbreaking studies have emerged in the field of ADSP for CPS, DT, and robotics. However, most existing research addresses isolated aspects of AI, CPS, DTs, or robotics, lacking a holistic view of how ADSP expands and optimizes these technologies. To address this gap, this survey provides a comprehensive and structured review of the existing methodologies, tools, and techniques at each step of the ADSP workflow, focusing on their applications in CPS, DT, and robotics. Firstly, this survey analyzes the data ingestion phase, focusing on raw data collection architecture, advanced data pre-processing techniques, and data lake integration, while identifying integration and scalability challenges. Secondly, the development methodologies and data analysis techniques within experimental workflows are explored by highlighting widely used tools and real-world case studies. Thirdly, a detailed overview of optimization techniques and deployment strategies is presented, including cloud, edge, and hybrid models, supported by practical deployment examples. Finally, the continuous learning mechanisms are investigated for adaptive system updates, challenges in real-time adaptation, and expanding model performance. Additionally, this survey focuses on evaluation metrics, benchmark studies, and performance comparisons to assess platform efficiency across various domains. This study also explores emerging challenges such as data quality & availability, platform expandability, model transparency, and security, offering insights into future research directions. To ensure the rigor and breadth, this survey is conducted on 316 high-quality studies, state-of-the-art methodologies, and platforms that are selected using the PRISMA methodology from over 600 reviewed publications. By presenting a structured overview of ADSPs, this work aims to serve as a valuable resource for researchers, engineers, and industry professionals aiming to utilize data science for innovation in CPS, DT, and robotics.","2169-3536","","10.1109/ACCESS.2025.3619776","Commissioned Research Fund provided by Fukushima Institute for Research, Education, and Innovation (F-REI)(grant numbers:JPFR24010102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11197470","Cyber-physical systems (CPS);digital twins (DT);robotics;advanced data science platforms (ADSP);machine learning (ML);deep learning (DL);artificial intelligence (AI);edge computing;cloud computing;Internet of Things (IoT);real-time data processing;automation;security and privacy;decision-making systems;benchmarking;data-driven optimization;reinforcement learning (RL);performance evaluation;smart manufacturing;industrial automation","Service robots;Real-time systems;Data models;Surveys;Adaptation models;Optimization;Security;Data science;Benchmark testing;Technological innovation","","","","316","CCBY","9 Oct 2025","2025","","IEEE","IEEE Journals"
"Artificial Intelligence in Finance: Coffee Commodity Trading Big Data for Informed Decision Making","N. -B. -v. Le; Y. -S. Seo; J. -H. Huh","Department of Data Informatics, National Korea Maritime and Ocean University, Busan, Republic of Korea; School of Computer Science and Engineering, Yeungnam University, Gyeongsan, Republic of Korea; Interdisciplinary Major of Ocean Renewable Energy Engineering, National Korea Maritime and Ocean University, Busan, Republic of Korea",IEEE Access,"10 Jul 2024","2024","12","","91780","91792","Coffee, the second-largest global soft commodity, can take advantage of a comprehensive mining of daily and historical market data for more effective informed trading decisions. Advanced ICT and data mining technologies can change the trading market operation. The existing systems are confronted with certain constraints, including incomplete data, insufficient documentation for storage, and a requirement for a scalable infrastructure for big data analytics, such as a data warehouse or data lakehouse. To address this issue, the paper presents a design and implementation of a coffee commodity trading big data warehouse capable of analyzing various essential parameters for supporting informed decision-making. First, the designed system can automatically collect coffee trading data for New York Arabica coffee futures prices from selected worldwide reports and financial data portals. Next, the Extract, transform, and load (ETL) process is adopted to ingest coffee futures trading crawled data into the 3 layers data warehouse. Finally, the analytical system will extract and visualize selected key dimensions that influence coffee futures prices within different observation windows and perspectives. As a result, we implement a prototype of a coffee trading data warehouse on the crawled data from January 2000 to October 2022 and visualize trends in coffee futures prices based on the collected data for informed decision-making. The construction system is capable of stably operating and processing large volumes of transaction data. This paper will be valuable documentation for reference and decision support for coffee commodity trading enterprises and contribute to the development of future forecasting algorithms.","2169-3536","","10.1109/ACCESS.2024.3409762","National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:NRF-2023R1A2C1008134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549867","Coffee big data;data warehouse;coffee commodity trading;ETL process;informed decision-making;data visualization;big data","Data warehouses;Contracts;Data visualization;Predictive models;Data models;Data mining;Big Data;Market research;Pricing;Stock markets;Financial services","","6","","40","CCBYNCND","5 Jun 2024","2024","","IEEE","IEEE Journals"
"Simplify Big Data Analytics with Amazon EMR: A beginner’s guide to learning and implementing Amazon EMR for building data analytics solutions","S. Mishra",NA,Simplify Big Data Analytics with Amazon EMR: A beginner’s guide to learning and implementing Amazon EMR for building data analytics solutions,"","2022","","","","","Design scalable big data solutions using Hadoop, Spark, and AWS cloud native servicesKey FeaturesBuild data pipelines that require distributed processing capabilities on a large volume of dataDiscover the security features of EMR such as data protection and granular permission managementExplore best practices and optimization techniques for building data analytics solutions in Amazon EMRBook DescriptionAmazon EMR, formerly Amazon Elastic MapReduce, provides a managed Hadoop cluster in Amazon Web Services (AWS) that you can use to implement batch or streaming data pipelines. By gaining expertise in Amazon EMR, you can design and implement data analytics pipelines with persistent or transient EMR clusters in AWS. This book is a practical guide to Amazon EMR for building data pipelines. You'll start by understanding the Amazon EMR architecture, cluster nodes, features, and deployment options, along with their pricing. Next, the book covers the various big data applications that EMR supports. You'll then focus on the advanced configuration of EMR applications, hardware, networking, security, troubleshooting, logging, and the different SDKs and APIs it provides. Later chapters will show you how to implement common Amazon EMR use cases, including batch ETL with Spark, real-time streaming with Spark Streaming, and handling UPSERT in S3 Data Lake with Apache Hudi. Finally, you'll orchestrate your EMR jobs and strategize on-premises Hadoop cluster migration to EMR. In addition to this, you'll explore best practices and cost optimization techniques while implementing your data analytics pipeline in EMR. By the end of this book, you'll be able to build and deploy Hadoop- or Spark-based apps on Amazon EMR and also migrate your existing on-premises Hadoop workloads to AWS.What you will learnExplore Amazon EMR features, architecture, Hadoop interfaces, and EMR StudioConfigure, deploy, and orchestrate Hadoop or Spark jobs in productionImplement the security, data governance, and monitoring capabilities of EMRBuild applications for batch and real-time streaming data analytics solutionsPerform interactive development with a persistent EMR cluster and NotebookOrchestrate an EMR Spark job using AWS Step Functions and Apache AirflowWho this book is forThis book is for data engineers, data analysts, data scientists, and solution architects who are interested in building data analytics solutions with the Hadoop ecosystem services and Amazon EMR. Prior experience in either Python programming, Scala, or the Java programming language and a basic understanding of Hadoop and AWS will help you make the most out of this book.","","9781801077729","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163079.pdf&bkn=10163079&pdfType=book","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Tsdat: An Open-Source Data Standardization Framework for Marine Energy and Beyond","C. Lansing; M. Levin; C. Sivaraman; R. Fao; F. Driscoll","Advanced Computing, Mathematics, and Data Division Pacific Northwest National Laboratory, Richland, U.S.A.; Advanced Computing, Mathematics, and Data Division Pacific Northwest National Laboratory, Richland, U.S.A.; Advanced Computing, Mathematics, and Data Division Pacific Northwest National Laboratory, Richland, U.S.A.; Water Power, Energy Conversion & Storage Systems National Renewable Energy Laboratory, Golden, U.S.A.; Water Power, Energy Conversion & Storage Systems National Renewable Energy Laboratory, Golden, U.S.A.",OCEANS 2021: San Diego – Porto,"15 Feb 2022","2021","","","1","6","Many organizations are tasked with the collection and processing of large quantities of data from various measurement devices. Data reported from these sources are often not interoperable with datasets and software used by analysts and other organizations in the same domain, introducing barriers for collaboration on large-scale projects. This poses a particular problem for cross-device comparisons and machine learning applications, which rely on large quantities of data from multiple sources. To address these challenges, the open-source Time-Series Data Pipelines (Tsdat) Python framework was developed by Pacific Northwest National Laboratory, with strategic guidance and direction provided by the National Renewable Energy Laboratory and Sandia National Laboratories to facilitate collaboration and accelerate advancements in the marine energy domain through the development of an open-source ecosystem of tools. This paper will describe the Tsdat framework and the data standards within which it operates. A beta version of Tsdat has been released and is being used by several projects in marine energy, wind energy, and building energy systems.","0197-7385","978-0-692-93559-0","10.23919/OCEANS44145.2021.9706101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706101","Python;open-source;data standards;data lake;big data;interoperability","Wind energy;Instruments;Standards organizations;Buildings;Collaboration;Organizations;Task analysis","","1","","23","","15 Feb 2022","20-23 Sept. 2021","20-23 Sept. 2021","IEEE","IEEE Conferences"
"A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance","W. Yu; T. Dillon; F. Mostafa; W. Rahayu; Y. Liu","Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia",IEEE Transactions on Industrial Informatics,"8 Jan 2020","2020","16","1","183","192","Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.","1941-0050","","10.1109/TII.2019.2915846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710319","Big data analytics;cloud computing;fault detection;Internet of Things (IoT);Industry 4.0;manufacturing ecosystem;predictive maintenance (PdM)","Big Data;Manufacturing;Predictive maintenance;Ecosystems;Computer architecture;Real-time systems;Cloud computing","","193","","28","IEEE","9 May 2019","Jan. 2020","","IEEE","IEEE Journals"
"An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques","W. Yu; Y. Liu; T. Dillon; W. Rahayu; F. Mostafa","Philips Research, Shanghai, China; Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia; Enterprise Data Services, Australian Energy Market Operator, Melbourne, VIC, Australia",IEEE Internet of Things Journal,"24 Jan 2022","2022","9","3","2443","2454","With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.","2327-4662","","10.1109/JIOT.2021.3096637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9481251","Big data;health state monitoring;Internet of Things (IoT);noisy data cleaning;real-time systems;sensor selection","Big Data;Internet of Things;Intelligent sensors;Data analysis;Cloud computing;Smart manufacturing;Sensor phenomena and characterization","","79","","37","IEEE","12 Jul 2021","1 Feb.1, 2022","","IEEE","IEEE Journals"
